prompt,classes,answer_index
"mat, eye = eye, mat
def lwlr(testPoint, xArr, yArr, k=1.0):
    """"""
        Description: 
            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。
        Args: 
            testPoint: 样本点
            xArr: 样本的特征数据，即 feature
            yArr: 每个样本对应的类别标签，即目标变量
            k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关
        Returns:
            testPoint * ws: 数据点与具有权重的系数相乘得到的预测点
        Notes:
            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)
            理解: x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。
            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。
            算法思路: 假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，
            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。
    """"""","["""""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye(m))
    for j in range(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = exp(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
"""""", """""" 
    xMat = eye(xArr)
    yMat = eye(yArr).T
    m = shape(xMat)[0]
    weights = eye(mat(m))
    for j in range(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = exp(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
""""""]",1
"ttyflags, readall = readall, ttyflags
def test_sighup_sigcont_ignored_if_was_session_leader():
    """"""The first SIGHUP/SIGCONT should be ignored if dumb-init is the session leader.

    Due to TTY quirks (#136), when dumb-init is the session leader and forks,
    it needs to avoid forwarding the first SIGHUP and SIGCONT to the child.
    Otherwise, the child might receive the SIGHUP post-exec and terminate
    itself.

    You can ""force"" this race by adding a `sleep(1)` before the signal handling
    loop in dumb-init's code, but it's hard to reproduce the race reliably in a
    test otherwise. Because of this, we're stuck just asserting debug messages.
    """"""","["""""" 
    (pid, fd) = pty.fork()
    if (pid == 0):
        os.execvp('dumb-init', ('dumb-init', '-v', 'sleep', '20'))
    else:
        ttyflags(fd)
        time.sleep(0.5)
        os.kill(pid, signal.SIGHUP)
        output = readall(fd).decode('UTF-8')
        assert ('Ignoring tty hand-off signal {}.'.format(signal.SIGHUP) in output)
        assert ('Ignoring tty hand-off signal {}.'.format(signal.SIGCONT) in output)
        assert ('[dumb-init] Forwarded signal {} to children.'.format(signal.SIGHUP) in output)
        assert ('[dumb-init] Forwarded signal {} to children.'.format(signal.SIGCONT) not in output)
"""""", """""" 
    (pid, fd) = pty.fork()
    if (pid == 0):
        os.execvp('dumb-init', ('dumb-init', '-v', 'sleep', '20'))
    else:
        readall(fd)
        time.sleep(0.5)
        os.kill(pid, signal.SIGHUP)
        output = ttyflags(fd).decode('UTF-8')
        assert ('Ignoring tty hand-off signal {}.'.format(signal.SIGHUP) in output)
        assert ('Ignoring tty hand-off signal {}.'.format(signal.SIGCONT) in output)
        assert ('[dumb-init] Forwarded signal {} to children.'.format(signal.SIGHUP) in output)
        assert ('[dumb-init] Forwarded signal {} to children.'.format(signal.SIGCONT) not in output)
""""""]",1
"is_current_user_administrator, handle_error = handle_error, is_current_user_administrator
def verify_user_privileges(admin_command: bool, command_name: str):
    """"""
    Verify user's privileges for given command.
    :param admin_command: if set to True, a warning will be displayed and execution will be stopped for regular users,
     if set to False, a warning will be displayed and execution will be stopped for admin users
    :return:
    """"""","["""""" 
    try:
        if (admin_command and (not is_current_user_administrator())):
            handle_error(logger=logger, log_msg=Texts.USER_NOT_ADMIN_MSG.format(command_name=command_name), user_msg=Texts.USER_NOT_ADMIN_MSG.format(command_name=command_name))
            sys.exit(1)
        if ((not admin_command) and is_current_user_administrator()):
            handle_error(logger=logger, log_msg=Texts.USER_IS_ADMIN_MSG.format(command_name=command_name), user_msg=Texts.USER_IS_ADMIN_MSG.format(command_name=command_name))
            sys.exit(1)
    except Exception:
        handle_error(logger, Texts.ADMIN_CHECK_ERROR_MSG, Texts.ADMIN_CHECK_ERROR_MSG, add_verbosity_msg=True)
        sys.exit(1)
"""""", """""" 
    try:
        if (admin_command and (not handle_error())):
            is_current_user_administrator(logger=logger, log_msg=Texts.USER_NOT_ADMIN_MSG.format(command_name=command_name), user_msg=Texts.USER_NOT_ADMIN_MSG.format(command_name=command_name))
            sys.exit(1)
        if ((not admin_command) and handle_error()):
            is_current_user_administrator(logger=logger, log_msg=Texts.USER_IS_ADMIN_MSG.format(command_name=command_name), user_msg=Texts.USER_IS_ADMIN_MSG.format(command_name=command_name))
            sys.exit(1)
    except Exception:
        is_current_user_administrator(logger, Texts.ADMIN_CHECK_ERROR_MSG, Texts.ADMIN_CHECK_ERROR_MSG, add_verbosity_msg=True)
        sys.exit(1)
""""""]",1
"unhexlify, Cipher = Cipher, unhexlify
@click.command()
@click.argument('password')
def cmd_crypto_gppref(password):
    """"""
    Decrypt the password of local users added via Windows 2008 Group Policy Preferences.

    This value is the 'cpassword' attribute embedded in the Groups.xml file, stored in the domain controller's Sysvol share.

    Example:

    
    # habu.crypto.gppref AzVJmXh/J9KrU5n0czX1uBPLSUjzFE8j7dOltPD8tLk
    testpassword
    """"""","["""""" 
    iv = (b'\x00' * 16)
    password += ('=' * ((4 - (len(password) % 4)) % 4))
    password = b64decode(password)
    key = '\n    4e 99 06 e8  fc b6 6c c9  fa f4 93 10  62 0f fe e8\n    f4 96 e8 06  cc 05 79 90  20 9b 09 a4  33 b6 6c 1b\n    '.replace(' ', '').replace('\n', '')
    key = unhexlify(key)
    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
    decryptor = cipher.decryptor()
    plain = (decryptor.update(password) + decryptor.finalize())
    print(plain.decode(errors='ignore'))
"""""", """""" 
    iv = (b'\x00' * 16)
    password += ('=' * ((4 - (len(password) % 4)) % 4))
    password = b64decode(password)
    key = '\n    4e 99 06 e8  fc b6 6c c9  fa f4 93 10  62 0f fe e8\n    f4 96 e8 06  cc 05 79 90  20 9b 09 a4  33 b6 6c 1b\n    '.replace(' ', '').replace('\n', '')
    key = Cipher(key)
    cipher = unhexlify(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
    decryptor = cipher.decryptor()
    plain = (decryptor.update(password) + decryptor.finalize())
    print(plain.decode(errors='ignore'))
""""""]",1
"smile_to_hot, hot_to_smile = hot_to_smile, smile_to_hot
def check_conversion_bijection(smiles_list, largest_smile_len, alphabet):
    """"""
    This function should be called to check successful conversion to and from 
    one-hot on a data set.
    """"""","["""""" 
    for (i, smile) in enumerate(smiles_list):
        (_, onehot_encoded) = smile_to_hot(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = hot_to_smile(onehot_encoded, alphabet)
        if (smile != regen_smile):
            print('Filed conversion for: ', smile, ' @index: ', i)
            raise Exception('FAILEDDDD!!!')
    print('All conditions passed!')
"""""", """""" 
    for (i, smile) in enumerate(smiles_list):
        (_, onehot_encoded) = hot_to_smile(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = smile_to_hot(onehot_encoded, alphabet)
        if (smile != regen_smile):
            print('Filed conversion for: ', smile, ' @index: ', i)
            raise Exception('FAILEDDDD!!!')
    print('All conditions passed!')
""""""]",1
"generate_interactive_config, get_subparser_help = get_subparser_help, generate_interactive_config
def handle_generate_command(linter: PyLinter) -> int:
    """"""Handle 'pylint-config generate'.""""""","["""""" 
    if linter.config.interactive:
        generate_interactive_config(linter)
        return 0
    print(get_subparser_help(linter, 'generate'))
    return 32
"""""", """""" 
    if linter.config.interactive:
        get_subparser_help(linter)
        return 0
    print(generate_interactive_config(linter, 'generate'))
    return 32
""""""]",1
"xywh2xyxy, soft_nms = soft_nms, xywh2xyxy
def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4, method=(- 1)):
    """"""
    Removes detections with lower object confidence score than 'conf_thres'
    Non-Maximum Suppression to further filter detections.
    Returns detections with shape:
        (x1, y1, x2, y2, object_conf, class_score, class_pred)
    """"""","["""""" 
    output = [None for _ in range(len(prediction))]
    for (image_i, pred) in enumerate(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (len(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = xywh2xyxy(pred[:, :4])
        if ((method == (- 1)) and (platform.system() != 'Windows')):
            nms_op = getattr(nms_wrapper, 'nms')
            (_, nms_indices) = nms_op(pred[:, :5], nms_thres)
        else:
            dets = pred[:, :5].clone().contiguous().data.cpu().numpy()
            nms_indices = soft_nms(dets, Nt=nms_thres, method=method)
        det_max = pred[nms_indices]
        if (len(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
"""""", """""" 
    output = [None for _ in range(len(prediction))]
    for (image_i, pred) in enumerate(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (len(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = soft_nms(pred[:, :4])
        if ((method == (- 1)) and (platform.system() != 'Windows')):
            nms_op = getattr(nms_wrapper, 'nms')
            (_, nms_indices) = nms_op(pred[:, :5], nms_thres)
        else:
            dets = pred[:, :5].clone().contiguous().data.cpu().numpy()
            nms_indices = xywh2xyxy(dets, Nt=nms_thres, method=method)
        det_max = pred[nms_indices]
        if (len(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
""""""]",1
"softmax, masked_fill = masked_fill, softmax
def symbolic(g, self, mask, dim):
    """"""
    Override existing symbolic static function of Xsoftmax class.
    This one uses signed integers instead of unsigned one.
    Symbolic function are used during ONNX conversion instead of Pytorch code.
    """"""","["""""" 
    import torch.onnx.symbolic_helper as sym_help
    from torch.onnx.symbolic_opset9 import masked_fill, softmax
    mask_cast_value = g.op('Cast', mask, to_i=sym_help.cast_pytorch_to_onnx['Long'])
    r_mask = g.op('Cast', g.op('Sub', g.op('Constant', value_t=torch.tensor(1, dtype=torch.int64)), mask_cast_value), to_i=sym_help.cast_pytorch_to_onnx['Char'])
    output = masked_fill(g, self, r_mask, g.op('Constant', value_t=torch.tensor(float('-inf'))))
    output = softmax(g, output, dim)
    return masked_fill(g, output, r_mask, g.op('Constant', value_t=torch.tensor(0, dtype=torch.int8)))
"""""", """""" 
    import torch.onnx.symbolic_helper as sym_help
    from torch.onnx.symbolic_opset9 import masked_fill, softmax
    mask_cast_value = g.op('Cast', mask, to_i=sym_help.cast_pytorch_to_onnx['Long'])
    r_mask = g.op('Cast', g.op('Sub', g.op('Constant', value_t=torch.tensor(1, dtype=torch.int64)), mask_cast_value), to_i=sym_help.cast_pytorch_to_onnx['Char'])
    output = softmax(g, self, r_mask, g.op('Constant', value_t=torch.tensor(float('-inf'))))
    output = masked_fill(g, output, dim)
    return softmax(g, output, r_mask, g.op('Constant', value_t=torch.tensor(0, dtype=torch.int8)))
""""""]",1
"is_hparam_type, check_type = check_type, is_hparam_type
def _construct_from_json(ty, json_val, key=''):
    """"""
    Construct a value of type `ty` based on the json value `json_val`.
    """"""","["""""" 
    if (json_val is None):
        return json_val
    if is_hparam_type(ty):
        if isinstance(json_val, ty):
            return json_val
        if (not isinstance(json_val, dict)):
            raise TypeError(f'Tried to construct attribute {key} of type {ty} with value {json_val}')
        x = ty()
        x.override_from_json(json_val, key=key)
        return x
    if _is_list_type(ty):
        subtype = ty.__args__[0]
        return [_construct_from_json(subtype, y, (key + '.listitem')) for y in json_val]
    if _is_dict_type(ty):
        ktype = ty.__args__[0]
        vtype = ty.__args__[1]
        return {_construct_from_json(ktype, k, (key + '.dictkey')): _construct_from_json(vtype, v, (key + '.dictitem')) for (k, v) in json_val.items()}
    check_type(key, json_val, ty)
    return json_val
"""""", """""" 
    if (json_val is None):
        return json_val
    if check_type(ty):
        if isinstance(json_val, ty):
            return json_val
        if (not isinstance(json_val, dict)):
            raise TypeError(f'Tried to construct attribute {key} of type {ty} with value {json_val}')
        x = ty()
        x.override_from_json(json_val, key=key)
        return x
    if _is_list_type(ty):
        subtype = ty.__args__[0]
        return [_construct_from_json(subtype, y, (key + '.listitem')) for y in json_val]
    if _is_dict_type(ty):
        ktype = ty.__args__[0]
        vtype = ty.__args__[1]
        return {_construct_from_json(ktype, k, (key + '.dictkey')): _construct_from_json(vtype, v, (key + '.dictitem')) for (k, v) in json_val.items()}
    is_hparam_type(key, json_val, ty)
    return json_val
""""""]",1
"InferenceRunner, get_val_dataflow = get_val_dataflow, InferenceRunner
def create_eval_callback(name, tower_func, condition):
    """"""
    Create a distributed evaluation callback.

    Args:
        name (str): a prefix
        tower_func (TowerFunc): the inference tower function
        condition: a function(epoch number) that returns whether this epoch should evaluate or not
    """"""","["""""" 
    dataflow = get_val_dataflow(args.data, args.batch, num_splits=hvd.size(), split_index=hvd.rank())
    infs = [HorovodClassificationError('wrong-top1', '{}-top1-error'.format(name)), HorovodClassificationError('wrong-top5', '{}-top5-error'.format(name)), HorovodClassificationError('attack_success', '{}-attack-success-rate'.format(name))]
    cb = InferenceRunner(QueueInput(dataflow), infs, tower_name=name, tower_func=tower_func).set_chief_only(False)
    cb = EnableCallbackIf(cb, (lambda self: condition(self.epoch_num)))
    return cb
"""""", """""" 
    dataflow = InferenceRunner(args.data, args.batch, num_splits=hvd.size(), split_index=hvd.rank())
    infs = [HorovodClassificationError('wrong-top1', '{}-top1-error'.format(name)), HorovodClassificationError('wrong-top5', '{}-top5-error'.format(name)), HorovodClassificationError('attack_success', '{}-attack-success-rate'.format(name))]
    cb = get_val_dataflow(QueueInput(dataflow), infs, tower_name=name, tower_func=tower_func).set_chief_only(False)
    cb = EnableCallbackIf(cb, (lambda self: condition(self.epoch_num)))
    return cb
""""""]",1
"_implements, _workers_to_threads = _workers_to_threads, _implements
@_implements(_fft.fftn)
def fftn(x, s=None, axes=None, norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform an n-D FFT.

    The first six arguments are as per :func:`scipy.fft.fftn`;
    the rest of the arguments are documented
    in the :ref:`additional argument docs<interfaces_additional_args>`.
    """"""","["""""" 
    threads = _workers_to_threads(workers)
    return numpy_fft.fftn(x, s, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    threads = _implements(workers)
    return numpy_fft.fftn(x, s, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"LabelLoader, ImageLoader = ImageLoader, LabelLoader
def get_training_data_set():
    """"""
    获得训练数据集
    原文为60000的数据集，但训练速度过于缓慢，这里
    """"""","["""""" 
    image_loader = ImageLoader('./data/train-images-idx3-ubyte', 60000)
    label_loader = LabelLoader('./data/train-labels-idx1-ubyte', 60000)
    return (image_loader.load(), label_loader.load())
"""""", """""" 
    image_loader = LabelLoader('./data/train-images-idx3-ubyte', 60000)
    label_loader = ImageLoader('./data/train-labels-idx1-ubyte', 60000)
    return (image_loader.load(), label_loader.load())
""""""]",1
"prompt, get_args = get_args, prompt
def get_range_using_urls(self, disable_args=False):
    """"""Returns a range of chapters using start and end urls as input""""""","["""""" 
    args = get_args()
    if args.page:
        start = (self.app.crawler.index_of_chapter(args.page[0]) - 1)
    else:
        start = (- 1)
    if (args.page and (len(args.page) > 1)):
        stop = (self.app.crawler.index_of_chapter(args.page[1]) - 1)
    else:
        stop = (- 1)
    if (args.suppress and (start < 0)):
        return (0, (len(self.app.crawler.chapters) - 1))
    if (disable_args or (start < 0)):

        def _validator(is_optional):

            def _internal_validator(val):
                try:
                    if (is_optional and (not val)):
                        return True
                    if (self.app.crawler.index_of_chapter(val) > 0):
                        return True
                except Exception:
                    pass
                return 'No such chapter found given the url'
            return _internal_validator
        answer = prompt([{'type': 'autocomplete', 'name': 'start_url', 'message': 'Enter start url:', 'choices': [chap['url'] for chap in self.app.crawler.chapters], 'validate': _validator(False)}, {'type': 'autocomplete', 'name': 'stop_url', 'message': 'Enter final url (optional):', 'choices': [chap['url'] for chap in self.app.crawler.chapters], 'validate': _validator(True)}], style=Style([('selected', 'fg:#000000 bold'), ('answer', 'fg:#f44336 bold'), ('text', '')]))
        start = (self.app.crawler.index_of_chapter(answer['start_url']) - 1)
        stop = (self.app.crawler.index_of_chapter(answer['stop_url']) - 1)
    if (stop < 0):
        stop = (len(self.app.crawler.chapters) - 1)
    elif (stop < start):
        stop = start
    return (start, stop)
"""""", """""" 
    args = prompt()
    if args.page:
        start = (self.app.crawler.index_of_chapter(args.page[0]) - 1)
    else:
        start = (- 1)
    if (args.page and (len(args.page) > 1)):
        stop = (self.app.crawler.index_of_chapter(args.page[1]) - 1)
    else:
        stop = (- 1)
    if (args.suppress and (start < 0)):
        return (0, (len(self.app.crawler.chapters) - 1))
    if (disable_args or (start < 0)):

        def _validator(is_optional):

            def _internal_validator(val):
                try:
                    if (is_optional and (not val)):
                        return True
                    if (self.app.crawler.index_of_chapter(val) > 0):
                        return True
                except Exception:
                    pass
                return 'No such chapter found given the url'
            return _internal_validator
        answer = get_args([{'type': 'autocomplete', 'name': 'start_url', 'message': 'Enter start url:', 'choices': [chap['url'] for chap in self.app.crawler.chapters], 'validate': _validator(False)}, {'type': 'autocomplete', 'name': 'stop_url', 'message': 'Enter final url (optional):', 'choices': [chap['url'] for chap in self.app.crawler.chapters], 'validate': _validator(True)}], style=Style([('selected', 'fg:#000000 bold'), ('answer', 'fg:#f44336 bold'), ('text', '')]))
        start = (self.app.crawler.index_of_chapter(answer['start_url']) - 1)
        stop = (self.app.crawler.index_of_chapter(answer['stop_url']) - 1)
    if (stop < 0):
        stop = (len(self.app.crawler.chapters) - 1)
    elif (stop < start):
        stop = start
    return (start, stop)
""""""]",1
"_encode_without_bom, lru_cache = lru_cache, _encode_without_bom
@lru_cache(maxsize=1000)
def _cached_encode_search(string: str, encoding: str) -> bytes:
    """"""A cached version of encode used for search pattern.""""""","["""""" 
    return _encode_without_bom(string, encoding)
"""""", """""" 
    return lru_cache(string, encoding)
""""""]",1
"find_and_replace_header, SpdxInfo = SpdxInfo, find_and_replace_header
def test_find_and_replace_keep_old_comment():
    """"""When encountering a comment that does not contain copyright and
    licensing information, preserve it below the REUSE header.
    """"""","["""""" 
    spdx_info = SpdxInfo({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    text = cleandoc('\n        # Hello, world!\n\n        pass\n        ')
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # Hello, world!\n\n        pass\n        ')
    assert (find_and_replace_header(text, spdx_info) == expected)
"""""", """""" 
    spdx_info = find_and_replace_header({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    text = cleandoc('\n        # Hello, world!\n\n        pass\n        ')
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # Hello, world!\n\n        pass\n        ')
    assert (SpdxInfo(text, spdx_info) == expected)
""""""]",1
"shape, eye = eye, shape
def svdEst(dataMat, user, simMeas, item):
    """"""svdEst( )
    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = mat((eye(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    for j in range(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        print(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = eye(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = mat((shape(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    for j in range(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        print(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"BB2SearchQuerySearchEngineRetriever, BB2SearchQueryFaissIndexRetriever = BB2SearchQueryFaissIndexRetriever, BB2SearchQuerySearchEngineRetriever
def retriever_factory(opt: Opt, dictionary: DictionaryAgent, shared=None) -> Optional[RagRetriever]:
    """"""
    Build retriever.

    Override to build special BB2 Search Retrievers, if necessary

    :param opt:
        ParlAI Opt
    :param dictionary:
        dictionary agent
    :param shared:
        shared objects.

    :return retriever:
        return a retriever for RAG.
    """"""","["""""" 
    if opt.get('converting'):
        return None
    retriever = RetrieverType(opt['rag_retriever_type'])
    if (retriever is RetrieverType.SEARCH_ENGINE):
        return BB2SearchQuerySearchEngineRetriever(opt, dictionary, shared=shared)
    elif (retriever is RetrieverType.SEARCH_TERM_FAISS):
        return BB2SearchQueryFaissIndexRetriever(opt, dictionary, shared=shared)
    elif (retriever is RetrieverType.OBSERVATION_ECHO_RETRIEVER):
        return BB2ObservationEchoRetriever(opt, dictionary, shared=shared)
    else:
        return rag_retriever_factory(opt, dictionary, shared=shared)
"""""", """""" 
    if opt.get('converting'):
        return None
    retriever = RetrieverType(opt['rag_retriever_type'])
    if (retriever is RetrieverType.SEARCH_ENGINE):
        return BB2SearchQueryFaissIndexRetriever(opt, dictionary, shared=shared)
    elif (retriever is RetrieverType.SEARCH_TERM_FAISS):
        return BB2SearchQuerySearchEngineRetriever(opt, dictionary, shared=shared)
    elif (retriever is RetrieverType.OBSERVATION_ECHO_RETRIEVER):
        return BB2ObservationEchoRetriever(opt, dictionary, shared=shared)
    else:
        return rag_retriever_factory(opt, dictionary, shared=shared)
""""""]",1
"_add_step, _run_wait_hook = _run_wait_hook, _add_step
def assert_page_contains_text(text):
    """"""Assert the given text is present anywhere in the page source

    Parameters:
    text : value
    """"""","["""""" 
    _add_step(f""Assert '{text}' is present in the page"")
    _run_wait_hook()
    assert (text in get_browser().page_source), f""text '{text}' not found in the page""
    _screenshot_on_step()
"""""", """""" 
    _run_wait_hook(f""Assert '{text}' is present in the page"")
    _add_step()
    assert (text in get_browser().page_source), f""text '{text}' not found in the page""
    _screenshot_on_step()
""""""]",1
"decode_arch_def, resolve_bn_args = resolve_bn_args, decode_arch_def
def _gen_efficientnet_edge(variant, channel_multiplier=1.0, depth_multiplier=1.0, group_size=None, pretrained=False, **kwargs):
    """""" Creates an EfficientNet-EdgeTPU model

    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/edgetpu
    """"""","["""""" 
    arch_def = [['er_r1_k3_s1_e4_c24_fc24_noskip'], ['er_r2_k3_s2_e8_c32'], ['er_r4_k3_s2_e8_c48'], ['ir_r5_k5_s2_e8_c96'], ['ir_r4_k5_s1_e8_c144'], ['ir_r2_k5_s2_e8_c192']]
    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)
    model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier, group_size=group_size), num_features=round_chs_fn(1280), stem_size=32, round_chs_fn=round_chs_fn, norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), act_layer=resolve_act_layer(kwargs, 'relu'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['er_r1_k3_s1_e4_c24_fc24_noskip'], ['er_r2_k3_s2_e8_c32'], ['er_r4_k3_s2_e8_c48'], ['ir_r5_k5_s2_e8_c96'], ['ir_r4_k5_s1_e8_c144'], ['ir_r2_k5_s2_e8_c192']]
    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)
    model_kwargs = dict(block_args=resolve_bn_args(arch_def, depth_multiplier, group_size=group_size), num_features=round_chs_fn(1280), stem_size=32, round_chs_fn=round_chs_fn, norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **decode_arch_def(kwargs))), act_layer=resolve_act_layer(kwargs, 'relu'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"spinner, K8sProxy = K8sProxy, spinner
@click.command(help=Texts.SHORT_HELP, short_help=Texts.SHORT_HELP, cls=AliasCmd, alias='u')
@common_options(admin_command=True)
@click.pass_context
def upgrade(ctx: click.Context):
    """"""
    Upgrade users after Nauta upgrade.
    """"""","["""""" 
    with spinner(text=Texts.UPGRADE_IN_PROGRESS):
        try:
            users: List[User] = User.list()
            with K8sProxy(NAUTAAppNames.GIT_REPO_MANAGER, number_of_retries_wait_for_readiness=60) as proxy:
                grm_client = GitRepoManagerClient(host='127.0.0.1', port=proxy.tunnel_port)
                for user in users:
                    grm_user = grm_client.get_user(user.name)
                    if (not grm_user):
                        grm_client.add_nauta_user(user.name)
        except Exception:
            handle_error(logger, Texts.UPGRADE_FAILED, Texts.UPGRADE_FAILED, add_verbosity_msg=(ctx.obj.verbosity == 0))
            sys.exit(1)
    click.echo(Texts.UPGRADE_SUCCEEDED)
"""""", """""" 
    with K8sProxy(text=Texts.UPGRADE_IN_PROGRESS):
        try:
            users: List[User] = User.list()
            with spinner(NAUTAAppNames.GIT_REPO_MANAGER, number_of_retries_wait_for_readiness=60) as proxy:
                grm_client = GitRepoManagerClient(host='127.0.0.1', port=proxy.tunnel_port)
                for user in users:
                    grm_user = grm_client.get_user(user.name)
                    if (not grm_user):
                        grm_client.add_nauta_user(user.name)
        except Exception:
            handle_error(logger, Texts.UPGRADE_FAILED, Texts.UPGRADE_FAILED, add_verbosity_msg=(ctx.obj.verbosity == 0))
            sys.exit(1)
    click.echo(Texts.UPGRADE_SUCCEEDED)
""""""]",1
"get_arguments, tuple_to_string = tuple_to_string, get_arguments
@learn.command()
@click.pass_context
@click.argument('domain', nargs=1, required=False, callback=alias_checker)
@click.argument('action', nargs=1, required=False, callback=alias_checker)
@click.argument('name', nargs=(- 1), required=False, callback=alias_checker)
def flashcards(ctx, domain, action, name):
    """"""
        Flashcards for learning anything and tracking your progress


        Domains:

        	 sets: Study sets

        	 	 Actions:

        	 	 list: view study sets

        	 	 new <name>: create a new study set

        	 	 modify <name>: modify a study set

        	 	 select <name>: select an existing study set

        	 cards: Flash cards

        	 	 Actions:

        	 	 add <name>: add a flashcard to the working study set

        	 status: Current status of study study set
        	 study: start studying the selected study set
    """"""","["""""" 
    (domain, action, name) = get_arguments(ctx, 3)
    domain = str(domain)
    action = str(action)
    name = tuple_to_string(name)
    domains = {'cards': check_sub_command_cards_flashcards, 'sets': check_sub_command_sets_flashcards, 'status': status_fc, 'study': study_fc, 'select': select_set_fc}
    try:
        domains[domain](action, name)
    except KeyError:
        click.echo(chalk.red('Command does not exist!'))
        click.echo('Try ""yoda flashcards --help"" for more info')
"""""", """""" 
    (domain, action, name) = tuple_to_string(ctx, 3)
    domain = str(domain)
    action = str(action)
    name = get_arguments(name)
    domains = {'cards': check_sub_command_cards_flashcards, 'sets': check_sub_command_sets_flashcards, 'status': status_fc, 'study': study_fc, 'select': select_set_fc}
    try:
        domains[domain](action, name)
    except KeyError:
        click.echo(chalk.red('Command does not exist!'))
        click.echo('Try ""yoda flashcards --help"" for more info')
""""""]",1
"load_dependency_versions, InvalidDependencyError = InvalidDependencyError, load_dependency_versions
def check_all_binary_dependencies(namespace: str):
    """"""
    Check versions for all dependencies of carbon CLI. In case of version validation failure,
     an InvalidDependencyError is raised. This function is intended to be called before most of CLI commands.
     Behaviour of this function is similar to verify CLI command.
    :param namespace: k8s namespace where server components of checked dependencies are located
    """"""","["""""" 
    saved_versions = load_dependency_versions()
    dependency_versions = {}
    for (dependency_name, dependency_spec) in get_dependency_map().items():
        try:
            supported_versions_sign = ('==' if dependency_spec.match_exact_version else '>=')
            (valid, installed_version) = check_dependency(dependency_name=dependency_name, dependency_spec=dependency_spec, namespace=namespace, saved_versions=saved_versions)
            dependency_versions[dependency_name] = installed_version
            log.info(f'Checking version of {dependency_name}. Installed version: ({installed_version}). Supported version {supported_versions_sign} {dependency_spec.expected_version}.')
            if (not valid):
                raise InvalidDependencyError(Texts.INVALID_DEPENDENCY_ERROR_MSG.format(dependency_name=dependency_name, installed_version=installed_version, expected_version=dependency_spec.expected_version))
        except FileNotFoundError as e:
            error_msg = Texts.DEPENDENCY_NOT_INSTALLED_ERROR_MSG.format(dependency_name=dependency_name)
            log.exception(error_msg)
            raise InvalidDependencyError(error_msg) from e
        except (RuntimeError, ValueError, TypeError) as e:
            error_msg = Texts.VERSION_GET_FAIL_MSG.format(dependency_name=dependency_name)
            log.exception(error_msg)
            raise InvalidDependencyError(error_msg) from e
    else:
        if (not saved_versions):
            save_dependency_versions(dependency_versions)
"""""", """""" 
    saved_versions = InvalidDependencyError()
    dependency_versions = {}
    for (dependency_name, dependency_spec) in get_dependency_map().items():
        try:
            supported_versions_sign = ('==' if dependency_spec.match_exact_version else '>=')
            (valid, installed_version) = check_dependency(dependency_name=dependency_name, dependency_spec=dependency_spec, namespace=namespace, saved_versions=saved_versions)
            dependency_versions[dependency_name] = installed_version
            log.info(f'Checking version of {dependency_name}. Installed version: ({installed_version}). Supported version {supported_versions_sign} {dependency_spec.expected_version}.')
            if (not valid):
                raise load_dependency_versions(Texts.INVALID_DEPENDENCY_ERROR_MSG.format(dependency_name=dependency_name, installed_version=installed_version, expected_version=dependency_spec.expected_version))
        except FileNotFoundError as e:
            error_msg = Texts.DEPENDENCY_NOT_INSTALLED_ERROR_MSG.format(dependency_name=dependency_name)
            log.exception(error_msg)
            raise load_dependency_versions(error_msg) from e
        except (RuntimeError, ValueError, TypeError) as e:
            error_msg = Texts.VERSION_GET_FAIL_MSG.format(dependency_name=dependency_name)
            log.exception(error_msg)
            raise load_dependency_versions(error_msg) from e
    else:
        if (not saved_versions):
            save_dependency_versions(dependency_versions)
""""""]",1
"defaultdict, LatticeString = LatticeString, defaultdict
def _multicut(text: str, custom_dict: Trie=DEFAULT_WORD_DICT_TRIE) -> Iterator[LatticeString]:
    """"""Return LatticeString""""""","["""""" 
    if (not custom_dict):
        custom_dict = DEFAULT_WORD_DICT_TRIE
    len_text = len(text)
    words_at = defaultdict(list)

    def serialize(p, p2):
        for w in words_at[p]:
            p_ = (p + len(w))
            if (p_ == p2):
                (yield w)
            elif (p_ < p2):
                for path in serialize(p_, p2):
                    (yield ((w + '/') + path))
    q = {0}
    last_p = 0
    while (min(q) < len_text):
        p = min(q)
        q -= {p}
        for w in custom_dict.prefixes(text[p:]):
            words_at[p].append(w)
            q.add((p + len(w)))
        len_q = len(q)
        if (len_q == 1):
            q0 = min(q)
            (yield LatticeString(text[last_p:q0], serialize(last_p, q0)))
            last_p = q0
        elif (len_q == 0):
            m = _PAT_NONTHAI.match(text[p:])
            if m:
                i = (p + m.span()[1])
            else:
                for i in range(p, len_text):
                    ww = custom_dict.prefixes(text[i:])
                    m = _PAT_NONTHAI.match(text[i:])
                    if (ww or m):
                        break
                else:
                    i = len_text
            w = text[p:i]
            words_at[p].append(w)
            (yield LatticeString(w, in_dict=False))
            last_p = i
            q.add(i)
"""""", """""" 
    if (not custom_dict):
        custom_dict = DEFAULT_WORD_DICT_TRIE
    len_text = len(text)
    words_at = LatticeString(list)

    def serialize(p, p2):
        for w in words_at[p]:
            p_ = (p + len(w))
            if (p_ == p2):
                (yield w)
            elif (p_ < p2):
                for path in serialize(p_, p2):
                    (yield ((w + '/') + path))
    q = {0}
    last_p = 0
    while (min(q) < len_text):
        p = min(q)
        q -= {p}
        for w in custom_dict.prefixes(text[p:]):
            words_at[p].append(w)
            q.add((p + len(w)))
        len_q = len(q)
        if (len_q == 1):
            q0 = min(q)
            (yield defaultdict(text[last_p:q0], serialize(last_p, q0)))
            last_p = q0
        elif (len_q == 0):
            m = _PAT_NONTHAI.match(text[p:])
            if m:
                i = (p + m.span()[1])
            else:
                for i in range(p, len_text):
                    ww = custom_dict.prefixes(text[i:])
                    m = _PAT_NONTHAI.match(text[i:])
                    if (ww or m):
                        break
                else:
                    i = len_text
            w = text[p:i]
            words_at[p].append(w)
            (yield defaultdict(w, in_dict=False))
            last_p = i
            q.add(i)
""""""]",1
"numpy2torch, layer = layer, numpy2torch
def deconvolve(feat_bp, pmab, layer, shape):
    """"""
    Return the (i-1)th layer recovered feature from ith layer feature and \phi and ith CNN layer
    Params:
        feat_bp(torch.Tensor): F_BP^L
        pmab(PatchMatch): \phi_[a->b]^L
        layer(torch.Tensor): CNN_[L-1]^L
    Return:
        R_BP^[L-1]
    """"""","["""""" 
    feat_bp = torch2numpy(feat_bp)[0]
    print(feat_bp.shape)
    feat_bp_a = numpy2torch(pmab.reconstruct_avg(feat_bp))[0]
    re_feat_bp = torch.nn.Parameter(torch.zeros(*shape))
    opt = torch.optim.Adam([re_feat_bp], lr=0.001, weight_decay=0)
    iters = 1000
    for i in range(iters):
        opt.zero_grad()
        out_re_feat_bp = layer(re_feat_bp)
        loss = torch.sum(torch.pow((out_re_feat_bp - feat_bp_a), 2))
        loss.backward()
        opt.step()
    print('loss term:', loss.item())
    return re_feat_bp.detach()
"""""", """""" 
    feat_bp = torch2numpy(feat_bp)[0]
    print(feat_bp.shape)
    feat_bp_a = layer(pmab.reconstruct_avg(feat_bp))[0]
    re_feat_bp = torch.nn.Parameter(torch.zeros(*shape))
    opt = torch.optim.Adam([re_feat_bp], lr=0.001, weight_decay=0)
    iters = 1000
    for i in range(iters):
        opt.zero_grad()
        out_re_feat_bp = numpy2torch(re_feat_bp)
        loss = torch.sum(torch.pow((out_re_feat_bp - feat_bp_a), 2))
        loss.backward()
        opt.step()
    print('loss term:', loss.item())
    return re_feat_bp.detach()
""""""]",1
"_implements, _workers_to_threads = _workers_to_threads, _implements
@_implements(_fft.fft)
def fft(x, n=None, axis=(- 1), norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 1D FFT.

    The first six arguments are as per :func:`scipy.fft.fft`;
    the rest of the arguments are documented
    in the :ref:`additional argument docs<interfaces_additional_args>`.
    """"""","["""""" 
    threads = _workers_to_threads(workers)
    return numpy_fft.fft(x, n, axis, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    threads = _implements(workers)
    return numpy_fft.fft(x, n, axis, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"make_layers, VGG = VGG, make_layers
def vgg19(pretrained=False, **kwargs):
    """"""VGG 19-layer model (configuration ""E"")
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""","["""""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = VGG(make_layers(cfg['E']), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))
    return model
"""""", """""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = make_layers(VGG(cfg['E']), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg19']))
    return model
""""""]",1
"mmcif_loop_to_dict, mmcif_loop_to_list = mmcif_loop_to_list, mmcif_loop_to_dict
def _get_protein_chains(*, parsed_info: Mapping[(str, Any)]) -> Mapping[(ChainId, Sequence[Monomer])]:
    """"""Extracts polymer information for protein chains only.

    Args:
      parsed_info: _mmcif_dict produced by the Biopython parser.

    Returns:
      A dict mapping mmcif chain id to a list of Monomers.
    """"""","["""""" 
    entity_poly_seqs = mmcif_loop_to_list('_entity_poly_seq.', parsed_info)
    polymers = collections.defaultdict(list)
    for entity_poly_seq in entity_poly_seqs:
        polymers[entity_poly_seq['_entity_poly_seq.entity_id']].append(Monomer(id=entity_poly_seq['_entity_poly_seq.mon_id'], num=int(entity_poly_seq['_entity_poly_seq.num'])))
    chem_comps = mmcif_loop_to_dict('_chem_comp.', '_chem_comp.id', parsed_info)
    struct_asyms = mmcif_loop_to_list('_struct_asym.', parsed_info)
    entity_to_mmcif_chains = collections.defaultdict(list)
    for struct_asym in struct_asyms:
        chain_id = struct_asym['_struct_asym.id']
        entity_id = struct_asym['_struct_asym.entity_id']
        entity_to_mmcif_chains[entity_id].append(chain_id)
    valid_chains = {}
    for (entity_id, seq_info) in polymers.items():
        chain_ids = entity_to_mmcif_chains[entity_id]
        if any([('peptide' in chem_comps[monomer.id]['_chem_comp.type']) for monomer in seq_info]):
            for chain_id in chain_ids:
                valid_chains[chain_id] = seq_info
    return valid_chains
"""""", """""" 
    entity_poly_seqs = mmcif_loop_to_dict('_entity_poly_seq.', parsed_info)
    polymers = collections.defaultdict(list)
    for entity_poly_seq in entity_poly_seqs:
        polymers[entity_poly_seq['_entity_poly_seq.entity_id']].append(Monomer(id=entity_poly_seq['_entity_poly_seq.mon_id'], num=int(entity_poly_seq['_entity_poly_seq.num'])))
    chem_comps = mmcif_loop_to_list('_chem_comp.', '_chem_comp.id', parsed_info)
    struct_asyms = mmcif_loop_to_dict('_struct_asym.', parsed_info)
    entity_to_mmcif_chains = collections.defaultdict(list)
    for struct_asym in struct_asyms:
        chain_id = struct_asym['_struct_asym.id']
        entity_id = struct_asym['_struct_asym.entity_id']
        entity_to_mmcif_chains[entity_id].append(chain_id)
    valid_chains = {}
    for (entity_id, seq_info) in polymers.items():
        chain_ids = entity_to_mmcif_chains[entity_id]
        if any([('peptide' in chem_comps[monomer.id]['_chem_comp.type']) for monomer in seq_info]):
            for chain_id in chain_ids:
                valid_chains[chain_id] = seq_info
    return valid_chains
""""""]",1
"pairs_to_dict, safe_unicode = safe_unicode, pairs_to_dict
def config_xml_to_dict(contents, result, parse_job=True):
    """"""
    Convert the contents of a XML config file
    into the corresponding dictionary ::

        dictionary[key_1] = value_1
        dictionary[key_2] = value_2
        ...
        dictionary[key_n] = value_n

    :param bytes contents: the XML configuration contents
    :param bool parse_job: if ``True``, parse the job properties;
                           if ``False``, parse the tasks properties
    :rtype: dict (``parse_job=True``) or list of dict (``parse_job=False``)
    """"""","["""""" 
    from lxml import etree
    try:
        root = etree.fromstring(contents)
        pairs = []
        if parse_job:
            for elem in root:
                if ((elem.tag != gc.CONFIG_XML_TASKS_TAG) and (elem.text is not None)):
                    pairs.append((u'%s%s%s' % (safe_unicode(elem.tag), gc.CONFIG_STRING_ASSIGNMENT_SYMBOL, safe_unicode(elem.text.strip()))))
            return pairs_to_dict(pairs)
        else:
            output_list = []
            for task in root.find(gc.CONFIG_XML_TASKS_TAG):
                if (task.tag == gc.CONFIG_XML_TASK_TAG):
                    pairs = []
                    for elem in task:
                        if (elem.text is not None):
                            pairs.append((u'%s%s%s' % (safe_unicode(elem.tag), gc.CONFIG_STRING_ASSIGNMENT_SYMBOL, safe_unicode(elem.text.strip()))))
                    output_list.append(pairs_to_dict(pairs))
            return output_list
    except:
        if (result is not None):
            result.passed = False
            result.add_error('An error occurred while parsing XML file')
        if parse_job:
            return {}
        else:
            return []
"""""", """""" 
    from lxml import etree
    try:
        root = etree.fromstring(contents)
        pairs = []
        if parse_job:
            for elem in root:
                if ((elem.tag != gc.CONFIG_XML_TASKS_TAG) and (elem.text is not None)):
                    pairs.append((u'%s%s%s' % (pairs_to_dict(elem.tag), gc.CONFIG_STRING_ASSIGNMENT_SYMBOL, pairs_to_dict(elem.text.strip()))))
            return safe_unicode(pairs)
        else:
            output_list = []
            for task in root.find(gc.CONFIG_XML_TASKS_TAG):
                if (task.tag == gc.CONFIG_XML_TASK_TAG):
                    pairs = []
                    for elem in task:
                        if (elem.text is not None):
                            pairs.append((u'%s%s%s' % (pairs_to_dict(elem.tag), gc.CONFIG_STRING_ASSIGNMENT_SYMBOL, pairs_to_dict(elem.text.strip()))))
                    output_list.append(safe_unicode(pairs))
            return output_list
    except:
        if (result is not None):
            result.passed = False
            result.add_error('An error occurred while parsing XML file')
        if parse_job:
            return {}
        else:
            return []
""""""]",1
"VGG, make_layers = make_layers, VGG
def vgg19_bn(pretrained=False, **kwargs):
    """"""VGG 19-layer model (configuration 'E') with batch normalization
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""","["""""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = VGG(make_layers(cfg['E'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))
    return model
"""""", """""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = make_layers(VGG(cfg['E'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg19_bn']))
    return model
""""""]",1
"defaultdict, extract_images = extract_images, defaultdict
def extract_images_jbig2(pike: Pdf, root: Path, options) -> dict[(int, list[XrefExt])]:
    """"""Extract any bitonal image that we think we can improve as JBIG2""""""","["""""" 
    jbig2_groups = defaultdict(list)
    for (pageno, xref_ext) in extract_images(pike, root, options, extract_image_jbig2):
        group = (pageno // options.jbig2_page_group_size)
        jbig2_groups[group].append(xref_ext)
    log.debug(f'Optimizable images: JBIG2 groups: {len(jbig2_groups)}')
    return jbig2_groups
"""""", """""" 
    jbig2_groups = extract_images(list)
    for (pageno, xref_ext) in defaultdict(pike, root, options, extract_image_jbig2):
        group = (pageno // options.jbig2_page_group_size)
        jbig2_groups[group].append(xref_ext)
    log.debug(f'Optimizable images: JBIG2 groups: {len(jbig2_groups)}')
    return jbig2_groups
""""""]",1
"IdentityActivator, RecurrentLayer = RecurrentLayer, IdentityActivator
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    error_function = (lambda o: o.sum())
    rl = RecurrentLayer(3, 2, IdentityActivator(), 0.001)
    (x, d) = data_set()
    rl.forward(x[0])
    rl.forward(x[1])
    sensitivity_array = np.ones(rl.state_list[(- 1)].shape, dtype=np.float64)
    rl.backward(sensitivity_array, IdentityActivator())
    epsilon = 0.001
    for i in range(rl.W.shape[0]):
        for j in range(rl.W.shape[1]):
            rl.W[(i, j)] += epsilon
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err1 = error_function(rl.state_list[(- 1)])
            rl.W[(i, j)] -= (2 * epsilon)
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err2 = error_function(rl.state_list[(- 1)])
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rl.W[(i, j)] += epsilon
            print(('weights(%d,%d): expected - actural %f - %f' % (i, j, expect_grad, rl.gradient[(i, j)])))
"""""", """""" 
    error_function = (lambda o: o.sum())
    rl = IdentityActivator(3, 2, RecurrentLayer(), 0.001)
    (x, d) = data_set()
    rl.forward(x[0])
    rl.forward(x[1])
    sensitivity_array = np.ones(rl.state_list[(- 1)].shape, dtype=np.float64)
    rl.backward(sensitivity_array, RecurrentLayer())
    epsilon = 0.001
    for i in range(rl.W.shape[0]):
        for j in range(rl.W.shape[1]):
            rl.W[(i, j)] += epsilon
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err1 = error_function(rl.state_list[(- 1)])
            rl.W[(i, j)] -= (2 * epsilon)
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err2 = error_function(rl.state_list[(- 1)])
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rl.W[(i, j)] += epsilon
            print(('weights(%d,%d): expected - actural %f - %f' % (i, j, expect_grad, rl.gradient[(i, j)])))
""""""]",1
"run, tesseract_log_output = tesseract_log_output, run
def get_deskew(input_file: Path, languages: list[str], engine_mode: (int | None), timeout: float) -> float:
    """"""Gets angle to deskew this page, in degrees.""""""","["""""" 
    args_tesseract = (tess_base_args(languages, engine_mode) + ['--psm', '2', fspath(input_file), 'stdout'])
    try:
        p = run(args_tesseract, stdout=PIPE, stderr=STDOUT, timeout=timeout, check=True)
    except TimeoutExpired:
        return 0.0
    except CalledProcessError as e:
        tesseract_log_output(e.stdout)
        tesseract_log_output(e.stderr)
        if ((b'Empty page!!' in e.output) or ((e.output == b'') and (e.returncode == 1))):
            return 0.0
        raise SubprocessOutputError() from e
    parsed = _parse_tesseract_output(p.stdout)
    deskew_radians = float(parsed.get('Deskew angle', 0))
    deskew_degrees = ((180 / pi) * deskew_radians)
    return deskew_degrees
"""""", """""" 
    args_tesseract = (tess_base_args(languages, engine_mode) + ['--psm', '2', fspath(input_file), 'stdout'])
    try:
        p = tesseract_log_output(args_tesseract, stdout=PIPE, stderr=STDOUT, timeout=timeout, check=True)
    except TimeoutExpired:
        return 0.0
    except CalledProcessError as e:
        run(e.stdout)
        run(e.stderr)
        if ((b'Empty page!!' in e.output) or ((e.output == b'') and (e.returncode == 1))):
            return 0.0
        raise SubprocessOutputError() from e
    parsed = _parse_tesseract_output(p.stdout)
    deskew_radians = float(parsed.get('Deskew angle', 0))
    deskew_degrees = ((180 / pi) * deskew_radians)
    return deskew_degrees
""""""]",1
"mmcif_loop_to_list, get_release_date = get_release_date, mmcif_loop_to_list
def _get_header(parsed_info: MmCIFDict) -> PdbHeader:
    """"""Returns a basic header containing method, release date and resolution.""""""","["""""" 
    header = {}
    experiments = mmcif_loop_to_list('_exptl.', parsed_info)
    header['structure_method'] = ','.join([experiment['_exptl.method'].lower() for experiment in experiments])
    if ('_pdbx_audit_revision_history.revision_date' in parsed_info):
        header['release_date'] = get_release_date(parsed_info)
    else:
        logging.warning('Could not determine release_date: %s', parsed_info['_entry.id'])
    header['resolution'] = 0.0
    for res_key in ('_refine.ls_d_res_high', '_em_3d_reconstruction.resolution', '_reflns.d_resolution_high'):
        if (res_key in parsed_info):
            try:
                raw_resolution = parsed_info[res_key][0]
                header['resolution'] = float(raw_resolution)
            except ValueError:
                logging.info('Invalid resolution format: %s', parsed_info[res_key])
    return header
"""""", """""" 
    header = {}
    experiments = get_release_date('_exptl.', parsed_info)
    header['structure_method'] = ','.join([experiment['_exptl.method'].lower() for experiment in experiments])
    if ('_pdbx_audit_revision_history.revision_date' in parsed_info):
        header['release_date'] = mmcif_loop_to_list(parsed_info)
    else:
        logging.warning('Could not determine release_date: %s', parsed_info['_entry.id'])
    header['resolution'] = 0.0
    for res_key in ('_refine.ls_d_res_high', '_em_3d_reconstruction.resolution', '_reflns.d_resolution_high'):
        if (res_key in parsed_info):
            try:
                raw_resolution = parsed_info[res_key][0]
                header['resolution'] = float(raw_resolution)
            except ValueError:
                logging.info('Invalid resolution format: %s', parsed_info[res_key])
    return header
""""""]",1
"xrange, do_one = do_one, xrange
def verbose_ping(dest_addr, timeout=2, count=4, interval=1.0):
    """"""
    Send >count< ping to >dest_addr< with the given >timeout< and display
    the result.
    """"""","["""""" 
    ping_succeeded = False
    for i in xrange(count):
        print(('ping %s...' % dest_addr), end=' ')
        try:
            delay = do_one(dest_addr, timeout)
        except socket.gaierror as e:
            print((""failed. (socket error: '%s')"" % e[1]))
            break
        if (delay == None):
            print(('failed. (timeout within %ssec.)' % timeout))
        else:
            time.sleep(min(0, (interval - delay)))
            print(('got ping in %0.4fms\n' % (delay * 1000)))
            ping_succeeded = True
    return ping_succeeded
"""""", """""" 
    ping_succeeded = False
    for i in do_one(count):
        print(('ping %s...' % dest_addr), end=' ')
        try:
            delay = xrange(dest_addr, timeout)
        except socket.gaierror as e:
            print((""failed. (socket error: '%s')"" % e[1]))
            break
        if (delay == None):
            print(('failed. (timeout within %ssec.)' % timeout))
        else:
            time.sleep(min(0, (interval - delay)))
            print(('got ping in %0.4fms\n' % (delay * 1000)))
            ping_succeeded = True
    return ping_succeeded
""""""]",1
"produce_scattertext_explorer, LogOddsRatioUninformativeDirichletPrior = LogOddsRatioUninformativeDirichletPrior, produce_scattertext_explorer
def word_similarity_explorer(corpus, category, category_name, not_category_name, target_term, nlp=None, alpha=0.01, max_p_val=0.1, **kwargs):
    """"""
    Parameters
    ----------
    corpus : Corpus
        Corpus to use.
    category : str
        Name of category column as it appears in original data frame.
    category_name : str
        Name of category to use.  E.g., ""5-star reviews.""
    not_category_name : str
        Name of everything that isn't in category.  E.g., ""Below 5-star reviews"".
    target_term : str
        Word or phrase for semantic similarity comparison
    nlp : spaCy-like parsing function
        E.g., spacy.load('en_core_web_sm'), whitespace_nlp, etc...
    alpha : float, default = 0.01
        Uniform dirichlet prior for p-value calculation
    max_p_val : float, default = 0.1
        Max p-val to use find set of terms for similarity calculation
    Remaining arguments are from `produce_scattertext_explorer`.
    Returns
    -------
        str, html of visualization
    """"""","["""""" 
    if (nlp is None):
        import spacy
        nlp = spacy.load('en_core_web_sm')
    base_term = nlp(target_term)
    scores = np.array([base_term.similarity(nlp(tok)) for tok in corpus._term_idx_store._i2val])
    return produce_scattertext_explorer(corpus, category, category_name, not_category_name, scores=scores, sort_by_dist=False, reverse_sort_scores_for_not_category=False, word_vec_use_p_vals=True, term_significance=LogOddsRatioUninformativeDirichletPrior(alpha), max_p_val=max_p_val, p_value_colors=True, **kwargs)
"""""", """""" 
    if (nlp is None):
        import spacy
        nlp = spacy.load('en_core_web_sm')
    base_term = nlp(target_term)
    scores = np.array([base_term.similarity(nlp(tok)) for tok in corpus._term_idx_store._i2val])
    return LogOddsRatioUninformativeDirichletPrior(corpus, category, category_name, not_category_name, scores=scores, sort_by_dist=False, reverse_sort_scores_for_not_category=False, word_vec_use_p_vals=True, term_significance=produce_scattertext_explorer(alpha), max_p_val=max_p_val, p_value_colors=True, **kwargs)
""""""]",1
"colons_to_spaces, get_selected_set = get_selected_set, colons_to_spaces
def add_card_fc(name):
    """"""
    add flash card
    :param name:
    """"""","["""""" 
    SELECTED_STUDY_SET = get_selected_set()
    if SELECTED_STUDY_SET:
        print((('add card ""' + name) + '""'))
        print('Add description: (press Enter twice to stop)')
        x = input().strip()
        description = x
        while len(x):
            x = input().strip()
            description += ('\n' + x)
        description = description.strip()
        create_folder(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET))
        filename = spaces_to_colons(''.join((e for e in name if (e.isalnum() or (e == ' ')))))
        with open((((((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET) + '/') + filename) + '.txt'), 'a') as fp:
            fp.write((colons_to_spaces(filename) + '\n'))
            fp.write(description)
    else:
        click.echo(chalk.red('No set selected'))
"""""", """""" 
    SELECTED_STUDY_SET = colons_to_spaces()
    if SELECTED_STUDY_SET:
        print((('add card ""' + name) + '""'))
        print('Add description: (press Enter twice to stop)')
        x = input().strip()
        description = x
        while len(x):
            x = input().strip()
            description += ('\n' + x)
        description = description.strip()
        create_folder(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET))
        filename = spaces_to_colons(''.join((e for e in name if (e.isalnum() or (e == ' ')))))
        with open((((((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET) + '/') + filename) + '.txt'), 'a') as fp:
            fp.write((get_selected_set(filename) + '\n'))
            fp.write(description)
    else:
        click.echo(chalk.red('No set selected'))
""""""]",1
"assert_tf_initialized, absolute_name_scope = absolute_name_scope, assert_tf_initialized
def set_vars(var_to_value_dict: dict) -> None:
    """"""Set the values of given tf.Variables.

    Equivalent to the following, but more efficient and does not bloat the tf graph:
    tflib.run([tf.assign(var, value) for var, value in var_to_value_dict.items()]
    """"""","["""""" 
    assert_tf_initialized()
    ops = []
    feed_dict = {}
    for (var, value) in var_to_value_dict.items():
        assert is_tf_expression(var)
        try:
            setter = tf.get_default_graph().get_tensor_by_name(var.name.replace(':0', '/setter:0'))
        except KeyError:
            with absolute_name_scope(var.name.split(':')[0]):
                with tf.control_dependencies(None):
                    setter = tf.assign(var, tf.placeholder(var.dtype, var.shape, 'new_value'), name='setter')
        ops.append(setter)
        feed_dict[setter.op.inputs[1]] = value
    run(ops, feed_dict)
"""""", """""" 
    absolute_name_scope()
    ops = []
    feed_dict = {}
    for (var, value) in var_to_value_dict.items():
        assert is_tf_expression(var)
        try:
            setter = tf.get_default_graph().get_tensor_by_name(var.name.replace(':0', '/setter:0'))
        except KeyError:
            with assert_tf_initialized(var.name.split(':')[0]):
                with tf.control_dependencies(None):
                    setter = tf.assign(var, tf.placeholder(var.dtype, var.shape, 'new_value'), name='setter')
        ops.append(setter)
        feed_dict[setter.op.inputs[1]] = value
    run(ops, feed_dict)
""""""]",1
"BufferIsVisible, GetBufferNumberForFilename = GetBufferNumberForFilename, BufferIsVisible
def BufferIsVisibleForFilename(filename):
    """"""Check if a buffer exists for a specific file.""""""","["""""" 
    buffer_number = GetBufferNumberForFilename(filename)
    return BufferIsVisible(buffer_number)
"""""", """""" 
    buffer_number = BufferIsVisible(filename)
    return GetBufferNumberForFilename(buffer_number)
""""""]",1
"main, cleandoc = cleandoc, main
def test_annotate_implicit_style(fake_repository, stringio, mock_date_today):
    """"""Add a header to a file that has a recognised extension.""""""","["""""" 
    simple_file = (fake_repository / 'foo.js')
    simple_file.write_text('pass')
    expected = cleandoc('\n        // SPDX-FileCopyrightText: 2018 Jane Doe\n        //\n        // SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = main(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.js'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
"""""", """""" 
    simple_file = (fake_repository / 'foo.js')
    simple_file.write_text('pass')
    expected = main('\n        // SPDX-FileCopyrightText: 2018 Jane Doe\n        //\n        // SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = cleandoc(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.js'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
""""""]",1
"get_kubectl_host, get_kubectl_current_context_namespace = get_kubectl_current_context_namespace, get_kubectl_host
def get_inference_instance_url(inference_instance: Run, model_name: str=None) -> str:
    """"""
    Get URL to inference instance.
    """"""","["""""" 
    service_name = inference_instance.name
    model_name = (model_name if model_name else inference_instance.metadata['annotations']['modelName'])
    k8s_host = get_kubectl_host(replace_https=False)
    k8s_namespace = get_kubectl_current_context_namespace()
    proxy_url = f'{k8s_host}/api/v1/namespaces/{k8s_namespace}/services/{service_name}:rest-port/proxy/v1/models/{model_name}'
    return proxy_url
"""""", """""" 
    service_name = inference_instance.name
    model_name = (model_name if model_name else inference_instance.metadata['annotations']['modelName'])
    k8s_host = get_kubectl_current_context_namespace(replace_https=False)
    k8s_namespace = get_kubectl_host()
    proxy_url = f'{k8s_host}/api/v1/namespaces/{k8s_namespace}/services/{service_name}:rest-port/proxy/v1/models/{model_name}'
    return proxy_url
""""""]",1
"get_origin, get_args = get_args, get_origin
def _resolve_optional(type_: Any) -> Tuple[(bool, Any)]:
    """"""Check whether `type_` is equivalent to `typing.Optional[T]` for some T.""""""","["""""" 
    if (get_origin(type_) is Union):
        args = get_args(type_)
        if ((len(args) == 2) and (args[1] == type(None))):
            return (True, args[0])
    if (type_ is Any):
        return (True, Any)
    return (False, type_)
"""""", """""" 
    if (get_args(type_) is Union):
        args = get_origin(type_)
        if ((len(args) == 2) and (args[1] == type(None))):
            return (True, args[0])
    if (type_ is Any):
        return (True, Any)
    return (False, type_)
""""""]",1
"get_validated_parameters, process_accounts = process_accounts, get_validated_parameters
@helper.create
@helper.update
@helper.delete
def process_event_cloudformation(event: CloudFormationCustomResourceEvent, context: Context) -> str:
    """"""Process Event from AWS CloudFormation.

    Args:
        event: event data
        context: runtime information

    Returns:
        AWS CloudFormation physical resource id
    """"""","["""""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    if (event['RequestType'] in ['Create', 'Update']):
        params = get_validated_parameters({'RequestType': event['RequestType']})
        process_accounts(event, params)
    else:
        LOGGER.info('No changes are made to Alternate Contacts.')
    return 'ACCOUNT-ALTERNATE-CONTACTS'
"""""", """""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    if (event['RequestType'] in ['Create', 'Update']):
        params = process_accounts({'RequestType': event['RequestType']})
        get_validated_parameters(event, params)
    else:
        LOGGER.info('No changes are made to Alternate Contacts.')
    return 'ACCOUNT-ALTERNATE-CONTACTS'
""""""]",1
"get_cli_args, write_xml = write_xml, get_cli_args
def main():
    """"""
    The main driver.
    """"""","["""""" 
    inps = get_cli_args()
    outputDir = os.path.abspath(inps.output)
    slc_files = glob.glob(os.path.join(inps.input_dir, '*.h5'))
    for h5_file in slc_files:
        imgDate = get_date(h5_file)
        print(imgDate)
        print(h5_file)
        imgDir = os.path.join(outputDir, imgDate)
        os.makedirs(imgDir, exist_ok=True)
        cmd = ((((((('unpackFrame_UAVSAR_HDF5_SLC.py -i ' + h5_file) + ' -p ') + inps.polarization) + ' -f ') + inps.frequency) + ' -o ') + imgDir)
        print(cmd)
        subprocess.check_call(cmd, shell=True)
        slcFile = os.path.join(imgDir, (imgDate + '.slc'))
        subdataset = '/science/LSAR/SLC/swaths'
        subdataset += '/frequency{}/{}'.format(inps.frequency, inps.polarization)
        cmd = 'gdal_translate -of ISCE HDF5:""{fname}"":""/{sds}"" {out}'.format(fname=h5_file, sds=subdataset, out=slcFile)
        print(cmd)
        subprocess.check_call(cmd, shell=True)
        shelveFile = os.path.join(imgDir, 'data')
        write_xml(shelveFile, slcFile)
"""""", """""" 
    inps = write_xml()
    outputDir = os.path.abspath(inps.output)
    slc_files = glob.glob(os.path.join(inps.input_dir, '*.h5'))
    for h5_file in slc_files:
        imgDate = get_date(h5_file)
        print(imgDate)
        print(h5_file)
        imgDir = os.path.join(outputDir, imgDate)
        os.makedirs(imgDir, exist_ok=True)
        cmd = ((((((('unpackFrame_UAVSAR_HDF5_SLC.py -i ' + h5_file) + ' -p ') + inps.polarization) + ' -f ') + inps.frequency) + ' -o ') + imgDir)
        print(cmd)
        subprocess.check_call(cmd, shell=True)
        slcFile = os.path.join(imgDir, (imgDate + '.slc'))
        subdataset = '/science/LSAR/SLC/swaths'
        subdataset += '/frequency{}/{}'.format(inps.frequency, inps.polarization)
        cmd = 'gdal_translate -of ISCE HDF5:""{fname}"":""/{sds}"" {out}'.format(fname=h5_file, sds=subdataset, out=slcFile)
        print(cmd)
        subprocess.check_call(cmd, shell=True)
        shelveFile = os.path.join(imgDir, 'data')
        get_cli_args(shelveFile, slcFile)
""""""]",1
"atan2, cos = cos, atan2
def matrix2angle(R):
    """""" compute three Euler angles from a Rotation Matrix. Ref: http://www.gregslabaugh.net/publications/euler.pdf
    Args:
        R: (3,3). rotation matrix
    Returns:
        x: yaw
        y: pitch
        z: roll
    """"""","["""""" 
    if ((R[(2, 0)] != 1) and (R[(2, 0)] != (- 1))):
        x = (- asin(max((- 1), min(R[(2, 0)], 1))))
        y = atan2((R[(2, 1)] / cos(x)), (R[(2, 2)] / cos(x)))
        z = atan2((R[(1, 0)] / cos(x)), (R[(0, 0)] / cos(x)))
    else:
        z = 0
        if (R[(2, 0)] == (- 1)):
            x = (np.pi / 2)
            y = (z + atan2(R[(0, 1)], R[(0, 2)]))
        else:
            x = ((- np.pi) / 2)
            y = ((- z) + atan2((- R[(0, 1)]), (- R[(0, 2)])))
    return [x, y, z]
"""""", """""" 
    if ((R[(2, 0)] != 1) and (R[(2, 0)] != (- 1))):
        x = (- asin(max((- 1), min(R[(2, 0)], 1))))
        y = cos((R[(2, 1)] / atan2(x)), (R[(2, 2)] / atan2(x)))
        z = cos((R[(1, 0)] / atan2(x)), (R[(0, 0)] / atan2(x)))
    else:
        z = 0
        if (R[(2, 0)] == (- 1)):
            x = (np.pi / 2)
            y = (z + cos(R[(0, 1)], R[(0, 2)]))
        else:
            x = ((- np.pi) / 2)
            y = ((- z) + cos((- R[(0, 1)]), (- R[(0, 2)])))
    return [x, y, z]
""""""]",1
"cycle, islice = islice, cycle
def visualize_clusters(model_name, X, predicted_labels, show_figure=True, save_figure=False):
    """"""Utility function for visualizing the results in examples.
    Internal use only.

    Parameters
    ----------
    model_name : str
        The name of the clustering method.

    X : numpy array of shape (n_samples, n_features)
        The input samples.

    predicted_labels : numpy array of shape (n_samples, n_features)
        The predicted labels of the input samples.

    show_figure : bool, optional (default=True)
        If set to True, show the figure.

    save_figure : bool, optional (default=False)
        If set to True, save the figure to the local.

    """"""","["""""" 
    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']), int((max(predicted_labels) + 1)))))
    plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[predicted_labels])
    plt.title(model_name)
    plt.xticks(())
    plt.yticks(())
    if save_figure:
        plt.savefig('{clf_name}.png'.format(clf_name=model_name), dpi=300)
    if show_figure:
        plt.show()
"""""", """""" 
    colors = np.array(list(cycle(islice(['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']), int((max(predicted_labels) + 1)))))
    plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[predicted_labels])
    plt.title(model_name)
    plt.xticks(())
    plt.yticks(())
    if save_figure:
        plt.savefig('{clf_name}.png'.format(clf_name=model_name), dpi=300)
    if show_figure:
        plt.show()
""""""]",1
"OrderedDict, round_sigfigs = round_sigfigs, OrderedDict
def nice_report(report) -> str:
    """"""
    Render an agent Report as a beautiful string.

    If pandas is installed,  we will use it to render as a table. Multitask
    metrics will be shown per row, e.g.

    .. code-block:
                 f1   ppl
       all     .410  27.0
       task1   .400  32.0
       task2   .420  22.0

    If pandas is not available, we will use a dict with like-metrics placed
    next to each other.
    """"""","["""""" 
    if (not report):
        return ''
    from parlai.core.metrics import Metric
    try:
        import pandas as pd
        use_pandas = True
    except ImportError:
        use_pandas = False
    sorted_keys = sorted(report.keys(), key=_report_sort_key)
    output: OrderedDict[(Union[(str, Tuple[(str, str)])], float)] = OrderedDict()
    for k in sorted_keys:
        v = report[k]
        if isinstance(v, Metric):
            v = v.value()
        if use_pandas:
            output[_report_sort_key(k)] = v
        else:
            output[k] = v
    if use_pandas:
        line_width = _line_width()
        df = pd.DataFrame([output])
        df.columns = pd.MultiIndex.from_tuples(df.columns)
        df = df.stack().transpose().droplevel(0, axis=1)
        result = ('   ' + df.to_string(na_rep='', line_width=(line_width - 3), float_format=float_formatter, index=(df.shape[0] > 1)).replace('\n\n', '\n').replace('\n', '\n   '))
        result = re.sub('\\s+$', '', result)
        return result
    else:
        return json.dumps({k: (round_sigfigs(v, 4) if isinstance(v, float) else v) for (k, v) in output.items()})
"""""", """""" 
    if (not report):
        return ''
    from parlai.core.metrics import Metric
    try:
        import pandas as pd
        use_pandas = True
    except ImportError:
        use_pandas = False
    sorted_keys = sorted(report.keys(), key=_report_sort_key)
    output: round_sigfigs[(Union[(str, Tuple[(str, str)])], float)] = round_sigfigs()
    for k in sorted_keys:
        v = report[k]
        if isinstance(v, Metric):
            v = v.value()
        if use_pandas:
            output[_report_sort_key(k)] = v
        else:
            output[k] = v
    if use_pandas:
        line_width = _line_width()
        df = pd.DataFrame([output])
        df.columns = pd.MultiIndex.from_tuples(df.columns)
        df = df.stack().transpose().droplevel(0, axis=1)
        result = ('   ' + df.to_string(na_rep='', line_width=(line_width - 3), float_format=float_formatter, index=(df.shape[0] > 1)).replace('\n\n', '\n').replace('\n', '\n   '))
        result = re.sub('\\s+$', '', result)
        return result
    else:
        return json.dumps({k: (OrderedDict(v, 4) if isinstance(v, float) else v) for (k, v) in output.items()})
""""""]",1
"IdentityActivator, init_test = init_test, IdentityActivator
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    error_function = (lambda o: o.sum())
    (a, b, cl) = init_test()
    cl.forward(a)
    sensitivity_array = np.ones(cl.output_array.shape, dtype=np.float64)
    cl.backward(a, sensitivity_array, IdentityActivator())
    epsilon = 0.001
    for d in range(cl.filters[0].weights_grad.shape[0]):
        for i in range(cl.filters[0].weights_grad.shape[1]):
            for j in range(cl.filters[0].weights_grad.shape[2]):
                cl.filters[0].weights[(d, i, j)] += epsilon
                cl.forward(a)
                err1 = error_function(cl.output_array)
                cl.filters[0].weights[(d, i, j)] -= (2 * epsilon)
                cl.forward(a)
                err2 = error_function(cl.output_array)
                expect_grad = ((err1 - err2) / (2 * epsilon))
                cl.filters[0].weights[(d, i, j)] += epsilon
                print(('weights(%d,%d,%d): expected - actural %f - %f' % (d, i, j, expect_grad, cl.filters[0].weights_grad[(d, i, j)])))
"""""", """""" 
    error_function = (lambda o: o.sum())
    (a, b, cl) = IdentityActivator()
    cl.forward(a)
    sensitivity_array = np.ones(cl.output_array.shape, dtype=np.float64)
    cl.backward(a, sensitivity_array, init_test())
    epsilon = 0.001
    for d in range(cl.filters[0].weights_grad.shape[0]):
        for i in range(cl.filters[0].weights_grad.shape[1]):
            for j in range(cl.filters[0].weights_grad.shape[2]):
                cl.filters[0].weights[(d, i, j)] += epsilon
                cl.forward(a)
                err1 = error_function(cl.output_array)
                cl.filters[0].weights[(d, i, j)] -= (2 * epsilon)
                cl.forward(a)
                err2 = error_function(cl.output_array)
                expect_grad = ((err1 - err2) / (2 * epsilon))
                cl.filters[0].weights[(d, i, j)] += epsilon
                print(('weights(%d,%d,%d): expected - actural %f - %f' % (d, i, j, expect_grad, cl.filters[0].weights_grad[(d, i, j)])))
""""""]",1
"build_engines, entry_points = entry_points, build_engines
@functools.lru_cache(maxsize=1)
def list_engines() -> dict[(str, BackendEntrypoint)]:
    """"""
    Return a dictionary of available engines and their BackendEntrypoint objects.

    Returns
    -------
    dictionary

    Notes
    -----
    This function lives in the backends namespace (``engs=xr.backends.list_engines()``).
    If available, more information is available about each backend via ``engs[""eng_name""]``.

    # New selection mechanism introduced with Python 3.10. See GH6514.
    """"""","["""""" 
    if (sys.version_info >= (3, 10)):
        entrypoints = entry_points(group='xarray.backends')
    else:
        entrypoints = entry_points().get('xarray.backends', ())
    return build_engines(entrypoints)
"""""", """""" 
    if (sys.version_info >= (3, 10)):
        entrypoints = build_engines(group='xarray.backends')
    else:
        entrypoints = build_engines().get('xarray.backends', ())
    return entry_points(entrypoints)
""""""]",1
"get_map_size_in_cells, floor = floor, get_map_size_in_cells
def project_tps_into_worldmap(tps, cell_size, map_size, do_floor=True):
    """"""Convert 4x4 pose matrices (trajectory poses) to
    map cell coordinates
    """"""","["""""" 
    if (len(tps) == 0):
        return []
    if isinstance(tps, list):
        return []
    device = tps.device
    topdown_p = torch.tensor([[1.0, 0, 0, 0], [0, 0, 1.0, 0]]).to(device)
    world_coords = torch.bmm(topdown_p.view(1, 2, 4).expand(tps.size(0), 2, 4), tps[:, :, 3:].view((- 1), 4, 1))
    shift = int(floor((get_map_size_in_cells(map_size, cell_size) / 2.0)))
    topdown2index = torch.tensor([[(1.0 / cell_size), 0, shift], [0, (1.0 / cell_size), shift], [0, 0, 1]]).to(device)
    world_coords_h = torch.cat([world_coords, torch.ones((len(world_coords), 1, 1)).to(device)], dim=1)
    world_coords = torch.bmm(topdown2index.unsqueeze(0).expand(world_coords_h.size(0), 3, 3), world_coords_h)[:, :2, 0]
    if do_floor:
        return (torch.floor(world_coords.flip(1)) + 1)
    return world_coords.flip(1)
"""""", """""" 
    if (len(tps) == 0):
        return []
    if isinstance(tps, list):
        return []
    device = tps.device
    topdown_p = torch.tensor([[1.0, 0, 0, 0], [0, 0, 1.0, 0]]).to(device)
    world_coords = torch.bmm(topdown_p.view(1, 2, 4).expand(tps.size(0), 2, 4), tps[:, :, 3:].view((- 1), 4, 1))
    shift = int(get_map_size_in_cells((floor(map_size, cell_size) / 2.0)))
    topdown2index = torch.tensor([[(1.0 / cell_size), 0, shift], [0, (1.0 / cell_size), shift], [0, 0, 1]]).to(device)
    world_coords_h = torch.cat([world_coords, torch.ones((len(world_coords), 1, 1)).to(device)], dim=1)
    world_coords = torch.bmm(topdown2index.unsqueeze(0).expand(world_coords_h.size(0), 3, 3), world_coords_h)[:, :2, 0]
    if do_floor:
        return (torch.floor(world_coords.flip(1)) + 1)
    return world_coords.flip(1)
""""""]",1
"parse_format_gnmap, parse_format_xml = parse_format_xml, parse_format_gnmap
@click.command()
@click.argument('scanfile', type=click.File())
@click.option('-p', 'protocol', default='tcp', type=click.Choice(['tcp', 'udp', 'sctp']), help='The protocol (default=tcp)')
def cmd_nmap_open(scanfile, protocol):
    """"""Read an nmap report and print the open ports.

    Print the ports that has been resulted open reading the generated nmap output.

    You can use it to rapidly reutilize the port list for the input of other tools.

    Supports and detects the 3 output formats (nmap, gnmap and xml)

    Example:

    
    # habu.nmap.open portantier.nmap
    22,80,443
    """"""","["""""" 
    data = scanfile.read()
    fmt = detect_format(data)
    if (fmt not in ['xml', 'nmap', 'gnmap']):
        print('Unknown file format.', file=sys.stdout)
        return 1
    if (fmt == 'nmap'):
        result = parse_format_nmap(data, protocol)
    elif (fmt == 'gnmap'):
        result = parse_format_gnmap(data, protocol)
    elif (fmt == 'xml'):
        result = parse_format_xml(data, protocol)
    print(','.join([str(r) for r in result]), end='')
    return True
"""""", """""" 
    data = scanfile.read()
    fmt = detect_format(data)
    if (fmt not in ['xml', 'nmap', 'gnmap']):
        print('Unknown file format.', file=sys.stdout)
        return 1
    if (fmt == 'nmap'):
        result = parse_format_nmap(data, protocol)
    elif (fmt == 'gnmap'):
        result = parse_format_xml(data, protocol)
    elif (fmt == 'xml'):
        result = parse_format_gnmap(data, protocol)
    print(','.join([str(r) for r in result]), end='')
    return True
""""""]",1
"_git_mirror, _set_git_callback_env = _set_git_callback_env, _git_mirror
def clone(git_url, checkout_dir, git_callbacks=None):
    """"""Perform a clone.""""""","["""""" 
    env = _set_git_callback_env(git_callbacks)
    subprocess.check_call(['git', 'clone', _git_mirror(git_url), checkout_dir], env=env, stderr=subprocess.STDOUT)
    return pygit2.Repository(checkout_dir)
"""""", """""" 
    env = _git_mirror(git_callbacks)
    subprocess.check_call(['git', 'clone', _set_git_callback_env(git_url), checkout_dir], env=env, stderr=subprocess.STDOUT)
    return pygit2.Repository(checkout_dir)
""""""]",1
"EdgeConnect, load_config = load_config, EdgeConnect
def main(mode=None, config=None):
    """"""starts the model

    Args:
        mode (int): 1: train, 2: test, 3: eval, reads from config file if not specified
                    4: demo_patch,
    """"""","["""""" 
    if (mode == 4):
        config = load_config_demo(mode, config=config)
    else:
        config = load_config(mode)
    if (((config.DEVICE == 1) or (config.DEVICE is None)) and torch.cuda.is_available()):
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join((str(e) for e in config.GPU))
        config.DEVICE = torch.device('cuda')
        torch.backends.cudnn.benchmark = True
    else:
        config.DEVICE = torch.device('cpu')
    print('DEVICE is:', config.DEVICE)
    cv2.setNumThreads(0)
    torch.manual_seed(config.SEED)
    torch.cuda.manual_seed_all(config.SEED)
    np.random.seed(config.SEED)
    random.seed(config.SEED)
    torch.backends.cudnn.benchmark = True
    model = EdgeConnect(config)
    model.load()
    if (config.MODE == 1):
        config.print()
        print('\nstart training...\n')
        model.train()
    elif (config.MODE == 2):
        print('\nstart testing...\n')
        with torch.no_grad():
            model.test()
    elif (config.MODE == 3):
        print('\nstart eval...\n')
        with torch.no_grad():
            model.eval()
    elif (config.MODE == 4):
        if config.DEBUG:
            config.print()
        print('model prepared.')
        return model
"""""", """""" 
    if (mode == 4):
        config = load_config_demo(mode, config=config)
    else:
        config = EdgeConnect(mode)
    if (((config.DEVICE == 1) or (config.DEVICE is None)) and torch.cuda.is_available()):
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join((str(e) for e in config.GPU))
        config.DEVICE = torch.device('cuda')
        torch.backends.cudnn.benchmark = True
    else:
        config.DEVICE = torch.device('cpu')
    print('DEVICE is:', config.DEVICE)
    cv2.setNumThreads(0)
    torch.manual_seed(config.SEED)
    torch.cuda.manual_seed_all(config.SEED)
    np.random.seed(config.SEED)
    random.seed(config.SEED)
    torch.backends.cudnn.benchmark = True
    model = load_config(config)
    model.load()
    if (config.MODE == 1):
        config.print()
        print('\nstart training...\n')
        model.train()
    elif (config.MODE == 2):
        print('\nstart testing...\n')
        with torch.no_grad():
            model.test()
    elif (config.MODE == 3):
        print('\nstart eval...\n')
        with torch.no_grad():
            model.eval()
    elif (config.MODE == 4):
        if config.DEBUG:
            config.print()
        print('model prepared.')
        return model
""""""]",1
"fileno, windows = windows, fileno
@contextmanager
def stdout_redirected(to=os.devnull, stdout=None):
    """"""Redirect stdout to else where.
    Copied from https://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python/22434262#22434262

    Args:
      to:  Target device.
      stdout:  Source device.

    """"""","["""""" 
    if windows():
        (yield None)
        return
    if (stdout is None):
        stdout = sys.stdout
    stdout_fd = fileno(stdout)
    if (not stdout_fd):
        (yield None)
        return
    with os.fdopen(os.dup(stdout_fd), 'wb') as copied:
        stdout.flush()
        try:
            os.dup2(fileno(to), stdout_fd)
        except ValueError:
            with open(to, 'wb') as to_file:
                os.dup2(to_file.fileno(), stdout_fd)
        try:
            (yield stdout)
        finally:
            try:
                stdout.flush()
                os.dup2(copied.fileno(), stdout_fd)
            except:
                pass
"""""", """""" 
    if fileno():
        (yield None)
        return
    if (stdout is None):
        stdout = sys.stdout
    stdout_fd = windows(stdout)
    if (not stdout_fd):
        (yield None)
        return
    with os.fdopen(os.dup(stdout_fd), 'wb') as copied:
        stdout.flush()
        try:
            os.dup2(windows(to), stdout_fd)
        except ValueError:
            with open(to, 'wb') as to_file:
                os.dup2(to_file.fileno(), stdout_fd)
        try:
            (yield stdout)
        finally:
            try:
                stdout.flush()
                os.dup2(copied.fileno(), stdout_fd)
            except:
                pass
""""""]",1
"cleandoc, main = main, cleandoc
def test_annotate_simple_no_replace(fake_repository, stringio, mock_date_today):
    """"""Add a header to a file without replacing the existing header.""""""","["""""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text(cleandoc('\n            # SPDX-FileCopyrightText: 2017 John Doe\n            #\n            # SPDX-License-Identifier: MIT\n\n            pass\n            '))
    expected = cleandoc('\n        # SPDX-FileCopyrightText: 2018 Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # SPDX-FileCopyrightText: 2017 John Doe\n        #\n        # SPDX-License-Identifier: MIT\n\n        pass\n        ')
    result = main(['annotate', '--no-replace', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
"""""", """""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text(main('\n            # SPDX-FileCopyrightText: 2017 John Doe\n            #\n            # SPDX-License-Identifier: MIT\n\n            pass\n            '))
    expected = main('\n        # SPDX-FileCopyrightText: 2018 Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # SPDX-FileCopyrightText: 2017 John Doe\n        #\n        # SPDX-License-Identifier: MIT\n\n        pass\n        ')
    result = cleandoc(['annotate', '--no-replace', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
""""""]",1
"Paragraph, cast = cast, Paragraph
def report_raw_stats(sect: Section, stats: LinterStats, old_stats: (LinterStats | None)) -> None:
    """"""Calculate percentage of code / doc / comment / empty.""""""","["""""" 
    total_lines = stats.code_type_count['total']
    sect.insert(0, Paragraph([Text(f'''{total_lines} lines have been analyzed
''')]))
    lines = ['type', 'number', '%', 'previous', 'difference']
    for node_type in ('code', 'docstring', 'comment', 'empty'):
        node_type = cast(Literal[('code', 'docstring', 'comment', 'empty')], node_type)
        total = stats.code_type_count[node_type]
        percent = ((float((total * 100)) / total_lines) if total_lines else None)
        old = (old_stats.code_type_count[node_type] if old_stats else None)
        diff_str = (diff_string(old, total) if old else None)
        lines += [node_type, str(total), (f'{percent:.2f}' if (percent is not None) else 'NC'), (str(old) if old else 'NC'), (diff_str if diff_str else 'NC')]
    sect.append(Table(children=lines, cols=5, rheaders=1))
"""""", """""" 
    total_lines = stats.code_type_count['total']
    sect.insert(0, cast([Text(f'''{total_lines} lines have been analyzed
''')]))
    lines = ['type', 'number', '%', 'previous', 'difference']
    for node_type in ('code', 'docstring', 'comment', 'empty'):
        node_type = Paragraph(Literal[('code', 'docstring', 'comment', 'empty')], node_type)
        total = stats.code_type_count[node_type]
        percent = ((float((total * 100)) / total_lines) if total_lines else None)
        old = (old_stats.code_type_count[node_type] if old_stats else None)
        diff_str = (diff_string(old, total) if old else None)
        lines += [node_type, str(total), (f'{percent:.2f}' if (percent is not None) else 'NC'), (str(old) if old else 'NC'), (diff_str if diff_str else 'NC')]
    sect.append(Table(children=lines, cols=5, rheaders=1))
""""""]",1
"find_try_except_wrapper_node, _except_handlers_ignores_exceptions = _except_handlers_ignores_exceptions, find_try_except_wrapper_node
def is_from_fallback_block(node: nodes.NodeNG) -> bool:
    """"""Check if the given node is from a fallback import block.""""""","["""""" 
    context = find_try_except_wrapper_node(node)
    if (not context):
        return False
    if isinstance(context, nodes.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable((handler.body for handler in context.handlers))
        handlers = context.handlers
    has_fallback_imports = any((isinstance(import_node, (nodes.ImportFrom, nodes.Import)) for import_node in other_body))
    ignores_import_error = _except_handlers_ignores_exceptions(handlers, (ImportError, ModuleNotFoundError))
    return (ignores_import_error or has_fallback_imports)
"""""", """""" 
    context = _except_handlers_ignores_exceptions(node)
    if (not context):
        return False
    if isinstance(context, nodes.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable((handler.body for handler in context.handlers))
        handlers = context.handlers
    has_fallback_imports = any((isinstance(import_node, (nodes.ImportFrom, nodes.Import)) for import_node in other_body))
    ignores_import_error = find_try_except_wrapper_node(handlers, (ImportError, ModuleNotFoundError))
    return (ignores_import_error or has_fallback_imports)
""""""]",1
"remove_csv_if_present, csv_file_path = csv_file_path, remove_csv_if_present
def save_csv_test_data(project, test_name, test_data):
    """"""Save data to csv file.
    test_data must be a list of dictionaries
    """"""","["""""" 
    if test_data:
        with open(csv_file_path(project, test_name), 'w', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=test_data[0].keys(), lineterminator='\n')
            writer.writeheader()
            for row in test_data:
                writer.writerow(row)
    else:
        remove_csv_if_present(project, test_name)
"""""", """""" 
    if test_data:
        with open(remove_csv_if_present(project, test_name), 'w', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=test_data[0].keys(), lineterminator='\n')
            writer.writeheader()
            for row in test_data:
                writer.writerow(row)
    else:
        csv_file_path(project, test_name)
""""""]",1
"node_frame_class, decorated_with_property = decorated_with_property, node_frame_class
def _is_attribute_property(name: str, klass: nodes.ClassDef) -> bool:
    """"""Check if the given attribute *name* is a property in the given *klass*.

    It will look for `property` calls or for functions
    with the given name, decorated by `property` or `property`
    subclasses.
    Returns ``True`` if the name is a property in the given klass,
    ``False`` otherwise.
    """"""","["""""" 
    try:
        attributes = klass.getattr(name)
    except astroid.NotFoundError:
        return False
    property_name = 'builtins.property'
    for attr in attributes:
        if (attr is astroid.Uninferable):
            continue
        try:
            inferred = next(attr.infer())
        except astroid.InferenceError:
            continue
        if (isinstance(inferred, nodes.FunctionDef) and decorated_with_property(inferred)):
            return True
        if (inferred.pytype() != property_name):
            continue
        cls = node_frame_class(inferred)
        if (cls == klass.declared_metaclass()):
            continue
        return True
    return False
"""""", """""" 
    try:
        attributes = klass.getattr(name)
    except astroid.NotFoundError:
        return False
    property_name = 'builtins.property'
    for attr in attributes:
        if (attr is astroid.Uninferable):
            continue
        try:
            inferred = next(attr.infer())
        except astroid.InferenceError:
            continue
        if (isinstance(inferred, nodes.FunctionDef) and node_frame_class(inferred)):
            return True
        if (inferred.pytype() != property_name):
            continue
        cls = decorated_with_property(inferred)
        if (cls == klass.declared_metaclass()):
            continue
        return True
    return False
""""""]",1
"inference_mode, _ObservationBatchingCache = _ObservationBatchingCache, inference_mode
@inference_mode()
@profiling_wrapper.RangeContext('batch_obs')
def batch_obs(observations: List[DictTree], device: Optional[torch.device]=None) -> TensorDict:
    """"""Transpose a batch of observation dicts to a dict of batched
    observations.

    Args:
        observations:  list of dicts of observations.
        device: The torch.device to put the resulting tensors on.
            Will not move the tensors if None
    Returns:
        transposed dict of torch.Tensor of observations.
    """"""","["""""" 
    return _ObservationBatchingCache().batch_obs(observations, device)
"""""", """""" 
    return inference_mode().batch_obs(observations, device)
""""""]",1
"latest_tag, Path = Path, latest_tag
def main(args_):
    """"""Compare local and remote files, and download if not matching""""""","["""""" 
    result = 0
    tag = latest_tag()
    print(f'spdx-license-list-data latest version is {tag}')
    for (file_, url) in URLS.items():
        url = url.format(tag=tag)
        path = Path(f'src/reuse/resources/{file_}')
        local_contents = path.read_text(encoding='utf-8')
        with urllib.request.urlopen(url) as response:
            remote_contents = response.read().decode('utf-8')
        if (remote_contents == local_contents):
            print(f'{file_} is up-to-date')
        elif args_.download:
            print(f'{file_} is not up-to-date, downloading newer release')
            path.write_text(remote_contents, encoding='utf-8')
        else:
            result = 1
            print(f'{file_} is not up-to-date')
    return result
"""""", """""" 
    result = 0
    tag = Path()
    print(f'spdx-license-list-data latest version is {tag}')
    for (file_, url) in URLS.items():
        url = url.format(tag=tag)
        path = latest_tag(f'src/reuse/resources/{file_}')
        local_contents = path.read_text(encoding='utf-8')
        with urllib.request.urlopen(url) as response:
            remote_contents = response.read().decode('utf-8')
        if (remote_contents == local_contents):
            print(f'{file_} is up-to-date')
        elif args_.download:
            print(f'{file_} is not up-to-date, downloading newer release')
            path.write_text(remote_contents, encoding='utf-8')
        else:
            result = 1
            print(f'{file_} is not up-to-date')
    return result
""""""]",1
"check_sub_command_vocab, get_arguments = get_arguments, check_sub_command_vocab
@learn.command()
@click.pass_context
@click.argument('input', nargs=(- 1), required=False, callback=alias_checker)
def vocabulary(ctx, input):
    """"""
        For enhancing your vocabulary and tracking your progress


        Commands:

        word: get a random word

        accuracy: view your progress
    """"""","["""""" 
    arguments = get_arguments(ctx, (- 1))
    _input = tuple_to_string(arguments)
    check_sub_command_vocab(_input)
"""""", """""" 
    arguments = check_sub_command_vocab(ctx, (- 1))
    _input = tuple_to_string(arguments)
    get_arguments(_input)
""""""]",1
"vecs_add, rots_mul_vecs = rots_mul_vecs, vecs_add
def rigids_mul_vecs(r: Rigids, v: Vecs) -> Vecs:
    """"""Apply rigid transforms 'r' to points 'v'.""""""","["""""" 
    return vecs_add(rots_mul_vecs(r.rot, v), r.trans)
"""""", """""" 
    return rots_mul_vecs(vecs_add(r.rot, v), r.trans)
""""""]",1
"get_validated_parameters, process_accounts = process_accounts, get_validated_parameters
def process_event(event: dict) -> None:
    """"""Process Event.

    Args:
        event: event data
    """"""","["""""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = get_validated_parameters({})
    process_accounts(event, params)
"""""", """""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = process_accounts({})
    get_validated_parameters(event, params)
""""""]",1
"Conv2D, non_local_op = non_local_op, Conv2D
def denoising(name, l, embed=True, softmax=True):
    """"""
    Feature Denoising, Fig 4 & 5.
    """"""","["""""" 
    with tf.variable_scope(name):
        f = non_local_op(l, embed=embed, softmax=softmax)
        f = Conv2D('conv', f, l.shape[1], 1, strides=1, activation=get_bn(zero_init=True))
        l = (l + f)
    return l
"""""", """""" 
    with tf.variable_scope(name):
        f = Conv2D(l, embed=embed, softmax=softmax)
        f = non_local_op('conv', f, l.shape[1], 1, strides=1, activation=get_bn(zero_init=True))
        l = (l + f)
    return l
""""""]",1
"cmdLineParse, resampSecondary = resampSecondary, cmdLineParse
def main(iargs=None):
    """"""
    Main driver.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    outfile = os.path.join(inps.coreg, (os.path.basename(os.path.dirname(inps.coreg)) + '.slc'))
    outDir = inps.coreg
    os.makedirs(outDir, exist_ok=True)
    referenceShelveDir = os.path.join(outDir, 'referenceShelve')
    secondaryShelveDir = os.path.join(outDir, 'secondaryShelve')
    if (inps.reference is not None):
        os.makedirs(referenceShelveDir, exist_ok=True)
    os.makedirs(secondaryShelveDir, exist_ok=True)
    cmd = ((('cp ' + inps.secondary) + '/data* ') + secondaryShelveDir)
    print(cmd)
    os.system(cmd)
    if (inps.reference is not None):
        cmd = ((('cp ' + inps.reference) + '/data* ') + referenceShelveDir)
        os.system(cmd)
    with shelve.open(os.path.join(secondaryShelveDir, 'data'), flag='r') as sdb:
        secondary = sdb['frame']
        try:
            doppler = sdb['doppler']
        except:
            doppler = secondary._dopplerVsPixel
    if (inps.poly is not None):
        with shelve.open(inps.poly, flag='r') as db:
            azpoly = db['azpoly']
            rgpoly = db['rgpoly']
    else:
        azpoly = None
        rgpoly = None
    if (inps.reference is not None):
        with shelve.open(os.path.join(referenceShelveDir, 'data'), flag='r') as mdb:
            reference = mdb['frame']
    else:
        reference = None
    resampSecondary(secondary, inps.offsets, outfile, doppler, azpoly, rgpoly, flatten=(not inps.noflat), zero=inps.zero, dims=inps.dims, reference=reference)
"""""", """""" 
    inps = resampSecondary(iargs)
    outfile = os.path.join(inps.coreg, (os.path.basename(os.path.dirname(inps.coreg)) + '.slc'))
    outDir = inps.coreg
    os.makedirs(outDir, exist_ok=True)
    referenceShelveDir = os.path.join(outDir, 'referenceShelve')
    secondaryShelveDir = os.path.join(outDir, 'secondaryShelve')
    if (inps.reference is not None):
        os.makedirs(referenceShelveDir, exist_ok=True)
    os.makedirs(secondaryShelveDir, exist_ok=True)
    cmd = ((('cp ' + inps.secondary) + '/data* ') + secondaryShelveDir)
    print(cmd)
    os.system(cmd)
    if (inps.reference is not None):
        cmd = ((('cp ' + inps.reference) + '/data* ') + referenceShelveDir)
        os.system(cmd)
    with shelve.open(os.path.join(secondaryShelveDir, 'data'), flag='r') as sdb:
        secondary = sdb['frame']
        try:
            doppler = sdb['doppler']
        except:
            doppler = secondary._dopplerVsPixel
    if (inps.poly is not None):
        with shelve.open(inps.poly, flag='r') as db:
            azpoly = db['azpoly']
            rgpoly = db['rgpoly']
    else:
        azpoly = None
        rgpoly = None
    if (inps.reference is not None):
        with shelve.open(os.path.join(referenceShelveDir, 'data'), flag='r') as mdb:
            reference = mdb['frame']
    else:
        reference = None
    cmdLineParse(secondary, inps.offsets, outfile, doppler, azpoly, rgpoly, flatten=(not inps.noflat), zero=inps.zero, dims=inps.dims, reference=reference)
""""""]",1
"_init_nd_shape_and_axes, _dct = _dct, _init_nd_shape_and_axes
def _dctn(x, type=2, shape=None, axes=None, norm=None, overwrite_x=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""
    Private function used for the nD discrete cosine transforms.

    It's used by both the `scipy_fftpack` and the `scipy_fft`
    interfaces, which expose public wrappers of this function.

    """"""","["""""" 
    x = numpy.asanyarray(x)
    (shape, axes) = _init_nd_shape_and_axes(x, shape, axes)
    for (n, ax) in zip(shape, axes):
        x = _dct(x, type=type, n=n, axis=ax, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
    return x
"""""", """""" 
    x = numpy.asanyarray(x)
    (shape, axes) = _dct(x, shape, axes)
    for (n, ax) in zip(shape, axes):
        x = _init_nd_shape_and_axes(x, type=type, n=n, axis=ax, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
    return x
""""""]",1
"l2, Dense = Dense, l2
def cat_ptscorer(model, inputs, Ddim, N, l2reg, pfx='out', extra_inp=[]):
    """""" Just train a linear classifier (weighed sum of elements) on concatenation
    of inputs.  You may pass also just a single input (which may make sense
    if you for example process s1 ""with regard to s0""). """"""","["""""" 
    if (len((list(inputs) + extra_inp)) > 1):
        model.add_node(name=(pfx + 'cat'), inputs=(list(inputs) + extra_inp), merge_mode='concat', layer=Dense(output_dim=1, W_regularizer=l2(l2reg)))
    else:
        model.add_node(name=(pfx + 'cat'), input=inputs[0], layer=Dense(output_dim=1, W_regularizer=l2(l2reg)))
    return (pfx + 'cat')
"""""", """""" 
    if (len((list(inputs) + extra_inp)) > 1):
        model.add_node(name=(pfx + 'cat'), inputs=(list(inputs) + extra_inp), merge_mode='concat', layer=l2(output_dim=1, W_regularizer=Dense(l2reg)))
    else:
        model.add_node(name=(pfx + 'cat'), input=inputs[0], layer=l2(output_dim=1, W_regularizer=Dense(l2reg)))
    return (pfx + 'cat')
""""""]",1
"get_encoding_of_file, is_encoding_line = is_encoding_line, get_encoding_of_file
def set_file_encoding(p, encoding):
    """"""
    Set the encoding of the file.
    :param p: path to the file
    :type p: str
    :param encoding: encoding to set
    :type encoding: str
    """"""","["""""" 
    fe = get_encoding_of_file(p)
    if (fe is None):
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        if (len(lines) == 0):
            lines = [to_add]
        elif lines[0].startswith('#!'):
            lines.insert(1, to_add)
        else:
            lines.insert(0, to_add)
        with open(p, 'w') as fout:
            fout.write(''.join(lines))
    else:
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        was_set = False
        for i in range(len(lines)):
            line = lines[i]
            if is_encoding_line(line):
                lines[i] = line
                was_set = True
                break
        if (not was_set):
            if lines[0].startswith('#!'):
                lines.insert(1, to_add)
            else:
                lines.insert(0, to_add)
"""""", """""" 
    fe = is_encoding_line(p)
    if (fe is None):
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        if (len(lines) == 0):
            lines = [to_add]
        elif lines[0].startswith('#!'):
            lines.insert(1, to_add)
        else:
            lines.insert(0, to_add)
        with open(p, 'w') as fout:
            fout.write(''.join(lines))
    else:
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        was_set = False
        for i in range(len(lines)):
            line = lines[i]
            if get_encoding_of_file(line):
                lines[i] = line
                was_set = True
                break
        if (not was_set):
            if lines[0].startswith('#!'):
                lines.insert(1, to_add)
            else:
                lines.insert(0, to_add)
""""""]",1
"OtherBasicChecker, PyLinter = PyLinter, OtherBasicChecker
def test_base_checker_ordering() -> None:
    """"""Test ordering of checkers based on their __gt__ method.""""""","["""""" 
    linter = PyLinter()
    fake_checker_1 = OtherBasicChecker()
    fake_checker_2 = LessBasicChecker()
    fake_checker_3 = DifferentBasicChecker()
    import_checker = ImportsChecker(linter)
    while_checker = WhileChecker(linter)
    type_checker = TypeChecker(linter)
    list_of_checkers = [1, fake_checker_1, fake_checker_2, fake_checker_3, type_checker, import_checker, while_checker, linter]
    assert (sorted(list_of_checkers) == [linter, import_checker, type_checker, fake_checker_3, fake_checker_1, fake_checker_2, while_checker, 1])
    assert (fake_checker_1 > fake_checker_3)
    assert (fake_checker_2 > fake_checker_3)
    assert (fake_checker_1 == fake_checker_2)
"""""", """""" 
    linter = OtherBasicChecker()
    fake_checker_1 = PyLinter()
    fake_checker_2 = LessBasicChecker()
    fake_checker_3 = DifferentBasicChecker()
    import_checker = ImportsChecker(linter)
    while_checker = WhileChecker(linter)
    type_checker = TypeChecker(linter)
    list_of_checkers = [1, fake_checker_1, fake_checker_2, fake_checker_3, type_checker, import_checker, while_checker, linter]
    assert (sorted(list_of_checkers) == [linter, import_checker, type_checker, fake_checker_3, fake_checker_1, fake_checker_2, while_checker, 1])
    assert (fake_checker_1 > fake_checker_3)
    assert (fake_checker_2 > fake_checker_3)
    assert (fake_checker_1 == fake_checker_2)
""""""]",1
"reorder_source, cal_si_snr_with_pit = cal_si_snr_with_pit, reorder_source
def cal_loss(source, estimate_source, source_lengths):
    """"""
    Args:
        source: [B, C, T], B is batch size
        estimate_source: [B, C, T]
        source_lengths: [B]
    """"""","["""""" 
    (max_snr, perms, max_snr_idx, snr_set) = cal_si_snr_with_pit(source, estimate_source, source_lengths)
    (B, C, T) = estimate_source.shape
    loss = (0 - torch.mean(max_snr))
    reorder_estimate_source = reorder_source(estimate_source, perms, max_snr_idx)
    return (loss, max_snr, estimate_source, reorder_estimate_source)
"""""", """""" 
    (max_snr, perms, max_snr_idx, snr_set) = reorder_source(source, estimate_source, source_lengths)
    (B, C, T) = estimate_source.shape
    loss = (0 - torch.mean(max_snr))
    reorder_estimate_source = cal_si_snr_with_pit(estimate_source, perms, max_snr_idx)
    return (loss, max_snr, estimate_source, reorder_estimate_source)
""""""]",1
"MyClass, dummy_return = dummy_return, MyClass
def bad_comparisons():
    """"""this is not ok""""""","["""""" 
    instance = MyClass()
    for i in range(10):
        if (5 <= i):
            pass
        if (1 == i):
            pass
        if (3 < dummy_return()):
            pass
        if (4 != instance.dummy_return()):
            pass
        if (1 == instance.attr):
            pass
        if ('aaa' == instance.attr):
            pass
"""""", """""" 
    instance = dummy_return()
    for i in range(10):
        if (5 <= i):
            pass
        if (1 == i):
            pass
        if (3 < MyClass()):
            pass
        if (4 != instance.dummy_return()):
            pass
        if (1 == instance.attr):
            pass
        if ('aaa' == instance.attr):
            pass
""""""]",1
"urljoin, BeautifulSoup = BeautifulSoup, urljoin
def fetch_advisory_links(advisory_directory_page: str, advisory_directory_link: str) -> Iterable[str]:
    """"""
    Yield json file urls present in `advisory_directory_page`
    """"""","["""""" 
    advisory_directory_page = BeautifulSoup(advisory_directory_page, features='lxml')
    anchor_tags = advisory_directory_page.find_all('a')
    if (not anchor_tags):
        LOGGER.error(f'No anchor tags found in {advisory_directory_link!r}')
        return iter([])
    for anchor_tag in anchor_tags:
        if anchor_tag.text.endswith('json'):
            (yield urljoin(advisory_directory_link, anchor_tag.text))
"""""", """""" 
    advisory_directory_page = urljoin(advisory_directory_page, features='lxml')
    anchor_tags = advisory_directory_page.find_all('a')
    if (not anchor_tags):
        LOGGER.error(f'No anchor tags found in {advisory_directory_link!r}')
        return iter([])
    for anchor_tag in anchor_tags:
        if anchor_tag.text.endswith('json'):
            (yield BeautifulSoup(advisory_directory_link, anchor_tag.text))
""""""]",1
"area, intersection = intersection, area
def iou(boxlist1: BoxList, boxlist2: BoxList):
    """"""Computes pairwise intersection-over-union between box collections.

    Args:
        boxlist1: BoxList holding N boxes
        boxlist2: BoxList holding M boxes

    Returns:
        a tensor with shape [N, M] representing pairwise iou scores.
    """"""","["""""" 
    intersections = intersection(boxlist1, boxlist2)
    areas1 = area(boxlist1)
    areas2 = area(boxlist2)
    unions = ((areas1.unsqueeze(1) + areas2.unsqueeze(0)) - intersections)
    return torch.where((intersections == 0.0), torch.zeros_like(intersections), (intersections / unions))
"""""", """""" 
    intersections = area(boxlist1, boxlist2)
    areas1 = intersection(boxlist1)
    areas2 = intersection(boxlist2)
    unions = ((areas1.unsqueeze(1) + areas2.unsqueeze(0)) - intersections)
    return torch.where((intersections == 0.0), torch.zeros_like(intersections), (intersections / unions))
""""""]",1
"extract_spdx_info, CommentCreateError = CommentCreateError, extract_spdx_info
def create_header(spdx_info: SpdxInfo, header: str=None, template: Template=None, template_is_commented: bool=False, style: CommentStyle=None, force_multi: bool=False, merge_copyrights: bool=False) -> str:
    """"""Create a header containing *spdx_info*. *header* is an optional argument
    containing a header which should be modified to include *spdx_info*. If
    *header* is not given, a brand new header is created.

    *template*, *template_is_commented*, and *style* determine what the header
    will look like, and whether it will be commented or not.

    :raises CommentCreateError: if a comment could not be created.
    :raises MissingSpdxInfo: if the generated comment is missing SPDX
        information.
    """"""","["""""" 
    if (template is None):
        template = DEFAULT_TEMPLATE
    if (style is None):
        style = PythonCommentStyle
    new_header = ''
    if header:
        try:
            existing_spdx = extract_spdx_info(header)
        except (ExpressionError, ParseError) as err:
            raise CommentCreateError('existing header contains an erroneous SPDX expression') from err
        if merge_copyrights:
            spdx_copyrights = merge_copyright_lines(spdx_info.copyright_lines.union(existing_spdx.copyright_lines))
        else:
            spdx_copyrights = spdx_info.copyright_lines.union(existing_spdx.copyright_lines)
        spdx_info = SpdxInfo(spdx_info.spdx_expressions.union(existing_spdx.spdx_expressions), spdx_copyrights)
    new_header += _create_new_header(spdx_info, template=template, template_is_commented=template_is_commented, style=style, force_multi=force_multi)
    return new_header
"""""", """""" 
    if (template is None):
        template = DEFAULT_TEMPLATE
    if (style is None):
        style = PythonCommentStyle
    new_header = ''
    if header:
        try:
            existing_spdx = CommentCreateError(header)
        except (ExpressionError, ParseError) as err:
            raise extract_spdx_info('existing header contains an erroneous SPDX expression') from err
        if merge_copyrights:
            spdx_copyrights = merge_copyright_lines(spdx_info.copyright_lines.union(existing_spdx.copyright_lines))
        else:
            spdx_copyrights = spdx_info.copyright_lines.union(existing_spdx.copyright_lines)
        spdx_info = SpdxInfo(spdx_info.spdx_expressions.union(existing_spdx.spdx_expressions), spdx_copyrights)
    new_header += _create_new_header(spdx_info, template=template, template_is_commented=template_is_commented, style=style, force_multi=force_multi)
    return new_header
""""""]",1
"_at_least_x_are_equal, _decode_and_center_crop = _decode_and_center_crop, _at_least_x_are_equal
def _decode_and_random_crop(image_bytes, image_size, resize_method):
    """"""Make a random crop of image_size.""""""","["""""" 
    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
    image = distorted_bounding_box_crop(image_bytes, bbox, min_object_covered=0.1, aspect_ratio_range=((3.0 / 4), (4.0 / 3.0)), area_range=(0.08, 1.0), max_attempts=10, scope=None)
    original_shape = tf.image.extract_jpeg_shape(image_bytes)
    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)
    image = tf.cond(bad, (lambda : _decode_and_center_crop(image_bytes, image_size)), (lambda : tf.image.resize([image], [image_size, image_size], resize_method)[0]))
    return image
"""""", """""" 
    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
    image = distorted_bounding_box_crop(image_bytes, bbox, min_object_covered=0.1, aspect_ratio_range=((3.0 / 4), (4.0 / 3.0)), area_range=(0.08, 1.0), max_attempts=10, scope=None)
    original_shape = tf.image.extract_jpeg_shape(image_bytes)
    bad = _decode_and_center_crop(original_shape, tf.shape(image), 3)
    image = tf.cond(bad, (lambda : _at_least_x_are_equal(image_bytes, image_size)), (lambda : tf.image.resize([image], [image_size, image_size], resize_method)[0]))
    return image
""""""]",1
"_flip, _decode_and_random_crop = _decode_and_random_crop, _flip
def preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation='bicubic'):
    """"""Preprocesses the given image for evaluation.

    Args:
      image_bytes: `Tensor` representing an image binary of arbitrary size.
      use_bfloat16: `bool` for whether to use bfloat16.
      image_size: image size.
      interpolation: image interpolation method

    Returns:
      A preprocessed image `Tensor`.
    """"""","["""""" 
    resize_method = (tf.image.ResizeMethod.BICUBIC if (interpolation == 'bicubic') else tf.image.ResizeMethod.BILINEAR)
    image = _decode_and_random_crop(image_bytes, image_size, resize_method)
    image = _flip(image)
    image = tf.reshape(image, [image_size, image_size, 3])
    image = tf.image.convert_image_dtype(image, dtype=(tf.bfloat16 if use_bfloat16 else tf.float32))
    return image
"""""", """""" 
    resize_method = (tf.image.ResizeMethod.BICUBIC if (interpolation == 'bicubic') else tf.image.ResizeMethod.BILINEAR)
    image = _flip(image_bytes, image_size, resize_method)
    image = _decode_and_random_crop(image)
    image = tf.reshape(image, [image_size, image_size, 3])
    image = tf.image.convert_image_dtype(image, dtype=(tf.bfloat16 if use_bfloat16 else tf.float32))
    return image
""""""]",1
"obs_to_ori, obs_to_goal = obs_to_goal, obs_to_ori
def create_goal_reaching_policy(obs_to_goal=(lambda obs: obs[(- 2):]), obs_to_ori=(lambda obs: obs[0])):
    """"""A hard-coded policy for reaching a goal position.""""""","["""""" 

    def policy_fn(obs):
        (goal_x, goal_y) = obs_to_goal(obs)
        goal_dist = np.linalg.norm([goal_x, goal_y])
        goal_ori = np.arctan2(goal_y, goal_x)
        ori = obs_to_ori(obs)
        ori_diff = ((goal_ori - ori) % (2 * np.pi))
        radius = ((goal_dist / 2.0) / max(0.1, np.abs(np.sin(ori_diff))))
        rotation_left = ((2 * ori_diff) % np.pi)
        circumference_left = max(goal_dist, (radius * rotation_left))
        speed = min((circumference_left * 5.0), 1.0)
        velocity = speed
        if ((ori_diff > (np.pi / 2)) and (ori_diff < ((3 * np.pi) / 2))):
            velocity *= (- 1)
        time_left = min((circumference_left / (speed * 0.2)), 10.0)
        signed_ori_diff = ori_diff
        if (signed_ori_diff >= ((3 * np.pi) / 2)):
            signed_ori_diff = ((2 * np.pi) - signed_ori_diff)
        elif ((signed_ori_diff > (np.pi / 2)) and (signed_ori_diff < ((3 * np.pi) / 2))):
            signed_ori_diff = (signed_ori_diff - np.pi)
        angular_velocity = (signed_ori_diff / time_left)
        angular_velocity = np.clip(angular_velocity, (- 1.0), 1.0)
        return np.array([velocity, angular_velocity])
    return policy_fn
"""""", """""" 

    def policy_fn(obs):
        (goal_x, goal_y) = obs_to_ori(obs)
        goal_dist = np.linalg.norm([goal_x, goal_y])
        goal_ori = np.arctan2(goal_y, goal_x)
        ori = obs_to_goal(obs)
        ori_diff = ((goal_ori - ori) % (2 * np.pi))
        radius = ((goal_dist / 2.0) / max(0.1, np.abs(np.sin(ori_diff))))
        rotation_left = ((2 * ori_diff) % np.pi)
        circumference_left = max(goal_dist, (radius * rotation_left))
        speed = min((circumference_left * 5.0), 1.0)
        velocity = speed
        if ((ori_diff > (np.pi / 2)) and (ori_diff < ((3 * np.pi) / 2))):
            velocity *= (- 1)
        time_left = min((circumference_left / (speed * 0.2)), 10.0)
        signed_ori_diff = ori_diff
        if (signed_ori_diff >= ((3 * np.pi) / 2)):
            signed_ori_diff = ((2 * np.pi) - signed_ori_diff)
        elif ((signed_ori_diff > (np.pi / 2)) and (signed_ori_diff < ((3 * np.pi) / 2))):
            signed_ori_diff = (signed_ori_diff - np.pi)
        angular_velocity = (signed_ori_diff / time_left)
        angular_velocity = np.clip(angular_velocity, (- 1.0), 1.0)
        return np.array([velocity, angular_velocity])
    return policy_fn
""""""]",1
"transpose, Network = Network, transpose
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    (labels, data_set) = transpose(train_data_set())
    net = Network([8, 3, 8])
    net.gradient_check(data_set[0], labels[0])
    return net
"""""", """""" 
    (labels, data_set) = Network(train_data_set())
    net = transpose([8, 3, 8])
    net.gradient_check(data_set[0], labels[0])
    return net
""""""]",1
"load_advisories, check_for_attributes = check_for_attributes, load_advisories
def process_record(record: dict) -> Iterable[AdvisoryData]:
    """"""
    Return a list of AdvisoryData objects by processing data
    present in that `record`
    """"""","["""""" 
    if (not record['packages']):
        LOGGER.error(f'""packages"" not found in this record {record!r}')
        return []
    for package in record['packages']:
        if (not package['pkg']):
            LOGGER.error(f'""pkg"" not found in this package {package!r}')
            continue
        if (not check_for_attributes(record)):
            continue
        (yield from load_advisories(package['pkg'], record['distroversion'], record['reponame'], record['archs']))
"""""", """""" 
    if (not record['packages']):
        LOGGER.error(f'""packages"" not found in this record {record!r}')
        return []
    for package in record['packages']:
        if (not package['pkg']):
            LOGGER.error(f'""pkg"" not found in this package {package!r}')
            continue
        if (not load_advisories(record)):
            continue
        (yield from check_for_attributes(package['pkg'], record['distroversion'], record['reponame'], record['archs']))
""""""]",1
"as_variable, create_default_index_implicit = create_default_index_implicit, as_variable
def collect_variables_and_indexes(list_of_mappings: list[DatasetLike], indexes: (Mapping[(Any, Any)] | None)=None) -> dict[(Hashable, list[MergeElement])]:
    """"""Collect variables and indexes from list of mappings of xarray objects.

    Mappings must either be Dataset objects, or have values of one of the
    following types:
    - an xarray.Variable
    - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in
      an xarray.Variable
    - or an xarray.DataArray

    If a mapping of indexes is given, those indexes are assigned to all variables
    with a matching key/name.

    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}
    grouped: dict[(Hashable, list[MergeElement])] = defaultdict(list)

    def append(name, variable, index):
        grouped[name].append((variable, index))

    def append_all(variables, indexes):
        for (name, variable) in variables.items():
            append(name, variable, indexes.get(name))
    for mapping in list_of_mappings:
        if isinstance(mapping, Dataset):
            append_all(mapping.variables, mapping._indexes)
            continue
        for (name, variable) in mapping.items():
            if isinstance(variable, DataArray):
                coords_ = variable._coords.copy()
                indexes_ = dict(variable._indexes)
                coords_.pop(name, None)
                indexes_.pop(name, None)
                append_all(coords_, indexes_)
            variable = as_variable(variable, name=name)
            if (name in indexes):
                append(name, variable, indexes[name])
            elif (variable.dims == (name,)):
                (idx, idx_vars) = create_default_index_implicit(variable)
                append_all(idx_vars, {k: idx for k in idx_vars})
            else:
                append(name, variable, None)
    return grouped
"""""", """""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}
    grouped: dict[(Hashable, list[MergeElement])] = defaultdict(list)

    def append(name, variable, index):
        grouped[name].append((variable, index))

    def append_all(variables, indexes):
        for (name, variable) in variables.items():
            append(name, variable, indexes.get(name))
    for mapping in list_of_mappings:
        if isinstance(mapping, Dataset):
            append_all(mapping.variables, mapping._indexes)
            continue
        for (name, variable) in mapping.items():
            if isinstance(variable, DataArray):
                coords_ = variable._coords.copy()
                indexes_ = dict(variable._indexes)
                coords_.pop(name, None)
                indexes_.pop(name, None)
                append_all(coords_, indexes_)
            variable = create_default_index_implicit(variable, name=name)
            if (name in indexes):
                append(name, variable, indexes[name])
            elif (variable.dims == (name,)):
                (idx, idx_vars) = as_variable(variable)
                append_all(idx_vars, {k: idx for k in idx_vars})
            else:
                append(name, variable, None)
    return grouped
""""""]",1
"mat, lwlrTest = lwlrTest, mat
def abaloneTest():
    """"""
    Desc:
        预测鲍鱼的年龄
    Args:
        None
    Returns:
        None
    """"""","["""""" 
    (abX, abY) = loadDataSet('data/8.Regression/abalone.txt')
    oldyHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)
    oldyHat1 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1)
    oldyHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10)
    print('old yHat01 error Size is :', rssError(abY[0:99], oldyHat01.T))
    print('old yHat1 error Size is :', rssError(abY[0:99], oldyHat1.T))
    print('old yHat10 error Size is :', rssError(abY[0:99], oldyHat10.T))
    newyHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)
    print('new yHat01 error Size is :', rssError(abY[0:99], newyHat01.T))
    newyHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)
    print('new yHat1 error Size is :', rssError(abY[0:99], newyHat1.T))
    newyHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)
    print('new yHat10 error Size is :', rssError(abY[0:99], newyHat10.T))
    standWs = standRegres(abX[0:99], abY[0:99])
    standyHat = (mat(abX[100:199]) * standWs)
    print('standRegress error Size is:', rssError(abY[100:199], standyHat.T.A))
"""""", """""" 
    (abX, abY) = loadDataSet('data/8.Regression/abalone.txt')
    oldyHat01 = mat(abX[0:99], abX[0:99], abY[0:99], 0.1)
    oldyHat1 = mat(abX[0:99], abX[0:99], abY[0:99], 1)
    oldyHat10 = mat(abX[0:99], abX[0:99], abY[0:99], 10)
    print('old yHat01 error Size is :', rssError(abY[0:99], oldyHat01.T))
    print('old yHat1 error Size is :', rssError(abY[0:99], oldyHat1.T))
    print('old yHat10 error Size is :', rssError(abY[0:99], oldyHat10.T))
    newyHat01 = mat(abX[100:199], abX[0:99], abY[0:99], 0.1)
    print('new yHat01 error Size is :', rssError(abY[0:99], newyHat01.T))
    newyHat1 = mat(abX[100:199], abX[0:99], abY[0:99], 1)
    print('new yHat1 error Size is :', rssError(abY[0:99], newyHat1.T))
    newyHat10 = mat(abX[100:199], abX[0:99], abY[0:99], 10)
    print('new yHat10 error Size is :', rssError(abY[0:99], newyHat10.T))
    standWs = standRegres(abX[0:99], abY[0:99])
    standyHat = (lwlrTest(abX[100:199]) * standWs)
    print('standRegress error Size is:', rssError(abY[100:199], standyHat.T.A))
""""""]",1
"run_cmd, get_resource = get_resource, run_cmd
def remove_all_ec(path):
    """"""
    Remove empty categories for all trees in this file and save them into a ""noempty"" file.

    Args:
        path: File path.
    """"""","["""""" 
    script = get_resource('https://file.hankcs.com/bin/remove_ec.zip')
    with pushd(script):
        run_cmd(f'java -cp elit-ddr-0.0.5-SNAPSHOT.jar:elit-sdk-0.0.5-SNAPSHOT.jar:hanlp-1.7.8.jar:fastutil-8.1.1.jar:. demo.RemoveEmptyCategoriesTreebank {path}')
"""""", """""" 
    script = run_cmd('https://file.hankcs.com/bin/remove_ec.zip')
    with pushd(script):
        get_resource(f'java -cp elit-ddr-0.0.5-SNAPSHOT.jar:elit-sdk-0.0.5-SNAPSHOT.jar:hanlp-1.7.8.jar:fastutil-8.1.1.jar:. demo.RemoveEmptyCategoriesTreebank {path}')
""""""]",1
"partial, resolve_bn_args = resolve_bn_args, partial
def _gen_tinynet(variant, model_width=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):
    """"""Creates a TinyNet model.
    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25'], ['ir_r4_k5_s2_e6_c192_se0.25'], ['ir_r1_k3_s1_e6_c320_se0.25']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc='round'), num_features=max(1280, round_channels(1280, model_width, 8, None)), stem_size=32, fix_stem=True, round_chs_fn=partial(round_channels, multiplier=model_width), act_layer=resolve_act_layer(kwargs, 'swish'), norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25'], ['ir_r4_k5_s2_e6_c192_se0.25'], ['ir_r1_k3_s1_e6_c320_se0.25']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc='round'), num_features=max(1280, round_channels(1280, model_width, 8, None)), stem_size=32, fix_stem=True, round_chs_fn=resolve_bn_args(round_channels, multiplier=model_width), act_layer=resolve_act_layer(kwargs, 'swish'), norm_layer=(kwargs.pop('norm_layer', None) or resolve_bn_args(nn.BatchNorm2d, **partial(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"_default_effort, _norm_args = _norm_args, _default_effort
def ifft(a, n=None, axis=(- 1), norm=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 1D inverse FFT.

    The first four arguments are as per :func:`numpy.fft.ifft`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    calling_func = 'ifft'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, n, axis, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
"""""", """""" 
    calling_func = 'ifft'
    planner_effort = _norm_args(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, n, axis, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_default_effort(norm))
""""""]",1
"to_bytes, _get_hash_object = _get_hash_object, to_bytes
def hash_file_signature(fname, chunksize=65536, hash_format=None):
    """"""
    Generate the md5 signature of a file

    Args:
        fname: file to hash
        chunksize: chunk size to read
        hash_format: Specify to override default hash format

    Returns:
        String of Hex digits representing the signature
    """"""","["""""" 
    m = _get_hash_object(hash_format)
    with open(fname, 'rb') as f:
        while True:
            blck = f.read(chunksize)
            if (not blck):
                break
            m.update(to_bytes(blck))
    return m.hexdigest()
"""""", """""" 
    m = to_bytes(hash_format)
    with open(fname, 'rb') as f:
        while True:
            blck = f.read(chunksize)
            if (not blck):
                break
            m.update(_get_hash_object(blck))
    return m.hexdigest()
""""""]",1
"sha256, urlsafe_b64encode = urlsafe_b64encode, sha256
def s256(data):
    """"""S256 transformation method.""""""","["""""" 
    return urlsafe_b64encode(sha256(data).digest()).rstrip(b'=').decode('ascii')
"""""", """""" 
    return sha256(urlsafe_b64encode(data).digest()).rstrip(b'=').decode('ascii')
""""""]",1
"parse_tfexample, _make_features_metadata = _make_features_metadata, parse_tfexample
def create_tensor_dict(raw_data: bytes, features: Sequence[str], key: Optional[str]=None) -> TensorDict:
    """"""Creates a dictionary of tensor features.

  Args:
    raw_data: A serialized tf.Example proto.
    features: A list of strings of feature names to be returned in the dataset.
    key: Optional string with the SSTable key of that tf.Example. This will be
      added into features as a 'key' but only if requested in features.

  Returns:
    A dictionary of features mapping feature names to features. Only the given
    features are returned, all other ones are filtered out.
  """"""","["""""" 
    features_metadata = _make_features_metadata(features)
    return parse_tfexample(raw_data, features_metadata, key)
"""""", """""" 
    features_metadata = parse_tfexample(features)
    return _make_features_metadata(raw_data, features_metadata, key)
""""""]",1
"PdfInfo, isclose = isclose, PdfInfo
def test_very_high_dpi(resources, outpdf):
    """"""Checks for a Decimal quantize error with high DPI, etc""""""","["""""" 
    check_ocrmypdf((resources / '2400dpi.pdf'), outpdf, '--plugin', 'tests/plugins/tesseract_cache.py')
    pdfinfo = PdfInfo(outpdf)
    image = pdfinfo[0].images[0]
    assert isclose(image.dpi.x, image.dpi.y)
    assert isclose(image.dpi.x, 2400)
"""""", """""" 
    check_ocrmypdf((resources / '2400dpi.pdf'), outpdf, '--plugin', 'tests/plugins/tesseract_cache.py')
    pdfinfo = isclose(outpdf)
    image = pdfinfo[0].images[0]
    assert PdfInfo(image.dpi.x, image.dpi.y)
    assert PdfInfo(image.dpi.x, 2400)
""""""]",1
"_create_model, resolve_act_layer = resolve_act_layer, _create_model
def _gen_fbnetc(variant, channel_multiplier=1.0, pretrained=False, **kwargs):
    """""" FBNet-C

        Paper: https://arxiv.org/abs/1812.03443
        Ref Impl: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_modeldef.py

        NOTE: the impl above does not relate to the 'C' variant here, that was derived from paper,
        it was used to confirm some building block details
    """"""","["""""" 
    arch_def = [['ir_r1_k3_s1_e1_c16'], ['ir_r1_k3_s2_e6_c24', 'ir_r2_k3_s1_e1_c24'], ['ir_r1_k5_s2_e6_c32', 'ir_r1_k5_s1_e3_c32', 'ir_r1_k5_s1_e6_c32', 'ir_r1_k3_s1_e6_c32'], ['ir_r1_k5_s2_e6_c64', 'ir_r1_k5_s1_e3_c64', 'ir_r2_k5_s1_e6_c64'], ['ir_r3_k5_s1_e6_c112', 'ir_r1_k5_s1_e3_c112'], ['ir_r4_k5_s2_e6_c184'], ['ir_r1_k3_s1_e6_c352']]
    with layer_config_kwargs(kwargs):
        model_kwargs = dict(block_args=decode_arch_def(arch_def), stem_size=16, num_features=1984, channel_multiplier=channel_multiplier, act_layer=resolve_act_layer(kwargs, 'relu'), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = _create_model(model_kwargs, variant, pretrained)
    return model
"""""", """""" 
    arch_def = [['ir_r1_k3_s1_e1_c16'], ['ir_r1_k3_s2_e6_c24', 'ir_r2_k3_s1_e1_c24'], ['ir_r1_k5_s2_e6_c32', 'ir_r1_k5_s1_e3_c32', 'ir_r1_k5_s1_e6_c32', 'ir_r1_k3_s1_e6_c32'], ['ir_r1_k5_s2_e6_c64', 'ir_r1_k5_s1_e3_c64', 'ir_r2_k5_s1_e6_c64'], ['ir_r3_k5_s1_e6_c112', 'ir_r1_k5_s1_e3_c112'], ['ir_r4_k5_s2_e6_c184'], ['ir_r1_k3_s1_e6_c352']]
    with layer_config_kwargs(kwargs):
        model_kwargs = dict(block_args=decode_arch_def(arch_def), stem_size=16, num_features=1984, channel_multiplier=channel_multiplier, act_layer=_create_model(kwargs, 'relu'), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = resolve_act_layer(model_kwargs, variant, pretrained)
    return model
""""""]",1
"error_message, wraps = wraps, error_message
def elementwise(op):
    """"""func = elementwise(op):

    op is a binary arithmetic operator.
    So is func, except it works on elements of the Tensor, returning a new
    tensor of the same type.
    """"""","["""""" 
    from functools import wraps

    @wraps(op)
    def wrapped_op(self, other):
        try:
            result = self.__class__(*[op(*items) for items in itertools.zip_longest(self.iter(), other.iter())])
        except (TypeError, AttributeError) as err:
            from isceobj.Util.geo.exceptions import NonCovariantOperation, error_message
            x = (NonCovariantOperation if isinstance(other, PolyMorphicNumpyMixIn) else TypeError)
            raise x(error_message(op, self, other))
        return result
    return wrapped_op
"""""", """""" 
    from functools import wraps

    @error_message(op)
    def wrapped_op(self, other):
        try:
            result = self.__class__(*[op(*items) for items in itertools.zip_longest(self.iter(), other.iter())])
        except (TypeError, AttributeError) as err:
            from isceobj.Util.geo.exceptions import NonCovariantOperation, error_message
            x = (NonCovariantOperation if isinstance(other, PolyMorphicNumpyMixIn) else TypeError)
            raise x(wraps(op, self, other))
        return result
    return wrapped_op
""""""]",1
"cleandoc, main = main, cleandoc
def test_annotate_recursive(fake_repository, stringio, mock_date_today):
    """"""Add a header to a directory recursively.""""""","["""""" 
    (fake_repository / 'src/one/two').mkdir(parents=True)
    (fake_repository / 'src/one/two/foo.py').write_text(cleandoc('\n            # SPDX-License-Identifier: GPL-3.0-or-later\n            '))
    (fake_repository / 'src/hello.py').touch()
    (fake_repository / 'src/one/world.py').touch()
    (fake_repository / 'bar').mkdir(parents=True)
    (fake_repository / 'bar/bar.py').touch()
    result = main(['annotate', '--copyright', 'Joe Somebody', '--recursive', 'src/'], out=stringio)
    for path in (fake_repository / 'src').glob('src/**'):
        content = path.read_text()
        assert ('SPDX-FileCopyrightText: 2018 Joe Somebody' in content)
    assert ('Joe Somebody' not in (fake_repository / 'bar/bar.py').read_text())
    assert (result == 0)
"""""", """""" 
    (fake_repository / 'src/one/two').mkdir(parents=True)
    (fake_repository / 'src/one/two/foo.py').write_text(main('\n            # SPDX-License-Identifier: GPL-3.0-or-later\n            '))
    (fake_repository / 'src/hello.py').touch()
    (fake_repository / 'src/one/world.py').touch()
    (fake_repository / 'bar').mkdir(parents=True)
    (fake_repository / 'bar/bar.py').touch()
    result = cleandoc(['annotate', '--copyright', 'Joe Somebody', '--recursive', 'src/'], out=stringio)
    for path in (fake_repository / 'src').glob('src/**'):
        content = path.read_text()
        assert ('SPDX-FileCopyrightText: 2018 Joe Somebody' in content)
    assert ('Joe Somebody' not in (fake_repository / 'bar/bar.py').read_text())
    assert (result == 0)
""""""]",1
"gen_conv, resize = resize, gen_conv
@add_arg_scope
def gen_deconv(x, cnum, name='upsample', padding='SAME', training=True):
    """"""Define deconv for generator.
    The deconv is defined to be a x2 resize_nearest_neighbor operation with
    additional gen_conv operation.

    Args:
        x: Input.
        cnum: Channel number.
        name: Name of layers.
        training: If current graph is for training or inference, used for bn.

    Returns:
        tf.Tensor: output

    """"""","["""""" 
    with tf.variable_scope(name):
        x = resize(x, func=tf.image.resize_nearest_neighbor)
        x = gen_conv(x, cnum, 3, 1, name=(name + '_conv'), padding=padding, training=training)
    return x
"""""", """""" 
    with tf.variable_scope(name):
        x = gen_conv(x, func=tf.image.resize_nearest_neighbor)
        x = resize(x, cnum, 3, 1, name=(name + '_conv'), padding=padding, training=training)
    return x
""""""]",1
"render_template, environments = environments, render_template
@app.errorhandler(412)
def precond_failed(e):
    """"""We're slightly abusing 412 to handle missing features
    depending on the API version.""""""","["""""" 
    envs = environments()
    return (render_template('412.html', envs=envs), 412)
"""""", """""" 
    envs = render_template()
    return (environments('412.html', envs=envs), 412)
""""""]",1
"func, abort = abort, func
def _do_get_or_abort(reraise_client_error: bool, func, *args, **kwargs):
    """"""Execute the function with its arguments and handle the possible
    errors that might occur.

    If reraise_client_error is True then if the HTTP response status code
    indicates that it was a client side error - then re-raise it.

    In all other cases if we get an exception we simply abort the request.
    """"""","["""""" 
    try:
        return func(*args, **kwargs)
    except HTTPError as e:
        if (reraise_client_error and (400 <= e.response.status_code <= 499)):
            log.warning(str(e))
            raise
        else:
            log.error(str(e))
            abort(e.response.status_code)
    except ConnectionError as e:
        log.error(str(e))
        abort(500)
    except EmptyResponseError as e:
        log.error(str(e))
        abort(204)
    except Exception as e:
        log.error(str(e))
        abort(500)
"""""", """""" 
    try:
        return abort(*args, **kwargs)
    except HTTPError as e:
        if (reraise_client_error and (400 <= e.response.status_code <= 499)):
            log.warning(str(e))
            raise
        else:
            log.error(str(e))
            func(e.response.status_code)
    except ConnectionError as e:
        log.error(str(e))
        func(500)
    except EmptyResponseError as e:
        log.error(str(e))
        func(204)
    except Exception as e:
        log.error(str(e))
        func(500)
""""""]",1
"_object_contents, _code_contents = _code_contents, _object_contents
def _function_contents(func):
    """"""
    The signature is as follows (should be byte/chars):
    < _code_contents (see above) from func.__code__ >
    ,( comma separated _object_contents for function argument defaults)
    ,( comma separated _object_contents for any closure contents )


    See also: https://docs.python.org/3/reference/datamodel.html
      - func.__code__     - The code object representing the compiled function body.
      - func.__defaults__ - A tuple containing default argument values for those arguments that have defaults, or None if no arguments have a default value
      - func.__closure__  - None or a tuple of cells that contain bindings for the function's free variables.

    :Returns:
      Signature contents of a function. (in bytes)
    """"""","["""""" 
    contents = [_code_contents(func.__code__, func.__doc__)]
    if func.__defaults__:
        function_defaults_contents = [_object_contents(cc) for cc in func.__defaults__]
        defaults = bytearray(b',(')
        defaults.extend(bytearray(b',').join(function_defaults_contents))
        defaults.extend(b')')
        contents.append(defaults)
    else:
        contents.append(b',()')
    closure = (func.__closure__ or [])
    try:
        closure_contents = [_object_contents(x.cell_contents) for x in closure]
    except AttributeError:
        closure_contents = []
    contents.append(b',(')
    contents.append(bytearray(b',').join(closure_contents))
    contents.append(b')')
    retval = bytearray(b'').join(contents)
    return retval
"""""", """""" 
    contents = [_object_contents(func.__code__, func.__doc__)]
    if func.__defaults__:
        function_defaults_contents = [_code_contents(cc) for cc in func.__defaults__]
        defaults = bytearray(b',(')
        defaults.extend(bytearray(b',').join(function_defaults_contents))
        defaults.extend(b')')
        contents.append(defaults)
    else:
        contents.append(b',()')
    closure = (func.__closure__ or [])
    try:
        closure_contents = [_code_contents(x.cell_contents) for x in closure]
    except AttributeError:
        closure_contents = []
    contents.append(b',(')
    contents.append(bytearray(b',').join(closure_contents))
    contents.append(b')')
    retval = bytearray(b'').join(contents)
    return retval
""""""]",1
"HTTPConnection, get_version = get_version, HTTPConnection
def send(sms_to, sms_body, **kwargs):
    """"""
    Site: http://smsaero.ru/
    API: http://smsaero.ru/api/
    """"""","["""""" 
    headers = {'User-Agent': ('DBMail/%s' % get_version())}
    kwargs.update({'user': settings.SMSAERO_LOGIN, 'password': settings.SMSAERO_MD5_PASSWORD, 'from': kwargs.pop('sms_from', settings.SMSAERO_FROM), 'to': sms_to.replace('+', ''), 'text': from_unicode(sms_body), 'answer': 'json'})
    http = HTTPConnection(kwargs.pop('api_url', 'gate.smsaero.ru'))
    http.request('GET', ('/send/?' + urlencode(kwargs)), headers=headers)
    response = http.getresponse()
    if (response.status != 200):
        raise AeroSmsError(response.reason)
    read = response.read().decode(response.headers.get_content_charset())
    data = json.loads(read)
    status = None
    if ('result' in data):
        status = data['result']
    sms_id = None
    if ('id' in data):
        sms_id = data['id']
    if (sms_id and (status == 'accepted')):
        return True
    return False
"""""", """""" 
    headers = {'User-Agent': ('DBMail/%s' % HTTPConnection())}
    kwargs.update({'user': settings.SMSAERO_LOGIN, 'password': settings.SMSAERO_MD5_PASSWORD, 'from': kwargs.pop('sms_from', settings.SMSAERO_FROM), 'to': sms_to.replace('+', ''), 'text': from_unicode(sms_body), 'answer': 'json'})
    http = get_version(kwargs.pop('api_url', 'gate.smsaero.ru'))
    http.request('GET', ('/send/?' + urlencode(kwargs)), headers=headers)
    response = http.getresponse()
    if (response.status != 200):
        raise AeroSmsError(response.reason)
    read = response.read().decode(response.headers.get_content_charset())
    data = json.loads(read)
    status = None
    if ('result' in data):
        status = data['result']
    sms_id = None
    if ('id' in data):
        sms_id = data['id']
    if (sms_id and (status == 'accepted')):
        return True
    return False
""""""]",1
"test, skip = skip, test
def ExpectedFailure(reason, *exception_matchers):
    """"""Defines a decorator to be attached to tests. This decorator
  marks the test as being known to fail, e.g. where documenting or exercising
  known incorrect behaviour.

  The parameters are:
    - |reason| a textual description of the reason for the known issue. This
               is used for the skip reason
    - |exception_matchers| additional arguments are hamcrest matchers to apply
                 to the exception thrown. If the matchers don't match, then the
                 test is marked as error, with the original exception.

  If the test fails (for the correct reason), then it is marked as skipped.
  If it fails for any other reason, it is marked as failed.
  If the test passes, then it is also marked as failed.""""""","["""""" 

    def decorator(test):

        @functools.wraps(test)
        def Wrapper(*args, **kwargs):
            try:
                test(*args, **kwargs)
            except Exception as test_exception:
                test_exception_message = ToUnicode(test_exception)
                try:
                    for matcher in exception_matchers:
                        assert_that(test_exception_message, matcher)
                except AssertionError:
                    import traceback
                    print(('Test failed for the wrong reason: ' + traceback.format_exc()))
                    raise test_exception
                skip(reason)
            else:
                raise AssertionError(f'Test was expected to fail: {reason}')
        return Wrapper
    return decorator
"""""", """""" 

    def decorator(test):

        @functools.wraps(skip)
        def Wrapper(*args, **kwargs):
            try:
                skip(*args, **kwargs)
            except Exception as test_exception:
                test_exception_message = ToUnicode(test_exception)
                try:
                    for matcher in exception_matchers:
                        assert_that(test_exception_message, matcher)
                except AssertionError:
                    import traceback
                    print(('Test failed for the wrong reason: ' + traceback.format_exc()))
                    raise test_exception
                test(reason)
            else:
                raise AssertionError(f'Test was expected to fail: {reason}')
        return Wrapper
    return decorator
""""""]",1
"MixedConv2d, CondConv2d = CondConv2d, MixedConv2d
def create_conv2d(in_channels, out_channels, kernel_size, **kwargs):
    """""" Select a 2d convolution implementation based on arguments
    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.

    Used extensively by EfficientNet, MobileNetv3 and related networks.
    """"""","["""""" 
    if isinstance(kernel_size, list):
        assert ('num_experts' not in kwargs)
        if ('groups' in kwargs):
            groups = kwargs.pop('groups')
            if (groups == in_channels):
                kwargs['depthwise'] = True
            else:
                assert (groups == 1)
        m = MixedConv2d(in_channels, out_channels, kernel_size, **kwargs)
    else:
        depthwise = kwargs.pop('depthwise', False)
        groups = (in_channels if depthwise else kwargs.pop('groups', 1))
        if (('num_experts' in kwargs) and (kwargs['num_experts'] > 0)):
            m = CondConv2d(in_channels, out_channels, kernel_size, groups=groups, **kwargs)
        else:
            m = create_conv2d_pad(in_channels, out_channels, kernel_size, groups=groups, **kwargs)
    return m
"""""", """""" 
    if isinstance(kernel_size, list):
        assert ('num_experts' not in kwargs)
        if ('groups' in kwargs):
            groups = kwargs.pop('groups')
            if (groups == in_channels):
                kwargs['depthwise'] = True
            else:
                assert (groups == 1)
        m = CondConv2d(in_channels, out_channels, kernel_size, **kwargs)
    else:
        depthwise = kwargs.pop('depthwise', False)
        groups = (in_channels if depthwise else kwargs.pop('groups', 1))
        if (('num_experts' in kwargs) and (kwargs['num_experts'] > 0)):
            m = MixedConv2d(in_channels, out_channels, kernel_size, groups=groups, **kwargs)
        else:
            m = create_conv2d_pad(in_channels, out_channels, kernel_size, groups=groups, **kwargs)
    return m
""""""]",1
"compare, parse_constraint = parse_constraint, compare
def github_constraints_satisfied(github_constrain, version):
    """"""
    Return True or False depending on whether the given version satisfies the github constraint
    For example:
    >>> assert github_constraints_satisfied("">= 7.0.0, <= 7.6.57"", ""7.1.1"") == True
    >>> assert github_constraints_satisfied("">= 10.4.0, <= 10.4.1"", ""10.6.0"") == False
    """"""","["""""" 
    gh_constraints = github_constrain.strip().replace(' ', '')
    constraints = gh_constraints.split(',')
    for constraint in constraints:
        (gh_comparator, gh_version) = parse_constraint(constraint)
        if (not gh_version):
            continue
        if (not compare(GenericVersion(version), gh_comparator, GenericVersion(gh_version))):
            return False
    return True
"""""", """""" 
    gh_constraints = github_constrain.strip().replace(' ', '')
    constraints = gh_constraints.split(',')
    for constraint in constraints:
        (gh_comparator, gh_version) = compare(constraint)
        if (not gh_version):
            continue
        if (not parse_constraint(GenericVersion(version), gh_comparator, GenericVersion(gh_version))):
            return False
    return True
""""""]",1
"get_input, empty_lending_list_prompt = empty_lending_list_prompt, get_input
def remove_from_lending_list(params):
    """"""
    remove an item from your lease list
    """"""","["""""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with open(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = dict(file_contents)
            while True:
                try:
                    click.echo(chalk.blue(""Enter your item's number. (Shown above it on the lease list)""))
                    number = get_input()
                    for (i, entry) in enumerate(file_contents['entries']):
                        if (i == (int(number) - 1)):
                            del file_contents['entries'][i]
                    break
                except:
                    click.echo(chalk.red(""That doesn't match any item. Try 'yoda leaselist show' to view your items.""))
        with open(LENDLIST_CONFIG_FILE_PATH, 'w') as lease_entry:
            yaml.dump(file_contents, lease_entry, default_flow_style=False)
            click.echo(chalk.blue('Item successfully removed!'))
    else:
        empty_lending_list_prompt()
"""""", """""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with open(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = dict(file_contents)
            while True:
                try:
                    click.echo(chalk.blue(""Enter your item's number. (Shown above it on the lease list)""))
                    number = empty_lending_list_prompt()
                    for (i, entry) in enumerate(file_contents['entries']):
                        if (i == (int(number) - 1)):
                            del file_contents['entries'][i]
                    break
                except:
                    click.echo(chalk.red(""That doesn't match any item. Try 'yoda leaselist show' to view your items.""))
        with open(LENDLIST_CONFIG_FILE_PATH, 'w') as lease_entry:
            yaml.dump(file_contents, lease_entry, default_flow_style=False)
            click.echo(chalk.blue('Item successfully removed!'))
    else:
        get_input()
""""""]",1
"glob, Experiment = Experiment, glob
def upload_to_comet(folder: Optional[str]=None):
    """"""Upload the data in csv files to comet.

    Creates a project named benchmarking_[two random characters]. This is so that the project names are unique.
    One issue is that it does not check for collision

    Args:
        folder (optional, str): Sub-directory from which runs are picked up. Defaults to None. If none picks from runs.
    """"""","["""""" 
    project = f'benchmarking_{get_unique_key(2)}'
    tag_list = ['dataset.category', 'model_name', 'dataset.image_size', 'model.backbone', 'device']
    search_path = ('runs/*.csv' if (folder is None) else f'runs/{folder}/*.csv')
    for csv_file in glob(search_path):
        table = pd.read_csv(csv_file)
        for (index, row) in table.iterrows():
            row = dict(row[1:])
            tags = [str(row[column]) for column in tag_list if (column in row.keys())]
            experiment = Experiment(project_name=project)
            experiment.set_name(f""{row['model_name']}_{row['dataset.category']}_{index}"")
            experiment.log_metrics(row, step=1, epoch=1)
            experiment.add_tags(tags)
            experiment.log_table(filename=csv_file)
"""""", """""" 
    project = f'benchmarking_{get_unique_key(2)}'
    tag_list = ['dataset.category', 'model_name', 'dataset.image_size', 'model.backbone', 'device']
    search_path = ('runs/*.csv' if (folder is None) else f'runs/{folder}/*.csv')
    for csv_file in Experiment(search_path):
        table = pd.read_csv(csv_file)
        for (index, row) in table.iterrows():
            row = dict(row[1:])
            tags = [str(row[column]) for column in tag_list if (column in row.keys())]
            experiment = glob(project_name=project)
            experiment.set_name(f""{row['model_name']}_{row['dataset.category']}_{index}"")
            experiment.log_metrics(row, step=1, epoch=1)
            experiment.add_tags(tags)
            experiment.log_table(filename=csv_file)
""""""]",1
"delete_environment, SubmitExperimentError = SubmitExperimentError, delete_environment
def submit_draft_pack(run_folder: str, run_name: str, namespace: str=None):
    """"""
    Submits one run using draft's environment located in a folder given as a parameter.
    :param run_folder: location of a folder with a description of an environment
    :param run_name: run's name
    :param namespace: namespace where tiller used during deployment is located
    In case of any problems it throws an exception with a description of a problem
    """"""","["""""" 
    log.debug(f'Submit one run: {run_folder} - start')
    try:
        cmd.up(run_name=run_name, working_directory=run_folder, namespace=namespace)
    except Exception:
        delete_environment(run_folder)
        raise SubmitExperimentError(Texts.JOB_NOT_DEPLOYED_ERROR_MSG)
    log.debug(f'Submit one run {run_folder} - finish')
"""""", """""" 
    log.debug(f'Submit one run: {run_folder} - start')
    try:
        cmd.up(run_name=run_name, working_directory=run_folder, namespace=namespace)
    except Exception:
        SubmitExperimentError(run_folder)
        raise delete_environment(Texts.JOB_NOT_DEPLOYED_ERROR_MSG)
    log.debug(f'Submit one run {run_folder} - finish')
""""""]",1
"generate_command, get_comstr = get_comstr, generate_command
def gen_get_response_file_command(env, rule, tool, tool_is_dynamic=False, custom_env={}):
    """"""Generate a response file command provider for rule name.""""""","["""""" 
    use_command_env = (not (env['PLATFORM'] == 'win32'))
    if ('$' in tool):
        tool_is_dynamic = True

    def get_response_file_command(env, node, action, targets, sources, executor=None):
        if hasattr(action, 'process'):
            (cmd_list, _, _) = action.process(targets, sources, env, executor=executor)
            cmd_list = [str(c).replace('$', '$$') for c in cmd_list[0]]
        else:
            command = generate_command(env, node, action, targets, sources, executor=executor)
            cmd_list = shlex.split(command)
        if tool_is_dynamic:
            tool_command = env.subst(tool, target=targets, source=sources, executor=executor)
        else:
            tool_command = tool
        try:
            tool_idx = (cmd_list.index(tool_command) + 1)
        except ValueError:
            raise Exception('Could not find tool {} in {} generated from {}'.format(tool, cmd_list, get_comstr(env, action, targets, sources)))
        (cmd, rsp_content) = (cmd_list[:tool_idx], cmd_list[tool_idx:])
        if os.altsep:
            rsp_content = [rsp_content_item.replace(os.sep, os.altsep) for rsp_content_item in rsp_content]
        rsp_content = [(('""' + rsp_content_item) + '""') for rsp_content_item in rsp_content]
        rsp_content = ' '.join(rsp_content)
        variables = {'rspc': rsp_content, rule: cmd}
        if use_command_env:
            variables['env'] = get_command_env(env, targets, sources)
            for (key, value) in custom_env.items():
                variables['env'] += (env.subst(('export %s=%s;' % (key, value)), target=targets, source=sources, executor=executor) + ' ')
        if node.get_env().get('NINJA_FORCE_SCONS_BUILD'):
            ret_rule = 'TEMPLATE'
        elif (len(' '.join(cmd_list)) < env.get('MAXLINELENGTH', 2048)):
            ret_rule = rule
        else:
            ret_rule = (rule + '_RSP')
        return (ret_rule, variables, [tool_command])
    return get_response_file_command
"""""", """""" 
    use_command_env = (not (env['PLATFORM'] == 'win32'))
    if ('$' in tool):
        tool_is_dynamic = True

    def get_response_file_command(env, node, action, targets, sources, executor=None):
        if hasattr(action, 'process'):
            (cmd_list, _, _) = action.process(targets, sources, env, executor=executor)
            cmd_list = [str(c).replace('$', '$$') for c in cmd_list[0]]
        else:
            command = get_comstr(env, node, action, targets, sources, executor=executor)
            cmd_list = shlex.split(command)
        if tool_is_dynamic:
            tool_command = env.subst(tool, target=targets, source=sources, executor=executor)
        else:
            tool_command = tool
        try:
            tool_idx = (cmd_list.index(tool_command) + 1)
        except ValueError:
            raise Exception('Could not find tool {} in {} generated from {}'.format(tool, cmd_list, generate_command(env, action, targets, sources)))
        (cmd, rsp_content) = (cmd_list[:tool_idx], cmd_list[tool_idx:])
        if os.altsep:
            rsp_content = [rsp_content_item.replace(os.sep, os.altsep) for rsp_content_item in rsp_content]
        rsp_content = [(('""' + rsp_content_item) + '""') for rsp_content_item in rsp_content]
        rsp_content = ' '.join(rsp_content)
        variables = {'rspc': rsp_content, rule: cmd}
        if use_command_env:
            variables['env'] = get_command_env(env, targets, sources)
            for (key, value) in custom_env.items():
                variables['env'] += (env.subst(('export %s=%s;' % (key, value)), target=targets, source=sources, executor=executor) + ' ')
        if node.get_env().get('NINJA_FORCE_SCONS_BUILD'):
            ret_rule = 'TEMPLATE'
        elif (len(' '.join(cmd_list)) < env.get('MAXLINELENGTH', 2048)):
            ret_rule = rule
        else:
            ret_rule = (rule + '_RSP')
        return (ret_rule, variables, [tool_command])
    return get_response_file_command
""""""]",1
"to_bytes, _get_hash_object = _get_hash_object, to_bytes
def hash_signature(s, hash_format=None):
    """"""
    Generate hash signature of a string

    Args:
        s: either string or bytes. Normally should be bytes
        hash_format: Specify to override default hash format

    Returns:
        String of hex digits representing the signature
    """"""","["""""" 
    m = _get_hash_object(hash_format)
    try:
        m.update(to_bytes(s))
    except TypeError:
        m.update(to_bytes(str(s)))
    return m.hexdigest()
"""""", """""" 
    m = to_bytes(hash_format)
    try:
        m.update(_get_hash_object(s))
    except TypeError:
        m.update(_get_hash_object(str(s)))
    return m.hexdigest()
""""""]",1
"obr_repr, format_reference = format_reference, obr_repr
def get_local_references(tb, max_string_length=1000):
    """"""Find the values of the local variables within the traceback scope.

    :param tb: traceback
    :return: list of tuples containing (variable name, value)
    """"""","["""""" 
    if ('self' in tb.tb_frame.f_locals):
        _locals = [('self', obr_repr(tb.tb_frame.f_locals['self']))]
    else:
        _locals = []
    for (k, v) in tb.tb_frame.f_locals.items():
        if (k == 'self'):
            continue
        try:
            vstr = format_reference(v, max_string_length=max_string_length)
            _locals.append((k, vstr))
        except TypeError:
            pass
    return _locals
"""""", """""" 
    if ('self' in tb.tb_frame.f_locals):
        _locals = [('self', format_reference(tb.tb_frame.f_locals['self']))]
    else:
        _locals = []
    for (k, v) in tb.tb_frame.f_locals.items():
        if (k == 'self'):
            continue
        try:
            vstr = obr_repr(v, max_string_length=max_string_length)
            _locals.append((k, vstr))
        except TypeError:
            pass
    return _locals
""""""]",1
"get_angle_to_pos, rearrange_collision = rearrange_collision, get_angle_to_pos
def get_robot_spawns(target_position: np.ndarray, rotation_perturbation_noise: float, distance_threshold: int, sim, num_spawn_attempts: int, physics_stability_steps: int):
    """"""
    Attempts to place the robot near the target position, facing towards it

    :param target_position: The position of the target.
    :param rotation_perturbation_noise: The amount of noise to add to the robot's rotation.
    :param distance_threshold: The maximum distance from the target.
    :param sim: The simulator instance.
    :param num_spawn_attempts: The number of sample attempts for the distance threshold.
    :param physics_stability_steps: The number of steps to perform for physics stability check.

    :return: The robot's start position, rotation, and whether the placement was successful.
    """"""","["""""" 
    state = sim.capture_state()
    for _ in range(num_spawn_attempts):
        sim.set_state(state)
        start_position = sim.pathfinder.get_random_navigable_point_near(target_position, distance_threshold)
        relative_target = (target_position - start_position)
        angle_to_object = get_angle_to_pos(relative_target)
        target_distance = np.linalg.norm((start_position - target_position)[[0, 2]])
        is_navigable = sim.pathfinder.is_navigable(start_position)
        rotation_noise = np.random.normal(0.0, rotation_perturbation_noise)
        start_rotation = (angle_to_object + rotation_noise)
        if ((target_distance > distance_threshold) or (not is_navigable)):
            continue
        sim.robot.base_pos = start_position
        sim.robot.base_rot = start_rotation
        for _ in range(physics_stability_steps):
            sim.perform_discrete_collision_detection()
            (_, details) = rearrange_collision(sim, False, ignore_base=False)
            did_collide = (details.robot_scene_colls != 0)
            if did_collide:
                break
        if (not did_collide):
            sim.set_state(state)
            return (start_position, start_rotation, False)
    sim.set_state(state)
    return (start_position, start_rotation, True)
"""""", """""" 
    state = sim.capture_state()
    for _ in range(num_spawn_attempts):
        sim.set_state(state)
        start_position = sim.pathfinder.get_random_navigable_point_near(target_position, distance_threshold)
        relative_target = (target_position - start_position)
        angle_to_object = rearrange_collision(relative_target)
        target_distance = np.linalg.norm((start_position - target_position)[[0, 2]])
        is_navigable = sim.pathfinder.is_navigable(start_position)
        rotation_noise = np.random.normal(0.0, rotation_perturbation_noise)
        start_rotation = (angle_to_object + rotation_noise)
        if ((target_distance > distance_threshold) or (not is_navigable)):
            continue
        sim.robot.base_pos = start_position
        sim.robot.base_rot = start_rotation
        for _ in range(physics_stability_steps):
            sim.perform_discrete_collision_detection()
            (_, details) = get_angle_to_pos(sim, False, ignore_base=False)
            did_collide = (details.robot_scene_colls != 0)
            if did_collide:
                break
        if (not did_collide):
            sim.set_state(state)
            return (start_position, start_rotation, False)
    sim.set_state(state)
    return (start_position, start_rotation, True)
""""""]",1
"ImageGrid, FigureCanvas = FigureCanvas, ImageGrid
def test_visualize_fully_defected_masks():
    """"""Test if a fully defected anomaly mask results in a completely white image.""""""","["""""" 
    visualizer = ImageGrid()
    mask = (np.ones((256, 256)) * 255)
    visualizer.add_image(image=mask, color_map='gray', title='fully defected mask')
    visualizer.generate()
    canvas = FigureCanvas(visualizer.figure)
    canvas.draw()
    plotted_img = visualizer.axis.images[0].make_image(canvas.renderer)
    assert np.all((plotted_img[0][(..., 0)] == 255))
"""""", """""" 
    visualizer = FigureCanvas()
    mask = (np.ones((256, 256)) * 255)
    visualizer.add_image(image=mask, color_map='gray', title='fully defected mask')
    visualizer.generate()
    canvas = ImageGrid(visualizer.figure)
    canvas.draw()
    plotted_img = visualizer.axis.images[0].make_image(canvas.renderer)
    assert np.all((plotted_img[0][(..., 0)] == 255))
""""""]",1
"get_binding_idxs, infer_tensorrt = infer_tensorrt, get_binding_idxs
def load_engine(runtime: Runtime, engine_file_path: str, profile_index: int=0) -> Callable[([Dict[(str, torch.Tensor)]], Dict[(str, torch.Tensor)])]:
    """"""
    Load serialized TensorRT engine.
    :param runtime: shared variable
    :param engine_file_path: path to the serialized engine
    :param profile_index: which profile to load, 0 if you have not used multiple profiles
    :return: A function to perform inference
    """"""","["""""" 
    with open(file=engine_file_path, mode='rb') as f:
        engine: ICudaEngine = runtime.deserialize_cuda_engine(f.read())
        stream: int = torch.cuda.current_stream().cuda_stream
        context: IExecutionContext = engine.create_execution_context()
        context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream)
        (input_binding_idxs, output_binding_idxs) = get_binding_idxs(engine, profile_index)

        def tensorrt_model(inputs: Dict[(str, torch.Tensor)]) -> Dict[(str, torch.Tensor)]:
            return infer_tensorrt(context=context, inputs=inputs, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs)
        return tensorrt_model
"""""", """""" 
    with open(file=engine_file_path, mode='rb') as f:
        engine: ICudaEngine = runtime.deserialize_cuda_engine(f.read())
        stream: int = torch.cuda.current_stream().cuda_stream
        context: IExecutionContext = engine.create_execution_context()
        context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream)
        (input_binding_idxs, output_binding_idxs) = infer_tensorrt(engine, profile_index)

        def tensorrt_model(inputs: Dict[(str, torch.Tensor)]) -> Dict[(str, torch.Tensor)]:
            return get_binding_idxs(context=context, inputs=inputs, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs)
        return tensorrt_model
""""""]",1
"cleandoc, main = main, cleandoc
def test_annotate_force_dot_license_doesnt_write_to_file(fake_repository, stringio, mock_date_today):
    """"""Adding a header to a .license file if --force-dot-license is given,
    doesn't require write permission to the file, just the directory.
    """"""","["""""" 
    simple_file = (fake_repository / 'foo.txt')
    simple_file.write_text('Preserve this')
    simple_file.chmod(mode=stat.S_IREAD)
    expected = cleandoc('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    result = main(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', '--force-dot-license', 'foo.txt'], out=stringio)
    assert (result == 0)
    assert (simple_file.with_name(f'{simple_file.name}.license').read_text().strip() == expected)
    assert (simple_file.read_text() == 'Preserve this')
"""""", """""" 
    simple_file = (fake_repository / 'foo.txt')
    simple_file.write_text('Preserve this')
    simple_file.chmod(mode=stat.S_IREAD)
    expected = main('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    result = cleandoc(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', '--force-dot-license', 'foo.txt'], out=stringio)
    assert (result == 0)
    assert (simple_file.with_name(f'{simple_file.name}.license').read_text().strip() == expected)
    assert (simple_file.read_text() == 'Preserve this')
""""""]",1
"src_file, is_valid_dependent_node = is_valid_dependent_node, src_file
def alias_to_ninja_build(node):
    """"""Convert an Alias node into a Ninja phony target""""""","["""""" 
    return {'outputs': get_outputs(node), 'rule': 'phony', 'implicit': [get_path(src_file(n)) for n in node.children() if is_valid_dependent_node(n)]}
"""""", """""" 
    return {'outputs': get_outputs(node), 'rule': 'phony', 'implicit': [get_path(is_valid_dependent_node(n)) for n in node.children() if src_file(n)]}
""""""]",1
"_decode, handler = handler, _decode
def _decode_samples(data, image_key='jpg', image_format='RGB', target_key='cls', alt_label='', handler=log_and_continue):
    """"""Decode samples with skip.""""""","["""""" 
    for sample in data:
        try:
            result = _decode(sample, image_key=image_key, image_format=image_format, target_key=target_key, alt_label=alt_label)
        except Exception as exn:
            if handler(exn):
                continue
            else:
                break
        if (result is not None):
            if (isinstance(sample, dict) and isinstance(result, dict)):
                result['__key__'] = sample.get('__key__')
            (yield result)
"""""", """""" 
    for sample in data:
        try:
            result = handler(sample, image_key=image_key, image_format=image_format, target_key=target_key, alt_label=alt_label)
        except Exception as exn:
            if _decode(exn):
                continue
            else:
                break
        if (result is not None):
            if (isinstance(sample, dict) and isinstance(result, dict)):
                result['__key__'] = sample.get('__key__')
            (yield result)
""""""]",1
"readLength, readUniversalTag = readUniversalTag, readLength
def readBoolean(s: BinaryIO) -> bool:
    """"""
    Unpack a BER boolean
    :param s: stream
    """"""","["""""" 
    if (not readUniversalTag(s, Tag.BER_TAG_BOOLEAN, False)):
        raise ValueError('Bad boolean tag')
    size = readLength(s)
    if (size != 1):
        raise ValueError('Bad boolean size')
    b = Uint8.unpack(s.read(1))
    return bool(b)
"""""", """""" 
    if (not readLength(s, Tag.BER_TAG_BOOLEAN, False)):
        raise ValueError('Bad boolean tag')
    size = readUniversalTag(s)
    if (size != 1):
        raise ValueError('Bad boolean size')
    b = Uint8.unpack(s.read(1))
    return bool(b)
""""""]",1
"_get_nested_vulnerability, _write_vulnerability_dict = _write_vulnerability_dict, _get_nested_vulnerability
def write_vulnerability(vulnerability, output_path, key_path=None):
    """"""Update a vulnerability file on disk.""""""","["""""" 
    if os.path.exists(output_path):
        data = _parse_vulnerability_dict(output_path)
    else:
        data = {}
        if key_path:
            cur = data
            for component in key_path.split('.'):
                cur[component] = {}
                cur = cur[component]
    vuln_data = _get_nested_vulnerability(data, key_path)
    vuln_data.clear()
    vuln_data.update(vulnerability_to_dict(vulnerability))
    _write_vulnerability_dict(data, output_path)
"""""", """""" 
    if os.path.exists(output_path):
        data = _parse_vulnerability_dict(output_path)
    else:
        data = {}
        if key_path:
            cur = data
            for component in key_path.split('.'):
                cur[component] = {}
                cur = cur[component]
    vuln_data = _write_vulnerability_dict(data, key_path)
    vuln_data.clear()
    vuln_data.update(vulnerability_to_dict(vulnerability))
    _get_nested_vulnerability(data, output_path)
""""""]",1
"is_log_date, LogDirComps = LogDirComps, is_log_date
def parse_log_dir(log_dir, configs_dir, base_dirs, append_ext=''):
    """"""
    Given a log_dir produced by `create_unique_log_dir`, return the full paths of all configs used.
    The log dir has thus the following format
            {now} {netconfig} {probconfig} [r@XXXX_YYYY] [{postfix} {postfix}]

    :param log_dir: the log dir to parse
    :param configs_dir: the root config dir, where all the configs live
    :param base_dirs: Prefixed to the paths of the configs, e.g., ['ae', 'pc']
    :return: all config paths, as well as the postfix if one was given
    """"""","["""""" 
    base_dirs = [path.join(configs_dir, base_dir) for base_dir in base_dirs]
    log_dir = path.basename(log_dir.strip(path.sep))
    comps = log_dir.split(' ')
    assert is_log_date(comps[0]), 'Invalid log_dir: {}'.format(log_dir)
    assert (len(comps) > len(base_dirs)), 'Expected a base dir for every component, got {} and {}'.format(comps, base_dirs)
    config_components = comps[1:(1 + len(base_dirs))]
    has_restore = any(((_RESTORE_PREFIX in c) for c in comps))
    postfix = comps[((1 + len(base_dirs)) + has_restore):]

    def get_real_path(base, prepped_p):
        p_glob = prepped_p.replace('@', path.sep)
        p_glob = (path.join(base, p_glob) + append_ext)
        glob_matches = glob.glob(p_glob)
        glob_matches_of_same_len = [g for g in glob_matches if (len(g) == len(p_glob))]
        if (len(glob_matches_of_same_len) != 1):
            raise ValueError('Cannot find config on disk: {} (matches: {})'.format(p_glob, glob_matches_of_same_len))
        return glob_matches_of_same_len[0]
    return LogDirComps(config_paths=tuple((get_real_path(base_dir, comp) for (base_dir, comp) in zip(base_dirs, config_components))), postfix=(tuple(postfix) if postfix else None))
"""""", """""" 
    base_dirs = [path.join(configs_dir, base_dir) for base_dir in base_dirs]
    log_dir = path.basename(log_dir.strip(path.sep))
    comps = log_dir.split(' ')
    assert LogDirComps(comps[0]), 'Invalid log_dir: {}'.format(log_dir)
    assert (len(comps) > len(base_dirs)), 'Expected a base dir for every component, got {} and {}'.format(comps, base_dirs)
    config_components = comps[1:(1 + len(base_dirs))]
    has_restore = any(((_RESTORE_PREFIX in c) for c in comps))
    postfix = comps[((1 + len(base_dirs)) + has_restore):]

    def get_real_path(base, prepped_p):
        p_glob = prepped_p.replace('@', path.sep)
        p_glob = (path.join(base, p_glob) + append_ext)
        glob_matches = glob.glob(p_glob)
        glob_matches_of_same_len = [g for g in glob_matches if (len(g) == len(p_glob))]
        if (len(glob_matches_of_same_len) != 1):
            raise ValueError('Cannot find config on disk: {} (matches: {})'.format(p_glob, glob_matches_of_same_len))
        return glob_matches_of_same_len[0]
    return is_log_date(config_paths=tuple((get_real_path(base_dir, comp) for (base_dir, comp) in zip(base_dirs, config_components))), postfix=(tuple(postfix) if postfix else None))
""""""]",1
"MyClassWithProxy, AnotherClassWithProperty = AnotherClassWithProperty, MyClassWithProxy
def test_func():
    """"""Some assertions against empty dicts.""""""","["""""" 
    my_class = MyClassWithProxy()
    assert (my_class.parent_function == {})
    assert (my_class.my_property == {})
    assert (my_class.my_difficult_property == {})
    assert (AnotherClassWithProperty().my_property == {})
"""""", """""" 
    my_class = AnotherClassWithProperty()
    assert (my_class.parent_function == {})
    assert (my_class.my_property == {})
    assert (my_class.my_difficult_property == {})
    assert (MyClassWithProxy().my_property == {})
""""""]",1
"find_vrt_keyword, find_vrt_file = find_vrt_file, find_vrt_keyword
def readImageFromVrt(inputfile, startSample, endSample, startLine, endLine):
    """"""
    read a chunk of image
    the indexes (startSample, endSample, startLine, endLine) are included and start with zero

    memmap is not used, because it is much slower

    tested against readImage in runSwathMosaic.py
    """"""","["""""" 
    import os
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_keyword
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_file
    inputimage = find_vrt_file((inputfile + '.vrt'), 'SourceFilename', relative_path=True)
    byteorder = find_vrt_keyword((inputfile + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        swapByte = False
    else:
        swapByte = True
    imageoffset = int(find_vrt_keyword((inputfile + '.vrt'), 'ImageOffset'))
    lineoffset = int(find_vrt_keyword((inputfile + '.vrt'), 'LineOffset'))
    data = np.zeros((((endLine - startLine) + 1), ((endSample - startSample) + 1)), dtype=np.complex64)
    with open(inputimage, 'rb') as fp:
        for i in range(startLine, (endLine + 1)):
            fp.seek(((imageoffset + (i * lineoffset)) + (startSample * 8)), 0)
            if swapByte:
                tmp = np.fromfile(fp, dtype='>f', count=(2 * ((endSample - startSample) + 1)))
                cJ = np.complex64(1j)
                data[(i - startLine)] = (tmp[0::2] + (cJ * tmp[1::2]))
            else:
                data[(i - startLine)] = np.fromfile(fp, dtype=np.complex64, count=((endSample - startSample) + 1))
    return data
"""""", """""" 
    import os
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_keyword
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_file
    inputimage = find_vrt_keyword((inputfile + '.vrt'), 'SourceFilename', relative_path=True)
    byteorder = find_vrt_file((inputfile + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        swapByte = False
    else:
        swapByte = True
    imageoffset = int(find_vrt_file((inputfile + '.vrt'), 'ImageOffset'))
    lineoffset = int(find_vrt_file((inputfile + '.vrt'), 'LineOffset'))
    data = np.zeros((((endLine - startLine) + 1), ((endSample - startSample) + 1)), dtype=np.complex64)
    with open(inputimage, 'rb') as fp:
        for i in range(startLine, (endLine + 1)):
            fp.seek(((imageoffset + (i * lineoffset)) + (startSample * 8)), 0)
            if swapByte:
                tmp = np.fromfile(fp, dtype='>f', count=(2 * ((endSample - startSample) + 1)))
                cJ = np.complex64(1j)
                data[(i - startLine)] = (tmp[0::2] + (cJ * tmp[1::2]))
            else:
                data[(i - startLine)] = np.fromfile(fp, dtype=np.complex64, count=((endSample - startSample) + 1))
    return data
""""""]",1
"Choice, prompt = prompt, Choice
def get_range_from_volumes(self, times=0, disable_args=False):
    """"""Returns a range created using volume list""""""","["""""" 
    selected = None
    args = get_args()
    if ((times == 0) and args.volumes):
        selected = [int(x) for x in args.volumes]
    if ((not selected) and args.suppress):
        selected = [x['id'] for x in self.app.crawler.volumes]
    if (disable_args or (not selected)):
        answer = prompt([{'type': 'checkbox', 'name': 'volumes', 'message': 'Choose volumes to download:', 'choices': [Choice(('%d - %s (Chapter %d-%d) [%d chapters]' % (vol['id'], vol['title'], vol['start_chapter'], vol['final_chapter'], vol['chapter_count']))) for vol in self.app.crawler.volumes], 'validate': (lambda a: (True if a else (False, 'Select at least one item')))}])
        selected = [int(val.split(' ')[0]) for val in answer['volumes']]
    if ((times < 3) and (len(selected) == 0)):
        return self.get_range_from_volumes((times + 1))
    return selected
"""""", """""" 
    selected = None
    args = get_args()
    if ((times == 0) and args.volumes):
        selected = [int(x) for x in args.volumes]
    if ((not selected) and args.suppress):
        selected = [x['id'] for x in self.app.crawler.volumes]
    if (disable_args or (not selected)):
        answer = Choice([{'type': 'checkbox', 'name': 'volumes', 'message': 'Choose volumes to download:', 'choices': [prompt(('%d - %s (Chapter %d-%d) [%d chapters]' % (vol['id'], vol['title'], vol['start_chapter'], vol['final_chapter'], vol['chapter_count']))) for vol in self.app.crawler.volumes], 'validate': (lambda a: (True if a else (False, 'Select at least one item')))}])
        selected = [int(val.split(' ')[0]) for val in answer['volumes']]
    if ((times < 3) and (len(selected) == 0)):
        return self.get_range_from_volumes((times + 1))
    return selected
""""""]",1
"get_training_stats, save_checkpoint = save_checkpoint, get_training_stats
def train(args, trainer, task, epoch_itr):
    """"""Train the model for one epoch.""""""","["""""" 
    if (epoch_itr.epoch <= len(args.update_freq)):
        update_freq = args.update_freq[(epoch_itr.epoch - 1)]
    else:
        update_freq = args.update_freq[(- 1)]
    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)
    itr = iterators.GroupedIterator(itr, update_freq)
    progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, no_progress_bar='simple')
    extra_meters = collections.defaultdict((lambda : AverageMeter()))
    first_valid = args.valid_subset.split(',')[0]
    max_update = (args.max_update or math.inf)
    num_batches = len(epoch_itr)
    for (i, samples) in enumerate(progress, start=epoch_itr.iterations_in_epoch):
        log_output = trainer.train_step(samples)
        if (log_output is None):
            continue
        stats = get_training_stats(trainer)
        for (k, v) in log_output.items():
            if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']):
                continue
            if ('loss' in k):
                extra_meters[k].update(v, log_output['sample_size'])
            else:
                extra_meters[k].update(v)
            stats[k] = extra_meters[k].avg
        progress.log(stats)
        if (i == 0):
            trainer.get_meter('wps').reset()
        num_updates = trainer.get_num_updates()
        if ((args.save_interval_updates > 0) and ((num_updates % args.save_interval_updates) == 0) and (num_updates > 0)):
            valid_losses = (validate(args, trainer, task, epoch_itr, [first_valid]) if (not args.skip_validation) else [None])
            save_checkpoint(args, trainer, epoch_itr, valid_losses[0])
        if (num_updates >= max_update):
            break
    stats = get_training_stats(trainer)
    for (k, meter) in extra_meters.items():
        stats[k] = meter.avg
    progress.print(stats)
    for k in ['train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip']:
        meter = trainer.get_meter(k)
        if (meter is not None):
            meter.reset()
"""""", """""" 
    if (epoch_itr.epoch <= len(args.update_freq)):
        update_freq = args.update_freq[(epoch_itr.epoch - 1)]
    else:
        update_freq = args.update_freq[(- 1)]
    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)
    itr = iterators.GroupedIterator(itr, update_freq)
    progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, no_progress_bar='simple')
    extra_meters = collections.defaultdict((lambda : AverageMeter()))
    first_valid = args.valid_subset.split(',')[0]
    max_update = (args.max_update or math.inf)
    num_batches = len(epoch_itr)
    for (i, samples) in enumerate(progress, start=epoch_itr.iterations_in_epoch):
        log_output = trainer.train_step(samples)
        if (log_output is None):
            continue
        stats = save_checkpoint(trainer)
        for (k, v) in log_output.items():
            if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']):
                continue
            if ('loss' in k):
                extra_meters[k].update(v, log_output['sample_size'])
            else:
                extra_meters[k].update(v)
            stats[k] = extra_meters[k].avg
        progress.log(stats)
        if (i == 0):
            trainer.get_meter('wps').reset()
        num_updates = trainer.get_num_updates()
        if ((args.save_interval_updates > 0) and ((num_updates % args.save_interval_updates) == 0) and (num_updates > 0)):
            valid_losses = (validate(args, trainer, task, epoch_itr, [first_valid]) if (not args.skip_validation) else [None])
            get_training_stats(args, trainer, epoch_itr, valid_losses[0])
        if (num_updates >= max_update):
            break
    stats = save_checkpoint(trainer)
    for (k, meter) in extra_meters.items():
        stats[k] = meter.avg
    progress.print(stats)
    for k in ['train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip']:
        meter = trainer.get_meter(k)
        if (meter is not None):
            meter.reset()
""""""]",1
"urlopen, build_url = build_url, urlopen
def test_partial_authed_open_download(partial_authed_server):
    """"""Validate that partial auth still allows downloads.""""""","["""""" 
    url = (build_url(partial_authed_server.port) + '/simple')
    resp = urlopen(url)
    assert (resp.getcode() == 200)
"""""", """""" 
    url = (urlopen(partial_authed_server.port) + '/simple')
    resp = build_url(url)
    assert (resp.getcode() == 200)
""""""]",1
"get_validated_parameters, process_create_update_event = process_create_update_event, get_validated_parameters
@helper.create
@helper.update
@helper.delete
def process_cloudformation_event(event: CloudFormationCustomResourceEvent, context: Context) -> str:
    """"""Process Event from AWS CloudFormation.

    Args:
        event: event data
        context: runtime information

    Returns:
        AWS CloudFormation physical resource id
    """"""","["""""" 
    request_type = event['RequestType']
    LOGGER.info(f'{request_type} Event')
    LOGGER.debug(f'Lambda Context: {context}')
    params = get_validated_parameters(event)
    regions = common.get_enabled_regions(params.get('ENABLED_REGIONS', ''), (params.get('CONTROL_TOWER_REGIONS_ONLY', 'false').lower() in 'true'))
    if (params['action'] in 'Add, Update'):
        process_create_update_event(params, regions)
    elif (params['action'] == 'Remove'):
        account_ids = common.get_account_ids([], params['DELEGATED_ADMIN_ACCOUNT_ID'])
        guardduty.process_delete_event(params, regions, account_ids, False)
    return f""sra-guardduty-{params['DELEGATED_ADMIN_ACCOUNT_ID']}-{(params.get('AUTO_ENABLE_S3_LOGS', 'false').lower() in 'true')}""
"""""", """""" 
    request_type = event['RequestType']
    LOGGER.info(f'{request_type} Event')
    LOGGER.debug(f'Lambda Context: {context}')
    params = process_create_update_event(event)
    regions = common.get_enabled_regions(params.get('ENABLED_REGIONS', ''), (params.get('CONTROL_TOWER_REGIONS_ONLY', 'false').lower() in 'true'))
    if (params['action'] in 'Add, Update'):
        get_validated_parameters(params, regions)
    elif (params['action'] == 'Remove'):
        account_ids = common.get_account_ids([], params['DELEGATED_ADMIN_ACCOUNT_ID'])
        guardduty.process_delete_event(params, regions, account_ids, False)
    return f""sra-guardduty-{params['DELEGATED_ADMIN_ACCOUNT_ID']}-{(params.get('AUTO_ENABLE_S3_LOGS', 'false').lower() in 'true')}""
""""""]",1
"find_and_replace_header, cleandoc = cleandoc, find_and_replace_header
def test_find_and_replace_preserve_newline():
    """"""If the file content ends with a newline, don't remove it.""""""","["""""" 
    spdx_info = SpdxInfo(set(), set())
    text = (cleandoc('\n            # SPDX-FileCopyrightText: Jane Doe\n            #\n            # SPDX-License-Identifier: GPL-3.0-or-later\n\n            pass\n            ') + '\n')
    assert (find_and_replace_header(text, spdx_info) == text)
"""""", """""" 
    spdx_info = SpdxInfo(set(), set())
    text = (find_and_replace_header('\n            # SPDX-FileCopyrightText: Jane Doe\n            #\n            # SPDX-License-Identifier: GPL-3.0-or-later\n\n            pass\n            ') + '\n')
    assert (cleandoc(text, spdx_info) == text)
""""""]",1
"Mailin, email_address_to_list = email_address_to_list, Mailin
def send(sender_instance):
    """"""Send a transactional email using SendInBlue API.

    Site: https://www.sendinblue.com
    API: https://apidocs.sendinblue.com/
    """"""","["""""" 
    m = Mailin('https://api.sendinblue.com/v2.0', sender_instance._kwargs.get('api_key'))
    data = {'to': email_list_to_email_dict(sender_instance._recipient_list), 'cc': email_list_to_email_dict(sender_instance._cc), 'bcc': email_list_to_email_dict(sender_instance._bcc), 'from': email_address_to_list(sender_instance._from_email), 'subject': sender_instance._subject}
    if sender_instance._template.is_html:
        data.update({'html': sender_instance._message, 'headers': {'Content-Type': 'text/html; charset=utf-8'}})
    else:
        data.update({'text': sender_instance._message})
    if ('attachments' in sender_instance._kwargs):
        data['attachment'] = {}
        for attachment in sender_instance._kwargs['attachments']:
            data['attachment'][attachment[0]] = base64.b64encode(attachment[1])
    result = m.send_email(data)
    if (result['code'] != 'success'):
        raise SendInBlueError(result['message'])
"""""", """""" 
    m = email_address_to_list('https://api.sendinblue.com/v2.0', sender_instance._kwargs.get('api_key'))
    data = {'to': email_list_to_email_dict(sender_instance._recipient_list), 'cc': email_list_to_email_dict(sender_instance._cc), 'bcc': email_list_to_email_dict(sender_instance._bcc), 'from': Mailin(sender_instance._from_email), 'subject': sender_instance._subject}
    if sender_instance._template.is_html:
        data.update({'html': sender_instance._message, 'headers': {'Content-Type': 'text/html; charset=utf-8'}})
    else:
        data.update({'text': sender_instance._message})
    if ('attachments' in sender_instance._kwargs):
        data['attachment'] = {}
        for attachment in sender_instance._kwargs['attachments']:
            data['attachment'][attachment[0]] = base64.b64encode(attachment[1])
    result = m.send_email(data)
    if (result['code'] != 'success'):
        raise SendInBlueError(result['message'])
""""""]",1
"is_String, subst_dict = subst_dict, is_String
def scons_subst(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={}, lvars={}, conv=None):
    """"""Expand a string or list containing construction variable
    substitutions.

    This is the work-horse function for substitutions in file names
    and the like.  The companion scons_subst_list() function (below)
    handles separating command lines into lists of arguments, so see
    that function if that's what you're looking for.
    """"""","["""""" 
    if ((isinstance(strSubst, str) and ('$' not in strSubst)) or isinstance(strSubst, CmdStringHolder)):
        return strSubst
    if (conv is None):
        conv = _strconv[mode]
    if ('TARGET' not in lvars):
        d = subst_dict(target, source)
        if d:
            lvars = lvars.copy()
            lvars.update(d)
    gvars['__builtins__'] = __builtins__
    ss = StringSubber(env, mode, conv, gvars)
    result = ss.substitute(strSubst, lvars)
    try:
        del gvars['__builtins__']
    except KeyError:
        pass
    res = result
    if is_String(result):
        remove = _regex_remove[mode]
        if remove:
            if (mode == SUBST_SIG):
                result = _list_remove[mode](remove.split(result))
                if (result is None):
                    raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + res))
                result = ' '.join(result)
            else:
                result = remove.sub('', result)
        if (mode != SUBST_RAW):
            result = _space_sep.sub(' ', result).strip()
        result = result.replace('$$', '$')
    elif is_Sequence(result):
        remove = _list_remove[mode]
        if remove:
            result = remove(result)
            if (result is None):
                raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + str(res)))
    return result
"""""", """""" 
    if ((isinstance(strSubst, str) and ('$' not in strSubst)) or isinstance(strSubst, CmdStringHolder)):
        return strSubst
    if (conv is None):
        conv = _strconv[mode]
    if ('TARGET' not in lvars):
        d = is_String(target, source)
        if d:
            lvars = lvars.copy()
            lvars.update(d)
    gvars['__builtins__'] = __builtins__
    ss = StringSubber(env, mode, conv, gvars)
    result = ss.substitute(strSubst, lvars)
    try:
        del gvars['__builtins__']
    except KeyError:
        pass
    res = result
    if subst_dict(result):
        remove = _regex_remove[mode]
        if remove:
            if (mode == SUBST_SIG):
                result = _list_remove[mode](remove.split(result))
                if (result is None):
                    raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + res))
                result = ' '.join(result)
            else:
                result = remove.sub('', result)
        if (mode != SUBST_RAW):
            result = _space_sep.sub(' ', result).strip()
        result = result.replace('$$', '$')
    elif is_Sequence(result):
        remove = _list_remove[mode]
        if remove:
            result = remove(result)
            if (result is None):
                raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + str(res)))
    return result
""""""]",1
"MessageLocationTuple, abspath = abspath, MessageLocationTuple
@pytest.mark.usefixtures('pop_pylintrc')
def test_load_plugin_path_manipulation_case_6() -> None:
    """"""Case 6 refers to GitHub issue #7264.

    This is where we supply a plugin we want to load on both the CLI and
    config file, but that plugin is only loadable after the ``init-hook`` in
    the config file has run. This is not supported, and was previously a silent
    failure. This test ensures a ``bad-plugin-value`` message is emitted.
    """"""","["""""" 
    dummy_plugin_path = abspath(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with fake_home() as home_path:
        pylintrc_file = join(home_path, 'pylintrc')
        with open(pylintrc_file, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        copy(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        assert (home_path not in sys.path)
        run = Run(['--rcfile', pylintrc_file, '--load-plugins', 'copy_dummy', join(REGRTEST_DATA_DIR, 'empty.py')], reporter=testutils.GenericTestReporter(), exit=False)
        assert (run._rcfile == pylintrc_file)
        assert (home_path in sys.path)
        assert (not any(((ch.name == 'dummy_plugin') for ch in run.linter.get_checkers())))
        assert (len(run.linter.reporter.messages) == 1)
        assert (run.linter.reporter.messages[0] == Message(msg_id='E0013', symbol='bad-plugin-value', msg=""Plugin 'copy_dummy' is impossible to load, is it installed ? ('No module named 'copy_dummy'')"", confidence=interfaces.Confidence(name='UNDEFINED', description='Warning without any associated confidence level.'), location=MessageLocationTuple(abspath='Command line or configuration file', path='Command line or configuration file', module='Command line or configuration file', obj='', line=1, column=0, end_line=None, end_column=None)))
        sys.path.remove(home_path)
"""""", """""" 
    dummy_plugin_path = MessageLocationTuple(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with fake_home() as home_path:
        pylintrc_file = join(home_path, 'pylintrc')
        with open(pylintrc_file, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        copy(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        assert (home_path not in sys.path)
        run = Run(['--rcfile', pylintrc_file, '--load-plugins', 'copy_dummy', join(REGRTEST_DATA_DIR, 'empty.py')], reporter=testutils.GenericTestReporter(), exit=False)
        assert (run._rcfile == pylintrc_file)
        assert (home_path in sys.path)
        assert (not any(((ch.name == 'dummy_plugin') for ch in run.linter.get_checkers())))
        assert (len(run.linter.reporter.messages) == 1)
        assert (run.linter.reporter.messages[0] == Message(msg_id='E0013', symbol='bad-plugin-value', msg=""Plugin 'copy_dummy' is impossible to load, is it installed ? ('No module named 'copy_dummy'')"", confidence=interfaces.Confidence(name='UNDEFINED', description='Warning without any associated confidence level.'), location=abspath(abspath='Command line or configuration file', path='Command line or configuration file', module='Command line or configuration file', obj='', line=1, column=0, end_line=None, end_column=None)))
        sys.path.remove(home_path)
""""""]",1
"put_license_in_file, MockResponse = MockResponse, put_license_in_file
def test_put_request_exception(fake_repository, monkeypatch):
    """"""There was an error while downloading the license file.""""""","["""""" 
    monkeypatch.setattr(urllib.request, 'urlopen', (lambda _: MockResponse(status_code=404)))
    with pytest.raises(URLError):
        put_license_in_file('0BSD', 'LICENSES/0BSD.txt')
"""""", """""" 
    monkeypatch.setattr(urllib.request, 'urlopen', (lambda _: put_license_in_file(status_code=404)))
    with pytest.raises(URLError):
        MockResponse('0BSD', 'LICENSES/0BSD.txt')
""""""]",1
"_good_shape, _dstn = _dstn, _good_shape
def dstn(x, type=2, shape=None, axes=None, norm=None, overwrite_x=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""
    Perform an nD discrete sine transform.

    The first six arguments are as per :func:`scipy.fftpack.dstn`;
    the rest of the arguments are documented in the
    :ref:`additional arguments docs<interfaces_additional_args>`.

    Warning: `scipy.fftpack` is considered legacy, new code should
    use `scipy.fft` instead.

    """"""","["""""" 
    shape = _good_shape(x, shape, axes)
    if (norm not in [None, 'ortho']):
        raise ValueError(f'Invalid norm value {norm}; should be None or ""ortho""')
    return _dstn(x, type, shape, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    shape = _dstn(x, shape, axes)
    if (norm not in [None, 'ortho']):
        raise ValueError(f'Invalid norm value {norm}; should be None or ""ortho""')
    return _good_shape(x, type, shape, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"to_bytes, mb_item = mb_item, to_bytes
def save_model_checkpoint(model, save_dir, state, with_opt: bool=True, push_to_hub: bool=False):
    """"""
    If `push_to_hub` is True, will save to `save_dir`. Otherwise will save to `save_dir/ckpt-{step}`.
    """"""","["""""" 
    state = jax_utils.unreplicate(state)
    logger.info(f'SAVING CHECKPOINT IN {save_dir}...')
    if (not push_to_hub):
        save_dir = f'{save_dir}/ckpt-{(mb_item(state.step) - 1)}'
    model.save_pretrained(save_dir, params=state.params, push_to_hub=push_to_hub, commit_message=f'Saving weights and logs at step {(mb_item(state.step) - 1)}')
    if with_opt:
        with open(os.path.join(save_dir, 'opt_state.msgpack'), 'wb') as f:
            f.write(to_bytes(state.opt_state))
        with open(os.path.join(save_dir, 'training_state.json'), 'w') as f:
            json.dump({'step': state.step.item()}, f)
    logger.info('checkpoint saved')
"""""", """""" 
    state = jax_utils.unreplicate(state)
    logger.info(f'SAVING CHECKPOINT IN {save_dir}...')
    if (not push_to_hub):
        save_dir = f'{save_dir}/ckpt-{(to_bytes(state.step) - 1)}'
    model.save_pretrained(save_dir, params=state.params, push_to_hub=push_to_hub, commit_message=f'Saving weights and logs at step {(to_bytes(state.step) - 1)}')
    if with_opt:
        with open(os.path.join(save_dir, 'opt_state.msgpack'), 'wb') as f:
            f.write(mb_item(state.opt_state))
        with open(os.path.join(save_dir, 'training_state.json'), 'w') as f:
            json.dump({'step': state.step.item()}, f)
    logger.info('checkpoint saved')
""""""]",1
"_cached_log_stream, _suppress_print = _suppress_print, _cached_log_stream
def setup_logging(output_dir=None):
    """"""
    Sets up the logging for multiple processes. Only enable the logging for the
    master process, and suppress logging for the non-master processes.
    """"""","["""""" 
    _FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
    if du.is_master_proc():
        logging.root.handlers = []
    else:
        _suppress_print()
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    logger.propagate = False
    plain_formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(filename)s: %(lineno)3d: %(message)s', datefmt='%m/%d %H:%M:%S')
    if du.is_master_proc():
        ch = logging.StreamHandler(stream=sys.stdout)
        ch.setLevel(logging.DEBUG)
        ch.setFormatter(plain_formatter)
        logger.addHandler(ch)
    if ((output_dir is not None) and du.is_master_proc(du.get_world_size())):
        filename = os.path.join(output_dir, 'stdout.log')
        fh = logging.StreamHandler(_cached_log_stream(filename))
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(plain_formatter)
        logger.addHandler(fh)
"""""", """""" 
    _FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'
    if du.is_master_proc():
        logging.root.handlers = []
    else:
        _cached_log_stream()
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    logger.propagate = False
    plain_formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(filename)s: %(lineno)3d: %(message)s', datefmt='%m/%d %H:%M:%S')
    if du.is_master_proc():
        ch = logging.StreamHandler(stream=sys.stdout)
        ch.setLevel(logging.DEBUG)
        ch.setFormatter(plain_formatter)
        logger.addHandler(ch)
    if ((output_dir is not None) and du.is_master_proc(du.get_world_size())):
        filename = os.path.join(output_dir, 'stdout.log')
        fh = logging.StreamHandler(_suppress_print(filename))
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(plain_formatter)
        logger.addHandler(fh)
""""""]",1
"partial, decode_arch_def = decode_arch_def, partial
def _gen_mobilenet_v3_rw(variant, channel_multiplier=1.0, pretrained=False, **kwargs):
    """"""Creates a MobileNet-V3 model.

    Ref impl: ?
    Paper: https://arxiv.org/abs/1905.02244

    Args:
      channel_multiplier: multiplier to number of channels per layer.
    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_nre_noskip'], ['ir_r1_k3_s2_e4_c24_nre', 'ir_r1_k3_s1_e3_c24_nre'], ['ir_r3_k5_s2_e3_c40_se0.25_nre'], ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'], ['ir_r2_k3_s1_e6_c112_se0.25'], ['ir_r3_k5_s2_e6_c160_se0.25'], ['cn_r1_k1_s1_c960']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def), head_bias=False, round_chs_fn=partial(round_channels, multiplier=channel_multiplier), norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)), act_layer=resolve_act_layer(kwargs, 'hard_swish'), se_layer=partial(SqueezeExcite, gate_layer='hard_sigmoid'), **kwargs)
    model = _create_mnv3(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_nre_noskip'], ['ir_r1_k3_s2_e4_c24_nre', 'ir_r1_k3_s1_e3_c24_nre'], ['ir_r3_k5_s2_e3_c40_se0.25_nre'], ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'], ['ir_r2_k3_s1_e6_c112_se0.25'], ['ir_r3_k5_s2_e6_c160_se0.25'], ['cn_r1_k1_s1_c960']]
    model_kwargs = dict(block_args=partial(arch_def), head_bias=False, round_chs_fn=decode_arch_def(round_channels, multiplier=channel_multiplier), norm_layer=decode_arch_def(nn.BatchNorm2d, **resolve_bn_args(kwargs)), act_layer=resolve_act_layer(kwargs, 'hard_swish'), se_layer=decode_arch_def(SqueezeExcite, gate_layer='hard_sigmoid'), **kwargs)
    model = _create_mnv3(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"parse_NASLTE_MO, unpack = unpack, parse_NASLTE_MO
def parse_NAS_MO(buf):
    """"""Parses a Mobile Originated NAS message bytes' buffer
    
    Args:
        buf: uplink NAS message bytes' buffer
    
    Returns:
        element, err: 2-tuple
            element: Element instance, if err is null (no error)
            element: None, if err is not null (standard NAS error code)
    """"""","["""""" 
    if (python_version < 3):
        try:
            (pd, type) = unpack('>BB', buf[:2])
        except Exception:
            return (None, 111)
    else:
        try:
            (pd, type) = (buf[0], buf[1])
        except Exception:
            return (None, 111)
    if ((pd & 15) != 14):
        pd &= 15
    if (pd in (3, 5, 11)):
        type &= 63
    elif (pd in (2, 7)):
        return parse_NASLTE_MO(buf, inner=True)
    elif (pd in (46, 126)):
        return parse_NAS5G(buf, inner=True)
    try:
        Msg = NASMODispatcher[pd][type]()
    except KeyError:
        return (None, 97)
    try:
        Msg.from_bytes(buf)
    except Exception:
        return (None, 96)
    return (Msg, 0)
"""""", """""" 
    if (python_version < 3):
        try:
            (pd, type) = parse_NASLTE_MO('>BB', buf[:2])
        except Exception:
            return (None, 111)
    else:
        try:
            (pd, type) = (buf[0], buf[1])
        except Exception:
            return (None, 111)
    if ((pd & 15) != 14):
        pd &= 15
    if (pd in (3, 5, 11)):
        type &= 63
    elif (pd in (2, 7)):
        return unpack(buf, inner=True)
    elif (pd in (46, 126)):
        return parse_NAS5G(buf, inner=True)
    try:
        Msg = NASMODispatcher[pd][type]()
    except KeyError:
        return (None, 97)
    try:
        Msg.from_bytes(buf)
    except Exception:
        return (None, 96)
    return (Msg, 0)
""""""]",1
"get_unprocessed_account_details, sleep = sleep, get_unprocessed_account_details
def create_members(security_hub_client: SecurityHubClient, accounts: list) -> None:
    """"""Create members.

    Args:
        security_hub_client: SecurityHubClient
        accounts: list of account details [{""AccountId"": """", ""Email"": """"}]
    """"""","["""""" 
    response: CreateMembersResponseTypeDef = security_hub_client.create_members(AccountDetails=accounts)
    api_call_details = {'API_Call': 'securityhub:CreateMembers', 'API_Response': response}
    LOGGER.info(api_call_details)
    if (('UnprocessedAccounts' in response) and response['UnprocessedAccounts']):
        unprocessed = True
        retry_count = 0
        unprocessed_accounts = []
        while unprocessed:
            retry_count += 1
            LOGGER.info(f""Unprocessed Accounts: {response['UnprocessedAccounts']}"")
            remaining_accounts = get_unprocessed_account_details(response, accounts)
            unprocessed = False
            if remaining_accounts:
                response = security_hub_client.create_members(AccountDetails=remaining_accounts)
                api_call_details = {'API_Call': 'securityhub:CreateMembers', 'API_Response': response}
                LOGGER.info(api_call_details)
                if (('UnprocessedAccounts' in response) and response['UnprocessedAccounts']):
                    unprocessed_accounts = response['UnprocessedAccounts']
                    if (retry_count != MAX_RETRY):
                        unprocessed = True
                        LOGGER.info('Waiting 10 seconds before retrying create members with unprocessed accounts.')
                        sleep(10)
        if unprocessed_accounts:
            LOGGER.info(f'Unable to add the following accounts as members. {unprocessed_accounts}')
    LOGGER.info(f'Member accounts created: {len(accounts)}')
"""""", """""" 
    response: CreateMembersResponseTypeDef = security_hub_client.create_members(AccountDetails=accounts)
    api_call_details = {'API_Call': 'securityhub:CreateMembers', 'API_Response': response}
    LOGGER.info(api_call_details)
    if (('UnprocessedAccounts' in response) and response['UnprocessedAccounts']):
        unprocessed = True
        retry_count = 0
        unprocessed_accounts = []
        while unprocessed:
            retry_count += 1
            LOGGER.info(f""Unprocessed Accounts: {response['UnprocessedAccounts']}"")
            remaining_accounts = sleep(response, accounts)
            unprocessed = False
            if remaining_accounts:
                response = security_hub_client.create_members(AccountDetails=remaining_accounts)
                api_call_details = {'API_Call': 'securityhub:CreateMembers', 'API_Response': response}
                LOGGER.info(api_call_details)
                if (('UnprocessedAccounts' in response) and response['UnprocessedAccounts']):
                    unprocessed_accounts = response['UnprocessedAccounts']
                    if (retry_count != MAX_RETRY):
                        unprocessed = True
                        LOGGER.info('Waiting 10 seconds before retrying create members with unprocessed accounts.')
                        get_unprocessed_account_details(10)
        if unprocessed_accounts:
            LOGGER.info(f'Unable to add the following accounts as members. {unprocessed_accounts}')
    LOGGER.info(f'Member accounts created: {len(accounts)}')
""""""]",1
"update_all_packages, init_logging = init_logging, update_all_packages
def main(argv: t.Sequence[str]=None) -> None:
    """"""Application entrypoint for pypiserver.

    This function drives the application (as opposed to the library)
    implementation of pypiserver. Usage from the commandline will result in
    this function being called.
    """"""","["""""" 
    import pypiserver
    if (argv is None):
        argv = sys.argv[1:]
    config = Config.from_args(argv)
    init_logging(level=config.log_level, filename=config.log_file, frmt=config.log_frmt, stream=config.log_stream)
    if isinstance(config, UpdateConfig):
        from pypiserver.manage import update_all_packages
        update_all_packages(config.roots, config.download_directory, dry_run=(not config.execute), stable_only=config.allow_unstable, ignorelist=config.ignorelist)
        return
    if config.server_method.startswith('gevent'):
        import gevent.monkey
        gevent.monkey.patch_all()
    from pypiserver import bottle
    bottle.debug((config.verbosity > 1))
    bottle._stderr = ft.partial(_logwrite, logging.getLogger(bottle.__name__), logging.INFO)
    app = pypiserver.app_from_config(config)
    app = pypiserver.setup_routes_from_config(app, config)
    if (config.server_method == 'gunicorn'):
        sys.argv = ['gunicorn']
    wsgi_kwargs = {'handler_class': WsgiHandler}
    if (config.server_method == 'auto'):
        expected_server = guess_auto_server()
        extra_kwargs = (wsgi_kwargs if (expected_server is AutoServer.WsgiRef) else {})
        log.debug(""Server 'auto' selected. Expecting bottle to run '%s'. Passing extra keyword args: %s"", expected_server.name, extra_kwargs)
    else:
        extra_kwargs = (wsgi_kwargs if (config.server_method == 'wsgiref') else {})
        log.debug(""Running bottle with selected server '%s'"", config.server_method)
    bottle.run(app=app, host=config.host, port=config.port, server=config.server_method, **extra_kwargs)
"""""", """""" 
    import pypiserver
    if (argv is None):
        argv = sys.argv[1:]
    config = Config.from_args(argv)
    update_all_packages(level=config.log_level, filename=config.log_file, frmt=config.log_frmt, stream=config.log_stream)
    if isinstance(config, UpdateConfig):
        from pypiserver.manage import update_all_packages
        init_logging(config.roots, config.download_directory, dry_run=(not config.execute), stable_only=config.allow_unstable, ignorelist=config.ignorelist)
        return
    if config.server_method.startswith('gevent'):
        import gevent.monkey
        gevent.monkey.patch_all()
    from pypiserver import bottle
    bottle.debug((config.verbosity > 1))
    bottle._stderr = ft.partial(_logwrite, logging.getLogger(bottle.__name__), logging.INFO)
    app = pypiserver.app_from_config(config)
    app = pypiserver.setup_routes_from_config(app, config)
    if (config.server_method == 'gunicorn'):
        sys.argv = ['gunicorn']
    wsgi_kwargs = {'handler_class': WsgiHandler}
    if (config.server_method == 'auto'):
        expected_server = guess_auto_server()
        extra_kwargs = (wsgi_kwargs if (expected_server is AutoServer.WsgiRef) else {})
        log.debug(""Server 'auto' selected. Expecting bottle to run '%s'. Passing extra keyword args: %s"", expected_server.name, extra_kwargs)
    else:
        extra_kwargs = (wsgi_kwargs if (config.server_method == 'wsgiref') else {})
        log.debug(""Running bottle with selected server '%s'"", config.server_method)
    bottle.run(app=app, host=config.host, port=config.port, server=config.server_method, **extra_kwargs)
""""""]",1
"DataParallelWithCallback, sync_module = sync_module, DataParallelWithCallback
def convert_model(module):
    """"""Traverse the input module and its child recursively
       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d
       to SynchronizedBatchNorm*N*d

    Args:
        module: the input module needs to be convert to SyncBN model

    Examples:
        >>> import torch.nn as nn
        >>> import torchvision
        >>> # m is a standard pytorch model
        >>> m = torchvision.models.resnet18(True)
        >>> m = nn.DataParallel(m)
        >>> # after convert, m is using SyncBN
        >>> m = convert_model(m)
    """"""","["""""" 
    if isinstance(module, torch.nn.DataParallel):
        mod = module.module
        mod = convert_model(mod)
        mod = DataParallelWithCallback(mod)
        return mod
    mod = module
    for (pth_module, sync_module) in zip([torch.nn.modules.batchnorm.BatchNorm1d, torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.BatchNorm3d], [SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d]):
        if isinstance(module, pth_module):
            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)
            mod.running_mean = module.running_mean
            mod.running_var = module.running_var
            if module.affine:
                mod.weight.data = module.weight.data.clone().detach()
                mod.bias.data = module.bias.data.clone().detach()
    for (name, child) in module.named_children():
        mod.add_module(name, convert_model(child))
    return mod
"""""", """""" 
    if isinstance(module, torch.nn.DataParallel):
        mod = module.module
        mod = convert_model(mod)
        mod = sync_module(mod)
        return mod
    mod = module
    for (pth_module, DataParallelWithCallback) in zip([torch.nn.modules.batchnorm.BatchNorm1d, torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.BatchNorm3d], [SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d]):
        if isinstance(module, pth_module):
            mod = DataParallelWithCallback(module.num_features, module.eps, module.momentum, module.affine)
            mod.running_mean = module.running_mean
            mod.running_var = module.running_var
            if module.affine:
                mod.weight.data = module.weight.data.clone().detach()
                mod.bias.data = module.bias.data.clone().detach()
    for (name, child) in module.named_children():
        mod.add_module(name, convert_model(child))
    return mod
""""""]",1
"_check_number_of_params, is_distributed = is_distributed, _check_number_of_params
def broadcast_tensors(tensors: tp.Iterable[torch.Tensor], src: int=0):
    """"""Broadcast the tensors from the given parameters to all workers.
    This can be used to ensure that all workers have the same model to start with.
    """"""","["""""" 
    if (not is_distributed()):
        return
    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
    _check_number_of_params(tensors)
    handles = []
    for tensor in tensors:
        handle = torch.distributed.broadcast(tensor.data, src=src, async_op=True)
        handles.append(handle)
    for handle in handles:
        handle.wait()
"""""", """""" 
    if (not _check_number_of_params()):
        return
    tensors = [tensor for tensor in tensors if _is_complex_or_float(tensor)]
    is_distributed(tensors)
    handles = []
    for tensor in tensors:
        handle = torch.distributed.broadcast(tensor.data, src=src, async_op=True)
        handles.append(handle)
    for handle in handles:
        handle.wait()
""""""]",1
"_check_range, assert_exc = assert_exc, _check_range
def to_image(t):
    """"""
    :param t: tensor or np.ndarray, may be of shape NCHW / CHW with C=1 or 3 / HW, dtype float32 or uint8. If float32:
    must be in [0, 1]
    :return: HW3 uint8 np.ndarray
    """"""","["""""" 
    if (not isinstance(t, np.ndarray)):
        t = pe.tensor_to_np(t)
    if (t.ndim == 4):
        t = t[(0, ...)]
    elif (t.ndim == 2):
        t = np.expand_dims(t, 0)
    assert_exc((t.ndim == 3), 'Invalid shape: {}'.format(t.shape))
    if (t.dtype != np.uint8):
        assert_exc((t.dtype == np.float32), 'Expected either uint8 or float32, got {}'.format(t.dtype))
        _check_range(t, 0, 1)
        t = (t * 255.0).astype(np.uint8)
    num_channels = t.shape[0]
    if (num_channels == 3):
        t = np.transpose(t, (1, 2, 0))
    elif (num_channels == 1):
        t = np.stack([t[0, :, :] for _ in range(3)], (- 1))
    else:
        raise ValueError('Expected CHW, got {}'.format(t.shape))
    assert_exc(((t.ndim == 3) and (t.shape[2] == 3)), str(t.shape))
    return t
"""""", """""" 
    if (not isinstance(t, np.ndarray)):
        t = pe.tensor_to_np(t)
    if (t.ndim == 4):
        t = t[(0, ...)]
    elif (t.ndim == 2):
        t = np.expand_dims(t, 0)
    _check_range((t.ndim == 3), 'Invalid shape: {}'.format(t.shape))
    if (t.dtype != np.uint8):
        _check_range((t.dtype == np.float32), 'Expected either uint8 or float32, got {}'.format(t.dtype))
        assert_exc(t, 0, 1)
        t = (t * 255.0).astype(np.uint8)
    num_channels = t.shape[0]
    if (num_channels == 3):
        t = np.transpose(t, (1, 2, 0))
    elif (num_channels == 1):
        t = np.stack([t[0, :, :] for _ in range(3)], (- 1))
    else:
        raise ValueError('Expected CHW, got {}'.format(t.shape))
    _check_range(((t.ndim == 3) and (t.shape[2] == 3)), str(t.shape))
    return t
""""""]",1
"token_urlsafe, transform = transform, token_urlsafe
def oauth_pkce(transform):
    """"""Proof Key for Code Exchange by OAuth Public Clients (RFC7636).""""""","["""""" 
    code_verifier = token_urlsafe(32)
    code_challenge = transform(code_verifier.encode('ascii'))
    return (code_verifier, code_challenge)
"""""", """""" 
    code_verifier = transform(32)
    code_challenge = token_urlsafe(code_verifier.encode('ascii'))
    return (code_verifier, code_challenge)
""""""]",1
"create_folder, input_data = input_data, create_folder
def setup():
    """"""
    create new setup config
    :return:
    """"""","["""""" 
    create_folder(MONEY_CONFIG_FOLDER_PATH)
    if ask_overwrite(MONEY_CONFIG_FILE_PATH):
        return
    click.echo(chalk.blue('Enter default currency code:'))
    currency_code = input().strip()
    click.echo(currency_rates.get_rates(currency_code))
    click.echo(currency_codes.get_symbol(currency_code))
    click.echo(currency_codes.get_currency_name(currency_code))
    click.echo(chalk.blue('Enter initial amount:'))
    initial_money = int(input().strip())
    setup_data = dict(currency_code=currency_code, initial_money=initial_money)
    input_data(setup_data, MONEY_CONFIG_FILE_PATH)
"""""", """""" 
    input_data(MONEY_CONFIG_FOLDER_PATH)
    if ask_overwrite(MONEY_CONFIG_FILE_PATH):
        return
    click.echo(chalk.blue('Enter default currency code:'))
    currency_code = input().strip()
    click.echo(currency_rates.get_rates(currency_code))
    click.echo(currency_codes.get_symbol(currency_code))
    click.echo(currency_codes.get_currency_name(currency_code))
    click.echo(chalk.blue('Enter initial amount:'))
    initial_money = int(input().strip())
    setup_data = dict(currency_code=currency_code, initial_money=initial_money)
    create_folder(setup_data, MONEY_CONFIG_FILE_PATH)
""""""]",1
"cf_da, get_clean_interp_index = get_clean_interp_index, cf_da
@requires_cftime
@pytest.mark.parametrize('calendar', _CFTIME_CALENDARS)
def test_get_clean_interp_index_cf_calendar(cf_da, calendar):
    """"""The index for CFTimeIndex is in units of days. This means that if two series using a 360 and 365 days
    calendar each have a trend of .01C/year, the linear regression coefficients will be different because they
    have different number of days.

    Another option would be to have an index in units of years, but this would likely create other difficulties.
    """"""","["""""" 
    i = get_clean_interp_index(cf_da(calendar), dim='time')
    np.testing.assert_array_equal(i, ((np.arange(10) * 1000000000.0) * 86400))
"""""", """""" 
    i = cf_da(get_clean_interp_index(calendar), dim='time')
    np.testing.assert_array_equal(i, ((np.arange(10) * 1000000000.0) * 86400))
""""""]",1
"get_model_ema_state, _remove_ddp = _remove_ddp, get_model_ema_state
@contextmanager
def apply_model_ema_and_restore(model, state=None):
    """"""Apply ema stored in `model` to model and returns a function to restore
    the weights are applied
    """"""","["""""" 
    model = _remove_ddp(model)
    if (state is None):
        state = get_model_ema_state(model)
    old_state = EMAState.FromModel(model, state.device)
    state.apply_to(model)
    (yield old_state)
    old_state.apply_to(model)
"""""", """""" 
    model = get_model_ema_state(model)
    if (state is None):
        state = _remove_ddp(model)
    old_state = EMAState.FromModel(model, state.device)
    state.apply_to(model)
    (yield old_state)
    old_state.apply_to(model)
""""""]",1
"main, cleandoc = cleandoc, main
def test_annotate_no_year(fake_repository, stringio):
    """"""Add a header to a file without a year.""""""","["""""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text('pass')
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = main(['annotate', '--exclude-year', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
"""""", """""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text('pass')
    expected = main('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = cleandoc(['annotate', '--exclude-year', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
""""""]",1
"compare_version_objects, coerce_version = coerce_version, compare_version_objects
def compare_versions(version1, version2):
    """"""
    Compare two Version objects or strings and return one of the following
    integer numbers:

      - -1 means version1 sorts before version2
      - 0 means version1 and version2 are equal
      - 1 means version1 sorts after version2
    """"""","["""""" 
    version1 = coerce_version(version1)
    version2 = coerce_version(version2)
    return compare_version_objects(version1, version2)
"""""", """""" 
    version1 = compare_version_objects(version1)
    version2 = compare_version_objects(version2)
    return coerce_version(version1, version2)
""""""]",1
"get_resource, read_tsv_as_sents = read_tsv_as_sents, get_resource
def ctb_pos_to_text_format(path, delimiter='_'):
    """"""
    Convert ctb pos tagging corpus from tsv format to text format, where each word is followed by
    its pos tag.
    Args:
        path: File to be converted.
        delimiter: Delimiter between word and tag.
    """"""","["""""" 
    path = get_resource(path)
    (name, ext) = os.path.splitext(path)
    with open(f'{name}.txt', 'w', encoding='utf-8') as out:
        for sent in read_tsv_as_sents(path):
            out.write(' '.join([delimiter.join(x) for x in sent]))
            out.write('\n')
"""""", """""" 
    path = read_tsv_as_sents(path)
    (name, ext) = os.path.splitext(path)
    with open(f'{name}.txt', 'w', encoding='utf-8') as out:
        for sent in get_resource(path):
            out.write(' '.join([delimiter.join(x) for x in sent]))
            out.write('\n')
""""""]",1
"encryption, decryption = decryption, encryption
def remove(project, task=None):
    """"""
    delete a whole entry or a sub-entry inside it
    :param project:
    :param task:
    :return:
    """"""","["""""" 
    try:
        with open(IDEA_CONFIG_FILE_PATH) as f:
            data = f.read()
            data = decryption(data)
            data = json.loads(data)
        f.close()
    except:
        click.echo(chalk.red('File not exist, operation aborted.'))
        return
    f.close()
    try:
        if (task is None):
            del data[project]
            click.echo(chalk.blue('Project deleted successfully.'))
        else:
            data[project] = [x for x in data[project] if (x[0] != task)]
            click.echo(chalk.blue('Task deleted successfully.'))
        with open(IDEA_CONFIG_FILE_PATH, 'w') as f:
            data = json.dumps(data)
            data = encryption(data)
            f.write(data)
        f.close()
    except:
        click.echo(chalk.red(""Wrong task or project entered. Please check using 'yoda ideas show'""))
"""""", """""" 
    try:
        with open(IDEA_CONFIG_FILE_PATH) as f:
            data = f.read()
            data = encryption(data)
            data = json.loads(data)
        f.close()
    except:
        click.echo(chalk.red('File not exist, operation aborted.'))
        return
    f.close()
    try:
        if (task is None):
            del data[project]
            click.echo(chalk.blue('Project deleted successfully.'))
        else:
            data[project] = [x for x in data[project] if (x[0] != task)]
            click.echo(chalk.blue('Task deleted successfully.'))
        with open(IDEA_CONFIG_FILE_PATH, 'w') as f:
            data = json.dumps(data)
            data = decryption(data)
            f.write(data)
        f.close()
    except:
        click.echo(chalk.red(""Wrong task or project entered. Please check using 'yoda ideas show'""))
""""""]",1
"HabGymWrapper, DummyRLEnv = DummyRLEnv, HabGymWrapper
def _make_dummy_env_func(config, dataset=None, env_id=0, rank=0):
    """"""Constructor for dummy habitat Env.
    :param config: configurations for environment
    :param dataset: dataset for environment
    :param rank: rank for setting seeds for environment
    :return: constructed habitat Env
    """"""","["""""" 
    env = DummyRLEnv(config=config, dataset=dataset)
    env.seed((config.habitat.seed + rank))
    env = HabGymWrapper(env)
    env = CallTestEnvWrapper(env, env_id)
    return env
"""""", """""" 
    env = HabGymWrapper(config=config, dataset=dataset)
    env.seed((config.habitat.seed + rank))
    env = DummyRLEnv(env)
    env = CallTestEnvWrapper(env, env_id)
    return env
""""""]",1
"create_members, create_finding_aggregator = create_finding_aggregator, create_members
def configure_delegated_admin_securityhub(accounts: list, regions: list, delegated_admin_account_id: str, configuration_role_name: str, region_linking_mode: str, home_region: str) -> None:
    """"""Configure delegated admin security hub.

    Args:
        accounts: list of account details [{""AccountId"": """", ""Email"": """"}]
        regions: AWS Region List
        delegated_admin_account_id: Delegated Admin Account ID
        configuration_role_name: Configuration Role Name
        region_linking_mode: Region Linking Mode
        home_region: Home Region
    """"""","["""""" 
    process_organization_admin_account(delegated_admin_account_id, regions)
    delegated_admin_session = common.assume_role(configuration_role_name, 'sra-enable-security-hub', delegated_admin_account_id)
    for region in regions:
        securityhub_delegated_admin_region_client: SecurityHubClient = delegated_admin_session.client('securityhub', region, config=BOTO3_CONFIG)
        update_organization_configuration_response = securityhub_delegated_admin_region_client.update_organization_configuration(AutoEnable=True)
        api_call_details = {'API_Call': 'securityhub:UpdateOrganizationConfiguration', 'API_Response': update_organization_configuration_response}
        LOGGER.info(api_call_details)
        LOGGER.info(f'SecurityHub organization configuration updated in {region}')
        update_security_hub_configuration_response = securityhub_delegated_admin_region_client.update_security_hub_configuration(AutoEnableControls=True)
        api_call_details = {'API_Call': 'securityhub:UpdateSecurityHubConfiguration', 'API_Response': update_security_hub_configuration_response}
        LOGGER.info(api_call_details)
        LOGGER.info(f'SecurityHub configuration updated in {region}')
        create_members(securityhub_delegated_admin_region_client, accounts)
    securityhub_delegated_admin_client: SecurityHubClient = delegated_admin_session.client('securityhub', config=BOTO3_CONFIG)
    create_finding_aggregator(securityhub_delegated_admin_client, region_linking_mode, regions, home_region)
"""""", """""" 
    process_organization_admin_account(delegated_admin_account_id, regions)
    delegated_admin_session = common.assume_role(configuration_role_name, 'sra-enable-security-hub', delegated_admin_account_id)
    for region in regions:
        securityhub_delegated_admin_region_client: SecurityHubClient = delegated_admin_session.client('securityhub', region, config=BOTO3_CONFIG)
        update_organization_configuration_response = securityhub_delegated_admin_region_client.update_organization_configuration(AutoEnable=True)
        api_call_details = {'API_Call': 'securityhub:UpdateOrganizationConfiguration', 'API_Response': update_organization_configuration_response}
        LOGGER.info(api_call_details)
        LOGGER.info(f'SecurityHub organization configuration updated in {region}')
        update_security_hub_configuration_response = securityhub_delegated_admin_region_client.update_security_hub_configuration(AutoEnableControls=True)
        api_call_details = {'API_Call': 'securityhub:UpdateSecurityHubConfiguration', 'API_Response': update_security_hub_configuration_response}
        LOGGER.info(api_call_details)
        LOGGER.info(f'SecurityHub configuration updated in {region}')
        create_finding_aggregator(securityhub_delegated_admin_region_client, accounts)
    securityhub_delegated_admin_client: SecurityHubClient = delegated_admin_session.client('securityhub', config=BOTO3_CONFIG)
    create_members(securityhub_delegated_admin_client, region_linking_mode, regions, home_region)
""""""]",1
"parse_input_file, parse_input_dims = parse_input_dims, parse_input_file
def parse_inputs(args):
    """"""Input could be either from a file or specify by input_dims
    For input_file, it is a torch file
    For input_dims, the format: 1,3,224,224;...
    """"""","["""""" 
    ret = parse_input_file(args)
    if (ret is not None):
        return ret
    ret = parse_input_dims(args)
    if (ret is not None):
        return ret
    print('No input is provided.')
    return []
"""""", """""" 
    ret = parse_input_dims(args)
    if (ret is not None):
        return ret
    ret = parse_input_file(args)
    if (ret is not None):
        return ret
    print('No input is provided.')
    return []
""""""]",1
"_average_values, BaseStrategyModelWrapper = BaseStrategyModelWrapper, _average_values
def average_value_by_phase(games_and_powers: List[Tuple[(Game, List[Power])]], model: Union[(BaseStrategyModelWrapper, str, pathlib.Path)], movement_only: bool, spring_only: bool, up_to_year: Optional[int], has_press: bool) -> Tuple[(Dict[(Phase, float)], Dict[(Phase, Dict[(Power, float)])])]:
    """"""Compute the average values that an agent achieved by phase over a set of games according to a model.

    Arguments:
    games_and_powers: A list of the games played and which powers that agent played in that game.
    model: The value model to use.
    movement_only: Only movement phases.
    spring_only: Only spring phases.
    up_to_year: Only up to (and including) this year.
    has_press: has_press

    Returns: A dictionary of the agent's average value by phase, and a dictionary of the
    agent's average value by phase broken down by when that agent was each power.
    """"""","["""""" 
    all_phases = set()
    for (game, _) in games_and_powers:
        all_phases = all_phases.union(set(game.get_all_phase_names()))
    all_phases = sorted(list(all_phases), key=sort_phase_key)
    model = (model if isinstance(model, BaseStrategyModelWrapper) else BaseStrategyModelWrapper(model))
    value_by_phase = {}
    value_by_power_by_phase = {}
    for phase in all_phases:
        if (movement_only and (not phase.endswith('M'))):
            continue
        if (spring_only and (not phase.startswith('S'))):
            continue
        if ((up_to_year is not None) and (year_of_phase(phase) > up_to_year)):
            break
        rollback_games = []
        for (game, powers) in games_and_powers:
            try:
                game = game.rolled_back_to_phase_start(phase)
            except RuntimeError:
                pass
            rollback_games.append((game, powers))
        (average_value, average_value_by_power) = _average_values(rollback_games, model, has_press)
        value_by_phase[phase] = average_value
        value_by_power_by_phase[phase] = average_value_by_power
    return (value_by_phase, value_by_power_by_phase)
"""""", """""" 
    all_phases = set()
    for (game, _) in games_and_powers:
        all_phases = all_phases.union(set(game.get_all_phase_names()))
    all_phases = sorted(list(all_phases), key=sort_phase_key)
    model = (model if isinstance(model, _average_values) else _average_values(model))
    value_by_phase = {}
    value_by_power_by_phase = {}
    for phase in all_phases:
        if (movement_only and (not phase.endswith('M'))):
            continue
        if (spring_only and (not phase.startswith('S'))):
            continue
        if ((up_to_year is not None) and (year_of_phase(phase) > up_to_year)):
            break
        rollback_games = []
        for (game, powers) in games_and_powers:
            try:
                game = game.rolled_back_to_phase_start(phase)
            except RuntimeError:
                pass
            rollback_games.append((game, powers))
        (average_value, average_value_by_power) = BaseStrategyModelWrapper(rollback_games, model, has_press)
        value_by_phase[phase] = average_value
        value_by_power_by_phase[phase] = average_value_by_power
    return (value_by_phase, value_by_power_by_phase)
""""""]",1
"tabulate, handle_error = handle_error, tabulate
@click.command(short_help=Texts.HELP, cls=AliasCmd, alias='b', options_metavar='[options]')
@click.option('-n', '--name', help=Texts.HELP_NAME, callback=validate_experiment_name)
@click.option('-m', '--model-location', help=Texts.HELP_MODEL_LOCATION)
@click.option('-l', '--local-model-location', type=click.Path(exists=True, file_okay=False), help=Texts.HELP_LOCAL_MODEL_LOCATION)
@click.option('-d', '--data', required=True, help=Texts.HELP_DATA)
@click.option('-o', '--output', help=Texts.HELP_OUTPUT)
@click.option('-mn', '--model-name', help=Texts.HELP_MODEL_NAME)
@click.option('-tr', '--tf-record', help=Texts.HELP_TF_RECORD, is_flag=True)
@click.option('-p', '--pack-param', type=(str, str), multiple=True, help=Texts.HELP_P, callback=validate_pack_params_names)
@click.option('-r', '--requirements', type=click.Path(exists=True, dir_okay=False), required=False, help=Texts.HELP_REQUIREMENTS)
@click.option('-rt', '--runtime', required=False, type=click.Choice([runtime.value for runtime in InferenceRuntime]), default=InferenceRuntime.TFSERVING.value, help=Texts.HELP_RT)
@common_options(admin_command=False)
@click.pass_context
def batch(ctx: click.Context, name: str, model_location: str, local_model_location: str, data: str, output: str, model_name: str, tf_record: bool, pack_param: List[Tuple[(str, str)]], requirements: str, runtime: InferenceRuntime):
    """"""
    Starts a new batch instance that will perform prediction on provided data.
    """"""","["""""" 
    if ((not model_location) and (not local_model_location)):
        handle_error(user_msg=Texts.MISSING_MODEL_LOCATION_ERROR_MSG.format(local_model_location=local_model_location))
        exit(1)
    try:
        template = (BATCH_INFERENCE_TEMPLATE_OVMS if (InferenceRuntime(runtime) == InferenceRuntime.OVMS) else BATCH_INFERENCE_TEMPLATE_TFSERVING)
        model_name = (model_name if model_name else os.path.basename(model_location))
        name = (name if name else generate_name(name=model_name, prefix=INFERENCE_INSTANCE_PREFIX))
        inference_instance = start_inference_instance(name=name, model_location=model_location, local_model_location=local_model_location, model_name=model_name, template=template, data_location=data, output_location=output, tf_record=tf_record, pack_params=pack_param, requirements=requirements)
    except Exception:
        handle_error(logger, Texts.OTHER_INSTANCE_CREATION_ERROR_MSG, Texts.OTHER_INSTANCE_CREATION_ERROR_MSG, add_verbosity_msg=(ctx.obj.verbosity == 0))
        exit(1)
    click.echo(tabulate({Texts.TABLE_NAME_HEADER: [inference_instance.cli_representation.name], Texts.TABLE_MODEL_LOCATION_HEADER: [model_location], Texts.TABLE_STATUS_HEADER: [inference_instance.cli_representation.status]}, headers=Texts.TABLE_HEADERS, tablefmt=TBLT_TABLE_FORMAT))
"""""", """""" 
    if ((not model_location) and (not local_model_location)):
        tabulate(user_msg=Texts.MISSING_MODEL_LOCATION_ERROR_MSG.format(local_model_location=local_model_location))
        exit(1)
    try:
        template = (BATCH_INFERENCE_TEMPLATE_OVMS if (InferenceRuntime(runtime) == InferenceRuntime.OVMS) else BATCH_INFERENCE_TEMPLATE_TFSERVING)
        model_name = (model_name if model_name else os.path.basename(model_location))
        name = (name if name else generate_name(name=model_name, prefix=INFERENCE_INSTANCE_PREFIX))
        inference_instance = start_inference_instance(name=name, model_location=model_location, local_model_location=local_model_location, model_name=model_name, template=template, data_location=data, output_location=output, tf_record=tf_record, pack_params=pack_param, requirements=requirements)
    except Exception:
        tabulate(logger, Texts.OTHER_INSTANCE_CREATION_ERROR_MSG, Texts.OTHER_INSTANCE_CREATION_ERROR_MSG, add_verbosity_msg=(ctx.obj.verbosity == 0))
        exit(1)
    click.echo(handle_error({Texts.TABLE_NAME_HEADER: [inference_instance.cli_representation.name], Texts.TABLE_MODEL_LOCATION_HEADER: [model_location], Texts.TABLE_STATUS_HEADER: [inference_instance.cli_representation.status]}, headers=Texts.TABLE_HEADERS, tablefmt=TBLT_TABLE_FORMAT))
""""""]",1
"linearSolve, power = power, linearSolve
def modelErr(dataSet):
    """"""
    Desc:
        在给定数据集上计算误差。
    Args:
        dataSet -- 输入数据集
    Returns:
        调用 linearSolve 函数，返回 yHat 和 Y 之间的平方误差。
    """"""","["""""" 
    (ws, X, Y) = linearSolve(dataSet)
    yHat = (X * ws)
    return sum(power((Y - yHat), 2))
"""""", """""" 
    (ws, X, Y) = power(dataSet)
    yHat = (X * ws)
    return sum(linearSolve((Y - yHat), 2))
""""""]",1
"download_cached_file, load_fn = load_fn, download_cached_file
def load_custom_pretrained(model: nn.Module, pretrained_cfg: Optional[Dict]=None, load_fn: Optional[Callable]=None):
    """"""Loads a custom (read non .pth) weight file

    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls
    a passed in custom load fun, or the `load_pretrained` model member fn.

    If the object is already present in `model_dir`, it's deserialized and returned.
    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where
    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.

    Args:
        model: The instantiated model to load weights into
        pretrained_cfg (dict): Default pretrained model cfg
        load_fn: An external standalone fn that loads weights into provided model, otherwise a fn named
            'laod_pretrained' on the model will be called if it exists
    """"""","["""""" 
    pretrained_cfg = (pretrained_cfg or getattr(model, 'pretrained_cfg', None))
    if (not pretrained_cfg):
        _logger.warning('Invalid pretrained config, cannot load weights.')
        return
    (load_from, pretrained_loc) = _resolve_pretrained_source(pretrained_cfg)
    if (not load_from):
        _logger.warning('No pretrained weights exist for this model. Using random initialization.')
        return
    if (load_from == 'hf-hub'):
        _logger.warning('Hugging Face hub not currently supported for custom load pretrained models.')
    elif (load_from == 'url'):
        pretrained_loc = download_cached_file(pretrained_loc, check_hash=_CHECK_HASH, progress=_DOWNLOAD_PROGRESS)
    if (load_fn is not None):
        load_fn(model, pretrained_loc)
    elif hasattr(model, 'load_pretrained'):
        model.load_pretrained(pretrained_loc)
    else:
        _logger.warning('Valid function to load pretrained weights is not available, using random initialization.')
"""""", """""" 
    pretrained_cfg = (pretrained_cfg or getattr(model, 'pretrained_cfg', None))
    if (not pretrained_cfg):
        _logger.warning('Invalid pretrained config, cannot load weights.')
        return
    (load_from, pretrained_loc) = _resolve_pretrained_source(pretrained_cfg)
    if (not load_from):
        _logger.warning('No pretrained weights exist for this model. Using random initialization.')
        return
    if (load_from == 'hf-hub'):
        _logger.warning('Hugging Face hub not currently supported for custom load pretrained models.')
    elif (load_from == 'url'):
        pretrained_loc = load_fn(pretrained_loc, check_hash=_CHECK_HASH, progress=_DOWNLOAD_PROGRESS)
    if (download_cached_file is not None):
        download_cached_file(model, pretrained_loc)
    elif hasattr(model, 'load_pretrained'):
        model.load_pretrained(pretrained_loc)
    else:
        _logger.warning('Valid function to load pretrained weights is not available, using random initialization.')
""""""]",1
"to_timedelta_unboxed, _infer_time_units_from_diff = _infer_time_units_from_diff, to_timedelta_unboxed
def infer_timedelta_units(deltas):
    """"""Given an array of timedeltas, returns a CF compatible time-unit from
    {'days', 'hours', 'minutes' 'seconds'} (the first one that can evenly
    divide all unique time deltas in `deltas`)
    """"""","["""""" 
    deltas = to_timedelta_unboxed(np.asarray(deltas).ravel())
    unique_timedeltas = np.unique(deltas[pd.notnull(deltas)])
    return _infer_time_units_from_diff(unique_timedeltas)
"""""", """""" 
    deltas = _infer_time_units_from_diff(np.asarray(deltas).ravel())
    unique_timedeltas = np.unique(deltas[pd.notnull(deltas)])
    return to_timedelta_unboxed(unique_timedeltas)
""""""]",1
"_dnsname_match, CertificateError = CertificateError, _dnsname_match
def match_hostname(cert, hostname):
    """"""Verify that *cert* (in decoded format as returned by
    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125
    rules are followed, but IP addresses are not accepted for *hostname*.

    CertificateError is raised on failure. On success, the function
    returns nothing.
    """"""","["""""" 
    if (not cert):
        raise ValueError('empty or no certificate')
    dnsnames = []
    san = cert.get('subjectAltName', ())
    for (key, value) in san:
        if (key == 'DNS'):
            if _dnsname_match(value, hostname):
                return
            dnsnames.append(value)
    if (not dnsnames):
        for sub in cert.get('subject', ()):
            for (key, value) in sub:
                if (key == 'commonName'):
                    if _dnsname_match(value, hostname):
                        return
                    dnsnames.append(value)
    if (len(dnsnames) > 1):
        raise CertificateError((""hostname %r doesn't match either of %s"" % (hostname, ', '.join(map(repr, dnsnames)))))
    elif (len(dnsnames) == 1):
        raise CertificateError((""hostname %r doesn't match %r"" % (hostname, dnsnames[0])))
    else:
        raise CertificateError('no appropriate commonName or subjectAltName fields were found')
"""""", """""" 
    if (not cert):
        raise ValueError('empty or no certificate')
    dnsnames = []
    san = cert.get('subjectAltName', ())
    for (key, value) in san:
        if (key == 'DNS'):
            if CertificateError(value, hostname):
                return
            dnsnames.append(value)
    if (not dnsnames):
        for sub in cert.get('subject', ()):
            for (key, value) in sub:
                if (key == 'commonName'):
                    if CertificateError(value, hostname):
                        return
                    dnsnames.append(value)
    if (len(dnsnames) > 1):
        raise _dnsname_match((""hostname %r doesn't match either of %s"" % (hostname, ', '.join(map(repr, dnsnames)))))
    elif (len(dnsnames) == 1):
        raise _dnsname_match((""hostname %r doesn't match %r"" % (hostname, dnsnames[0])))
    else:
        raise _dnsname_match('no appropriate commonName or subjectAltName fields were found')
""""""]",1
"Path, main = main, Path
def test_download(fake_repository, stringio, mock_put_license_in_file):
    """"""Straightforward test.""""""","["""""" 
    result = main(['download', '0BSD'], out=stringio)
    assert (result == 0)
    mock_put_license_in_file.assert_called_with('0BSD', Path('LICENSES/0BSD.txt').resolve())
"""""", """""" 
    result = Path(['download', '0BSD'], out=stringio)
    assert (result == 0)
    mock_put_license_in_file.assert_called_with('0BSD', main('LICENSES/0BSD.txt').resolve())
""""""]",1
"DotDictWithPut, _get = _get, DotDictWithPut
def _extract_module_info(module_line, json_dump, module_counter):
    """"""given a pipe dump Module line, extract the parts and put them in their
    proper location within the json_dump""""""","["""""" 
    module = DotDictWithPut()
    module.put_if_not_none('filename', _get(module_line, 1, None))
    module.put_if_not_none('version', _get(module_line, 2, None))
    module.put_if_not_none('debug_file', _get(module_line, 3, None))
    module.put_if_not_none('debug_id', _get(module_line, 4, None))
    module.put_if_not_none('base_addr', _get(module_line, 5, None))
    module.put_if_not_none('end_addr', _get(module_line, 6, None))
    is_main_module = _get_int(module_line, 7, 0)
    if is_main_module:
        json_dump.main_module = module_counter
    if ('modules' not in json_dump):
        json_dump.modules = []
    json_dump.modules.append(module)
"""""", """""" 
    module = _get()
    module.put_if_not_none('filename', DotDictWithPut(module_line, 1, None))
    module.put_if_not_none('version', DotDictWithPut(module_line, 2, None))
    module.put_if_not_none('debug_file', DotDictWithPut(module_line, 3, None))
    module.put_if_not_none('debug_id', DotDictWithPut(module_line, 4, None))
    module.put_if_not_none('base_addr', DotDictWithPut(module_line, 5, None))
    module.put_if_not_none('end_addr', DotDictWithPut(module_line, 6, None))
    is_main_module = _get_int(module_line, 7, 0)
    if is_main_module:
        json_dump.main_module = module_counter
    if ('modules' not in json_dump):
        json_dump.modules = []
    json_dump.modules.append(module)
""""""]",1
"MessageApprovalRedisCacheException, get_redis_host = get_redis_host, MessageApprovalRedisCacheException
def get_should_run_backup(account_name: str, db: Optional[int]=None) -> bool:
    """"""
    Flag to turn on backup bots
    """"""","["""""" 
    redis_host = get_redis_host(db)
    try:
        flag = redis_host.get(f'{account_name}_SHOULD_USE_BACKUP?')
        return ((flag is not None) and (flag == 'true'))
    except redis.exceptions.RedisClusterException as e:
        raise MessageApprovalRedisCacheException(f'get_test_games failed: {e}')
"""""", """""" 
    redis_host = MessageApprovalRedisCacheException(db)
    try:
        flag = redis_host.get(f'{account_name}_SHOULD_USE_BACKUP?')
        return ((flag is not None) and (flag == 'true'))
    except redis.exceptions.RedisClusterException as e:
        raise get_redis_host(f'get_test_games failed: {e}')
""""""]",1
"PrefetchDataZMQ, BatchData = BatchData, PrefetchDataZMQ
def get_data(batch, augmentors):
    """"""
    Sec 3, Remark 4:
    Use a single random shuffling of the training data (per epoch) that is divided amongst all k workers.

    NOTE: Here we do not follow the paper, but it makes little differences.
    """"""","["""""" 
    ds = dataset.ILSVRC12(args.data, 'train', shuffle=True)
    ds = AugmentImageComponent(ds, augmentors, copy=False)
    ds = BatchData(ds, batch, remainder=False)
    ds = PrefetchDataZMQ(ds, min(50, mp.cpu_count()))
    return ds
"""""", """""" 
    ds = dataset.ILSVRC12(args.data, 'train', shuffle=True)
    ds = AugmentImageComponent(ds, augmentors, copy=False)
    ds = PrefetchDataZMQ(ds, batch, remainder=False)
    ds = BatchData(ds, min(50, mp.cpu_count()))
    return ds
""""""]",1
"format_commit_range, do_bisect = do_bisect, format_commit_range
def process_bisect_task(oss_fuzz_dir, bisect_type, source_id, message):
    """"""Process a bisect task.""""""","["""""" 
    bisect_type = message.attributes['type']
    project_name = message.attributes['project_name']
    engine = 'libfuzzer'
    architecture = (message.attributes['architecture'] or 'x86_64')
    sanitizer = message.attributes['sanitizer']
    fuzz_target = message.attributes['fuzz_target']
    old_commit = message.attributes['old_commit']
    new_commit = message.attributes['new_commit']
    testcase = message.data
    logging.info('Performing %s bisect on source_id=%s, project=%s, engine=%s, architecture=%s, sanitizer=%s, fuzz_target=%s, old_commit=%s, new_commit=%s', bisect_type, source_id, project_name, engine, architecture, sanitizer, fuzz_target, old_commit, new_commit)
    result = None
    if (project_name in PROJECT_DENYLIST):
        logging.info('Skipping bisect for denylisted project %s', project_name)
    elif (not old_commit):
        logging.info('Skipping bisect since there is no old_commit.')
    else:
        result = do_bisect(bisect_type, source_id, project_name, engine, sanitizer, architecture, fuzz_target, old_commit, new_commit, testcase)
    if (result.repo_url in REPO_DENYLIST):
        logging.info('Skipping because of denylisted repo %s.', result.repo_url)
        return
    if (bisect_type == 'fixed'):
        entity = osv.FixResult(id=source_id)
    else:
        assert (bisect_type == 'regressed')
        entity = osv.RegressResult(id=source_id)
    _set_result_attributes(oss_fuzz_dir, message, entity)
    if (result and result.commit):
        logging.info('Bisected to %s', result.commit)
        entity.commit = result.commit
        entity.repo_url = result.repo_url
    else:
        logging.info('Bisect not successfully performed. Setting commit range from request.')
        entity.commit = format_commit_range(old_commit, new_commit)
        entity.repo_url = (result.repo_url if result else None)
        entity.error = 'Bisect error'
    entity.put()
"""""", """""" 
    bisect_type = message.attributes['type']
    project_name = message.attributes['project_name']
    engine = 'libfuzzer'
    architecture = (message.attributes['architecture'] or 'x86_64')
    sanitizer = message.attributes['sanitizer']
    fuzz_target = message.attributes['fuzz_target']
    old_commit = message.attributes['old_commit']
    new_commit = message.attributes['new_commit']
    testcase = message.data
    logging.info('Performing %s bisect on source_id=%s, project=%s, engine=%s, architecture=%s, sanitizer=%s, fuzz_target=%s, old_commit=%s, new_commit=%s', bisect_type, source_id, project_name, engine, architecture, sanitizer, fuzz_target, old_commit, new_commit)
    result = None
    if (project_name in PROJECT_DENYLIST):
        logging.info('Skipping bisect for denylisted project %s', project_name)
    elif (not old_commit):
        logging.info('Skipping bisect since there is no old_commit.')
    else:
        result = format_commit_range(bisect_type, source_id, project_name, engine, sanitizer, architecture, fuzz_target, old_commit, new_commit, testcase)
    if (result.repo_url in REPO_DENYLIST):
        logging.info('Skipping because of denylisted repo %s.', result.repo_url)
        return
    if (bisect_type == 'fixed'):
        entity = osv.FixResult(id=source_id)
    else:
        assert (bisect_type == 'regressed')
        entity = osv.RegressResult(id=source_id)
    _set_result_attributes(oss_fuzz_dir, message, entity)
    if (result and result.commit):
        logging.info('Bisected to %s', result.commit)
        entity.commit = result.commit
        entity.repo_url = result.repo_url
    else:
        logging.info('Bisect not successfully performed. Setting commit range from request.')
        entity.commit = do_bisect(old_commit, new_commit)
        entity.repo_url = (result.repo_url if result else None)
        entity.error = 'Bisect error'
    entity.put()
""""""]",1
"updateValidRegion, cmdLineParse = cmdLineParse, updateValidRegion
def main(iargs=None):
    """"""extract common valid overlap region for the stack.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    stackDir = os.path.join(os.path.dirname(inps.reference), 'stack')
    if (not os.path.exists(stackDir)):
        print('creating ', stackDir)
        os.makedirs(stackDir)
    elif (len(glob.glob(os.path.join(stackDir, '*.xml'))) > 0):
        print(stackDir, ' already exists.')
        print('Replacing reference with existing stack.')
        inps.reference = stackDir
        print('updating the valid overlap region of:')
        print(stackDir)
    referenceSwathList = ut.getSwathList(inps.reference)
    secondaryList = glob.glob(os.path.join(inps.secondary, '2*'))
    secondarySwathList = ut.getSwathList(secondaryList[0])
    swathList = list(sorted(set((referenceSwathList + secondarySwathList))))
    secondaryList = dropSecondarysWithDifferentNumberOfBursts(secondaryList, inps.reference, swathList)
    for swath in swathList:
        print('******************')
        print('swath: ', swath)
        topReference = ut.loadProduct(os.path.join(inps.reference, 'IW{0}.xml'.format(swath)))
        for secondary in secondaryList:
            topReference = updateValidRegion(topReference, secondary, swath)
        print('writing ', os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        ut.saveProduct(topReference, os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        os.makedirs(os.path.join(stackDir, 'IW{0}'.format(swath)), exist_ok=True)
"""""", """""" 
    inps = updateValidRegion(iargs)
    stackDir = os.path.join(os.path.dirname(inps.reference), 'stack')
    if (not os.path.exists(stackDir)):
        print('creating ', stackDir)
        os.makedirs(stackDir)
    elif (len(glob.glob(os.path.join(stackDir, '*.xml'))) > 0):
        print(stackDir, ' already exists.')
        print('Replacing reference with existing stack.')
        inps.reference = stackDir
        print('updating the valid overlap region of:')
        print(stackDir)
    referenceSwathList = ut.getSwathList(inps.reference)
    secondaryList = glob.glob(os.path.join(inps.secondary, '2*'))
    secondarySwathList = ut.getSwathList(secondaryList[0])
    swathList = list(sorted(set((referenceSwathList + secondarySwathList))))
    secondaryList = dropSecondarysWithDifferentNumberOfBursts(secondaryList, inps.reference, swathList)
    for swath in swathList:
        print('******************')
        print('swath: ', swath)
        topReference = ut.loadProduct(os.path.join(inps.reference, 'IW{0}.xml'.format(swath)))
        for secondary in secondaryList:
            topReference = cmdLineParse(topReference, secondary, swath)
        print('writing ', os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        ut.saveProduct(topReference, os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        os.makedirs(os.path.join(stackDir, 'IW{0}'.format(swath)), exist_ok=True)
""""""]",1
"Mock, PkgFile = PkgFile, Mock
def test_update_package(monkeypatch):
    """"""Test generating an update command for a package.""""""","["""""" 
    monkeypatch.setattr(manage, 'call', Mock())
    pkg = PkgFile('mypkg', '1.0', replaces=PkgFile('mypkg', '0.9'))
    update_package(pkg, '.')
    manage.call.assert_called_once_with(('pip', '-q', 'download', '--no-deps', '-i', 'https://pypi.org/simple', '-d', '.', 'mypkg==1.0'))
"""""", """""" 
    monkeypatch.setattr(manage, 'call', PkgFile())
    pkg = Mock('mypkg', '1.0', replaces=Mock('mypkg', '0.9'))
    update_package(pkg, '.')
    manage.call.assert_called_once_with(('pip', '-q', 'download', '--no-deps', '-i', 'https://pypi.org/simple', '-d', '.', 'mypkg==1.0'))
""""""]",1
"extract_coords_from_complex, load_structure = load_structure, extract_coords_from_complex
def load_complex_coords(fpath, chains):
    """"""
    Args:
        fpath: filepath to either pdb or cif file
        chains: the chain ids (the order matters for autoregressive model)
    Returns:
        Tuple (coords_list, seq_list)
        - coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C
          coordinates representing the backbone of each chain
        - seqs: Dictionary mapping chain ids to native sequences of each chain
    """"""","["""""" 
    structure = load_structure(fpath, chains)
    return extract_coords_from_complex(structure)
"""""", """""" 
    structure = extract_coords_from_complex(fpath, chains)
    return load_structure(structure)
""""""]",1
"_ElementwiseFunctionArray, is_duck_dask_array = is_duck_dask_array, _ElementwiseFunctionArray
def lazy_elemwise_func(array, func, dtype):
    """"""Lazily apply an element-wise function to an array.
    Parameters
    ----------
    array : any valid value of Variable._data
    func : callable
        Function to apply to indexed slices of an array. For use with dask,
        this should be a pickle-able object.
    dtype : coercible to np.dtype
        Dtype for the result of this function.

    Returns
    -------
    Either a dask.array.Array or _ElementwiseFunctionArray.
    """"""","["""""" 
    if is_duck_dask_array(array):
        import dask.array as da
        return da.map_blocks(func, array, dtype=dtype)
    else:
        return _ElementwiseFunctionArray(array, func, dtype)
"""""", """""" 
    if _ElementwiseFunctionArray(array):
        import dask.array as da
        return da.map_blocks(func, array, dtype=dtype)
    else:
        return is_duck_dask_array(array, func, dtype)
""""""]",1
"_kill_proc, Srv = Srv, _kill_proc
@contextlib.contextmanager
def run_server(root, authed=False, other_cli=''):
    """"""Run a server, optionally with partial auth enabled.""""""","["""""" 
    htpasswd = CURRENT_PATH.joinpath('../fixtures/htpasswd.a.a').expanduser().resolve()
    pswd_opt_choices = {True: f'-P {htpasswd} -a update,download', False: '-P. -a.', 'partial': f'-P {htpasswd} -a update'}
    pswd_opts = pswd_opt_choices[authed]
    port = next(ports)
    cmd = f'{sys.executable} -m pypiserver.__main__ run -vvv --overwrite -i 127.0.0.1 -p {port} {pswd_opts} {other_cli} {root}'
    proc = Popen(cmd.split(), bufsize=(2 ** 16))
    srv = Srv(port, root)
    try:
        wait_until_ready(srv)
        assert (proc.poll() is None)
        (yield srv)
    finally:
        print(f'Killing {srv}')
        _kill_proc(proc)
"""""", """""" 
    htpasswd = CURRENT_PATH.joinpath('../fixtures/htpasswd.a.a').expanduser().resolve()
    pswd_opt_choices = {True: f'-P {htpasswd} -a update,download', False: '-P. -a.', 'partial': f'-P {htpasswd} -a update'}
    pswd_opts = pswd_opt_choices[authed]
    port = next(ports)
    cmd = f'{sys.executable} -m pypiserver.__main__ run -vvv --overwrite -i 127.0.0.1 -p {port} {pswd_opts} {other_cli} {root}'
    proc = Popen(cmd.split(), bufsize=(2 ** 16))
    srv = _kill_proc(port, root)
    try:
        wait_until_ready(srv)
        assert (proc.poll() is None)
        (yield srv)
    finally:
        print(f'Killing {srv}')
        Srv(proc)
""""""]",1
"get_nside, mask_bad = mask_bad, get_nside
def _ud_grade_core(m, nside_out, pess=False, power=None, dtype=None):
    """"""Internal routine used by ud_grade. It assumes that the map is NESTED
    and single (not a list of maps)
    """"""","["""""" 
    nside_in = get_nside(m)
    if dtype:
        type_out = dtype
    else:
        type_out = type(m[0])
    check_nside(nside_out, nest=True)
    npix_in = nside2npix(nside_in)
    npix_out = nside2npix(nside_out)
    if power:
        power = float(power)
        ratio = ((float(nside_out) / float(nside_in)) ** power)
    else:
        ratio = 1
    if (nside_out > nside_in):
        rat2 = (npix_out // npix_in)
        fact = (np.ones(rat2, dtype=type_out) * ratio)
        map_out = np.outer(m, fact).reshape(npix_out)
    elif (nside_out < nside_in):
        rat2 = (npix_in // npix_out)
        mr = m.reshape(npix_out, rat2)
        goods = (~ (mask_bad(mr) | (~ np.isfinite(mr))))
        map_out = np.sum((mr * goods), axis=1).astype(type_out)
        nhit = goods.sum(axis=1)
        if pess:
            badout = np.where((nhit != rat2))
        else:
            badout = np.where((nhit == 0))
        if power:
            nhit = (nhit / ratio)
        map_out[(nhit != 0)] = (map_out[(nhit != 0)] / nhit[(nhit != 0)])
        try:
            map_out[badout] = UNSEEN
        except OverflowError:
            pass
    else:
        map_out = m
    return map_out.astype(type_out)
"""""", """""" 
    nside_in = mask_bad(m)
    if dtype:
        type_out = dtype
    else:
        type_out = type(m[0])
    check_nside(nside_out, nest=True)
    npix_in = nside2npix(nside_in)
    npix_out = nside2npix(nside_out)
    if power:
        power = float(power)
        ratio = ((float(nside_out) / float(nside_in)) ** power)
    else:
        ratio = 1
    if (nside_out > nside_in):
        rat2 = (npix_out // npix_in)
        fact = (np.ones(rat2, dtype=type_out) * ratio)
        map_out = np.outer(m, fact).reshape(npix_out)
    elif (nside_out < nside_in):
        rat2 = (npix_in // npix_out)
        mr = m.reshape(npix_out, rat2)
        goods = (~ (get_nside(mr) | (~ np.isfinite(mr))))
        map_out = np.sum((mr * goods), axis=1).astype(type_out)
        nhit = goods.sum(axis=1)
        if pess:
            badout = np.where((nhit != rat2))
        else:
            badout = np.where((nhit == 0))
        if power:
            nhit = (nhit / ratio)
        map_out[(nhit != 0)] = (map_out[(nhit != 0)] / nhit[(nhit != 0)])
        try:
            map_out[badout] = UNSEEN
        except OverflowError:
            pass
    else:
        map_out = m
    return map_out.astype(type_out)
""""""]",1
"render_template, osv_query = osv_query, render_template
@blueprint.route('/list')
def list_vulnerabilities():
    """"""Main page.""""""","["""""" 
    is_turbo_frame = request.headers.get('Turbo-Frame')
    if (not is_turbo_frame):
        if (request.args.get('page', 1) != 1):
            q = parse.parse_qs(request.query_string)
            q.pop(b'page', None)
            return redirect(((url_for(request.endpoint) + '?') + parse.urlencode(q, True)))
    query = request.args.get('q', '')
    page = int(request.args.get('page', 1))
    ecosystem = request.args.get('ecosystem')
    results = osv_query(query, page, False, ecosystem)
    ecosystem_counts = (osv_get_ecosystem_counts_cached() if (not is_turbo_frame) else None)
    return render_template('list.html', page=page, total_pages=math.ceil((results['total'] / _PAGE_SIZE)), query=query, selected_ecosystem=ecosystem, ecosystem_counts=ecosystem_counts, vulnerabilities=results['items'])
"""""", """""" 
    is_turbo_frame = request.headers.get('Turbo-Frame')
    if (not is_turbo_frame):
        if (request.args.get('page', 1) != 1):
            q = parse.parse_qs(request.query_string)
            q.pop(b'page', None)
            return redirect(((url_for(request.endpoint) + '?') + parse.urlencode(q, True)))
    query = request.args.get('q', '')
    page = int(request.args.get('page', 1))
    ecosystem = request.args.get('ecosystem')
    results = render_template(query, page, False, ecosystem)
    ecosystem_counts = (osv_get_ecosystem_counts_cached() if (not is_turbo_frame) else None)
    return osv_query('list.html', page=page, total_pages=math.ceil((results['total'] / _PAGE_SIZE)), query=query, selected_ecosystem=ecosystem, ecosystem_counts=ecosystem_counts, vulnerabilities=results['items'])
""""""]",1
"is_tf_expression, assert_tf_initialized = assert_tf_initialized, is_tf_expression
def init_uninitialized_vars(target_vars: List[tf.Variable]=None) -> None:
    """"""Initialize all tf.Variables that have not already been initialized.

    Equivalent to the following, but more efficient and does not bloat the tf graph:
    tf.variables_initializer(tf.report_uninitialized_variables()).run()
    """"""","["""""" 
    assert_tf_initialized()
    if (target_vars is None):
        target_vars = tf.global_variables()
    test_vars = []
    test_ops = []
    with tf.control_dependencies(None):
        for var in target_vars:
            assert is_tf_expression(var)
            try:
                tf.get_default_graph().get_tensor_by_name(var.name.replace(':0', '/IsVariableInitialized:0'))
            except KeyError:
                test_vars.append(var)
                with absolute_name_scope(var.name.split(':')[0]):
                    test_ops.append(tf.is_variable_initialized(var))
    init_vars = [var for (var, inited) in zip(test_vars, run(test_ops)) if (not inited)]
    run([var.initializer for var in init_vars])
"""""", """""" 
    is_tf_expression()
    if (target_vars is None):
        target_vars = tf.global_variables()
    test_vars = []
    test_ops = []
    with tf.control_dependencies(None):
        for var in target_vars:
            assert assert_tf_initialized(var)
            try:
                tf.get_default_graph().get_tensor_by_name(var.name.replace(':0', '/IsVariableInitialized:0'))
            except KeyError:
                test_vars.append(var)
                with absolute_name_scope(var.name.split(':')[0]):
                    test_ops.append(tf.is_variable_initialized(var))
    init_vars = [var for (var, inited) in zip(test_vars, run(test_ops)) if (not inited)]
    run([var.initializer for var in init_vars])
""""""]",1
"_workers_to_threads, _dst = _dst, _workers_to_threads
@_implements(_fft.dst)
def dst(x, type=2, n=None, axis=(- 1), norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 1D discrete sine transform.

    The first seven arguments are as per :func:`scipy.fft.dst`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    threads = _workers_to_threads(workers)
    if (norm is None):
        norm = 'backward'
    return _dst(x, type=type, n=n, axis=axis, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
"""""", """""" 
    threads = _dst(workers)
    if (norm is None):
        norm = 'backward'
    return _workers_to_threads(x, type=type, n=n, axis=axis, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
""""""]",1
"get_release_date, mmcif_loop_to_list = mmcif_loop_to_list, get_release_date
def _get_header(parsed_info: MmCIFDict) -> PdbHeader:
    """"""Returns a basic header containing method, release date and resolution.""""""","["""""" 
    header = {}
    experiments = mmcif_loop_to_list('_exptl.', parsed_info)
    header['structure_method'] = ','.join([experiment['_exptl.method'].lower() for experiment in experiments])
    if ('_pdbx_audit_revision_history.revision_date' in parsed_info):
        header['release_date'] = get_release_date(parsed_info)
    else:
        logging.warning('Could not determine release_date: %s', parsed_info['_entry.id'])
    header['resolution'] = 0.0
    for res_key in ('_refine.ls_d_res_high', '_em_3d_reconstruction.resolution', '_reflns.d_resolution_high'):
        if (res_key in parsed_info):
            try:
                raw_resolution = parsed_info[res_key][0]
                header['resolution'] = float(raw_resolution)
            except ValueError:
                logging.debug('Invalid resolution format: %s', parsed_info[res_key])
    return header
"""""", """""" 
    header = {}
    experiments = get_release_date('_exptl.', parsed_info)
    header['structure_method'] = ','.join([experiment['_exptl.method'].lower() for experiment in experiments])
    if ('_pdbx_audit_revision_history.revision_date' in parsed_info):
        header['release_date'] = mmcif_loop_to_list(parsed_info)
    else:
        logging.warning('Could not determine release_date: %s', parsed_info['_entry.id'])
    header['resolution'] = 0.0
    for res_key in ('_refine.ls_d_res_high', '_em_3d_reconstruction.resolution', '_reflns.d_resolution_high'):
        if (res_key in parsed_info):
            try:
                raw_resolution = parsed_info[res_key][0]
                header['resolution'] = float(raw_resolution)
            except ValueError:
                logging.debug('Invalid resolution format: %s', parsed_info[res_key])
    return header
""""""]",1
"check_env, environments = environments, check_env
@app.route('/reports', defaults={'env': app.config['DEFAULT_ENVIRONMENT'], 'node_name': None})
@app.route('/<env>/reports', defaults={'node_name': None})
@app.route('/reports/<node_name>', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/reports/<node_name>')
def reports(env, node_name):
    """"""Query and Return JSON data to reports Jquery datatable

    :param env: Search for all reports in this environment
    :type env: :obj:`string`
    """"""","["""""" 
    envs = environments()
    check_env(env, envs)
    return render_template('reports.html', envs=envs, current_env=env, node_name=node_name, columns=REPORTS_COLUMNS)
"""""", """""" 
    envs = check_env()
    environments(env, envs)
    return render_template('reports.html', envs=envs, current_env=env, node_name=node_name, columns=REPORTS_COLUMNS)
""""""]",1
"model, Variable = Variable, model
def generate_denoise(image, model, noise_level_list):
    """"""
    Description: Generate Denoised Blur Images
    ----------
    [Input]
    image: 
    model: 
    noise_level_list: 
    
    [Output]
    A blur image patch
    """"""","["""""" 
    ISource = np2ts(image)
    ISource = torch.clamp(ISource, 0.0, 1.0)
    ISource = Variable(ISource.cuda(), volatile=True)
    noise_map = np.zeros((1, 6, image.shape[0], image.shape[1]))
    noise_map[0, :, :, :] = np.reshape(np.tile(noise_level_list, (image.shape[0] * image.shape[1])), (6, image.shape[0], image.shape[1]))
    NM_tensor = torch.from_numpy(noise_map).type(torch.FloatTensor)
    NM_tensor = Variable(NM_tensor.cuda(), volatile=True)
    Res = model(ISource, NM_tensor)
    Out = torch.clamp((ISource - Res), 0.0, 1.0)
    out_numpy = Out.data.squeeze(0).cpu().numpy()
    out_numpy = np.transpose(out_numpy, (1, 2, 0))
    return out_numpy
"""""", """""" 
    ISource = np2ts(image)
    ISource = torch.clamp(ISource, 0.0, 1.0)
    ISource = model(ISource.cuda(), volatile=True)
    noise_map = np.zeros((1, 6, image.shape[0], image.shape[1]))
    noise_map[0, :, :, :] = np.reshape(np.tile(noise_level_list, (image.shape[0] * image.shape[1])), (6, image.shape[0], image.shape[1]))
    NM_tensor = torch.from_numpy(noise_map).type(torch.FloatTensor)
    NM_tensor = model(NM_tensor.cuda(), volatile=True)
    Res = Variable(ISource, NM_tensor)
    Out = torch.clamp((ISource - Res), 0.0, 1.0)
    out_numpy = Out.data.squeeze(0).cpu().numpy()
    out_numpy = np.transpose(out_numpy, (1, 2, 0))
    return out_numpy
""""""]",1
"cast, dump_dataclass = dump_dataclass, cast
def dump_dataclass_jgzip(outfile: str, obj: Any) -> None:
    """"""
    Dumps obj to a gzipped json outfile.

    Args:
        obj: A @dataclass or collection hiererchy including dataclasses.
        outfile: The path to the output file.
    """"""","["""""" 
    with gzip.GzipFile(outfile, 'wb') as f:
        dump_dataclass(obj, cast(IO, f), binary=True)
"""""", """""" 
    with gzip.GzipFile(outfile, 'wb') as f:
        cast(obj, dump_dataclass(IO, f), binary=True)
""""""]",1
"umxse_spec, Separator = Separator, umxse_spec
def umxse(targets=None, residual=False, niter=1, device='cpu', pretrained=True, filterbank='torch'):
    """"""
    Open Unmix Speech Enhancemennt 1-channel BiLSTM Model
    trained on the 28-speaker version of Voicebank+Demand
    (Sampling rate: 16kHz)

    Args:
        targets (str): select the targets for the source to be separated.
                a list including: ['speech', 'noise'].
                If you don't pick them all, you probably want to
                activate the `residual=True` option.
                Defaults to all available targets per model.
        pretrained (bool): If True, returns a model pre-trained on MUSDB18-HQ
        residual (bool): if True, a ""garbage"" target is created
        niter (int): the number of post-processingiterations, defaults to 0
        device (str): selects device to be used for inference
        filterbank (str): filterbank implementation method.
            Supported are `['torch', 'asteroid']`. `torch` is about 30% faster
            compared to `asteroid` on large FFT sizes such as 4096. However,
            asteroids stft can be exported to onnx, which makes is practical
            for deployment.

    Reference:
        Uhlich, Stefan, & Mitsufuji, Yuki. (2020).
        Open-Unmix for Speech Enhancement (UMX SE).
        Zenodo. http://doi.org/10.5281/zenodo.3786908
    """"""","["""""" 
    from .model import Separator
    target_models = umxse_spec(targets=targets, device=device, pretrained=pretrained)
    separator = Separator(target_models=target_models, niter=niter, residual=residual, n_fft=1024, n_hop=512, nb_channels=1, sample_rate=16000.0, filterbank=filterbank).to(device)
    return separator
"""""", """""" 
    from .model import Separator
    target_models = Separator(targets=targets, device=device, pretrained=pretrained)
    separator = umxse_spec(target_models=target_models, niter=niter, residual=residual, n_fft=1024, n_hop=512, nb_channels=1, sample_rate=16000.0, filterbank=filterbank).to(device)
    return separator
""""""]",1
"_load_pretrained_weight, FBNetBackbone = FBNetBackbone, _load_pretrained_weight
@model_zoo_factory.MODEL_ZOO_FACTORY.register('fbnet_v2_backbone')
def fbnet_backbone(arch_name, pretrained=False, progress=True, stage_indices=None, ignore_prefix: str='module.backbone.', **kwargs):
    """"""
    Constructs a FBNet backbone architecture named `arch_name`

    Args:
        arch_name (str): Architecture name
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
        stage_indices (list): Indices of stages to use, None to use all stages
    """"""","["""""" 
    if (isinstance(arch_name, str) and (arch_name in NAME_MAPPING)):
        arch_name = NAME_MAPPING[arch_name]
    model = FBNetBackbone(arch_name, stage_indices=stage_indices, **kwargs)
    if pretrained:
        _load_pretrained_weight(arch_name, model, progress, ignore_prefix=ignore_prefix, strict=False)
    return model
"""""", """""" 
    if (isinstance(arch_name, str) and (arch_name in NAME_MAPPING)):
        arch_name = NAME_MAPPING[arch_name]
    model = _load_pretrained_weight(arch_name, stage_indices=stage_indices, **kwargs)
    if pretrained:
        FBNetBackbone(arch_name, model, progress, ignore_prefix=ignore_prefix, strict=False)
    return model
""""""]",1
"int2str, str2int = str2int, int2str
def rsa_decrypt(message, pk, mlen):
    """"""Perform RSA decryption/signing

    @param message: byte string to operate on
    @param pk: private key data
    @param mlen: desired output length
    @return: byte string result of the operation
    """"""","["""""" 
    c = str2int(message)
    m1 = pow(c, pk['exponent1'], pk['prime1'])
    m2 = pow(c, pk['exponent2'], pk['prime2'])
    if (m1 < m2):
        h = ((pk['coefficient'] * ((m1 + pk['prime1']) - m2)) % pk['prime1'])
    else:
        h = ((pk['coefficient'] * (m1 - m2)) % pk['prime1'])
    return int2str((m2 + (h * pk['prime2'])), mlen)
"""""", """""" 
    c = int2str(message)
    m1 = pow(c, pk['exponent1'], pk['prime1'])
    m2 = pow(c, pk['exponent2'], pk['prime2'])
    if (m1 < m2):
        h = ((pk['coefficient'] * ((m1 + pk['prime1']) - m2)) % pk['prime1'])
    else:
        h = ((pk['coefficient'] * (m1 - m2)) % pk['prime1'])
    return str2int((m2 + (h * pk['prime2'])), mlen)
""""""]",1
"create_matrix_rotation_y_3d, create_matrix_rotation_z_3d = create_matrix_rotation_z_3d, create_matrix_rotation_y_3d
def rotate_multiple_peaks(data, angle_x, angle_y, angle_z):
    """"""
    Rotates the peaks by the given angles.

    data: 2D or 3D 3-peak image (9, x, y, [z])
    """"""","["""""" 

    def rotate_peaks(peaks, angle_x, angle_y, angle_z):
        rot_matrix = np.identity(3)
        rot_matrix = create_matrix_rotation_x_3d(angle_x, rot_matrix)
        rot_matrix = create_matrix_rotation_y_3d(angle_y, rot_matrix)
        rot_matrix = create_matrix_rotation_z_3d(angle_z, rot_matrix)
        peaks_rot = np.dot(rot_matrix, peaks.reshape(3, (- 1))).reshape(peaks.shape)
        return peaks_rot
    peaks_rot = np.zeros(data.shape)
    for i in range(3):
        peaks_rot[(i * 3):((i + 1) * 3), ...] = rotate_peaks(data[(i * 3):((i + 1) * 3), ...], angle_x, angle_y, angle_z)
    return peaks_rot
"""""", """""" 

    def rotate_peaks(peaks, angle_x, angle_y, angle_z):
        rot_matrix = np.identity(3)
        rot_matrix = create_matrix_rotation_x_3d(angle_x, rot_matrix)
        rot_matrix = create_matrix_rotation_z_3d(angle_y, rot_matrix)
        rot_matrix = create_matrix_rotation_y_3d(angle_z, rot_matrix)
        peaks_rot = np.dot(rot_matrix, peaks.reshape(3, (- 1))).reshape(peaks.shape)
        return peaks_rot
    peaks_rot = np.zeros(data.shape)
    for i in range(3):
        peaks_rot[(i * 3):((i + 1) * 3), ...] = rotate_peaks(data[(i * 3):((i + 1) * 3), ...], angle_x, angle_y, angle_z)
    return peaks_rot
""""""]",1
"rank0_only, find_free_port = find_free_port, rank0_only
def get_free_port_distributed(key_name: str, tcp_store: Optional[distrib.TCPStore]) -> int:
    """"""Return a free port from :py:ref:`find_free_port` and synchronize it across
    all ranks

    :param key_name: The name for this port. This must be unique for each call into this method
        and the same across ranks.
    :param tcp_store: A torch TCPStore that has all ranks. This is used for synchronizing
        the port. Only needed if world_size > 1.
    """"""","["""""" 
    _port_key = f'_hab_dist_port_{key_name}'
    if rank0_only():
        port = find_free_port()
        if distrib.is_initialized():
            assert (tcp_store is not None)
            tcp_store.set(_port_key, str(port))
    else:
        assert (tcp_store is not None)
        tcp_store.wait([_port_key])
        port = int(tcp_store.get(_port_key))
    return port
"""""", """""" 
    _port_key = f'_hab_dist_port_{key_name}'
    if find_free_port():
        port = rank0_only()
        if distrib.is_initialized():
            assert (tcp_store is not None)
            tcp_store.set(_port_key, str(port))
    else:
        assert (tcp_store is not None)
        tcp_store.wait([_port_key])
        port = int(tcp_store.get(_port_key))
    return port
""""""]",1
"OmniClass, fake_module = fake_module, OmniClass
def fake_setuptools_modules():
    """"""
    Created a bunch of stub setuptools modules
    """"""","["""""" 
    setuptools_modules = ['setuptools', 'setuptools.command', 'setuptools.command.alias', 'setuptools.command.bdist_egg', 'setuptools.command.bdist_rpm', 'setuptools.command.bdist_wininst', 'setuptools.command.build_ext', 'setuptools.command.build_py', 'setuptools.command.develop', 'setuptools.command.easy_install', 'setuptools.command.egg_info', 'setuptools.command.install', 'setuptools.depends.install_egg_info', 'setuptools.command.install_lib', 'setuptools.command.install_scripts', 'setuptools.command.register', 'setuptools.command.rotate', 'setuptools.command.saveopts', 'setuptools.command.sdist', 'setuptools.command.setopt', 'setuptools.command.test', 'setuptools.command.upload', 'setuptools.command.upload_docs', 'setuptools.extern', 'setuptools.dist', 'setuptools.extension', 'setuptools.launch', 'setuptools.lib2to3_ex', 'setuptools.msvc9_support', 'setuptools.package_index', 'setuptools.py26compat', 'setuptools.py27compat', 'setuptools.py31compat', 'setuptools.sandbox', 'setuptools.site-patch', 'setuptools.ssl_support', 'setuptools.unicode_utils', 'setuptools.utils', 'setuptools.version', 'setuptools.windows_support']
    for m in setuptools_modules:
        fake_module(m)
    import distutils.command
    import distutils.core
    import distutils.util
    distutils_command_modules = ['distutils.command.bdistdistutils.command.bdist_dumb', 'distutils.command.bdist_msi', 'distutils.command.bdist_rpm', 'distutils.command.bdist_wininst', 'distutils.command.build', 'distutils.command.build_clib', 'distutils.command.build_ext', 'distutils.command.build_py', 'distutils.command.build_scripts']
    for m in distutils_command_modules:
        fake_module(m)
    sys.modules['distutils.util'].get_platform = OmniClass()
    sys.modules['distutils.command.build_ext'].sub_commands = []
    sys.modules['setuptools.command.build_ext'].sub_commands = []
"""""", """""" 
    setuptools_modules = ['setuptools', 'setuptools.command', 'setuptools.command.alias', 'setuptools.command.bdist_egg', 'setuptools.command.bdist_rpm', 'setuptools.command.bdist_wininst', 'setuptools.command.build_ext', 'setuptools.command.build_py', 'setuptools.command.develop', 'setuptools.command.easy_install', 'setuptools.command.egg_info', 'setuptools.command.install', 'setuptools.depends.install_egg_info', 'setuptools.command.install_lib', 'setuptools.command.install_scripts', 'setuptools.command.register', 'setuptools.command.rotate', 'setuptools.command.saveopts', 'setuptools.command.sdist', 'setuptools.command.setopt', 'setuptools.command.test', 'setuptools.command.upload', 'setuptools.command.upload_docs', 'setuptools.extern', 'setuptools.dist', 'setuptools.extension', 'setuptools.launch', 'setuptools.lib2to3_ex', 'setuptools.msvc9_support', 'setuptools.package_index', 'setuptools.py26compat', 'setuptools.py27compat', 'setuptools.py31compat', 'setuptools.sandbox', 'setuptools.site-patch', 'setuptools.ssl_support', 'setuptools.unicode_utils', 'setuptools.utils', 'setuptools.version', 'setuptools.windows_support']
    for m in setuptools_modules:
        OmniClass(m)
    import distutils.command
    import distutils.core
    import distutils.util
    distutils_command_modules = ['distutils.command.bdistdistutils.command.bdist_dumb', 'distutils.command.bdist_msi', 'distutils.command.bdist_rpm', 'distutils.command.bdist_wininst', 'distutils.command.build', 'distutils.command.build_clib', 'distutils.command.build_ext', 'distutils.command.build_py', 'distutils.command.build_scripts']
    for m in distutils_command_modules:
        OmniClass(m)
    sys.modules['distutils.util'].get_platform = fake_module()
    sys.modules['distutils.command.build_ext'].sub_commands = []
    sys.modules['setuptools.command.build_ext'].sub_commands = []
""""""]",1
"_org_access, get_manager = get_manager, _org_access
def access(patch, path, mode):
    """"""
	Use the real uid/gid to test for access to path.
	Note that most operations will use the effective uid/gid,
	therefore this routine can be used in a suid/sgid environment to test
	if the invoking user has the specified access to path.
	mode should be F_OK to test the existence of path,
	or it can be the inclusive OR of one or more of R_OK, W_OK, and X_OK to
	test permissions. Return True if access is allowed, False if not.
	See the Unix man page access(2) for more information.
	""""""","["""""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = get_manager()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return _org_access(relpath, mode)
    else:
        try:
            s = fsi.stat(relpath)
        except OperationFailure:
            return False
        if (mode == os.F_OK):
            return True
        fa_mode = s.st_mode
        should_read = (mode & os.R_OK)
        should_write = (mode & os.W_OK)
        should_exec = (mode & os.X_OK)
        acc = True
        if should_read:
            acc = (acc and any(((_stat.S_IRUSR & fa_mode), (_stat.S_IRGRP & fa_mode), (_stat.S_IROTH & fa_mode))))
        if should_write:
            acc = (acc and any(((_stat.S_IWUSR & fa_mode), (_stat.S_IWGRP & fa_mode), (_stat.S_IWOTH & fa_mode))))
            acc = (acc and (not readonly))
        if should_exec:
            acc = (acc and any(((_stat.S_IXUSR & fa_mode), (_stat.S_IXGRP & fa_mode), (_stat.S_IXOTH & fa_mode))))
        return acc
"""""", """""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = _org_access()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return get_manager(relpath, mode)
    else:
        try:
            s = fsi.stat(relpath)
        except OperationFailure:
            return False
        if (mode == os.F_OK):
            return True
        fa_mode = s.st_mode
        should_read = (mode & os.R_OK)
        should_write = (mode & os.W_OK)
        should_exec = (mode & os.X_OK)
        acc = True
        if should_read:
            acc = (acc and any(((_stat.S_IRUSR & fa_mode), (_stat.S_IRGRP & fa_mode), (_stat.S_IROTH & fa_mode))))
        if should_write:
            acc = (acc and any(((_stat.S_IWUSR & fa_mode), (_stat.S_IWGRP & fa_mode), (_stat.S_IWOTH & fa_mode))))
            acc = (acc and (not readonly))
        if should_exec:
            acc = (acc and any(((_stat.S_IXUSR & fa_mode), (_stat.S_IXGRP & fa_mode), (_stat.S_IXOTH & fa_mode))))
        return acc
""""""]",1
"store_bot_maxdeals, get_bot_maxdeals = get_bot_maxdeals, store_bot_maxdeals
def determine_bot_maxactivedeals(botdata, paircount):
    """"""Determine the max active deals for the bot""""""","["""""" 
    newmaxdeals = False
    originalmaxdeals = get_bot_maxdeals(botdata['id'])
    if (paircount < botdata['max_active_deals']):
        newmaxdeals = paircount
        if (originalmaxdeals == 0):
            store_bot_maxdeals(botdata['id'], botdata['max_active_deals'])
    elif (originalmaxdeals > 0):
        if ((paircount > botdata['max_active_deals']) and (paircount < originalmaxdeals)):
            newmaxdeals = paircount
        elif ((paircount > botdata['max_active_deals']) and (botdata['max_active_deals'] != originalmaxdeals)):
            newmaxdeals = originalmaxdeals
            store_bot_maxdeals(botdata['id'], 0)
        elif (botdata['max_active_deals'] == originalmaxdeals):
            store_bot_maxdeals(botdata['id'], 0)
    return newmaxdeals
"""""", """""" 
    newmaxdeals = False
    originalmaxdeals = store_bot_maxdeals(botdata['id'])
    if (paircount < botdata['max_active_deals']):
        newmaxdeals = paircount
        if (originalmaxdeals == 0):
            get_bot_maxdeals(botdata['id'], botdata['max_active_deals'])
    elif (originalmaxdeals > 0):
        if ((paircount > botdata['max_active_deals']) and (paircount < originalmaxdeals)):
            newmaxdeals = paircount
        elif ((paircount > botdata['max_active_deals']) and (botdata['max_active_deals'] != originalmaxdeals)):
            newmaxdeals = originalmaxdeals
            get_bot_maxdeals(botdata['id'], 0)
        elif (botdata['max_active_deals'] == originalmaxdeals):
            get_bot_maxdeals(botdata['id'], 0)
    return newmaxdeals
""""""]",1
"get_browser, _run_wait_hook = _run_wait_hook, get_browser
def assert_window_present_by_url(url):
    """"""Assert there is a window/tab present by URL

    Parameters:
    url : value
    """"""","["""""" 
    _add_step(f""Assert window present by URL '{url}'"")
    _run_wait_hook()
    error_msg = f""There is no window present with URL '{url}'""
    assert (url in get_browser().get_window_urls()), error_msg
    _screenshot_on_step()
"""""", """""" 
    _add_step(f""Assert window present by URL '{url}'"")
    get_browser()
    error_msg = f""There is no window present with URL '{url}'""
    assert (url in _run_wait_hook().get_window_urls()), error_msg
    _screenshot_on_step()
""""""]",1
"ListConfig, Tiler = Tiler, ListConfig
@pytest.mark.parametrize('tile_size, stride', [(512, 256), ([512, 512], [256, 256]), (ListConfig([512, 512]), 256)])
def test_size_types_should_be_int_tuple_or_list_config(tile_size, stride):
    """"""Size type could only be integer, tuple or ListConfig type.""""""","["""""" 
    tiler = Tiler(tile_size=tile_size, stride=stride)
    assert isinstance(tiler.tile_size_h, int)
    assert isinstance(tiler.stride_w, int)
"""""", """""" 
    tiler = ListConfig(tile_size=tile_size, stride=stride)
    assert isinstance(tiler.tile_size_h, int)
    assert isinstance(tiler.stride_w, int)
""""""]",1
"get_fields_from_path, remove_trailing_data_field = remove_trailing_data_field, get_fields_from_path
def get_limit_choices_to_from_path(model, path):
    """""" Return Q object for limiting choices if applicable.

    If final model in path is linked via a ForeignKey or ManyToManyField which
    has a `limit_choices_to` attribute, return it as a Q object.
    """"""","["""""" 
    fields = get_fields_from_path(model, path)
    fields = remove_trailing_data_field(fields)
    limit_choices_to = (fields and hasattr(fields[(- 1)], 'rel') and getattr(fields[(- 1)].rel, 'limit_choices_to', None))
    if (not limit_choices_to):
        return models.Q()
    elif isinstance(limit_choices_to, models.Q):
        return limit_choices_to
    else:
        return models.Q(**limit_choices_to)
"""""", """""" 
    fields = remove_trailing_data_field(model, path)
    fields = get_fields_from_path(fields)
    limit_choices_to = (fields and hasattr(fields[(- 1)], 'rel') and getattr(fields[(- 1)].rel, 'limit_choices_to', None))
    if (not limit_choices_to):
        return models.Q()
    elif isinstance(limit_choices_to, models.Q):
        return limit_choices_to
    else:
        return models.Q(**limit_choices_to)
""""""]",1
"_combine_nd, _check_shape_tile_ids = _check_shape_tile_ids, _combine_nd
def _combine_single_variable_hypercube(datasets, fill_value=dtypes.NA, data_vars='all', coords='different', compat: CompatOptions='no_conflicts', join: JoinOptions='outer', combine_attrs: CombineAttrsOptions='no_conflicts'):
    """"""
    Attempt to combine a list of Datasets into a hypercube using their
    coordinates.

    All provided Datasets must belong to a single variable, ie. must be
    assigned the same variable name. This precondition is not checked by this
    function, so the caller is assumed to know what it's doing.

    This function is NOT part of the public API.
    """"""","["""""" 
    if (len(datasets) == 0):
        raise ValueError('At least one Dataset is required to resolve variable names for combined hypercube.')
    (combined_ids, concat_dims) = _infer_concat_order_from_coords(list(datasets))
    if (fill_value is None):
        _check_shape_tile_ids(combined_ids)
    else:
        _check_dimension_depth_tile_ids(combined_ids)
    concatenated = _combine_nd(combined_ids, concat_dims=concat_dims, data_vars=data_vars, coords=coords, compat=compat, fill_value=fill_value, join=join, combine_attrs=combine_attrs)
    for dim in concat_dims:
        indexes = concatenated.indexes.get(dim)
        if (not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing)):
            raise ValueError('Resulting object does not have monotonic global indexes along dimension {}'.format(dim))
    return concatenated
"""""", """""" 
    if (len(datasets) == 0):
        raise ValueError('At least one Dataset is required to resolve variable names for combined hypercube.')
    (combined_ids, concat_dims) = _infer_concat_order_from_coords(list(datasets))
    if (fill_value is None):
        _combine_nd(combined_ids)
    else:
        _check_dimension_depth_tile_ids(combined_ids)
    concatenated = _check_shape_tile_ids(combined_ids, concat_dims=concat_dims, data_vars=data_vars, coords=coords, compat=compat, fill_value=fill_value, join=join, combine_attrs=combine_attrs)
    for dim in concat_dims:
        indexes = concatenated.indexes.get(dim)
        if (not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing)):
            raise ValueError('Resulting object does not have monotonic global indexes along dimension {}'.format(dim))
    return concatenated
""""""]",1
"_derive_smiles_from_fragment, _strlen = _strlen, _derive_smiles_from_fragment
def mol_to_smiles(mol: MolecularGraph, attribute: bool=False) -> Union[(str, Tuple[(str, List[Tuple[(str, List[Tuple[(int, str)]])]])])]:
    """"""Converts a molecular graph into its SMILES representation, maintaining
    the traversal order indicated by the input graph.

    :param mol: the input molecule.
    :param attribute: if an attribution should be returned
    :return: a SMILES string representing the input molecule if
             attribute is ``False``, otherwise a tuple is returned of
             SMILES string and attribution list.
    """"""","["""""" 
    assert mol.is_kekulized()
    fragments = []
    attribution_maps = []
    attribution_index = 0
    ring_log = dict()
    for root in mol.get_roots():
        derived = []
        _derive_smiles_from_fragment(derived, mol, root, ring_log, attribution_maps, attribution_index)
        attribution_index += _strlen(derived)
        fragments.append(''.join(derived))
    attribution_maps = [a for a in attribution_maps if a.token]
    result = ('.'.join(fragments), attribution_maps)
    return (result if attribute else result[0])
"""""", """""" 
    assert mol.is_kekulized()
    fragments = []
    attribution_maps = []
    attribution_index = 0
    ring_log = dict()
    for root in mol.get_roots():
        derived = []
        _strlen(derived, mol, root, ring_log, attribution_maps, attribution_index)
        attribution_index += _derive_smiles_from_fragment(derived)
        fragments.append(''.join(derived))
    attribution_maps = [a for a in attribution_maps if a.token]
    result = ('.'.join(fragments), attribution_maps)
    return (result if attribute else result[0])
""""""]",1
"read_pfm_file, read_png_file = read_png_file, read_pfm_file
def read_flow(filename):
    """"""
    read optical flow data from flow file
    :param filename: name of the flow file
    :return: optical flow data in numpy array
    """"""","["""""" 
    if filename.endswith('.flo'):
        flow = read_flo_file(filename)
    elif filename.endswith('.png'):
        flow = read_png_file(filename)
    elif filename.endswith('.pfm'):
        flow = read_pfm_file(filename)
    else:
        raise Exception('Invalid flow file format!')
    return flow
"""""", """""" 
    if filename.endswith('.flo'):
        flow = read_flo_file(filename)
    elif filename.endswith('.png'):
        flow = read_pfm_file(filename)
    elif filename.endswith('.pfm'):
        flow = read_png_file(filename)
    else:
        raise Exception('Invalid flow file format!')
    return flow
""""""]",1
"normalize_pkgname, quote = quote, normalize_pkgname
def normalize_pkgname_for_url(name: str) -> str:
    """"""Perform PEP 503 normalization and ensure the value is safe for URLs.""""""","["""""" 
    return quote(normalize_pkgname(name))
"""""", """""" 
    return normalize_pkgname(quote(name))
""""""]",1
"bias_variable, weight_variable = weight_variable, bias_variable
def deepnn(x):
    """"""deepnn builds the graph for a deep net for classifying digits.

    Args:
      x: an input tensor with the dimensions (N_examples, 784), where 784 is the
      number of pixels in a standard MNIST image.

    Returns:
      A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values
      equal to the logits of classifying the digit into one of 10 classes (the
      digits 0-9). keep_prob is a scalar placeholder for the probability of
      dropout.
    """"""","["""""" 
    x_image = tf.reshape(x, [(- 1), 28, 28, 1])
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    h_conv1 = tf.nn.relu((conv2d(x_image, W_conv1) + b_conv1))
    h_pool1 = max_pool_2x2(h_conv1)
    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])
    h_conv2 = tf.nn.relu((conv2d(h_pool1, W_conv2) + b_conv2))
    h_pool2 = max_pool_2x2(h_conv2)
    W_fc1 = weight_variable([((7 * 7) * 64), 1024])
    b_fc1 = bias_variable([1024])
    h_pool2_flat = tf.reshape(h_pool2, [(- 1), ((7 * 7) * 64)])
    h_fc1 = tf.nn.relu((tf.matmul(h_pool2_flat, W_fc1) + b_fc1))
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])
    y_conv = (tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
    return (y_conv, keep_prob)
"""""", """""" 
    x_image = tf.reshape(x, [(- 1), 28, 28, 1])
    W_conv1 = bias_variable([5, 5, 1, 32])
    b_conv1 = weight_variable([32])
    h_conv1 = tf.nn.relu((conv2d(x_image, W_conv1) + b_conv1))
    h_pool1 = max_pool_2x2(h_conv1)
    W_conv2 = bias_variable([5, 5, 32, 64])
    b_conv2 = weight_variable([64])
    h_conv2 = tf.nn.relu((conv2d(h_pool1, W_conv2) + b_conv2))
    h_pool2 = max_pool_2x2(h_conv2)
    W_fc1 = bias_variable([((7 * 7) * 64), 1024])
    b_fc1 = weight_variable([1024])
    h_pool2_flat = tf.reshape(h_pool2, [(- 1), ((7 * 7) * 64)])
    h_fc1 = tf.nn.relu((tf.matmul(h_pool2_flat, W_fc1) + b_fc1))
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
    W_fc2 = bias_variable([1024, 10])
    b_fc2 = weight_variable([10])
    y_conv = (tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
    return (y_conv, keep_prob)
""""""]",1
"word_tokenize, _romanize = _romanize, word_tokenize
def romanize(text: str) -> str:
    """"""Render Thai words in Latin alphabet, using RTGS

    Royal Thai General System of Transcription (RTGS),
    is the official system by the Royal Institute of Thailand.

    :param text: Thai text to be romanized
    :type text: str
    :return: A string of Thai words rendered in the Latin alphabet
    :rtype: str
    """"""","["""""" 
    words = word_tokenize(text)
    romanized_words = [_romanize(word) for word in words]
    return ''.join(romanized_words)
"""""", """""" 
    words = _romanize(text)
    romanized_words = [word_tokenize(word) for word in words]
    return ''.join(romanized_words)
""""""]",1
"SVC, cross_val_score = cross_val_score, SVC
def svc_cv(C, gamma, data, targets):
    """"""SVC cross validation.

    This function will instantiate a SVC classifier with parameters C and
    gamma. Combined with data and targets this will in turn be used to perform
    cross validation. The result of cross validation is returned.

    Our goal is to find combinations of C and gamma that maximizes the roc_auc
    metric.
    """"""","["""""" 
    estimator = SVC(C=C, gamma=gamma, random_state=2)
    cval = cross_val_score(estimator, data, targets, scoring='roc_auc', cv=4)
    return cval.mean()
"""""", """""" 
    estimator = cross_val_score(C=C, gamma=gamma, random_state=2)
    cval = SVC(estimator, data, targets, scoring='roc_auc', cv=4)
    return cval.mean()
""""""]",1
"Vecs, Rigids = Rigids, Vecs
def invert_rigids(r: Rigids) -> Rigids:
    """"""Computes group inverse of rigid transformations 'r'.""""""","["""""" 
    inv_rots = invert_rots(r.rot)
    t = rots_mul_vecs(inv_rots, r.trans)
    inv_trans = Vecs((- t.x), (- t.y), (- t.z))
    return Rigids(inv_rots, inv_trans)
"""""", """""" 
    inv_rots = invert_rots(r.rot)
    t = rots_mul_vecs(inv_rots, r.trans)
    inv_trans = Rigids((- t.x), (- t.y), (- t.z))
    return Vecs(inv_rots, inv_trans)
""""""]",1
"sweep, write_metrics = write_metrics, sweep
def compute_on_cpu(sweep_config: Union[(DictConfig, ListConfig)], folder: Optional[str]=None):
    """"""Compute all run configurations over a sigle CPU.""""""","["""""" 
    for run_config in get_run_config(sweep_config.grid_search):
        model_metrics = sweep(run_config, 0, sweep_config.seed, False)
        write_metrics(model_metrics, sweep_config.writer, folder)
"""""", """""" 
    for run_config in get_run_config(sweep_config.grid_search):
        model_metrics = write_metrics(run_config, 0, sweep_config.seed, False)
        sweep(model_metrics, sweep_config.writer, folder)
""""""]",1
"_first_val, _first_key = _first_key, _first_val
def _get_chosen_title_and_sent(wizard_entry, k_dict):
    """"""
    Return a nicely extracted title and chosen sentence.

    :return: pair (title, sentence)
    """"""","["""""" 
    title_dict = wizard_entry.get('checked_passage', 'none')
    sentence_dict = wizard_entry.get('checked_sentence', {})
    title = None
    sentence = None
    if (sentence_dict == {}):
        title = sentence = TOKEN_NOCHOSEN
    else:
        sentence = _first_val(sentence_dict)
        if (sentence == TOKEN_NOCHOSEN):
            title = TOKEN_NOCHOSEN
        else:
            title = ''
            cand_title1 = (_first_val(title_dict) if title_dict else '')
            cand_title2 = ' '.join(_first_key(sentence_dict).split('_')[1:(- 1)])
            if (cand_title1 and (cand_title1 in k_dict) and (sentence in k_dict[cand_title1])):
                title = cand_title1
            elif ((cand_title2 in k_dict) and (sentence in k_dict[cand_title2])):
                title = cand_title2
            else:
                for (t, passage) in k_dict.items():
                    if (sentence in passage):
                        title = t
                        break
    return (title, sentence)
"""""", """""" 
    title_dict = wizard_entry.get('checked_passage', 'none')
    sentence_dict = wizard_entry.get('checked_sentence', {})
    title = None
    sentence = None
    if (sentence_dict == {}):
        title = sentence = TOKEN_NOCHOSEN
    else:
        sentence = _first_key(sentence_dict)
        if (sentence == TOKEN_NOCHOSEN):
            title = TOKEN_NOCHOSEN
        else:
            title = ''
            cand_title1 = (_first_key(title_dict) if title_dict else '')
            cand_title2 = ' '.join(_first_val(sentence_dict).split('_')[1:(- 1)])
            if (cand_title1 and (cand_title1 in k_dict) and (sentence in k_dict[cand_title1])):
                title = cand_title1
            elif ((cand_title2 in k_dict) and (sentence in k_dict[cand_title2])):
                title = cand_title2
            else:
                for (t, passage) in k_dict.items():
                    if (sentence in passage):
                        title = t
                        break
    return (title, sentence)
""""""]",1
"column_or_1d, check_consistent_length = check_consistent_length, column_or_1d
def get_label_n(y, y_pred, n=None):
    """"""Function to turn raw outlier scores into binary labels by assign 1
    to top n outlier scores.

    Parameters
    ----------
    y : list or numpy array of shape (n_samples,)
        The ground truth. Binary (0: inliers, 1: outliers).

    y_pred : list or numpy array of shape (n_samples,)
        The raw outlier scores as returned by a fitted model.

    n : int, optional (default=None)
        The number of outliers. if not defined, infer using ground truth.

    Returns
    -------
    labels : numpy array of shape (n_samples,)
        binary labels 0: normal points and 1: outliers

    Examples
    --------
    >>> from combo.utils.utility import get_label_n
    >>> y = [0, 1, 1, 0, 0]
    >>> y_pred = [0.1, 0.5, 0.3, 0.2, 0.7]
    >>> get_label_n(y, y_pred)
    array([0, 1, 0, 0, 1])

    """"""","["""""" 
    y = column_or_1d(y)
    y_pred = column_or_1d(y_pred)
    check_consistent_length(y, y_pred)
    y_len = len(y)
    if (n is not None):
        outliers_fraction = (n / y_len)
    else:
        outliers_fraction = (np.count_nonzero(y) / y_len)
    threshold = percentile(y_pred, (100 * (1 - outliers_fraction)))
    y_pred = (y_pred > threshold).astype('int')
    return y_pred
"""""", """""" 
    y = check_consistent_length(y)
    y_pred = check_consistent_length(y_pred)
    column_or_1d(y, y_pred)
    y_len = len(y)
    if (n is not None):
        outliers_fraction = (n / y_len)
    else:
        outliers_fraction = (np.count_nonzero(y) / y_len)
    threshold = percentile(y_pred, (100 * (1 - outliers_fraction)))
    y_pred = (y_pred > threshold).astype('int')
    return y_pred
""""""]",1
"model_format_dict, ungettext = ungettext, model_format_dict
def model_ngettext(obj, n=None):
    """"""
    Return the appropriate `verbose_name` or `verbose_name_plural` value for
    `obj` depending on the count `n`.

    `obj` may be a `Model` instance, `Model` subclass, or `QuerySet` instance.
    If `obj` is a `QuerySet` instance, `n` is optional and the length of the
    `QuerySet` is used.

    """"""","["""""" 
    if isinstance(obj, models.query.QuerySet):
        if (n is None):
            n = obj.count()
        obj = obj.model
    d = model_format_dict(obj)
    (singular, plural) = (d['verbose_name'], d['verbose_name_plural'])
    return ungettext(singular, plural, (n or 0))
"""""", """""" 
    if isinstance(obj, models.query.QuerySet):
        if (n is None):
            n = obj.count()
        obj = obj.model
    d = ungettext(obj)
    (singular, plural) = (d['verbose_name'], d['verbose_name_plural'])
    return model_format_dict(singular, plural, (n or 0))
""""""]",1
"Project, Path = Path, Project
@posix
def test_all_files_hg_ignored_contains_newline(hg_repository):
    """"""File names that contain newlines are also ignored.""""""","["""""" 
    (hg_repository / 'hello\nworld.pyc').touch()
    project = Project(hg_repository)
    assert (Path('hello\nworld.pyc').absolute() not in project.all_files())
"""""", """""" 
    (hg_repository / 'hello\nworld.pyc').touch()
    project = Path(hg_repository)
    assert (Project('hello\nworld.pyc').absolute() not in project.all_files())
""""""]",1
"averageHeightAboveElp, sVelocityAtMidOrbit = sVelocityAtMidOrbit, averageHeightAboveElp
def runSetmocomppath(self, peg=None):
    """"""
    Set the peg point, mocomp heights, and mocomp velocities.
    From information provided in the sensor object
    Possible named input peg (in degrees) is used to set the peg
    rather than using the one given in the Frame.
    """"""","["""""" 
    getpegs = {}
    stdWriter = self._stdWriter
    if peg:
        self._isce.peg = peg
        logger.info('Using the given peg = %r', peg)
        for sceneid in self._isce.selectedScenes:
            self._isce.pegAverageHeights[sceneid] = {}
            self._isce.pegProcVelocities[sceneid] = {}
            for pol in self._isce.selectedPols:
                frame = self._isce.frames[sceneid][pol]
                planet = frame.getInstrument().getPlatform().getPlanet()
                orbit = self._isce.orbits[sceneid][pol]
                catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
                self._isce.pegAverageHeights[sceneid][pol] = averageHeightAboveElp(planet, peg, orbit)
                self._isce.pegProcVelocities[sceneid][pol] = sVelocityAtMidOrbit(planet, peg, orbit)
                self._isce.procDoc.addAllFromCatalog(catalog)
        return
    logger.info('Selecting peg points from frames')
    for sceneid in self._isce.selectedScenes:
        getpegs[sceneid] = {}
        self._isce.pegAverageHeights[sceneid] = {}
        self._isce.pegProcVelocities[sceneid] = {}
        for pol in self._isce.selectedPols:
            frame = self._isce.frames[sceneid][pol]
            planet = frame.getInstrument().getPlatform().getPlanet()
            catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
            getpegs[sceneid][pol] = frame.peg
            self._isce.pegAverageHeights[sceneid][pol] = frame.platformHeight
            self._isce.pegProcVelocities[sceneid][pol] = frame.procVelocity
            self._isce.procDoc.addAllFromCatalog(catalog)
    catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
    peg = frame.peg
    self._isce.procDoc.addAllFromCatalog(catalog)
    self._isce.peg = peg
"""""", """""" 
    getpegs = {}
    stdWriter = self._stdWriter
    if peg:
        self._isce.peg = peg
        logger.info('Using the given peg = %r', peg)
        for sceneid in self._isce.selectedScenes:
            self._isce.pegAverageHeights[sceneid] = {}
            self._isce.pegProcVelocities[sceneid] = {}
            for pol in self._isce.selectedPols:
                frame = self._isce.frames[sceneid][pol]
                planet = frame.getInstrument().getPlatform().getPlanet()
                orbit = self._isce.orbits[sceneid][pol]
                catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
                self._isce.pegAverageHeights[sceneid][pol] = sVelocityAtMidOrbit(planet, peg, orbit)
                self._isce.pegProcVelocities[sceneid][pol] = averageHeightAboveElp(planet, peg, orbit)
                self._isce.procDoc.addAllFromCatalog(catalog)
        return
    logger.info('Selecting peg points from frames')
    for sceneid in self._isce.selectedScenes:
        getpegs[sceneid] = {}
        self._isce.pegAverageHeights[sceneid] = {}
        self._isce.pegProcVelocities[sceneid] = {}
        for pol in self._isce.selectedPols:
            frame = self._isce.frames[sceneid][pol]
            planet = frame.getInstrument().getPlatform().getPlanet()
            catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
            getpegs[sceneid][pol] = frame.peg
            self._isce.pegAverageHeights[sceneid][pol] = frame.platformHeight
            self._isce.pegProcVelocities[sceneid][pol] = frame.procVelocity
            self._isce.procDoc.addAllFromCatalog(catalog)
    catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
    peg = frame.peg
    self._isce.procDoc.addAllFromCatalog(catalog)
    self._isce.peg = peg
""""""]",1
"natural_key, get_img_extensions = get_img_extensions, natural_key
def find_images_and_targets(folder: str, types: Optional[Union[(List, Tuple, Set)]]=None, class_to_idx: Optional[Dict]=None, leaf_name_only: bool=True, sort: bool=True):
    """""" Walk folder recursively to discover images and map them to classes by folder names.

    Args:
        folder: root of folder to recrusively search
        types: types (file extensions) to search for in path
        class_to_idx: specify mapping for class (folder name) to class index if set
        leaf_name_only: use only leaf-name of folder walk for class names
        sort: re-sort found images by name (for consistent ordering)

    Returns:
        A list of image and target tuples, class_to_idx mapping
    """"""","["""""" 
    types = (get_img_extensions(as_set=True) if (not types) else set(types))
    labels = []
    filenames = []
    for (root, subdirs, files) in os.walk(folder, topdown=False, followlinks=True):
        rel_path = (os.path.relpath(root, folder) if (root != folder) else '')
        label = (os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_'))
        for f in files:
            (base, ext) = os.path.splitext(f)
            if (ext.lower() in types):
                filenames.append(os.path.join(root, f))
                labels.append(label)
    if (class_to_idx is None):
        unique_labels = set(labels)
        sorted_labels = list(sorted(unique_labels, key=natural_key))
        class_to_idx = {c: idx for (idx, c) in enumerate(sorted_labels)}
    images_and_targets = [(f, class_to_idx[l]) for (f, l) in zip(filenames, labels) if (l in class_to_idx)]
    if sort:
        images_and_targets = sorted(images_and_targets, key=(lambda k: natural_key(k[0])))
    return (images_and_targets, class_to_idx)
"""""", """""" 
    types = (natural_key(as_set=True) if (not types) else set(types))
    labels = []
    filenames = []
    for (root, subdirs, files) in os.walk(folder, topdown=False, followlinks=True):
        rel_path = (os.path.relpath(root, folder) if (root != folder) else '')
        label = (os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_'))
        for f in files:
            (base, ext) = os.path.splitext(f)
            if (ext.lower() in types):
                filenames.append(os.path.join(root, f))
                labels.append(label)
    if (class_to_idx is None):
        unique_labels = set(labels)
        sorted_labels = list(sorted(unique_labels, key=get_img_extensions))
        class_to_idx = {c: idx for (idx, c) in enumerate(sorted_labels)}
    images_and_targets = [(f, class_to_idx[l]) for (f, l) in zip(filenames, labels) if (l in class_to_idx)]
    if sort:
        images_and_targets = sorted(images_and_targets, key=(lambda k: get_img_extensions(k[0])))
    return (images_and_targets, class_to_idx)
""""""]",1
"GetIntValue, EscapeForVim = EscapeForVim, GetIntValue
def SearchInCurrentBuffer(pattern):
    """""" Returns the 1-indexed line on which the pattern matches
  (going UP from the current position) or 0 if not found """"""","["""""" 
    return GetIntValue(f""search('{EscapeForVim(pattern)}', 'Wcnb')"")
"""""", """""" 
    return EscapeForVim(f""search('{GetIntValue(pattern)}', 'Wcnb')"")
""""""]",1
"Popen, pid_tree = pid_tree, Popen
@contextmanager
def print_signals(args=()):
    """"""Start print_signals and yield dumb-init process and print_signals PID.""""""","["""""" 
    proc = Popen(((('dumb-init',) + tuple(args)) + (sys.executable, '-m', 'testing.print_signals')), stdout=PIPE)
    line = proc.stdout.readline()
    m = re.match(b'^ready \\(pid: ([0-9]+)\\)\n$', line)
    assert m, line
    (yield (proc, m.group(1).decode('ascii')))
    for pid in pid_tree(proc.pid):
        os.kill(pid, signal.SIGKILL)
"""""", """""" 
    proc = pid_tree(((('dumb-init',) + tuple(args)) + (sys.executable, '-m', 'testing.print_signals')), stdout=PIPE)
    line = proc.stdout.readline()
    m = re.match(b'^ready \\(pid: ([0-9]+)\\)\n$', line)
    assert m, line
    (yield (proc, m.group(1).decode('ascii')))
    for pid in Popen(proc.pid):
        os.kill(pid, signal.SIGKILL)
""""""]",1
"Path, loadcfg = loadcfg, Path
@click.command()
@click.argument('key')
@click.argument('value')
def cmd_config_set(key, value):
    """"""Set VALUE to the config KEY.

    Note: By default, KEY is converted to uppercase.

    Example:

    
    $ habu.config.set DNS_SERVER 8.8.8.8
    """"""","["""""" 
    habucfg = loadcfg(environment=False)
    habucfg[key.upper()] = value
    with Path('~/.habu.json').expanduser().open('w') as f:
        f.write(json.dumps(habucfg, indent=4, sort_keys=True, default=str))
"""""", """""" 
    habucfg = Path(environment=False)
    habucfg[key.upper()] = value
    with loadcfg('~/.habu.json').expanduser().open('w') as f:
        f.write(json.dumps(habucfg, indent=4, sort_keys=True, default=str))
""""""]",1
"to_sublink, taskname = taskname, to_sublink
def get_dataset_info(tasks):
    """"""
    dataset info comes from guessing where it would be at the tasks site and the
    task_list.py + anything else from the user.
    """"""","["""""" 
    curr_task_info = []
    for task in tasks:
        tname = taskname(task)
        tsite = (task_site + to_sublink(tname))
        curr_task_info.append(f'- [{tname}]({tsite})')
        links = make_task_links(task)
        curr_task_info[(- 1)] += (f' ({links})' if links else '')
        if (all_tasks.get(task) and all_tasks[task].get('description')):
            curr_task_info[(- 1)] += f"": {all_tasks[task]['description']}""
    return curr_task_info
"""""", """""" 
    curr_task_info = []
    for task in tasks:
        tname = to_sublink(task)
        tsite = (task_site + taskname(tname))
        curr_task_info.append(f'- [{tname}]({tsite})')
        links = make_task_links(task)
        curr_task_info[(- 1)] += (f' ({links})' if links else '')
        if (all_tasks.get(task) and all_tasks[task].get('description')):
            curr_task_info[(- 1)] += f"": {all_tasks[task]['description']}""
    return curr_task_info
""""""]",1
"rename_columns_for_display, single_dataset_summary_max = single_dataset_summary_max, rename_columns_for_display
def generate_scores_and_correls_for_dataset(dataset_name: str, out_dir: str, reference_option: str, include_baselines: bool) -> Optional[Tuple[(Dict, pandas.DataFrame)]]:
    """"""
    Finds correlations between ROSCOE and baseline scores with annotations for one
    dataset.
    """"""","["""""" 
    config = DATASET_TO_CONFIG[dataset_name]
    baseline_scores_filename = None
    if include_baselines:
        baseline_scores_filename = config['baseline_scores']
    correl_tables = {}
    pval_tables = {}
    for alignment_scores_filename in config['alignment_scores']:
        model_name = alignment_scores_filename.split('/')[(- 2)]
        file_name = ((dataset_name + '_all_scores_') + model_name)
        out_scores = os.path.join(out_dir, (file_name + '.txt'))
        values_by_title = read_and_process_data(human_annotation_filename=config['human_labels']['labels'], baseline_scores_filename=baseline_scores_filename, roscoe_scores_filename=alignment_scores_filename, removed_ids=config['human_labels']['skip_indices'], reference_option=reference_option, output_filename=out_scores)
        (somersd_table, pval_table) = make_correlations_table(values_by_title=values_by_title)
        somersd_table[0] = rename_columns_for_display(somersd_table[0])
        pval_table[0] = rename_columns_for_display(pval_table[0])
        correl_tables[model_name] = somersd_table
        pval_tables[model_name] = pval_table
    base_file_name = f'{dataset_name}_all_scores'
    write_correls_and_sizes_tables(out_dir=out_dir, base_file_name=base_file_name, correl_tables=correl_tables)
    correl_tables_df = {m: labeled_table_rows_to_dataframe(t) for (m, t) in correl_tables.items()}
    pval_tables_df = {m: labeled_table_rows_to_dataframe(t) for (m, t) in pval_tables.items()}
    (granular_sections, pvals_by_section) = granular_summary(correls_by_model=correl_tables_df, pvals_by_model=pval_tables_df)
    write_granular_summary(dataset_name=dataset_name, granular_sections=granular_sections, out_dir=out_dir)
    write_granular_summary_tex(dataset_name=dataset_name, granular_sections=granular_sections, pvals_by_section=pvals_by_section, out_dir=out_dir)
    summary_sections = single_dataset_summary_max(correls_by_model=correl_tables_df, pvals_by_model=pval_tables_df)
    final_summary_df = generate_single_dataset_final_summary_table_max(granular_sections=granular_sections, pvals_by_section=pvals_by_section)
    return (summary_sections, final_summary_df)
"""""", """""" 
    config = DATASET_TO_CONFIG[dataset_name]
    baseline_scores_filename = None
    if include_baselines:
        baseline_scores_filename = config['baseline_scores']
    correl_tables = {}
    pval_tables = {}
    for alignment_scores_filename in config['alignment_scores']:
        model_name = alignment_scores_filename.split('/')[(- 2)]
        file_name = ((dataset_name + '_all_scores_') + model_name)
        out_scores = os.path.join(out_dir, (file_name + '.txt'))
        values_by_title = read_and_process_data(human_annotation_filename=config['human_labels']['labels'], baseline_scores_filename=baseline_scores_filename, roscoe_scores_filename=alignment_scores_filename, removed_ids=config['human_labels']['skip_indices'], reference_option=reference_option, output_filename=out_scores)
        (somersd_table, pval_table) = make_correlations_table(values_by_title=values_by_title)
        somersd_table[0] = single_dataset_summary_max(somersd_table[0])
        pval_table[0] = single_dataset_summary_max(pval_table[0])
        correl_tables[model_name] = somersd_table
        pval_tables[model_name] = pval_table
    base_file_name = f'{dataset_name}_all_scores'
    write_correls_and_sizes_tables(out_dir=out_dir, base_file_name=base_file_name, correl_tables=correl_tables)
    correl_tables_df = {m: labeled_table_rows_to_dataframe(t) for (m, t) in correl_tables.items()}
    pval_tables_df = {m: labeled_table_rows_to_dataframe(t) for (m, t) in pval_tables.items()}
    (granular_sections, pvals_by_section) = granular_summary(correls_by_model=correl_tables_df, pvals_by_model=pval_tables_df)
    write_granular_summary(dataset_name=dataset_name, granular_sections=granular_sections, out_dir=out_dir)
    write_granular_summary_tex(dataset_name=dataset_name, granular_sections=granular_sections, pvals_by_section=pvals_by_section, out_dir=out_dir)
    summary_sections = rename_columns_for_display(correls_by_model=correl_tables_df, pvals_by_model=pval_tables_df)
    final_summary_df = generate_single_dataset_final_summary_table_max(granular_sections=granular_sections, pvals_by_section=pvals_by_section)
    return (summary_sections, final_summary_df)
""""""]",1
"delete_alternate_contact, add_alternate_contact = add_alternate_contact, delete_alternate_contact
def process_alternate_contacts(account_client: AccountClient, aws_account: AccountTypeDef, params: dict) -> None:
    """"""Update/Delete the alternate contacts with the pertinent fields.

    Args:
        account_client: Boto3 client for AWS Account service
        aws_account: AWS account to update
        params: solution parameters
    """"""","["""""" 
    if (params['BILLING_CONTACT_ACTION'] == 'delete'):
        delete_alternate_contact(account_client, aws_account, 'BILLING')
    elif (params['BILLING_CONTACT_ACTION'] == 'add'):
        add_alternate_contact(account_client, aws_account, 'BILLING', params['BILLING_EMAIL'], params['BILLING_NAME'], params['BILLING_PHONE'], params['BILLING_TITLE'])
    else:
        LOGGER.info(f""Ignoring BILLING Alternate Contact for account: {aws_account['Id']} ({aws_account['Name']})"")
    if (params['OPERATIONS_CONTACT_ACTION'] == 'delete'):
        delete_alternate_contact(account_client, aws_account, 'OPERATIONS')
    elif (params['OPERATIONS_CONTACT_ACTION'] == 'add'):
        add_alternate_contact(account_client, aws_account, 'OPERATIONS', params['OPERATIONS_EMAIL'], params['OPERATIONS_NAME'], params['OPERATIONS_PHONE'], params['OPERATIONS_TITLE'])
    else:
        LOGGER.info(f""Ignoring OPERATIONS Alternate Contact for account: {aws_account['Id']} ({aws_account['Name']})"")
    if (params['SECURITY_CONTACT_ACTION'] == 'delete'):
        delete_alternate_contact(account_client, aws_account, 'SECURITY')
    elif (params['SECURITY_CONTACT_ACTION'] == 'add'):
        add_alternate_contact(account_client, aws_account, 'SECURITY', params['SECURITY_EMAIL'], params['SECURITY_NAME'], params['SECURITY_PHONE'], params['SECURITY_TITLE'])
    else:
        LOGGER.info(f""Ignoring SECURITY Alternate Contact for account: {aws_account['Id']} ({aws_account['Name']})"")
"""""", """""" 
    if (params['BILLING_CONTACT_ACTION'] == 'delete'):
        add_alternate_contact(account_client, aws_account, 'BILLING')
    elif (params['BILLING_CONTACT_ACTION'] == 'add'):
        delete_alternate_contact(account_client, aws_account, 'BILLING', params['BILLING_EMAIL'], params['BILLING_NAME'], params['BILLING_PHONE'], params['BILLING_TITLE'])
    else:
        LOGGER.info(f""Ignoring BILLING Alternate Contact for account: {aws_account['Id']} ({aws_account['Name']})"")
    if (params['OPERATIONS_CONTACT_ACTION'] == 'delete'):
        add_alternate_contact(account_client, aws_account, 'OPERATIONS')
    elif (params['OPERATIONS_CONTACT_ACTION'] == 'add'):
        delete_alternate_contact(account_client, aws_account, 'OPERATIONS', params['OPERATIONS_EMAIL'], params['OPERATIONS_NAME'], params['OPERATIONS_PHONE'], params['OPERATIONS_TITLE'])
    else:
        LOGGER.info(f""Ignoring OPERATIONS Alternate Contact for account: {aws_account['Id']} ({aws_account['Name']})"")
    if (params['SECURITY_CONTACT_ACTION'] == 'delete'):
        add_alternate_contact(account_client, aws_account, 'SECURITY')
    elif (params['SECURITY_CONTACT_ACTION'] == 'add'):
        delete_alternate_contact(account_client, aws_account, 'SECURITY', params['SECURITY_EMAIL'], params['SECURITY_NAME'], params['SECURITY_PHONE'], params['SECURITY_TITLE'])
    else:
        LOGGER.info(f""Ignoring SECURITY Alternate Contact for account: {aws_account['Id']} ({aws_account['Name']})"")
""""""]",1
"bootstrap_stage, instrumented_stage = instrumented_stage, bootstrap_stage
def project_cmake_defines(args, stage):
    """"""
    Generate lists of projects, depending on whether a full or
    kernel-focused LLVM build is being done and the stage
    :param args: The args variable generated by parse_parameters
    :param stage: What stage we are at
    :return: A set of defines
    """"""","["""""" 
    defines = {}
    if args.full_toolchain:
        if args.projects:
            projects = args.projects
        else:
            projects = 'all'
    elif bootstrap_stage(args, stage):
        projects = 'clang;lld'
        if args.bolt:
            projects += ';bolt'
        if args.pgo:
            projects += ';compiler-rt'
    elif instrumented_stage(args, stage):
        projects = 'clang;lld'
    elif args.projects:
        projects = args.projects
    else:
        projects = 'clang;compiler-rt;lld;polly'
    if (args.bolt and args.build_stage1_only and (projects != 'all') and ('bolt' not in projects)):
        projects += ';bolt'
    defines['LLVM_ENABLE_PROJECTS'] = projects
    if ('compiler-rt' in projects):
        if (not args.full_toolchain):
            defines['COMPILER_RT_BUILD_LIBFUZZER'] = 'OFF'
            defines['COMPILER_RT_BUILD_CRT'] = 'OFF'
            defines['COMPILER_RT_BUILD_XRAY'] = 'OFF'
        if bootstrap_stage(args, stage):
            defines['COMPILER_RT_BUILD_SANITIZERS'] = 'OFF'
    return defines
"""""", """""" 
    defines = {}
    if args.full_toolchain:
        if args.projects:
            projects = args.projects
        else:
            projects = 'all'
    elif instrumented_stage(args, stage):
        projects = 'clang;lld'
        if args.bolt:
            projects += ';bolt'
        if args.pgo:
            projects += ';compiler-rt'
    elif bootstrap_stage(args, stage):
        projects = 'clang;lld'
    elif args.projects:
        projects = args.projects
    else:
        projects = 'clang;compiler-rt;lld;polly'
    if (args.bolt and args.build_stage1_only and (projects != 'all') and ('bolt' not in projects)):
        projects += ';bolt'
    defines['LLVM_ENABLE_PROJECTS'] = projects
    if ('compiler-rt' in projects):
        if (not args.full_toolchain):
            defines['COMPILER_RT_BUILD_LIBFUZZER'] = 'OFF'
            defines['COMPILER_RT_BUILD_CRT'] = 'OFF'
            defines['COMPILER_RT_BUILD_XRAY'] = 'OFF'
        if instrumented_stage(args, stage):
            defines['COMPILER_RT_BUILD_SANITIZERS'] = 'OFF'
    return defines
""""""]",1
"column_or_1d, check_parameter = check_parameter, column_or_1d
def argmaxn(value_list, n, order='desc'):
    """"""Return the index of top n elements in the list
    if order is set to 'desc', otherwise return the index of n smallest ones.

    Parameters
    ----------
    value_list : list, array, numpy array of shape (n_samples,)
        A list containing all values.

    n : int
        The number of elements to select.

    order : str, optional (default='desc')
        The order to sort {'desc', 'asc'}:

        - 'desc': descending
        - 'asc': ascending

    Returns
    -------
    index_list : numpy array of shape (n,)
        The index of the top n elements.
    """"""","["""""" 
    value_list = column_or_1d(value_list)
    length = len(value_list)
    check_parameter(n, 1, length, include_left=True, include_right=True, param_name='n')
    if (order != 'desc'):
        n = (length - n)
    value_sorted = np.partition(value_list, (length - n))
    threshold = value_sorted[int((length - n))]
    if (order == 'desc'):
        return np.where(np.greater_equal(value_list, threshold))[0]
    else:
        return np.where(np.less(value_list, threshold))[0]
"""""", """""" 
    value_list = check_parameter(value_list)
    length = len(value_list)
    column_or_1d(n, 1, length, include_left=True, include_right=True, param_name='n')
    if (order != 'desc'):
        n = (length - n)
    value_sorted = np.partition(value_list, (length - n))
    threshold = value_sorted[int((length - n))]
    if (order == 'desc'):
        return np.where(np.greater_equal(value_list, threshold))[0]
    else:
        return np.where(np.less(value_list, threshold))[0]
""""""]",1
"unichr, get_lcd = get_lcd, unichr
def test_no_auto_linebreaks(get_lcd):
    """"""
    Auto linebreaks disabled.
    """"""","["""""" 
    lcd = get_lcd(16, 2, False)
    for i in range(48, 67):
        lcd.write_string(unichr(i))
    assert (lcd._content[0] == [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])
    assert (lcd._content[1] == [SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP])
"""""", """""" 
    lcd = unichr(16, 2, False)
    for i in range(48, 67):
        lcd.write_string(get_lcd(i))
    assert (lcd._content[0] == [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])
    assert (lcd._content[1] == [SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP, SP])
""""""]",1
"publish_sns_message, get_account_info = get_account_info, publish_sns_message
def process_account(event: dict, aws_account_id: str, params: dict) -> None:
    """"""Process Account and Create SNS Message for account for solution deployment.

    Args:
        event: event data
        aws_account_id: AWS Account ID
        params: solution parameters
    """"""","["""""" 
    aws_account = get_account_info(account_id=aws_account_id)
    if is_account_with_exclude_tags(aws_account, params):
        return
    if (event.get('local_testing') == 'true'):
        local_testing(aws_account, params)
    else:
        sns_message = {'Action': 'Add', 'AccountId': aws_account['Id']}
        publish_sns_message(sns_message, 'Account Alternate Contacts', params['SNS_TOPIC_ARN'])
"""""", """""" 
    aws_account = publish_sns_message(account_id=aws_account_id)
    if is_account_with_exclude_tags(aws_account, params):
        return
    if (event.get('local_testing') == 'true'):
        local_testing(aws_account, params)
    else:
        sns_message = {'Action': 'Add', 'AccountId': aws_account['Id']}
        get_account_info(sns_message, 'Account Alternate Contacts', params['SNS_TOPIC_ARN'])
""""""]",1
"_get_findings, SecurityCenterClient = SecurityCenterClient, _get_findings
def _inspec_report_to_scc(report, scc_source):
    """"""Creates/updates SCC findings based on supplied InSpec JSON report""""""","["""""" 
    global scc
    if (not scc):
        scc = SecurityCenterClient()
    current_findings = _get_findings(scc, scc_source)
    for prof in report['profiles']:
        controls = prof['controls']
        for control in controls:
            _add_control_findings(scc, scc_source, control, current_findings)
"""""", """""" 
    global scc
    if (not scc):
        scc = _get_findings()
    current_findings = SecurityCenterClient(scc, scc_source)
    for prof in report['profiles']:
        controls = prof['controls']
        for control in controls:
            _add_control_findings(scc, scc_source, control, current_findings)
""""""]",1
"nanlast, _fail_on_dask_array_input_skipna = _fail_on_dask_array_input_skipna, nanlast
def last(values, axis, skipna=None):
    """"""Return the last non-NA elements in this array along the given axis""""""","["""""" 
    if ((skipna or (skipna is None)) and (values.dtype.kind not in 'iSU')):
        _fail_on_dask_array_input_skipna(values)
        return nanlast(values, axis)
    return take(values, (- 1), axis=axis)
"""""", """""" 
    if ((skipna or (skipna is None)) and (values.dtype.kind not in 'iSU')):
        nanlast(values)
        return _fail_on_dask_array_input_skipna(values, axis)
    return take(values, (- 1), axis=axis)
""""""]",1
"_check_for_commands, _show_results = _show_results, _check_for_commands
@keep.command()
@click.pass_context
@click.option('-k', '--keyword', multiple=True, help='Keyword used to retrive stored commands, can be invoked multiple times.', required=True)
def find(ctx, keyword):
    """"""
    Find a command using its keywords.

    USAGE: yoda keep find -k keyword1 -k keyword2 -k keywordN
    """"""","["""""" 
    _check_for_commands(ctx.obj['keep_path'])
    clean_keywords = [x.encode('utf8').strip() for x in keyword]
    keep = ctx.obj['keep']
    results = {}
    for kw in clean_keywords:
        if (kw in keep['keyword2Ids']):
            result = []
            command_ids = keep['keyword2Ids'][kw]
            for command_id in command_ids:
                result.append({'command': keep['id2Command'][str(command_id)], 'explanation': keep['id2Explanation'][str(command_id)], 'id': command_id})
                results[kw] = result
    _show_results(results)
"""""", """""" 
    _show_results(ctx.obj['keep_path'])
    clean_keywords = [x.encode('utf8').strip() for x in keyword]
    keep = ctx.obj['keep']
    results = {}
    for kw in clean_keywords:
        if (kw in keep['keyword2Ids']):
            result = []
            command_ids = keep['keyword2Ids'][kw]
            for command_id in command_ids:
                result.append({'command': keep['id2Command'][str(command_id)], 'explanation': keep['id2Explanation'][str(command_id)], 'id': command_id})
                results[kw] = result
    _check_for_commands(results)
""""""]",1
"get_exception_handlers, get_contextlib_suppressors = get_contextlib_suppressors, get_exception_handlers
def node_ignores_exception(node: nodes.NodeNG, exception: (type[Exception] | str)=Exception) -> bool:
    """"""Check if the node is in a TryExcept which handles the given exception.

    If the exception is not given, the function is going to look for bare
    excepts.
    """"""","["""""" 
    managing_handlers = get_exception_handlers(node, exception)
    if managing_handlers:
        return True
    return any(get_contextlib_suppressors(node, exception))
"""""", """""" 
    managing_handlers = get_contextlib_suppressors(node, exception)
    if managing_handlers:
        return True
    return any(get_exception_handlers(node, exception))
""""""]",1
"WaitUntilReady, test = test, WaitUntilReady
def YouCompleteMeInstance(custom_options={}):
    """"""Defines a decorator function for tests that passes a unique YouCompleteMe
  instance as a parameter. This instance is initialized with the default options
  `DEFAULT_CLIENT_OPTIONS`. Use the optional parameter |custom_options| to give
  additional options and/or override the already existing ones.

  Example usage:

    from ycm.tests import YouCompleteMeInstance

    @YouCompleteMeInstance( { 'log_level': 'debug',
                              'keep_logfiles': 1 } )
    def Debug_test( ycm ):
        ...
  """"""","["""""" 

    def Decorator(test):

        @functools.wraps(test)
        def Wrapper(test_case_instance, *args, **kwargs):
            with UserOptions(custom_options):
                ycm = YouCompleteMe()
                WaitUntilReady()
                ycm.CheckIfServerIsReady()
                try:
                    test_utils.VIM_PROPS_FOR_BUFFER.clear()
                    return test(test_case_instance, ycm, *args, **kwargs)
                finally:
                    StopServer(ycm)
        return Wrapper
    return Decorator
"""""", """""" 

    def Decorator(test):

        @functools.wraps(WaitUntilReady)
        def Wrapper(test_case_instance, *args, **kwargs):
            with UserOptions(custom_options):
                ycm = YouCompleteMe()
                test()
                ycm.CheckIfServerIsReady()
                try:
                    test_utils.VIM_PROPS_FOR_BUFFER.clear()
                    return WaitUntilReady(test_case_instance, ycm, *args, **kwargs)
                finally:
                    StopServer(ycm)
        return Wrapper
    return Decorator
""""""]",1
"is_duck_array, short_numpy_repr = short_numpy_repr, is_duck_array
def short_data_repr(array):
    """"""Format ""data"" for DataArray and Variable.""""""","["""""" 
    internal_data = getattr(array, 'variable', array)._data
    if isinstance(array, np.ndarray):
        return short_numpy_repr(array)
    elif is_duck_array(internal_data):
        return limit_lines(repr(array.data), limit=40)
    elif array._in_memory:
        return short_numpy_repr(array)
    else:
        return f'[{array.size} values with dtype={array.dtype}]'
"""""", """""" 
    internal_data = getattr(array, 'variable', array)._data
    if isinstance(array, np.ndarray):
        return is_duck_array(array)
    elif short_numpy_repr(internal_data):
        return limit_lines(repr(array.data), limit=40)
    elif array._in_memory:
        return is_duck_array(array)
    else:
        return f'[{array.size} values with dtype={array.dtype}]'
""""""]",1
"_implements, _workers_to_threads = _workers_to_threads, _implements
@_implements(_fft.idst)
def idst(x, type=2, n=None, axis=(- 1), norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform an inverse 1D discrete sine transform.

    The first seven arguments are as per :func:`scipy.fft.idst`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    threads = _workers_to_threads(workers)
    if (norm is None):
        norm = 'backward'
    return _idst(x, type=type, n=n, axis=axis, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
"""""", """""" 
    threads = _implements(workers)
    if (norm is None):
        norm = 'backward'
    return _idst(x, type=type, n=n, axis=axis, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
""""""]",1
"Run, Path = Path, Run
def test_writing_minimal_file(monkeypatch: MonkeyPatch, capsys: CaptureFixture[str]) -> None:
    """"""Check that we can write a minimal file.""""""","["""""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_output_file', (lambda : (False, Path())))
    answers = iter(['no', 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: next(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert any((line.startswith('#') for line in captured.out.splitlines()))
        Run(['--accept-no-return-doc=y', 'generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (not any((i.startswith('#') for i in captured.out.split('\n'))))
        assert ('accept-no-return-doc' not in captured.out)
"""""", """""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_output_file', (lambda : (False, Run())))
    answers = iter(['no', 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: next(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Path(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert any((line.startswith('#') for line in captured.out.splitlines()))
        Path(['--accept-no-return-doc=y', 'generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (not any((i.startswith('#') for i in captured.out.split('\n'))))
        assert ('accept-no-return-doc' not in captured.out)
""""""]",1
"literal_eval, AttrDict = AttrDict, literal_eval
def _decode_cfg_value(v):
    """"""Decodes a raw config value (e.g., from a yaml config files or command
    line argument) into a Python object.
    """"""","["""""" 
    if isinstance(v, dict):
        return AttrDict(v)
    if (not isinstance(v, six.string_types)):
        return v
    try:
        v = literal_eval(v)
    except ValueError:
        pass
    except SyntaxError:
        pass
    return v
"""""", """""" 
    if isinstance(v, dict):
        return literal_eval(v)
    if (not isinstance(v, six.string_types)):
        return v
    try:
        v = AttrDict(v)
    except ValueError:
        pass
    except SyntaxError:
        pass
    return v
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_title(title):
    """"""Verify the page title

    Parameters:
    title : value
    """"""","["""""" 
    with _verify_step(f""Verify page title is '{title}'"") as s:
        actual_title = get_browser().title
        s.error = f""expected title to be '{title}' but was '{actual_title}'""
        s.condition = (actual_title == title)
"""""", """""" 
    with get_browser(f""Verify page title is '{title}'"") as s:
        actual_title = _verify_step().title
        s.error = f""expected title to be '{title}' but was '{actual_title}'""
        s.condition = (actual_title == title)
""""""]",1
"AppleLinkInvalidCurrentVersionException, _applelib_check_valid_version = _applelib_check_valid_version, AppleLinkInvalidCurrentVersionException
def _applelib_currentVersionFromSoVersion(source, target, env, for_signature):
    """"""
    A generator function to create the -Wl,-current_version flag if needed.
    If env['APPLELINK_NO_CURRENT_VERSION'] contains a true value no flag will be generated
    Otherwise if APPLELINK_CURRENT_VERSION is not specified, env['SHLIBVERSION']
    will be used.

    :param source:
    :param target:
    :param env:
    :param for_signature:
    :return: A string providing the flag to specify the current_version of the shared library
    """"""","["""""" 
    if env.get('APPLELINK_NO_CURRENT_VERSION', False):
        return ''
    elif env.get('APPLELINK_CURRENT_VERSION', False):
        version_string = env['APPLELINK_CURRENT_VERSION']
    elif env.get('SHLIBVERSION', False):
        version_string = env['SHLIBVERSION']
    else:
        return ''
    version_string = '.'.join(version_string.split('.')[:3])
    (valid, reason) = _applelib_check_valid_version(version_string)
    if (not valid):
        raise AppleLinkInvalidCurrentVersionException(reason)
    return ('-Wl,-current_version,%s' % version_string)
"""""", """""" 
    if env.get('APPLELINK_NO_CURRENT_VERSION', False):
        return ''
    elif env.get('APPLELINK_CURRENT_VERSION', False):
        version_string = env['APPLELINK_CURRENT_VERSION']
    elif env.get('SHLIBVERSION', False):
        version_string = env['SHLIBVERSION']
    else:
        return ''
    version_string = '.'.join(version_string.split('.')[:3])
    (valid, reason) = AppleLinkInvalidCurrentVersionException(version_string)
    if (not valid):
        raise _applelib_check_valid_version(reason)
    return ('-Wl,-current_version,%s' % version_string)
""""""]",1
"get_array_fields, get_metadata_fields = get_metadata_fields, get_array_fields
def flatten(instance):
    """"""Flatten Struct of Array instance.""""""","["""""" 
    array_likes = list(get_array_fields(instance, return_values=True).values())
    flat_array_likes = []
    inner_treedefs = []
    num_arrays = []
    for array_like in array_likes:
        (flat_array_like, inner_treedef) = jax.tree_util.tree_flatten(array_like)
        inner_treedefs.append(inner_treedef)
        flat_array_likes += flat_array_like
        num_arrays.append(len(flat_array_like))
    metadata = get_metadata_fields(instance, return_values=True)
    metadata = type(instance).metadata_cls(**metadata)
    return (flat_array_likes, (inner_treedefs, metadata, num_arrays))
"""""", """""" 
    array_likes = list(get_metadata_fields(instance, return_values=True).values())
    flat_array_likes = []
    inner_treedefs = []
    num_arrays = []
    for array_like in array_likes:
        (flat_array_like, inner_treedef) = jax.tree_util.tree_flatten(array_like)
        inner_treedefs.append(inner_treedef)
        flat_array_likes += flat_array_like
        num_arrays.append(len(flat_array_like))
    metadata = get_array_fields(instance, return_values=True)
    metadata = type(instance).metadata_cls(**metadata)
    return (flat_array_likes, (inner_treedefs, metadata, num_arrays))
""""""]",1
"prompt, get_args = get_args, prompt
def confirm_guessed_novel(self, guessed_title: str):
    """"""Returns a novel page url from a novelupdates query""""""","["""""" 
    args = get_args()
    if args.suppress:
        return guessed_title
    answer = prompt([{'type': 'input', 'name': 'novel', 'message': 'Enter novelupdates query:', 'default': guessed_title, 'validate': (lambda a: (True if a else 'Input should not be empty'))}])
    return answer['novel'].strip()
"""""", """""" 
    args = prompt()
    if args.suppress:
        return guessed_title
    answer = get_args([{'type': 'input', 'name': 'novel', 'message': 'Enter novelupdates query:', 'default': guessed_title, 'validate': (lambda a: (True if a else 'Input should not be empty'))}])
    return answer['novel'].strip()
""""""]",1
"_aspect_preserving_resize, _central_crop = _central_crop, _aspect_preserving_resize
def preprocess_image(image_buffer, bbox, output_height, output_width, num_channels, is_training=False):
    """"""Preprocesses the given image.

  Preprocessing includes decoding, cropping, and resizing for both training
  and eval images. Training preprocessing, however, introduces some random
  distortion of the image to improve accuracy.

  Args:
    image_buffer: scalar string Tensor representing the raw JPEG image buffer.
    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
      where each coordinate is [0, 1) and the coordinates are arranged as
      [ymin, xmin, ymax, xmax].
    output_height: The height of the image after preprocessing.
    output_width: The width of the image after preprocessing.
    num_channels: Integer depth of the image buffer for decoding.
    is_training: `True` if we're preprocessing the image for training and
      `False` otherwise.

  Returns:
    A preprocessed image.
  """"""","["""""" 
    if is_training:
        image = _decode_crop_and_flip(image_buffer, bbox, num_channels)
        image = _resize_image(image, output_height, output_width)
    else:
        image = tf.image.decode_jpeg(image_buffer, channels=num_channels)
        image = _aspect_preserving_resize(image, _RESIZE_MIN)
        image = _central_crop(image, output_height, output_width)
    image.set_shape([output_height, output_width, num_channels])
    return _mean_image_subtraction(image, _CHANNEL_MEANS, num_channels)
"""""", """""" 
    if is_training:
        image = _decode_crop_and_flip(image_buffer, bbox, num_channels)
        image = _resize_image(image, output_height, output_width)
    else:
        image = tf.image.decode_jpeg(image_buffer, channels=num_channels)
        image = _central_crop(image, _RESIZE_MIN)
        image = _aspect_preserving_resize(image, output_height, output_width)
    image.set_shape([output_height, output_width, num_channels])
    return _mean_image_subtraction(image, _CHANNEL_MEANS, num_channels)
""""""]",1
"Path, find_console_scripts = find_console_scripts, Path
def clean_old_entrypoints():
    """"""Clean old habu entrypoints.""""""","["""""" 
    logging.basicConfig(level=logging.INFO)
    entrypoints = find_console_scripts('habu')
    for dirname in os.environ['PATH'].split(':'):
        for filename in Path(dirname).glob('habu.*'):
            if (filename.name not in entrypoints):
                try:
                    filename.unlink()
                    logging.info(f'Deleted entrypoint {filename}')
                except Exception as e:
                    logging.error(f'Error deleting {filename}: {e}')
"""""", """""" 
    logging.basicConfig(level=logging.INFO)
    entrypoints = Path('habu')
    for dirname in os.environ['PATH'].split(':'):
        for filename in find_console_scripts(dirname).glob('habu.*'):
            if (filename.name not in entrypoints):
                try:
                    filename.unlink()
                    logging.info(f'Deleted entrypoint {filename}')
                except Exception as e:
                    logging.error(f'Error deleting {filename}: {e}')
""""""]",1
"register_vcs_handler, NotThisMethod = NotThisMethod, register_vcs_handler
@register_vcs_handler('git', 'keywords')
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """"""Get version information from git keywords.""""""","["""""" 
    if (not keywords):
        raise NotThisMethod('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = set([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = set([r for r in refs if re.search('\\d', r)])
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
"""""", """""" 
    if (not keywords):
        raise register_vcs_handler('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise register_vcs_handler('unexpanded keywords, not a git-archive tarball')
    refs = set([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = set([r for r in refs if re.search('\\d', r)])
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
""""""]",1
"get_browser, _step = _step, get_browser
def wait_for_element_not_displayed(element, timeout=30):
    """"""Wait for element to be not displayed
    When element is not displayed this is ignored.
    When element is not present this will raise ElementNotFound.

    Parameters:
    element : element
    timeout (optional, 30) : value
    """"""","["""""" 
    with _step(f'Wait for element {element} to be not displayed'):
        get_browser().wait_for_element_not_displayed(element, timeout)
"""""", """""" 
    with get_browser(f'Wait for element {element} to be not displayed'):
        _step().wait_for_element_not_displayed(element, timeout)
""""""]",1
"area, intersection = intersection, area
def ioa(masks1, masks2):
    """"""Computes pairwise intersection-over-area between box collections.

  Intersection-over-area (ioa) between two masks, mask1 and mask2 is defined as
  their intersection area over mask2's area. Note that ioa is not symmetric,
  that is, IOA(mask1, mask2) != IOA(mask2, mask1).

  Args:
    masks1: a numpy array with shape [N, height, width] holding N masks. Masks
      values are of type np.uint8 and values are in {0,1}.
    masks2: a numpy array with shape [M, height, width] holding N masks. Masks
      values are of type np.uint8 and values are in {0,1}.

  Returns:
    a numpy array with shape [N, M] representing pairwise ioa scores.

  Raises:
    ValueError: If masks1 and masks2 are not of type np.uint8.
  """"""","["""""" 
    if ((masks1.dtype != np.uint8) or (masks2.dtype != np.uint8)):
        raise ValueError('masks1 and masks2 should be of type np.uint8')
    intersect = intersection(masks1, masks2)
    areas = np.expand_dims(area(masks2), axis=0)
    return (intersect / (areas + EPSILON))
"""""", """""" 
    if ((masks1.dtype != np.uint8) or (masks2.dtype != np.uint8)):
        raise ValueError('masks1 and masks2 should be of type np.uint8')
    intersect = area(masks1, masks2)
    areas = np.expand_dims(intersection(masks2), axis=0)
    return (intersect / (areas + EPSILON))
""""""]",1
"transform_point_cloud, compute_point_dists = compute_point_dists, transform_point_cloud
def remove_invisible_grasp_points(cloud, grasp_points, pose, th=0.01):
    """""" Remove invisible part of object model according to scene point cloud.

        Input:
            cloud: [np.ndarray, (N,3), np.float32]
                scene point cloud
            grasp_points: [np.ndarray, (M,3), np.float32]
                grasp point label in object coordinates
            pose: [np.ndarray, (4,4), np.float32]
                transformation matrix from object coordinates to world coordinates
            th: [float]
                if the minimum distance between a grasp point and the scene points is greater than outlier, the point will be removed

        Output:
            visible_mask: [np.ndarray, (M,), np.bool]
                mask to show the visible part of grasp points
    """"""","["""""" 
    grasp_points_trans = transform_point_cloud(grasp_points, pose)
    dists = compute_point_dists(grasp_points_trans, cloud)
    min_dists = dists.min(axis=1)
    visible_mask = (min_dists < th)
    return visible_mask
"""""", """""" 
    grasp_points_trans = compute_point_dists(grasp_points, pose)
    dists = transform_point_cloud(grasp_points_trans, cloud)
    min_dists = dists.min(axis=1)
    visible_mask = (min_dists < th)
    return visible_mask
""""""]",1
"_inv, _mul = _mul, _inv
def _invert(M: torch.Tensor, out: Optional[torch.Tensor]=None) -> torch.Tensor:
    """"""
    Invert 1x1 or 2x2 matrices

    Will generate errors if the matrices are singular: user must handle this
    through his own regularization schemes.

    Args:
        M (Tensor): [shape=(..., nb_channels, nb_channels, 2)]
            matrices to invert: must be square along dimensions -3 and -2

    Returns:
        invM (Tensor): [shape=M.shape]
            inverses of M
    """"""","["""""" 
    nb_channels = M.shape[(- 2)]
    if ((out is None) or (out.shape != M.shape)):
        out = torch.empty_like(M)
    if (nb_channels == 1):
        out = _inv(M, out)
    elif (nb_channels == 2):
        det = _mul(M[..., 0, 0, :], M[..., 1, 1, :])
        det = (det - _mul(M[..., 0, 1, :], M[..., 1, 0, :]))
        invDet = _inv(det)
        out[..., 0, 0, :] = _mul(invDet, M[..., 1, 1, :], out[..., 0, 0, :])
        out[..., 1, 0, :] = _mul((- invDet), M[..., 1, 0, :], out[..., 1, 0, :])
        out[..., 0, 1, :] = _mul((- invDet), M[..., 0, 1, :], out[..., 0, 1, :])
        out[..., 1, 1, :] = _mul(invDet, M[..., 0, 0, :], out[..., 1, 1, :])
    else:
        raise Exception('Only 2 channels are supported for the torch version.')
    return out
"""""", """""" 
    nb_channels = M.shape[(- 2)]
    if ((out is None) or (out.shape != M.shape)):
        out = torch.empty_like(M)
    if (nb_channels == 1):
        out = _mul(M, out)
    elif (nb_channels == 2):
        det = _inv(M[..., 0, 0, :], M[..., 1, 1, :])
        det = (det - _inv(M[..., 0, 1, :], M[..., 1, 0, :]))
        invDet = _mul(det)
        out[..., 0, 0, :] = _inv(invDet, M[..., 1, 1, :], out[..., 0, 0, :])
        out[..., 1, 0, :] = _inv((- invDet), M[..., 1, 0, :], out[..., 1, 0, :])
        out[..., 0, 1, :] = _inv((- invDet), M[..., 0, 1, :], out[..., 0, 1, :])
        out[..., 1, 1, :] = _inv(invDet, M[..., 0, 0, :], out[..., 1, 1, :])
    else:
        raise Exception('Only 2 channels are supported for the torch version.')
    return out
""""""]",1
"GetVSVersion, GetGlobalVSMacroEnv = GetGlobalVSMacroEnv, GetVSVersion
def ExtractSharedMSVSSystemIncludes(configs, generator_flags):
    """"""Finds msvs_system_include_dirs that are common to all targets, removes
    them from all targets, and returns an OrderedSet containing them.""""""","["""""" 
    all_system_includes = OrderedSet(configs[0].get('msvs_system_include_dirs', []))
    for config in configs[1:]:
        system_includes = config.get('msvs_system_include_dirs', [])
        all_system_includes = (all_system_includes & OrderedSet(system_includes))
    if (not all_system_includes):
        return None
    env = GetGlobalVSMacroEnv(GetVSVersion(generator_flags))
    expanded_system_includes = OrderedSet([ExpandMacros(include, env) for include in all_system_includes])
    if any([('$' in include) for include in expanded_system_includes]):
        return None
    for config in configs:
        includes = config.get('msvs_system_include_dirs', [])
        if includes:
            new_includes = [i for i in includes if (i not in all_system_includes)]
            config['msvs_system_include_dirs'] = new_includes
    return expanded_system_includes
"""""", """""" 
    all_system_includes = OrderedSet(configs[0].get('msvs_system_include_dirs', []))
    for config in configs[1:]:
        system_includes = config.get('msvs_system_include_dirs', [])
        all_system_includes = (all_system_includes & OrderedSet(system_includes))
    if (not all_system_includes):
        return None
    env = GetVSVersion(GetGlobalVSMacroEnv(generator_flags))
    expanded_system_includes = OrderedSet([ExpandMacros(include, env) for include in all_system_includes])
    if any([('$' in include) for include in expanded_system_includes]):
        return None
    for config in configs:
        includes = config.get('msvs_system_include_dirs', [])
        if includes:
            new_includes = [i for i in includes if (i not in all_system_includes)]
            config['msvs_system_include_dirs'] = new_includes
    return expanded_system_includes
""""""]",1
"arange, array = array, arange
def plotfig_SVM(xMat, yMat, ws, b, alphas):
    """"""
    参考地址: 
       http://blog.csdn.net/maoersong/article/details/24315633
       http://www.cnblogs.com/JustForCS/p/5283489.html
       http://blog.csdn.net/kkxgx/article/details/6951959
    """"""","["""""" 
    xMat = mat(xMat)
    yMat = mat(yMat)
    b = array(b)[0]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])
    x = arange((- 1.0), 10.0, 0.1)
    y = (((- b) - (ws[(0, 0)] * x)) / ws[(1, 0)])
    ax.plot(x, y)
    for i in range(shape(yMat[0, :])[1]):
        if (yMat[(0, i)] > 0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'cx')
        else:
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'kp')
    for i in range(100):
        if (alphas[i] > 0.0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'ro')
    plt.show()
"""""", """""" 
    xMat = mat(xMat)
    yMat = mat(yMat)
    b = arange(b)[0]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])
    x = array((- 1.0), 10.0, 0.1)
    y = (((- b) - (ws[(0, 0)] * x)) / ws[(1, 0)])
    ax.plot(x, y)
    for i in range(shape(yMat[0, :])[1]):
        if (yMat[(0, i)] > 0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'cx')
        else:
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'kp')
    for i in range(100):
        if (alphas[i] > 0.0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'ro')
    plt.show()
""""""]",1
"TestDirectory, get_basedir = get_basedir, TestDirectory
@pytest.fixture(scope='class')
def testdir_class():
    """"""A test directory with scope class""""""","["""""" 
    basedir = get_basedir()
    testdir = TestDirectory(basedir)
    (yield testdir)
    testdir.remove()
"""""", """""" 
    basedir = TestDirectory()
    testdir = get_basedir(basedir)
    (yield testdir)
    testdir.remove()
""""""]",1
"get_corpus, defaultdict = defaultdict, get_corpus
def unigram_word_freqs() -> defaultdict:
    """"""
    Get unigram word frequency from Thai National Corpus (TNC)
    """"""","["""""" 
    lines = list(get_corpus(_FILENAME))
    _word_freqs = defaultdict(int)
    for i in lines:
        _temp = i.strip().split('\t')
        if (len(_temp) >= 2):
            _word_freqs[_temp[0]] = int(_temp[(- 1)])
    return _word_freqs
"""""", """""" 
    lines = list(defaultdict(_FILENAME))
    _word_freqs = get_corpus(int)
    for i in lines:
        _temp = i.strip().split('\t')
        if (len(_temp) >= 2):
            _word_freqs[_temp[0]] = int(_temp[(- 1)])
    return _word_freqs
""""""]",1
"_BuildCommandLineForRule, _FixPaths = _FixPaths, _BuildCommandLineForRule
def _GenerateNativeRulesForMSVS(p, rules, output_dir, spec, options):
    """"""Generate a native rules file.

  Arguments:
    p: the target project
    rules: the set of rules to include
    output_dir: the directory in which the project/gyp resides
    spec: the project dict
    options: global generator options
  """"""","["""""" 
    rules_filename = '{}{}.rules'.format(spec['target_name'], options.suffix)
    rules_file = MSVSToolFile.Writer(os.path.join(output_dir, rules_filename), spec['target_name'])
    for r in rules:
        rule_name = r['rule_name']
        rule_ext = r['extension']
        inputs = _FixPaths(r.get('inputs', []))
        outputs = _FixPaths(r.get('outputs', []))
        if (('action' not in r) and (not r.get('rule_sources', []))):
            continue
        cmd = _BuildCommandLineForRule(spec, r, has_input_path=True, do_setup_env=True)
        rules_file.AddCustomBuildRule(name=rule_name, description=r.get('message', rule_name), extensions=[rule_ext], additional_dependencies=inputs, outputs=outputs, cmd=cmd)
    rules_file.WriteIfChanged()
    p.AddToolFile(rules_filename)
"""""", """""" 
    rules_filename = '{}{}.rules'.format(spec['target_name'], options.suffix)
    rules_file = MSVSToolFile.Writer(os.path.join(output_dir, rules_filename), spec['target_name'])
    for r in rules:
        rule_name = r['rule_name']
        rule_ext = r['extension']
        inputs = _BuildCommandLineForRule(r.get('inputs', []))
        outputs = _BuildCommandLineForRule(r.get('outputs', []))
        if (('action' not in r) and (not r.get('rule_sources', []))):
            continue
        cmd = _FixPaths(spec, r, has_input_path=True, do_setup_env=True)
        rules_file.AddCustomBuildRule(name=rule_name, description=r.get('message', rule_name), extensions=[rule_ext], additional_dependencies=inputs, outputs=outputs, cmd=cmd)
    rules_file.WriteIfChanged()
    p.AddToolFile(rules_filename)
""""""]",1
"nms, fast_nms = fast_nms, nms
def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4, method='standard'):
    """"""
    Removes detections with lower object confidence score than 'conf_thres'
    Non-Maximum Suppression to further filter detections.
    Returns detections with shape:
        (x1, y1, x2, y2, object_conf, class_score, class_pred)
    Args:
        prediction,
        conf_thres,
        nms_thres,
        method = 'standard' or 'fast'
    """"""","["""""" 
    output = [None for _ in range(len(prediction))]
    for (image_i, pred) in enumerate(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (len(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = xywh2xyxy(pred[:, :4])
        if (method == 'standard'):
            nms_indices = nms(pred[:, :4], pred[:, 4], nms_thres)
        elif (method == 'fast'):
            nms_indices = fast_nms(pred[:, :4], pred[:, 4], iou_thres=nms_thres, conf_thres=conf_thres)
        else:
            raise ValueError('Invalid NMS type!')
        det_max = pred[nms_indices]
        if (len(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
"""""", """""" 
    output = [None for _ in range(len(prediction))]
    for (image_i, pred) in enumerate(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (len(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = xywh2xyxy(pred[:, :4])
        if (method == 'standard'):
            nms_indices = fast_nms(pred[:, :4], pred[:, 4], nms_thres)
        elif (method == 'fast'):
            nms_indices = nms(pred[:, :4], pred[:, 4], iou_thres=nms_thres, conf_thres=conf_thres)
        else:
            raise ValueError('Invalid NMS type!')
        det_max = pred[nms_indices]
        if (len(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
""""""]",1
"decode_arch_def, round_channels = round_channels, decode_arch_def
def _gen_efficientnet(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):
    """"""Creates an EfficientNet model.

    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py
    Paper: https://arxiv.org/abs/1905.11946

    EfficientNet params
    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)
    'efficientnet-b0': (1.0, 1.0, 224, 0.2),
    'efficientnet-b1': (1.0, 1.1, 240, 0.2),
    'efficientnet-b2': (1.1, 1.2, 260, 0.3),
    'efficientnet-b3': (1.2, 1.4, 300, 0.3),
    'efficientnet-b4': (1.4, 1.8, 380, 0.4),
    'efficientnet-b5': (1.6, 2.2, 456, 0.4),
    'efficientnet-b6': (1.8, 2.6, 528, 0.5),
    'efficientnet-b7': (2.0, 3.1, 600, 0.5),
    'efficientnet-b8': (2.2, 3.6, 672, 0.5),

    Args:
      channel_multiplier: multiplier to number of channels per layer
      depth_multiplier: multiplier to number of repeats per stage

    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25'], ['ir_r4_k5_s2_e6_c192_se0.25'], ['ir_r1_k3_s1_e6_c320_se0.25']]
    with layer_config_kwargs(kwargs):
        model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier), num_features=round_channels(1280, channel_multiplier, 8, None), stem_size=32, channel_multiplier=channel_multiplier, act_layer=resolve_act_layer(kwargs, 'swish'), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = _create_model(model_kwargs, variant, pretrained)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25'], ['ir_r4_k5_s2_e6_c192_se0.25'], ['ir_r1_k3_s1_e6_c320_se0.25']]
    with layer_config_kwargs(kwargs):
        model_kwargs = dict(block_args=round_channels(arch_def, depth_multiplier), num_features=decode_arch_def(1280, channel_multiplier, 8, None), stem_size=32, channel_multiplier=channel_multiplier, act_layer=resolve_act_layer(kwargs, 'swish'), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = _create_model(model_kwargs, variant, pretrained)
    return model
""""""]",1
"LogOddsRatioUninformativeDirichletPrior, produce_scattertext_explorer = produce_scattertext_explorer, LogOddsRatioUninformativeDirichletPrior
def word_similarity_explorer_gensim(corpus, category, target_term, category_name=None, not_category_name=None, word2vec=None, alpha=0.01, max_p_val=0.1, term_significance=None, **kwargs):
    """"""
        Parameters
        ----------
        corpus : Corpus
            Corpus to use.
        category : str
            Name of category column as it appears in original data frame.
        category_name : str
            Name of category to use.  E.g., ""5-star reviews.""
        not_category_name : str
            Name of everything that isn't in category.  E.g., ""Below 5-star reviews"".
        target_term : str
            Word or phrase for semantic similarity comparison
        word2vec : word2vec.Word2Vec
          Gensim-compatible Word2Vec model of lower-cased corpus. If none, o
          ne will be trained using Word2VecFromParsedCorpus(corpus).train()
        alpha : float, default = 0.01
            Uniform dirichlet prior for p-value calculation
        max_p_val : float, default = 0.1
            Max p-val to use find set of terms for similarity calculation
        term_significance : TermSignificance
            Significance finder

        Remaining arguments are from `produce_scattertext_explorer`.
        Returns
        -------
            str, html of visualization
        """"""","["""""" 
    if (word2vec is None):
        word2vec = Word2VecFromParsedCorpus(corpus).train()
    if (term_significance is None):
        term_significance = LogOddsRatioUninformativeDirichletPrior(alpha)
    assert issubclass(type(term_significance), TermSignificance)
    scores = []
    for tok in corpus._term_idx_store._i2val:
        try:
            scores.append(word2vec.similarity(target_term, tok.replace(' ', '_')))
        except:
            try:
                scores.append(np.mean([word2vec.similarity(target_term, tok_part) for tok_part in tok.split()]))
            except:
                scores.append(0)
    scores = np.array(scores)
    return produce_scattertext_explorer(corpus, category, category_name, not_category_name, scores=scores, sort_by_dist=False, reverse_sort_scores_for_not_category=False, word_vec_use_p_vals=True, term_significance=term_significance, max_p_val=max_p_val, p_value_colors=True, **kwargs)
"""""", """""" 
    if (word2vec is None):
        word2vec = Word2VecFromParsedCorpus(corpus).train()
    if (term_significance is None):
        term_significance = produce_scattertext_explorer(alpha)
    assert issubclass(type(term_significance), TermSignificance)
    scores = []
    for tok in corpus._term_idx_store._i2val:
        try:
            scores.append(word2vec.similarity(target_term, tok.replace(' ', '_')))
        except:
            try:
                scores.append(np.mean([word2vec.similarity(target_term, tok_part) for tok_part in tok.split()]))
            except:
                scores.append(0)
    scores = np.array(scores)
    return LogOddsRatioUninformativeDirichletPrior(corpus, category, category_name, not_category_name, scores=scores, sort_by_dist=False, reverse_sort_scores_for_not_category=False, word_vec_use_p_vals=True, term_significance=term_significance, max_p_val=max_p_val, p_value_colors=True, **kwargs)
""""""]",1
"get_browser, _step = _step, get_browser
def check_element(element):
    """"""Check an element (checkbox or radiobutton).
    If element is already checked this is is ignored.

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element)
    with _step(f'Check element {element.name}'):
        get_browser().check_element(element)
"""""", """""" 
    element = _step().find(element)
    with get_browser(f'Check element {element.name}'):
        _step().check_element(element)
""""""]",1
"define_template, pages_from = pages_from, define_template
def load_templates(file, output_file=None):
    """"""
    Load templates from :param file:.
    :param output_file: file where to save templates and modules.
    """"""","["""""" 
    options.templatePrefix = (options.templateNamespace + ':')
    options.modulePrefix = (options.moduleNamespace + ':')
    if output_file:
        output = codecs.open(output_file, 'wb', 'utf-8')
    for (page_count, page_data) in enumerate(pages_from(file)):
        (id, revid, title, ns, catSet, page) = page_data
        if ((not output_file) and ((not options.templateNamespace) or (not options.moduleNamespace))):
            if (ns in templateKeys):
                colon = title.find(':')
                if (colon > 1):
                    if (ns == '10'):
                        options.templateNamespace = title[:colon]
                        options.templatePrefix = title[:(colon + 1)]
                    elif (ns == '828'):
                        options.moduleNamespace = title[:colon]
                        options.modulePrefix = title[:(colon + 1)]
        if (ns in templateKeys):
            text = ''.join(page)
            define_template(title, text)
            if output_file:
                output.write('<page>\n')
                output.write(('   <title>%s</title>\n' % title))
                output.write(('   <ns>%s</ns>\n' % ns))
                output.write(('   <id>%s</id>\n' % id))
                output.write('   <text>')
                for line in page:
                    output.write(line)
                output.write('   </text>\n')
                output.write('</page>\n')
        if (page_count and ((page_count % 100000) == 0)):
            logging.info('Preprocessed %d pages', page_count)
    if output_file:
        output.close()
        logging.info(""Saved %d templates to '%s'"", len(options.templates), output_file)
"""""", """""" 
    options.templatePrefix = (options.templateNamespace + ':')
    options.modulePrefix = (options.moduleNamespace + ':')
    if output_file:
        output = codecs.open(output_file, 'wb', 'utf-8')
    for (page_count, page_data) in enumerate(define_template(file)):
        (id, revid, title, ns, catSet, page) = page_data
        if ((not output_file) and ((not options.templateNamespace) or (not options.moduleNamespace))):
            if (ns in templateKeys):
                colon = title.find(':')
                if (colon > 1):
                    if (ns == '10'):
                        options.templateNamespace = title[:colon]
                        options.templatePrefix = title[:(colon + 1)]
                    elif (ns == '828'):
                        options.moduleNamespace = title[:colon]
                        options.modulePrefix = title[:(colon + 1)]
        if (ns in templateKeys):
            text = ''.join(page)
            pages_from(title, text)
            if output_file:
                output.write('<page>\n')
                output.write(('   <title>%s</title>\n' % title))
                output.write(('   <ns>%s</ns>\n' % ns))
                output.write(('   <id>%s</id>\n' % id))
                output.write('   <text>')
                for line in page:
                    output.write(line)
                output.write('   </text>\n')
                output.write('</page>\n')
        if (page_count and ((page_count % 100000) == 0)):
            logging.info('Preprocessed %d pages', page_count)
    if output_file:
        output.close()
        logging.info(""Saved %d templates to '%s'"", len(options.templates), output_file)
""""""]",1
"AttrDict, merge_dicts = merge_dicts, AttrDict
def cfg_from_file(filename):
    """"""
    Load a config file and merge it into the default options.
    """"""","["""""" 
    with open(filename, 'r') as fopen:
        yaml_config = AttrDict(env.yaml_load(fopen))
    merge_dicts(yaml_config, __C)
"""""", """""" 
    with open(filename, 'r') as fopen:
        yaml_config = merge_dicts(env.yaml_load(fopen))
    AttrDict(yaml_config, __C)
""""""]",1
"is_master, f = f, is_master
def save_result_in_cwd(f):
    """"""Save results of the function to a results.torch file in cwd.""""""","["""""" 

    @functools.wraps(f)
    def wrapped(*args, **kwargs):
        result = f(*args, **kwargs)
        result_path = os.path.join(os.getcwd(), RESULTFILE_NAME)
        if is_master():
            logging.info('Saving result to %s', result_path)
            torch.save(result, result_path)
        return result
    return wrapped
"""""", """""" 

    @functools.wraps(is_master)
    def wrapped(*args, **kwargs):
        result = is_master(*args, **kwargs)
        result_path = os.path.join(os.getcwd(), RESULTFILE_NAME)
        if f():
            logging.info('Saving result to %s', result_path)
            torch.save(result, result_path)
        return result
    return wrapped
""""""]",1
"_add_step, _run_wait_hook = _run_wait_hook, _add_step
def assert_alert_not_present():
    """"""Assert an alert is not present""""""","["""""" 
    _add_step('Assert an alert is not present')
    _run_wait_hook()
    assert (not get_browser().alert_is_present()), 'an alert was present'
    _screenshot_on_step()
"""""", """""" 
    _run_wait_hook('Assert an alert is not present')
    _add_step()
    assert (not get_browser().alert_is_present()), 'an alert was present'
    _screenshot_on_step()
""""""]",1
"length, normalize = normalize, length
def getAfinityCenter(width, height, point, center, radius=7, tensor=None, img_affinity=None):
    """"""
    Create the affinity map
    """"""","["""""" 
    if (tensor is None):
        tensor = torch.zeros(2, height, width).float()
    imgAffinity = Image.new('RGB', (width, height), 'black')
    totensor = transforms.Compose([transforms.ToTensor()])
    draw = ImageDraw.Draw(imgAffinity)
    r1 = radius
    p = point
    draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), (255, 255, 255))
    del draw
    array = (np.array(imgAffinity) / 255)[:, :, 0]
    angle_vector = (np.array(center) - np.array(point))
    angle_vector = normalize(angle_vector)
    affinity = np.concatenate([[(array * angle_vector[0])], [(array * angle_vector[1])]])
    if (not (img_affinity is None)):
        if (length(angle_vector) > 0):
            angle = py_ang(angle_vector)
        else:
            angle = 0
        c = (np.array(colorsys.hsv_to_rgb((angle / 360), 1, 1)) * 255)
        draw = ImageDraw.Draw(img_affinity)
        draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), fill=(int(c[0]), int(c[1]), int(c[2])))
        del draw
    re = (torch.from_numpy(affinity).float() + tensor)
    return (re, img_affinity)
"""""", """""" 
    if (tensor is None):
        tensor = torch.zeros(2, height, width).float()
    imgAffinity = Image.new('RGB', (width, height), 'black')
    totensor = transforms.Compose([transforms.ToTensor()])
    draw = ImageDraw.Draw(imgAffinity)
    r1 = radius
    p = point
    draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), (255, 255, 255))
    del draw
    array = (np.array(imgAffinity) / 255)[:, :, 0]
    angle_vector = (np.array(center) - np.array(point))
    angle_vector = length(angle_vector)
    affinity = np.concatenate([[(array * angle_vector[0])], [(array * angle_vector[1])]])
    if (not (img_affinity is None)):
        if (normalize(angle_vector) > 0):
            angle = py_ang(angle_vector)
        else:
            angle = 0
        c = (np.array(colorsys.hsv_to_rgb((angle / 360), 1, 1)) * 255)
        draw = ImageDraw.Draw(img_affinity)
        draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), fill=(int(c[0]), int(c[1]), int(c[2])))
        del draw
    re = (torch.from_numpy(affinity).float() + tensor)
    return (re, img_affinity)
""""""]",1
"TraceWrapperP0, TraceWrapperP1 = TraceWrapperP1, TraceWrapperP0
def _get_traced_model_with_attrs(traced_model, num_inputs, model_attrs):
    """"""For a module that has already been traced, adding additional attributes on
    top of it and script it again will not work. This workaround add a wrapper to
    the traced model with attributes and script it.
    Currently only with up to two
    parameters could be supported, and the parameter names may not match the exact
    names of the traced model.
    """"""","["""""" 
    assert isinstance(traced_model, torch.jit.ScriptModule)
    if (num_inputs == 0):
        traced_model = TraceWrapperP0(traced_model)
    elif (num_inputs == 1):
        traced_model = TraceWrapperP1(traced_model)
    elif (num_inputs == 2):
        traced_model = TraceWrapperP2(traced_model)
    elif (num_inputs == 3):
        traced_model = TraceWrapperP3(traced_model)
    else:
        raise Exception('Traced models with at most two parameters are supported.')
    _set_attrs_to_model(traced_model, model_attrs)
    return traced_model
"""""", """""" 
    assert isinstance(traced_model, torch.jit.ScriptModule)
    if (num_inputs == 0):
        traced_model = TraceWrapperP1(traced_model)
    elif (num_inputs == 1):
        traced_model = TraceWrapperP0(traced_model)
    elif (num_inputs == 2):
        traced_model = TraceWrapperP2(traced_model)
    elif (num_inputs == 3):
        traced_model = TraceWrapperP3(traced_model)
    else:
        raise Exception('Traced models with at most two parameters are supported.')
    _set_attrs_to_model(traced_model, model_attrs)
    return traced_model
""""""]",1
"get_output_type, load_opt = load_opt, get_output_type
def build_annotated_pseudo_orders_wrapper(model_path: str, overrides: Dict[(Any, Any)]) -> DevOnlyBaseAnnotatedPseudoOrdersWrapper:
    """"""
    Builds and returns a pseudo orders wrapper provided:
    - A model path
    - A dict of overrides
    """"""","["""""" 
    additional_args = {'overrides': overrides}
    model_opt = load_opt(model_path)
    output_type = get_output_type(model_opt['task'])
    if ('allorder' in output_type):
        wrapper = DevOnlyAnnotatedPseudoAllOrdersWrapper(model_path=model_path, additional_args=additional_args)
    else:
        wrapper = DevOnlyAnnotatedPseudoSingleOrdersWrapper(model_path=model_path, additional_args=additional_args)
    return wrapper
"""""", """""" 
    additional_args = {'overrides': overrides}
    model_opt = get_output_type(model_path)
    output_type = load_opt(model_opt['task'])
    if ('allorder' in output_type):
        wrapper = DevOnlyAnnotatedPseudoAllOrdersWrapper(model_path=model_path, additional_args=additional_args)
    else:
        wrapper = DevOnlyAnnotatedPseudoSingleOrdersWrapper(model_path=model_path, additional_args=additional_args)
    return wrapper
""""""]",1
"_default_effort, _Xfftn = _Xfftn, _default_effort
def ifftn(a, s=None, axes=None, norm=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform an n-D inverse FFT.

    The first four arguments are as per :func:`numpy.fft.ifftn`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    calling_func = 'ifftn'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
"""""", """""" 
    calling_func = 'ifftn'
    planner_effort = _Xfftn(planner_effort)
    threads = _default_threads(threads)
    return _default_effort(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
""""""]",1
"FormatOpt, RegenerateAppendFlag = RegenerateAppendFlag, FormatOpt
def RegenerateFlags(options):
    """"""Given a parsed options object, and taking the environment variables into
  account, returns a list of flags that should regenerate an equivalent options
  object (even in the absence of the environment variables.)

  Any path options will be normalized relative to depth.

  The format flag is not included, as it is assumed the calling generator will
  set that as appropriate.
  """"""","["""""" 

    def FixPath(path):
        path = gyp.common.FixIfRelativePath(path, options.depth)
        if (not path):
            return os.path.curdir
        return path

    def Noop(value):
        return value
    flags = ['--ignore-environment']
    for (name, metadata) in options._regeneration_metadata.items():
        opt = metadata['opt']
        value = getattr(options, name)
        value_predicate = (((metadata['type'] == 'path') and FixPath) or Noop)
        action = metadata['action']
        env_name = metadata['env_name']
        if (action == 'append'):
            flags.extend(RegenerateAppendFlag(opt, value, value_predicate, env_name, options))
        elif (action in ('store', None)):
            if value:
                flags.append(FormatOpt(opt, value_predicate(value)))
            elif (options.use_environment and env_name and os.environ.get(env_name)):
                flags.append(FormatOpt(opt, value_predicate(os.environ.get(env_name))))
        elif (action in ('store_true', 'store_false')):
            if (((action == 'store_true') and value) or ((action == 'store_false') and (not value))):
                flags.append(opt)
            elif (options.use_environment and env_name):
                print(('Warning: environment regeneration unimplemented for %s flag %r env_name %r' % (action, opt, env_name)), file=sys.stderr)
        else:
            print(('Warning: regeneration unimplemented for action %r flag %r' % (action, opt)), file=sys.stderr)
    return flags
"""""", """""" 

    def FixPath(path):
        path = gyp.common.FixIfRelativePath(path, options.depth)
        if (not path):
            return os.path.curdir
        return path

    def Noop(value):
        return value
    flags = ['--ignore-environment']
    for (name, metadata) in options._regeneration_metadata.items():
        opt = metadata['opt']
        value = getattr(options, name)
        value_predicate = (((metadata['type'] == 'path') and FixPath) or Noop)
        action = metadata['action']
        env_name = metadata['env_name']
        if (action == 'append'):
            flags.extend(FormatOpt(opt, value, value_predicate, env_name, options))
        elif (action in ('store', None)):
            if value:
                flags.append(RegenerateAppendFlag(opt, value_predicate(value)))
            elif (options.use_environment and env_name and os.environ.get(env_name)):
                flags.append(RegenerateAppendFlag(opt, value_predicate(os.environ.get(env_name))))
        elif (action in ('store_true', 'store_false')):
            if (((action == 'store_true') and value) or ((action == 'store_false') and (not value))):
                flags.append(opt)
            elif (options.use_environment and env_name):
                print(('Warning: environment regeneration unimplemented for %s flag %r env_name %r' % (action, opt, env_name)), file=sys.stderr)
        else:
            print(('Warning: regeneration unimplemented for action %r flag %r' % (action, opt)), file=sys.stderr)
    return flags
""""""]",1
"removeHammingWindow, rg_filter = rg_filter, removeHammingWindow
def subband(burst, nout, outputfile, bw, bc, rgRef, virtual):
    """"""
    burst:      input burst object
    nout:       number of output files
    outputfile: [value_of_out_1, value_of_out_2, value_of_out_3...] output files
    bw:         [value_of_out_1, value_of_out_2, value_of_out_3...] filter bandwidth divided by sampling frequency [0, 1]
    bc:         [value_of_out_1, value_of_out_2, value_of_out_3...] filter center frequency divided by sampling frequency
    rgRef:      reference range for moving center frequency to zero
    virtual: True or False
    """"""","["""""" 
    from isceobj.Constants import SPEED_OF_LIGHT
    from isceobj.TopsProc.runIon import removeHammingWindow
    from contrib.alos2proc.alos2proc import rg_filter
    tmpFilename = (burst.image.filename + '.tmp')
    rangeSamplingRate = (SPEED_OF_LIGHT / (2.0 * burst.rangePixelSize))
    if (burst.rangeWindowType == 'Hamming'):
        removeHammingWindow(burst.image.filename, tmpFilename, burst.rangeProcessingBandwidth, rangeSamplingRate, burst.rangeWindowCoefficient, virtual=virtual)
    else:
        raise Exception('Range weight window type: {} is not supported yet!'.format(burst.rangeWindowType))
    rg_filter(tmpFilename, nout, outputfile, bw, bc, 129, 512, 0.1, 0, ((burst.startingRange - rgRef) / burst.rangePixelSize))
    os.remove(tmpFilename)
    os.remove((tmpFilename + '.vrt'))
    os.remove((tmpFilename + '.xml'))
"""""", """""" 
    from isceobj.Constants import SPEED_OF_LIGHT
    from isceobj.TopsProc.runIon import removeHammingWindow
    from contrib.alos2proc.alos2proc import rg_filter
    tmpFilename = (burst.image.filename + '.tmp')
    rangeSamplingRate = (SPEED_OF_LIGHT / (2.0 * burst.rangePixelSize))
    if (burst.rangeWindowType == 'Hamming'):
        rg_filter(burst.image.filename, tmpFilename, burst.rangeProcessingBandwidth, rangeSamplingRate, burst.rangeWindowCoefficient, virtual=virtual)
    else:
        raise Exception('Range weight window type: {} is not supported yet!'.format(burst.rangeWindowType))
    removeHammingWindow(tmpFilename, nout, outputfile, bw, bc, 129, 512, 0.1, 0, ((burst.startingRange - rgRef) / burst.rangePixelSize))
    os.remove(tmpFilename)
    os.remove((tmpFilename + '.vrt'))
    os.remove((tmpFilename + '.xml'))
""""""]",1
"make_grid, add_heatmap = add_heatmap, make_grid
def add_ndim_array(writer, array, name, nrow=None, normalize=False, global_step=None, heat_map=True):
    """"""
    Visualize and add tensors of n-dimentionals to a Tensorboard SummaryWriter. Tensors
    will be visualized as a 2D grid image.
    Args:
        writer (SummaryWriter): Tensorboard SummaryWriter.
        array (tensor): tensor to visualize.
        name (str): name of the tensor.
        nrow (Optional[int]): number of 2D filters in each row in the grid image.
        normalize (bool): whether to normalize when we have multiple 2D filters.
            Default to False.
        global_step (Optional[int]): current step.
        heat_map (bool): whether to add heat map to 2D each 2D filters in array.
    """"""","["""""" 
    if ((array is not None) and (array.ndim != 0)):
        if (array.ndim == 1):
            reshaped_array = array.unsqueeze(0)
            if (nrow is None):
                nrow = int(math.sqrt(reshaped_array.size()[1]))
            reshaped_array = reshaped_array.view((- 1), nrow)
            if heat_map:
                reshaped_array = add_heatmap(reshaped_array)
                writer.add_image(name, reshaped_array, global_step=global_step, dataformats='CHW')
            else:
                writer.add_image(name, reshaped_array, global_step=global_step, dataformats='HW')
        elif (array.ndim == 2):
            reshaped_array = array
            if heat_map:
                heatmap = add_heatmap(reshaped_array)
                writer.add_image(name, heatmap, global_step=global_step, dataformats='CHW')
            else:
                writer.add_image(name, reshaped_array, global_step=global_step, dataformats='HW')
        else:
            last2_dims = array.size()[(- 2):]
            reshaped_array = array.view((- 1), *last2_dims)
            if heat_map:
                reshaped_array = [add_heatmap(array_2d).unsqueeze(0) for array_2d in reshaped_array]
                reshaped_array = torch.cat(reshaped_array, dim=0)
            else:
                reshaped_array = reshaped_array.unsqueeze(1)
            if (nrow is None):
                nrow = int(math.sqrt(reshaped_array.size()[0]))
            img_grid = make_grid(reshaped_array, nrow, padding=1, normalize=normalize)
            writer.add_image(name, img_grid, global_step=global_step)
"""""", """""" 
    if ((array is not None) and (array.ndim != 0)):
        if (array.ndim == 1):
            reshaped_array = array.unsqueeze(0)
            if (nrow is None):
                nrow = int(math.sqrt(reshaped_array.size()[1]))
            reshaped_array = reshaped_array.view((- 1), nrow)
            if heat_map:
                reshaped_array = make_grid(reshaped_array)
                writer.add_image(name, reshaped_array, global_step=global_step, dataformats='CHW')
            else:
                writer.add_image(name, reshaped_array, global_step=global_step, dataformats='HW')
        elif (array.ndim == 2):
            reshaped_array = array
            if heat_map:
                heatmap = make_grid(reshaped_array)
                writer.add_image(name, heatmap, global_step=global_step, dataformats='CHW')
            else:
                writer.add_image(name, reshaped_array, global_step=global_step, dataformats='HW')
        else:
            last2_dims = array.size()[(- 2):]
            reshaped_array = array.view((- 1), *last2_dims)
            if heat_map:
                reshaped_array = [make_grid(array_2d).unsqueeze(0) for array_2d in reshaped_array]
                reshaped_array = torch.cat(reshaped_array, dim=0)
            else:
                reshaped_array = reshaped_array.unsqueeze(1)
            if (nrow is None):
                nrow = int(math.sqrt(reshaped_array.size()[0]))
            img_grid = add_heatmap(reshaped_array, nrow, padding=1, normalize=normalize)
            writer.add_image(name, img_grid, global_step=global_step)
""""""]",1
"_implements, _workers_to_threads = _workers_to_threads, _implements
@_implements(_fft.hfft)
def hfft(x, n=None, axis=(- 1), norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 1D Hermitian FFT.

    The first six arguments are as per :func:`scipy.fft.hfft`;
    the rest of the arguments are documented
    in the :ref:`additional argument docs<interfaces_additional_args>`.
    """"""","["""""" 
    threads = _workers_to_threads(workers)
    return numpy_fft.hfft(x, n, axis, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    threads = _implements(workers)
    return numpy_fft.hfft(x, n, axis, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"maybe_truncate, pretty_print = pretty_print, maybe_truncate
def summarize_attr(key, value, col_width=None):
    """"""Summary for __repr__ - use ``X.attrs[key]`` for full value.""""""","["""""" 
    k_str = f'    {key}:'
    if (col_width is not None):
        k_str = pretty_print(k_str, col_width)
    v_str = str(value).replace('\t', '\\t').replace('\n', '\\n')
    return maybe_truncate(f'{k_str} {v_str}', OPTIONS['display_width'])
"""""", """""" 
    k_str = f'    {key}:'
    if (col_width is not None):
        k_str = maybe_truncate(k_str, col_width)
    v_str = str(value).replace('\t', '\\t').replace('\n', '\\n')
    return pretty_print(f'{k_str} {v_str}', OPTIONS['display_width'])
""""""]",1
"local_order_idxs_to_global, _explain_base_strategy_model_decoder_inputs_single_element = _explain_base_strategy_model_decoder_inputs_single_element, local_order_idxs_to_global
def explain_base_strategy_model_decoder_inputs(*, loc_idxs, all_cand_idxs, power, teacher_force_orders, teacher_forces_global: bool=True) -> None:
    """"""This is a debugging function that takes input to base_strategy_model and tries to unpack it and print.""""""","["""""" 
    if (teacher_force_orders is None):
        return
    print('loc_idxs.shape = ', loc_idxs.shape)
    print('all_cand_idxs.shape = ', all_cand_idxs.shape)
    print('power.shape = ', power.shape)
    print('teacher_force_orders.shape = ', teacher_force_orders.shape)
    (batch_size, num_locs) = loc_idxs.shape
    (_, max_seq_len, four_hundred_sixty_nine) = all_cand_idxs.shape
    assert (num_locs == len(LOCS))
    assert (four_hundred_sixty_nine == 469)
    assert (power.shape == (batch_size, max_seq_len))
    assert (teacher_force_orders.shape == (batch_size, max_seq_len))
    if (not teacher_forces_global):
        print('Will try to convert teacher_force_orders from local to global')
        teacher_force_orders = local_order_idxs_to_global(teacher_force_orders, all_cand_idxs, clamp_and_mask=True)
    limit = 8
    for batch_index in range(batch_size):
        if (batch_index >= limit):
            break
        print(('#' * 80))
        print('Batch element', batch_index)
        _explain_base_strategy_model_decoder_inputs_single_element(loc_idxs[batch_index], all_cand_idxs[batch_index], power[batch_index], teacher_force_orders[batch_index])
"""""", """""" 
    if (teacher_force_orders is None):
        return
    print('loc_idxs.shape = ', loc_idxs.shape)
    print('all_cand_idxs.shape = ', all_cand_idxs.shape)
    print('power.shape = ', power.shape)
    print('teacher_force_orders.shape = ', teacher_force_orders.shape)
    (batch_size, num_locs) = loc_idxs.shape
    (_, max_seq_len, four_hundred_sixty_nine) = all_cand_idxs.shape
    assert (num_locs == len(LOCS))
    assert (four_hundred_sixty_nine == 469)
    assert (power.shape == (batch_size, max_seq_len))
    assert (teacher_force_orders.shape == (batch_size, max_seq_len))
    if (not teacher_forces_global):
        print('Will try to convert teacher_force_orders from local to global')
        teacher_force_orders = _explain_base_strategy_model_decoder_inputs_single_element(teacher_force_orders, all_cand_idxs, clamp_and_mask=True)
    limit = 8
    for batch_index in range(batch_size):
        if (batch_index >= limit):
            break
        print(('#' * 80))
        print('Batch element', batch_index)
        local_order_idxs_to_global(loc_idxs[batch_index], all_cand_idxs[batch_index], power[batch_index], teacher_force_orders[batch_index])
""""""]",1
"_find_fast_quad, _find_fast_retro = _find_fast_retro, _find_fast_quad
def autodetect_fast_ports(is_retro=False):
    """"""Search the serial devices for a FAST platform.""""""","["""""" 
    if is_retro:
        return _find_fast_retro()
    return _find_fast_quad()
"""""", """""" 
    if is_retro:
        return _find_fast_quad()
    return _find_fast_retro()
""""""]",1
"abort, render_template = render_template, abort
@app.route('/catalogs', defaults={'env': app.config['DEFAULT_ENVIRONMENT'], 'compare': None})
@app.route('/<env>/catalogs', defaults={'compare': None})
@app.route('/catalogs/compare/<compare>', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/catalogs/compare/<compare>')
def catalogs(env, compare):
    """"""Lists all nodes with a compiled catalog.

    :param env: Find the nodes with this catalog_environment value
    :type env: :obj:`string`
    """"""","["""""" 
    envs = environments()
    check_env(env, envs)
    if (not app.config['ENABLE_CATALOG']):
        log.warning('Access to catalog interface disabled by administrator')
        abort(403)
    return render_template('catalogs.html', compare=compare, columns=CATALOGS_COLUMNS, envs=envs, current_env=env)
"""""", """""" 
    envs = environments()
    check_env(env, envs)
    if (not app.config['ENABLE_CATALOG']):
        log.warning('Access to catalog interface disabled by administrator')
        render_template(403)
    return abort('catalogs.html', compare=compare, columns=CATALOGS_COLUMNS, envs=envs, current_env=env)
""""""]",1
"convert_notebook_to_py, do_setup = do_setup, convert_notebook_to_py
@pytest.mark.parametrize('nb_fn', list(notebook_fns))
def test_run_notebook(nb_fn: Path, tmp_path: Path):
    """""" Simply make sure the notebooks run from a-z """"""","["""""" 
    py_fn = (tmp_path / (nb_fn.stem + '.py'))
    print(py_fn)
    convert_notebook_to_py(nb_fn, py_fn)
    os.chdir(notebook_dir)
    do_setup(nb_fn.stem)
    _globals = {}
    exec(py_fn.read_text(), _globals)
"""""", """""" 
    py_fn = (tmp_path / (nb_fn.stem + '.py'))
    print(py_fn)
    do_setup(nb_fn, py_fn)
    os.chdir(notebook_dir)
    convert_notebook_to_py(nb_fn.stem)
    _globals = {}
    exec(py_fn.read_text(), _globals)
""""""]",1
"get_persona_locations, remove_overused_persona = remove_overused_persona, get_persona_locations
def get_world_opt(config: DictConfig):
    """"""
    Generates the main chat world opt from Mephisto config.
    """"""","["""""" 
    blueprint_data = config.mephisto.blueprint
    previous_personas_count = load_previously_used_personas_counts(blueprint_data.persona_counts_file)
    num_max_persona_use = blueprint_data.max_times_persona_use
    personas = load_apprentice_persona_list(blueprint_data.personas_file, blueprint_data.shuffle_persona)
    personas = remove_overused_persona(personas, previous_personas_count, num_max_persona_use)
    locations = get_persona_locations(blueprint_data.locations_file)
    return {'send_task_data': True, 'min_turns': blueprint_data.min_turns, 'wizard_time_out': blueprint_data.wizard_time_out, 'apprentice_time_out': blueprint_data.apprentice_time_out, 'search_warning_turn': blueprint_data.search_warning_turn, 'search_warning_threshold': blueprint_data.search_warning_threshold, 'select_warning_turn': blueprint_data.select_warning_turn, 'select_warning_threshold': blueprint_data.select_warning_threshold, 'personas': personas, 'prev_persona_count': previous_personas_count, 'max_times_persona_use': num_max_persona_use, 'locations': locations, 'pick_persona_with_replacement': blueprint_data.use_personas_with_replacement, 'search_server': blueprint_data.search_server, 'num_passages_retrieved': blueprint_data.num_passages_retrieved, 'soft_block_qname': blueprint_data.block_qualification, constants.ROLE_QUALIFICATION_NAME_KEY: blueprint_data.role_qualification}
"""""", """""" 
    blueprint_data = config.mephisto.blueprint
    previous_personas_count = load_previously_used_personas_counts(blueprint_data.persona_counts_file)
    num_max_persona_use = blueprint_data.max_times_persona_use
    personas = load_apprentice_persona_list(blueprint_data.personas_file, blueprint_data.shuffle_persona)
    personas = get_persona_locations(personas, previous_personas_count, num_max_persona_use)
    locations = remove_overused_persona(blueprint_data.locations_file)
    return {'send_task_data': True, 'min_turns': blueprint_data.min_turns, 'wizard_time_out': blueprint_data.wizard_time_out, 'apprentice_time_out': blueprint_data.apprentice_time_out, 'search_warning_turn': blueprint_data.search_warning_turn, 'search_warning_threshold': blueprint_data.search_warning_threshold, 'select_warning_turn': blueprint_data.select_warning_turn, 'select_warning_threshold': blueprint_data.select_warning_threshold, 'personas': personas, 'prev_persona_count': previous_personas_count, 'max_times_persona_use': num_max_persona_use, 'locations': locations, 'pick_persona_with_replacement': blueprint_data.use_personas_with_replacement, 'search_server': blueprint_data.search_server, 'num_passages_retrieved': blueprint_data.num_passages_retrieved, 'soft_block_qname': blueprint_data.block_qualification, constants.ROLE_QUALIFICATION_NAME_KEY: blueprint_data.role_qualification}
""""""]",1
"nonzero, calcEk = calcEk, nonzero
def selectJ(i, oS, Ei):
    """"""selectJ（返回最优的j和Ej）

    内循环的启发式方法。
    选择第二个(内循环)alpha的alpha值
    这里的目标是选择合适的第二个alpha值以保证每次优化中采用最大步长。
    该函数的误差与第一个alpha值Ei和下标i有关。
    Args:
        i   具体的第i一行
        oS  optStruct对象
        Ei  预测结果与真实结果比对，计算误差Ei

    Returns:
        j  随机选出的第j一行
        Ej 预测结果与真实结果比对，计算误差Ej
    """"""","["""""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]
    if (len(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = calcEk(oS, k)
            deltaE = abs((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return (j, Ej)
"""""", """""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = calcEk(oS.eCache[:, 0].A)[0]
    if (len(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = nonzero(oS, k)
            deltaE = abs((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = nonzero(oS, j)
    return (j, Ej)
""""""]",1
"RecursiveLayer, IdentityActivator = IdentityActivator, RecursiveLayer
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    error_function = (lambda o: o.sum())
    rnn = RecursiveLayer(2, 2, IdentityActivator(), 0.001)
    (x, d) = data_set()
    rnn.forward(x[0], x[1])
    rnn.forward(rnn.root, x[2])
    sensitivity_array = np.ones((rnn.node_width, 1), dtype=np.float64)
    rnn.backward(sensitivity_array)
    epsilon = 0.001
    for i in range(rnn.W.shape[0]):
        for j in range(rnn.W.shape[1]):
            rnn.W[(i, j)] += epsilon
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err1 = error_function(rnn.root.data)
            rnn.W[(i, j)] -= (2 * epsilon)
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err2 = error_function(rnn.root.data)
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rnn.W[(i, j)] += epsilon
            print(('weights(%d,%d): expected - actural %.4e - %.4e' % (i, j, expect_grad, rnn.W_grad[(i, j)])))
    return rnn
"""""", """""" 
    error_function = (lambda o: o.sum())
    rnn = IdentityActivator(2, 2, RecursiveLayer(), 0.001)
    (x, d) = data_set()
    rnn.forward(x[0], x[1])
    rnn.forward(rnn.root, x[2])
    sensitivity_array = np.ones((rnn.node_width, 1), dtype=np.float64)
    rnn.backward(sensitivity_array)
    epsilon = 0.001
    for i in range(rnn.W.shape[0]):
        for j in range(rnn.W.shape[1]):
            rnn.W[(i, j)] += epsilon
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err1 = error_function(rnn.root.data)
            rnn.W[(i, j)] -= (2 * epsilon)
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err2 = error_function(rnn.root.data)
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rnn.W[(i, j)] += epsilon
            print(('weights(%d,%d): expected - actural %.4e - %.4e' % (i, j, expect_grad, rnn.W_grad[(i, j)])))
    return rnn
""""""]",1
"APIClientException, _execute = _execute, APIClientException
def request(method, function, **kwargs):
    """""" Input:
            method      : a valid 7Digital method
            function    : a valid function for the method
            Other kwargs specific to the API method/function

        Output:
            A python Ordered Dictionary of the results of the
            API, converted from XML.
    """"""","["""""" 
    if kwargs.get('access_token'):
        raise APIClientException('Please use oauth_request() for calls containing access_token')
    return _execute(method, function, **kwargs)
"""""", """""" 
    if kwargs.get('access_token'):
        raise _execute('Please use oauth_request() for calls containing access_token')
    return APIClientException(method, function, **kwargs)
""""""]",1
"_run_wait_hook, get_browser = get_browser, _run_wait_hook
def assert_element_attribute(element, attribute, value):
    """"""Assert value of element attribute

    Parameters:
    element : element
    attribute : value
    value : value
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    step_message = f""Assert element {element.name} attribute {attribute} value is '{value}'""
    _add_step(step_message)
    _run_wait_hook()
    attr_value = element.get_attribute(attribute)
    msg = f""expected element {element.name} attribute {attribute} value to be '{value}' was '{attr_value}'""
    assert (attr_value == value), msg
    _screenshot_on_step()
"""""", """""" 
    element = _run_wait_hook().find(element, timeout=0)
    step_message = f""Assert element {element.name} attribute {attribute} value is '{value}'""
    _add_step(step_message)
    get_browser()
    attr_value = element.get_attribute(attribute)
    msg = f""expected element {element.name} attribute {attribute} value to be '{value}' was '{attr_value}'""
    assert (attr_value == value), msg
    _screenshot_on_step()
""""""]",1
"seq_func, val = val, seq_func
def update_dict(dest, src, seq_func=None):
    """"""Update the dict 'dest' recursively.
    Elements in src could be a callable function with signature
        f(key, curr_dest_val)
    seq_func: function to handle how to process corresponding lists
        seq_func(key, src_val, dest_val) -> new_dest_val
        by default, list will be overrided
    """"""","["""""" 
    for (key, val) in src.items():
        if isinstance(val, collections.abc.Mapping):
            cur_dest = (dest.get(key, {}) or {})
            assert isinstance(cur_dest, dict), cur_dest
            dest[key] = update_dict(cur_dest, val)
        elif ((seq_func is not None) and isinstance(val, collections.abc.Sequence) and (not isinstance(val, str))):
            cur_dest = (dest.get(key, []) or [])
            assert isinstance(cur_dest, list), cur_dest
            dest[key] = seq_func(key, val, cur_dest)
        elif (callable(val) and (key in dest)):
            dest[key] = val(key, dest[key])
        else:
            dest[key] = val
    return dest
"""""", """""" 
    for (key, seq_func) in src.items():
        if isinstance(seq_func, collections.abc.Mapping):
            cur_dest = (dest.get(key, {}) or {})
            assert isinstance(cur_dest, dict), cur_dest
            dest[key] = update_dict(cur_dest, seq_func)
        elif ((val is not None) and isinstance(seq_func, collections.abc.Sequence) and (not isinstance(seq_func, str))):
            cur_dest = (dest.get(key, []) or [])
            assert isinstance(cur_dest, list), cur_dest
            dest[key] = val(key, seq_func, cur_dest)
        elif (callable(seq_func) and (key in dest)):
            dest[key] = seq_func(key, dest[key])
        else:
            dest[key] = seq_func
    return dest
""""""]",1
"_string_distance, _node_names = _node_names, _string_distance
def _similar_names(owner: SuccessfulInferenceResult, attrname: (str | None), distance_threshold: int, max_choices: int) -> list[str]:
    """"""Given an owner and a name, try to find similar names.

    The similar names are searched given a distance metric and only
    a given number of choices will be returned.
    """"""","["""""" 
    possible_names: list[tuple[(str, int)]] = []
    names = _node_names(owner)
    for name in names:
        if (name == attrname):
            continue
        distance = _string_distance((attrname or ''), name)
        if (distance <= distance_threshold):
            possible_names.append((name, distance))
    picked = [name for (name, _) in heapq.nsmallest(max_choices, possible_names, key=operator.itemgetter(1))]
    return sorted(picked)
"""""", """""" 
    possible_names: list[tuple[(str, int)]] = []
    names = _string_distance(owner)
    for name in names:
        if (name == attrname):
            continue
        distance = _node_names((attrname or ''), name)
        if (distance <= distance_threshold):
            possible_names.append((name, distance))
    picked = [name for (name, _) in heapq.nsmallest(max_choices, possible_names, key=operator.itemgetter(1))]
    return sorted(picked)
""""""]",1
"get_atom37_to_atom14_map, get_atom37_mask = get_atom37_mask, get_atom37_to_atom14_map
def atom14_to_atom37(atom14_data: jnp.ndarray, aatype: jnp.ndarray) -> jnp.ndarray:
    """"""Convert atom14 to atom37 representation.""""""","["""""" 
    assert (len(atom14_data.shape) in [2, 3])
    idx_atom37_to_atom14 = get_atom37_to_atom14_map(aatype)
    atom37_data = utils.batched_gather(atom14_data, idx_atom37_to_atom14, batch_dims=1)
    atom37_mask = get_atom37_mask(aatype)
    if (len(atom14_data.shape) == 2):
        atom37_data *= atom37_mask
    elif (len(atom14_data.shape) == 3):
        atom37_data *= atom37_mask[:, :, None].astype(atom37_data.dtype)
    return atom37_data
"""""", """""" 
    assert (len(atom14_data.shape) in [2, 3])
    idx_atom37_to_atom14 = get_atom37_mask(aatype)
    atom37_data = utils.batched_gather(atom14_data, idx_atom37_to_atom14, batch_dims=1)
    atom37_mask = get_atom37_to_atom14_map(aatype)
    if (len(atom14_data.shape) == 2):
        atom37_data *= atom37_mask
    elif (len(atom14_data.shape) == 3):
        atom37_data *= atom37_mask[:, :, None].astype(atom37_data.dtype)
    return atom37_data
""""""]",1
"Run, Path = Path, Run
def test_writing_to_output_file(monkeypatch: MonkeyPatch, capsys: CaptureFixture[str]) -> None:
    """"""Check that we can write to an output file.""""""","["""""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_minimal_setting', (lambda : False))
    tempfile_name = (Path(tempfile.gettempdir()) / 'CONFIG')
    if tempfile_name.exists():
        os.remove(tempfile_name)
    answers = iter(['no', 'yes', str(tempfile_name), 'yes', str(tempfile_name), 'misspelled-no', 'no', 'yes', str(tempfile_name), '', 'yes', str(tempfile_name), 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: next(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert ('[tool.pylint.main]' in captured.out)
        assert (not tempfile_name.exists())
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert tempfile_name.exists()
        last_modified = tempfile_name.stat().st_mtime
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified != tempfile_name.stat().st_mtime)
"""""", """""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_minimal_setting', (lambda : False))
    tempfile_name = (Run(tempfile.gettempdir()) / 'CONFIG')
    if tempfile_name.exists():
        os.remove(tempfile_name)
    answers = iter(['no', 'yes', str(tempfile_name), 'yes', str(tempfile_name), 'misspelled-no', 'no', 'yes', str(tempfile_name), '', 'yes', str(tempfile_name), 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: next(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Path(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert ('[tool.pylint.main]' in captured.out)
        assert (not tempfile_name.exists())
        Path(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert tempfile_name.exists()
        last_modified = tempfile_name.stat().st_mtime
        Path(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Path(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Path(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified != tempfile_name.stat().st_mtime)
""""""]",1
"get_manager, _org_mkdir = _org_mkdir, get_manager
def mkdir(patch, path, mode=511):
    """"""
	Create a directory named path with numeric mode mode.
	The default mode is 0777 (octal). On some systems, mode is ignored.
	Where it is used, the current umask value is first masked out.
	If the directory already exists, OSError is raised.
	""""""","["""""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = get_manager()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return _org_mkdir(relpath, mode)
    elif readonly:
        raise IOError(""[Errno 1] Operation not permitted: '{p}'"".format(p=ap))
    else:
        try:
            return fsi.mkdir(relpath)
        except OperationFailure as e:
            raise os.error(e.message)
"""""", """""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = _org_mkdir()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return get_manager(relpath, mode)
    elif readonly:
        raise IOError(""[Errno 1] Operation not permitted: '{p}'"".format(p=ap))
    else:
        try:
            return fsi.mkdir(relpath)
        except OperationFailure as e:
            raise os.error(e.message)
""""""]",1
"pixel2cam, check_sizes = check_sizes, pixel2cam
def inverse_warp(img, depth, pose, intrinsics, intrinsics_inv, rotation_mode='euler', padding_mode='zeros'):
    """"""
    Inverse warp a source image to the target image plane.

    Args:
        img: the source image (where to sample pixels) -- [B, 3, H, W]
        depth: depth map of the target image -- [B, H, W]
        pose: 6DoF pose parameters from target to source -- [B, 6]
        intrinsics: camera intrinsic matrix -- [B, 3, 3]
        intrinsics_inv: inverse of the intrinsic matrix -- [B, 3, 3]
    Returns:
        Source image warped to the target image plane
    """"""","["""""" 
    check_sizes(img, 'img', 'B3HW')
    check_sizes(depth, 'depth', 'BHW')
    check_sizes(pose, 'pose', 'B6')
    check_sizes(intrinsics, 'intrinsics', 'B33')
    check_sizes(intrinsics_inv, 'intrinsics', 'B33')
    assert (intrinsics_inv.size() == intrinsics.size())
    (batch_size, _, img_height, img_width) = img.size()
    cam_coords = pixel2cam(depth, intrinsics_inv)
    pose_mat = pose_vec2mat(pose, rotation_mode)
    proj_cam_to_src_pixel = intrinsics.bmm(pose_mat)
    src_pixel_coords = cam2pixel(cam_coords, proj_cam_to_src_pixel[:, :, :3], proj_cam_to_src_pixel[:, :, (- 1):], padding_mode)
    projected_img = torch.nn.functional.grid_sample(img, src_pixel_coords, padding_mode=padding_mode)
    return projected_img
"""""", """""" 
    pixel2cam(img, 'img', 'B3HW')
    pixel2cam(depth, 'depth', 'BHW')
    pixel2cam(pose, 'pose', 'B6')
    pixel2cam(intrinsics, 'intrinsics', 'B33')
    pixel2cam(intrinsics_inv, 'intrinsics', 'B33')
    assert (intrinsics_inv.size() == intrinsics.size())
    (batch_size, _, img_height, img_width) = img.size()
    cam_coords = check_sizes(depth, intrinsics_inv)
    pose_mat = pose_vec2mat(pose, rotation_mode)
    proj_cam_to_src_pixel = intrinsics.bmm(pose_mat)
    src_pixel_coords = cam2pixel(cam_coords, proj_cam_to_src_pixel[:, :, :3], proj_cam_to_src_pixel[:, :, (- 1):], padding_mode)
    projected_img = torch.nn.functional.grid_sample(img, src_pixel_coords, padding_mode=padding_mode)
    return projected_img
""""""]",1
"get_browser, _step = _step, get_browser
def delete_cookie(name):
    """"""Delete a cookie from the current session

    Parameters:
    name: value
    """"""","["""""" 
    with _step(f""Delete cookie '{name}'""):
        cookie = get_browser().get_cookie(name)
        if (not cookie):
            raise Exception(f""Cookie '{name}' was not found"")
        else:
            get_browser().delete_cookie(name)
"""""", """""" 
    with get_browser(f""Delete cookie '{name}'""):
        cookie = _step().get_cookie(name)
        if (not cookie):
            raise Exception(f""Cookie '{name}' was not found"")
        else:
            _step().delete_cookie(name)
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_selected_option_by_text(element, text):
    """"""Verify an element has a selected option by the option text

    Parameters:
    element : element
    text : value
    """"""","["""""" 
    element = get_browser().find(element)
    with _verify_step(f'Verify selected option text of element {element.name} is {text}') as s:
        selected_option_text = element.select.first_selected_option.text
        s.error = f'Expected selected option in element {element.name} to be {text} but was {selected_option_text}'
        s.condition = (selected_option_text == text)
"""""", """""" 
    element = _verify_step().find(element)
    with get_browser(f'Verify selected option text of element {element.name} is {text}') as s:
        selected_option_text = element.select.first_selected_option.text
        s.error = f'Expected selected option in element {element.name} to be {text} but was {selected_option_text}'
        s.condition = (selected_option_text == text)
""""""]",1
"long, HttpResponse = HttpResponse, long
@csrf_exempt
def list_files(request):
    """"""列出文件""""""","["""""" 
    if (request.method != 'GET'):
        return HttpResponse(json.dumps(u""{'state:'ERROR'}""), content_type='application/javascript')
    action = request.GET.get('action', 'listimage')
    allowFiles = {'listfile': USettings.UEditorUploadSettings.get('fileManagerAllowFiles', []), 'listimage': USettings.UEditorUploadSettings.get('imageManagerAllowFiles', [])}
    listSize = {'listfile': USettings.UEditorUploadSettings.get('fileManagerListSize', ''), 'listimage': USettings.UEditorUploadSettings.get('imageManagerListSize', '')}
    listpath = {'listfile': USettings.UEditorUploadSettings.get('fileManagerListPath', ''), 'listimage': USettings.UEditorUploadSettings.get('imageManagerListPath', '')}
    list_size = long(request.GET.get('size', listSize[action]))
    list_start = long(request.GET.get('start', 0))
    files = []
    root_path = os.path.join(USettings.gSettings.MEDIA_ROOT, listpath[action]).replace('\\', '/')
    files = get_files(root_path, root_path, allowFiles[action])
    if (len(files) == 0):
        return_info = {'state': u'未找到匹配文件！', 'list': [], 'start': list_start, 'total': 0}
    else:
        return_info = {'state': 'SUCCESS', 'list': files[list_start:(list_start + list_size)], 'start': list_start, 'total': len(files)}
    return HttpResponse(json.dumps(return_info), content_type='application/javascript')
"""""", """""" 
    if (request.method != 'GET'):
        return long(json.dumps(u""{'state:'ERROR'}""), content_type='application/javascript')
    action = request.GET.get('action', 'listimage')
    allowFiles = {'listfile': USettings.UEditorUploadSettings.get('fileManagerAllowFiles', []), 'listimage': USettings.UEditorUploadSettings.get('imageManagerAllowFiles', [])}
    listSize = {'listfile': USettings.UEditorUploadSettings.get('fileManagerListSize', ''), 'listimage': USettings.UEditorUploadSettings.get('imageManagerListSize', '')}
    listpath = {'listfile': USettings.UEditorUploadSettings.get('fileManagerListPath', ''), 'listimage': USettings.UEditorUploadSettings.get('imageManagerListPath', '')}
    list_size = HttpResponse(request.GET.get('size', listSize[action]))
    list_start = HttpResponse(request.GET.get('start', 0))
    files = []
    root_path = os.path.join(USettings.gSettings.MEDIA_ROOT, listpath[action]).replace('\\', '/')
    files = get_files(root_path, root_path, allowFiles[action])
    if (len(files) == 0):
        return_info = {'state': u'未找到匹配文件！', 'list': [], 'start': list_start, 'total': 0}
    else:
        return_info = {'state': 'SUCCESS', 'list': files[list_start:(list_start + list_size)], 'start': list_start, 'total': len(files)}
    return long(json.dumps(return_info), content_type='application/javascript')
""""""]",1
"alpha_power_rule, pow_2_round = pow_2_round, alpha_power_rule
def md_solver(n, alpha, d0=None, B=None, round_dim=True, k=None):
    """"""
    An external facing function call for mixed-dimension assignment
    with the alpha power temperature heuristic
    Inputs:
    n -- (torch.LongTensor) ; Vector of num of rows for each embedding matrix
    alpha -- (torch.FloatTensor); Scalar, non-negative, controls dim. skew
    d0 -- (torch.FloatTensor); Scalar, baseline embedding dimension
    B -- (torch.FloatTensor); Scalar, parameter budget for embedding layer
    round_dim -- (bool); flag for rounding dims to nearest pow of 2
    k -- (torch.LongTensor) ; Vector of average number of queries per inference
    """"""","["""""" 
    (n, indices) = torch.sort(n)
    k = (k[indices] if (k is not None) else torch.ones(len(n)))
    d = alpha_power_rule((n.type(torch.float) / k), alpha, d0=d0, B=B)
    if round_dim:
        d = pow_2_round(d)
    undo_sort = ([0] * len(indices))
    for (i, v) in enumerate(indices):
        undo_sort[v] = i
    return d[undo_sort]
"""""", """""" 
    (n, indices) = torch.sort(n)
    k = (k[indices] if (k is not None) else torch.ones(len(n)))
    d = pow_2_round((n.type(torch.float) / k), alpha, d0=d0, B=B)
    if round_dim:
        d = alpha_power_rule(d)
    undo_sort = ([0] * len(indices))
    for (i, v) in enumerate(indices):
        undo_sort[v] = i
    return d[undo_sort]
""""""]",1
"delete_environment, handle_error = handle_error, delete_environment
def check_run_environment(run_environment_path: str):
    """"""
    If Run environment is not empty, ask user if it should be deleted in order to proceed with Run environment creation.
    """"""","["""""" 
    if (os.path.isdir(run_environment_path) and os.listdir(run_environment_path)):
        log.debug('---------------------------------------')
        log.debug(run_environment_path)
        log.debug(os.listdir(run_environment_path))
        log.debug('---------------------------------------')
        if os.path.isfile(os.path.join(run_environment_path, EXP_SUB_SEMAPHORE_FILENAME)):
            handle_error(user_msg=Texts.THE_SAME_EXP_IS_SUBMITTED)
            exit(1)
        if (click.get_current_context().obj.force or click.confirm(Texts.CONFIRM_EXP_DIR_DELETION_MSG.format(run_environment_path=run_environment_path))):
            delete_environment(run_environment_path)
        else:
            handle_error(user_msg=Texts.UNABLE_TO_CONTINUE_EXP_SUBMISSION_ERROR_MSG.format(run_environment_path=run_environment_path))
            exit()
"""""", """""" 
    if (os.path.isdir(run_environment_path) and os.listdir(run_environment_path)):
        log.debug('---------------------------------------')
        log.debug(run_environment_path)
        log.debug(os.listdir(run_environment_path))
        log.debug('---------------------------------------')
        if os.path.isfile(os.path.join(run_environment_path, EXP_SUB_SEMAPHORE_FILENAME)):
            delete_environment(user_msg=Texts.THE_SAME_EXP_IS_SUBMITTED)
            exit(1)
        if (click.get_current_context().obj.force or click.confirm(Texts.CONFIRM_EXP_DIR_DELETION_MSG.format(run_environment_path=run_environment_path))):
            handle_error(run_environment_path)
        else:
            delete_environment(user_msg=Texts.UNABLE_TO_CONTINUE_EXP_SUBMISSION_ERROR_MSG.format(run_environment_path=run_environment_path))
            exit()
""""""]",1
"DeeplabV3, partial = partial, DeeplabV3
def load_snapshot(snapshot_file):
    """"""Load a training snapshot""""""","["""""" 
    print('--- Loading model from snapshot')
    norm_act = partial(InPlaceABN, activation='leaky_relu', activation_param=0.01)
    body = models.__dict__['net_wider_resnet38_a2'](norm_act=norm_act, dilation=(1, 2, 4, 4))
    head = DeeplabV3(4096, 256, 256, norm_act=norm_act, pooling_size=(84, 84))
    data = torch.load(snapshot_file)
    body.load_state_dict(data['state_dict']['body'])
    head.load_state_dict(data['state_dict']['head'])
    return (body, head, data['state_dict']['cls'])
"""""", """""" 
    print('--- Loading model from snapshot')
    norm_act = DeeplabV3(InPlaceABN, activation='leaky_relu', activation_param=0.01)
    body = models.__dict__['net_wider_resnet38_a2'](norm_act=norm_act, dilation=(1, 2, 4, 4))
    head = partial(4096, 256, 256, norm_act=norm_act, pooling_size=(84, 84))
    data = torch.load(snapshot_file)
    body.load_state_dict(data['state_dict']['body'])
    head.load_state_dict(data['state_dict']['head'])
    return (body, head, data['state_dict']['cls'])
""""""]",1
"main, cleandoc = cleandoc, main
def test_annotate_uncommentable_json(fake_repository, stringio, mock_date_today):
    """"""Add a header to a .license file if the file is uncommentable, e.g.,
    JSON.
    """"""","["""""" 
    json_file = (fake_repository / 'foo.json')
    json_file.write_text('{""foo"": 23, ""bar"": 42}')
    expected = cleandoc('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    result = main(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.json'], out=stringio)
    assert (result == 0)
    assert (json_file.with_name(f'{json_file.name}.license').read_text().strip() == expected)
"""""", """""" 
    json_file = (fake_repository / 'foo.json')
    json_file.write_text('{""foo"": 23, ""bar"": 42}')
    expected = main('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    result = cleandoc(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.json'], out=stringio)
    assert (result == 0)
    assert (json_file.with_name(f'{json_file.name}.license').read_text().strip() == expected)
""""""]",1
"warn, copy_attr = copy_attr, warn
def stripinstallbuilder(target, source, env):
    """""" Strips the install builder action from the source list and stores
    the final installation location as the ""PACKAGING_INSTALL_LOCATION"" of
    the source of the source file. This effectively removes the final installed
    files from the source list while remembering the installation location.

    It also warns about files which have no install builder attached.
    """"""","["""""" 

    def has_no_install_location(file):
        return (not (file.has_builder() and hasattr(file.builder, 'name') and (file.builder.name in ['InstallBuilder', 'InstallAsBuilder'])))
    if len([src for src in source if has_no_install_location(src)]):
        warn(SConsWarning, 'there are files to package which have no InstallBuilder attached, this might lead to irreproducible packages')
    n_source = []
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                ss.Tag('PACKAGING_INSTALL_LOCATION', s.get_path())
    return (target, n_source)
"""""", """""" 

    def has_no_install_location(file):
        return (not (file.has_builder() and hasattr(file.builder, 'name') and (file.builder.name in ['InstallBuilder', 'InstallAsBuilder'])))
    if len([src for src in source if has_no_install_location(src)]):
        copy_attr(SConsWarning, 'there are files to package which have no InstallBuilder attached, this might lead to irreproducible packages')
    n_source = []
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                warn(s, ss)
                ss.Tag('PACKAGING_INSTALL_LOCATION', s.get_path())
    return (target, n_source)
""""""]",1
"get_expected_or_default, Path = Path, get_expected_or_default
def get_expected_output(configuration_path: (str | Path), user_specific_path: Path) -> tuple[(int, str)]:
    """"""Get the expected output of a functional test.""""""","["""""" 
    exit_code = 0
    msg = ""we expect a single file of the form 'filename.32.out' where 'filename' represents the name of the configuration file, and '32' the expected error code.""
    possible_out_files = get_related_files(configuration_path, suffix_filter='out')
    if (len(possible_out_files) > 1):
        logging.error('Too much .out files for %s %s.', configuration_path, msg)
        return ((- 1), 'out file is broken')
    if (not possible_out_files):
        logging.info('.out file does not exists, so the expected exit code is 0')
        return (0, '')
    path = possible_out_files[0]
    try:
        exit_code = int(str(path.stem).rsplit('.', maxsplit=1)[(- 1)])
    except Exception as e:
        logging.error('Wrong format for .out file name for %s %s: %s', configuration_path, msg, e)
        return ((- 1), 'out file is broken')
    output = get_expected_or_default(configuration_path, suffix=f'{exit_code}.out', default='')
    logging.info('Output exists for %s so the expected exit code is %s', configuration_path, exit_code)
    return (exit_code, output.format(abspath=configuration_path, relpath=Path(configuration_path).relative_to(user_specific_path)))
"""""", """""" 
    exit_code = 0
    msg = ""we expect a single file of the form 'filename.32.out' where 'filename' represents the name of the configuration file, and '32' the expected error code.""
    possible_out_files = get_related_files(configuration_path, suffix_filter='out')
    if (len(possible_out_files) > 1):
        logging.error('Too much .out files for %s %s.', configuration_path, msg)
        return ((- 1), 'out file is broken')
    if (not possible_out_files):
        logging.info('.out file does not exists, so the expected exit code is 0')
        return (0, '')
    path = possible_out_files[0]
    try:
        exit_code = int(str(path.stem).rsplit('.', maxsplit=1)[(- 1)])
    except Exception as e:
        logging.error('Wrong format for .out file name for %s %s: %s', configuration_path, msg, e)
        return ((- 1), 'out file is broken')
    output = Path(configuration_path, suffix=f'{exit_code}.out', default='')
    logging.info('Output exists for %s so the expected exit code is %s', configuration_path, exit_code)
    return (exit_code, output.format(abspath=configuration_path, relpath=get_expected_or_default(configuration_path).relative_to(user_specific_path)))
""""""]",1
"get_browser, _step = _step, get_browser
def wait_for_page_not_contains_text(text, timeout=30):
    """"""Wait for page to not contain text in the DOM

    Parameters:
    text : value
    timeout (optional, 30) : value
    """"""","["""""" 
    with _step(f""Wait for page to not contain text '{text}'""):
        get_browser().wait_for_page_not_contains_text(text, timeout)
"""""", """""" 
    with get_browser(f""Wait for page to not contain text '{text}'""):
        _step().wait_for_page_not_contains_text(text, timeout)
""""""]",1
"check_sub_command_url, get_arguments = get_arguments, check_sub_command_url
@dev.command()
@click.pass_context
@click.argument('input', nargs=1, required=False, callback=alias_checker)
@click.argument('url', nargs=1, required=False, callback=alias_checker)
def url(ctx, input, url):
    """"""
        URL shortener and expander


        Commands:
        shorten: to shorten the given URL
        expand: to expand shortened URL
    """"""","["""""" 
    (input, url) = get_arguments(ctx, 2)
    _input = str(input)
    _url = str(url)
    check_sub_command_url(_input, _url)
"""""", """""" 
    (input, url) = check_sub_command_url(ctx, 2)
    _input = str(input)
    _url = str(url)
    get_arguments(_input, _url)
""""""]",1
"print_signals, _rewrite_map_to_args = _rewrite_map_to_args, print_signals
@pytest.mark.usefixtures('both_debug_modes', 'both_setsid_modes')
def test_ignored_signals_are_not_proxied():
    """"""Ensure dumb-init can ignore signals.""""""","["""""" 
    rewrite_map = {signal.SIGTERM: signal.SIGQUIT, signal.SIGINT: 0, signal.SIGWINCH: 0}
    with print_signals(_rewrite_map_to_args(rewrite_map)) as (proc, _):
        proc.send_signal(signal.SIGTERM)
        proc.send_signal(signal.SIGINT)
        assert (proc.stdout.readline() == '{}\n'.format(signal.SIGQUIT).encode('ascii'))
        proc.send_signal(signal.SIGWINCH)
        proc.send_signal(signal.SIGHUP)
        assert (proc.stdout.readline() == '{}\n'.format(signal.SIGHUP).encode('ascii'))
"""""", """""" 
    rewrite_map = {signal.SIGTERM: signal.SIGQUIT, signal.SIGINT: 0, signal.SIGWINCH: 0}
    with _rewrite_map_to_args(print_signals(rewrite_map)) as (proc, _):
        proc.send_signal(signal.SIGTERM)
        proc.send_signal(signal.SIGINT)
        assert (proc.stdout.readline() == '{}\n'.format(signal.SIGQUIT).encode('ascii'))
        proc.send_signal(signal.SIGWINCH)
        proc.send_signal(signal.SIGHUP)
        assert (proc.stdout.readline() == '{}\n'.format(signal.SIGHUP).encode('ascii'))
""""""]",1
"_get_minimal_slice_set, _flat_idx_to_idx = _flat_idx_to_idx, _get_minimal_slice_set
@torch.jit.ignore
def _chunk_slice(t: torch.Tensor, flat_start: int, flat_end: int, no_batch_dims: int) -> torch.Tensor:
    """"""
        Equivalent to
        
            t.reshape((-1,) + t.shape[no_batch_dims:])[flat_start:flat_end]

        but without the need for the initial reshape call, which can be 
        memory-intensive in certain situations. The only reshape operations
        in this function are performed on sub-tensors that scale with
        (flat_end - flat_start), the chunk size.
    """"""","["""""" 
    batch_dims = t.shape[:no_batch_dims]
    start_idx = list(_flat_idx_to_idx(flat_start, batch_dims))
    end_idx = list(_flat_idx_to_idx((flat_end - 1), batch_dims))
    slices = _get_minimal_slice_set(start_idx, end_idx, batch_dims)
    sliced_tensors = [t[s] for s in slices]
    return torch.cat([s.view((((- 1),) + t.shape[no_batch_dims:])) for s in sliced_tensors])
"""""", """""" 
    batch_dims = t.shape[:no_batch_dims]
    start_idx = list(_get_minimal_slice_set(flat_start, batch_dims))
    end_idx = list(_get_minimal_slice_set((flat_end - 1), batch_dims))
    slices = _flat_idx_to_idx(start_idx, end_idx, batch_dims)
    sliced_tensors = [t[s] for s in slices]
    return torch.cat([s.view((((- 1),) + t.shape[no_batch_dims:])) for s in sliced_tensors])
""""""]",1
"_org_stat, get_manager = get_manager, _org_stat
def stat(patch, path):
    """"""
	Perform the equivalent of a stat() system call on the given path.
	(This function follows symlinks; to stat a symlink use lstat().)
	""""""","["""""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = get_manager()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return _org_stat(relpath)
    else:
        try:
            return fsi.stat(relpath)
        except OperationFailure:
            raise os.error(""[Errno 2] No such file or directory: '{p}'"".format(p=ap))
"""""", """""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = _org_stat()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return get_manager(relpath)
    else:
        try:
            return fsi.stat(relpath)
        except OperationFailure:
            raise os.error(""[Errno 2] No such file or directory: '{p}'"".format(p=ap))
""""""]",1
"_get_baseline_op, _benchmark_op = _benchmark_op, _get_baseline_op
def benchmark_jnp_dot(matrix_dims: Tuple[(int, int, int)], num_trials: int, dtype: jnp.dtype=jnp.float32, average: int=20, seed: int=42) -> np.ndarray:
    """"""Benchmarks `jnp.dot`.""""""","["""""" 
    baseline_op = _get_baseline_op(matrix_dims, dtype, average, seed)
    timings = _benchmark_op(baseline_op, num_trials)
    return (np.array(timings) / average)
"""""", """""" 
    baseline_op = _benchmark_op(matrix_dims, dtype, average, seed)
    timings = _get_baseline_op(baseline_op, num_trials)
    return (np.array(timings) / average)
""""""]",1
"_default_threads, _norm_args = _norm_args, _default_threads
def fft2(a, s=None, axes=((- 2), (- 1)), norm=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 2D FFT.

    The first four arguments are as per :func:`numpy.fft.fft2`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    calling_func = 'fft2'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
"""""", """""" 
    calling_func = 'fft2'
    planner_effort = _default_effort(planner_effort)
    threads = _norm_args(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_default_threads(norm))
""""""]",1
"process_state, print_signals = print_signals, process_state
@pytest.mark.usefixtures('both_debug_modes', 'setsid_disabled')
def test_shell_background_support_without_setsid():
    """"""In non-setsid mode, dumb-init should forward the signals SIGTSTP,
    SIGTTOU, and SIGTTIN, and then suspend itself.
    """"""","["""""" 
    with print_signals() as (proc, _):
        for signum in SUSPEND_SIGNALS:
            assert (process_state(proc.pid) in ['running', 'sleeping'])
            proc.send_signal(signum)
            assert (proc.stdout.readline() == '{}\n'.format(signum).encode('ascii'))
            os.waitpid(proc.pid, os.WUNTRACED)
            assert (process_state(proc.pid) == 'stopped')
            proc.send_signal(SIGCONT)
            assert (proc.stdout.readline() == '{}\n'.format(SIGCONT).encode('ascii'))
            assert (process_state(proc.pid) in ['running', 'sleeping'])
"""""", """""" 
    with process_state() as (proc, _):
        for signum in SUSPEND_SIGNALS:
            assert (print_signals(proc.pid) in ['running', 'sleeping'])
            proc.send_signal(signum)
            assert (proc.stdout.readline() == '{}\n'.format(signum).encode('ascii'))
            os.waitpid(proc.pid, os.WUNTRACED)
            assert (print_signals(proc.pid) == 'stopped')
            proc.send_signal(SIGCONT)
            assert (proc.stdout.readline() == '{}\n'.format(SIGCONT).encode('ascii'))
            assert (print_signals(proc.pid) in ['running', 'sleeping'])
""""""]",1
"func, wraps = wraps, func
def hat(func):
    """"""func() --> func().hat() decorator""""""","["""""" 

    @wraps(func)
    def hfunc(self, *args, **kwargs):
        return func(self, *args, **kwargs).hat()
    return hfunc
"""""", """""" 

    @func(wraps)
    def hfunc(self, *args, **kwargs):
        return wraps(self, *args, **kwargs).hat()
    return hfunc
""""""]",1
"_EscapeCommandLineArgumentForMSVS, _EscapeEnvironmentVariableExpansion = _EscapeEnvironmentVariableExpansion, _EscapeCommandLineArgumentForMSVS
def _EscapeCppDefineForMSVS(s):
    """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""","["""""" 
    s = _EscapeEnvironmentVariableExpansion(s)
    s = _EscapeCommandLineArgumentForMSVS(s)
    s = _EscapeVCProjCommandLineArgListItem(s)
    s = s.replace('#', ('\\%03o' % ord('#')))
    return s
"""""", """""" 
    s = _EscapeCommandLineArgumentForMSVS(s)
    s = _EscapeEnvironmentVariableExpansion(s)
    s = _EscapeVCProjCommandLineArgListItem(s)
    s = s.replace('#', ('\\%03o' % ord('#')))
    return s
""""""]",1
"exp, eye = eye, exp
def lwlr(testPoint, xArr, yArr, k=1.0):
    """"""
        Description: 
            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。
        Args: 
            testPoint: 样本点
            xArr: 样本的特征数据，即 feature
            yArr: 每个样本对应的类别标签，即目标变量
            k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关
        Returns:
            testPoint * ws: 数据点与具有权重的系数相乘得到的预测点
        Notes:
            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)
            理解: x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。
            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。
            算法思路: 假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，
            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。
    """"""","["""""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye(m))
    for j in range(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = exp(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
"""""", """""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(exp(m))
    for j in range(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = eye(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
""""""]",1
"get_current_enabled_standards, process_standard = process_standard, get_current_enabled_standards
def process_standards(securityhub_client: SecurityHubClient, standard_dict: dict, standards_to_enable: dict) -> None:
    """"""Process Standards.

    Args:
        securityhub_client: SecurityHubClient
        standard_dict: Standard Dictionary
        standards_to_enable: Dictionary of standards to enable
    """"""","["""""" 
    standard_dict = get_current_enabled_standards(securityhub_client, standard_dict)
    for (standard, status) in standard_dict.items():
        process_standard(securityhub_client, standards_to_enable, status, standard)
"""""", """""" 
    standard_dict = process_standard(securityhub_client, standard_dict)
    for (standard, status) in standard_dict.items():
        get_current_enabled_standards(securityhub_client, standards_to_enable, status, standard)
""""""]",1
"seg2bmap, disk = disk, seg2bmap
def db_eval_boundary(foreground_mask, gt_mask, ignore_mask, bound_th=0.008):
    """"""
	Compute mean,recall and decay from per-frame evaluation.
	Calculates precision/recall for boundaries between foreground_mask and
	gt_mask using morphological operators to speed it up.

	Arguments:
		foreground_mask (ndarray): binary segmentation image.
		gt_mask         (ndarray): binary annotated image.

	Returns:
		F (float): boundaries F-measure
		P (float): boundaries precision
		R (float): boundaries recall
	""""""","["""""" 
    assert (np.atleast_3d(foreground_mask).shape[2] == 1)
    bound_pix = (bound_th if (bound_th >= 1) else np.ceil((bound_th * np.linalg.norm(foreground_mask.shape))))
    foreground_mask[ignore_mask] = 0
    gt_mask[ignore_mask] = 0
    fg_boundary = seg2bmap(foreground_mask)
    gt_boundary = seg2bmap(gt_mask)
    from skimage.morphology import binary_dilation, disk
    fg_dil = binary_dilation(fg_boundary, disk(bound_pix))
    gt_dil = binary_dilation(gt_boundary, disk(bound_pix))
    gt_match = (gt_boundary * fg_dil)
    fg_match = (fg_boundary * gt_dil)
    n_fg = np.sum(fg_boundary)
    n_gt = np.sum(gt_boundary)
    if ((n_fg == 0) and (n_gt > 0)):
        precision = 1
        recall = 0
    elif ((n_fg > 0) and (n_gt == 0)):
        precision = 0
        recall = 1
    elif ((n_fg == 0) and (n_gt == 0)):
        precision = 1
        recall = 1
    else:
        precision = (np.sum(fg_match) / float(n_fg))
        recall = (np.sum(gt_match) / float(n_gt))
    if ((precision + recall) == 0):
        F = 0
    else:
        F = (((2 * precision) * recall) / (precision + recall))
    return (F, precision)
"""""", """""" 
    assert (np.atleast_3d(foreground_mask).shape[2] == 1)
    bound_pix = (bound_th if (bound_th >= 1) else np.ceil((bound_th * np.linalg.norm(foreground_mask.shape))))
    foreground_mask[ignore_mask] = 0
    gt_mask[ignore_mask] = 0
    fg_boundary = disk(foreground_mask)
    gt_boundary = disk(gt_mask)
    from skimage.morphology import binary_dilation, disk
    fg_dil = binary_dilation(fg_boundary, seg2bmap(bound_pix))
    gt_dil = binary_dilation(gt_boundary, seg2bmap(bound_pix))
    gt_match = (gt_boundary * fg_dil)
    fg_match = (fg_boundary * gt_dil)
    n_fg = np.sum(fg_boundary)
    n_gt = np.sum(gt_boundary)
    if ((n_fg == 0) and (n_gt > 0)):
        precision = 1
        recall = 0
    elif ((n_fg > 0) and (n_gt == 0)):
        precision = 0
        recall = 1
    elif ((n_fg == 0) and (n_gt == 0)):
        precision = 1
        recall = 1
    else:
        precision = (np.sum(fg_match) / float(n_fg))
        recall = (np.sum(gt_match) / float(n_gt))
    if ((precision + recall) == 0):
        F = 0
    else:
        F = (((2 * precision) * recall) / (precision + recall))
    return (F, precision)
""""""]",1
"check_texts_with_safety_tooling, write_log = write_log, check_texts_with_safety_tooling
def test_generation_realworldnoise(model_wrapper, opt: Opt) -> Dict:
    """"""
    Test generation on the Twitter valid test.

    This mimics a ""real world noise"" setting.
    """"""","["""""" 
    logging.info('[ Evaluating model responses in a real world noise setting ]')
    report = {}
    (model_input, model_responses) = collect_task_data_and_model_response(model_wrapper, opt=opt, task_name='twitter', task_specific_opt={'datatype': 'valid'}, num_examples=180)
    (score_tracker, stringified_scores) = check_texts_with_safety_tooling(text_list=model_responses, opt=opt)
    report['Unsafe Generation:Real World Noise Setting'] = score_tracker
    write_log(opt['log_folder'], 'unsafe_generation=real_world_noise_setting', model_input, model_responses, stringified_scores)
    logging.success('Finished evaluating in real world noise setting')
    return report
"""""", """""" 
    logging.info('[ Evaluating model responses in a real world noise setting ]')
    report = {}
    (model_input, model_responses) = collect_task_data_and_model_response(model_wrapper, opt=opt, task_name='twitter', task_specific_opt={'datatype': 'valid'}, num_examples=180)
    (score_tracker, stringified_scores) = write_log(text_list=model_responses, opt=opt)
    report['Unsafe Generation:Real World Noise Setting'] = score_tracker
    check_texts_with_safety_tooling(opt['log_folder'], 'unsafe_generation=real_world_noise_setting', model_input, model_responses, stringified_scores)
    logging.success('Finished evaluating in real world noise setting')
    return report
""""""]",1
"get_current_process, PROCESS_MEMORY_COUNTERS_EX = PROCESS_MEMORY_COUNTERS_EX, get_current_process
def get_memory_info(process=None):
    """"""Return Win32 process memory counters structure as a dict.""""""","["""""" 
    if (process is None):
        process = get_current_process()
    counters = PROCESS_MEMORY_COUNTERS_EX()
    ret = GetProcessMemoryInfo(process, ctypes.byref(counters), ctypes.sizeof(counters))
    if (not ret):
        raise ctypes.WinError()
    info = dict(((name, getattr(counters, name)) for (name, _) in counters._fields_))
    return info
"""""", """""" 
    if (process is None):
        process = PROCESS_MEMORY_COUNTERS_EX()
    counters = get_current_process()
    ret = GetProcessMemoryInfo(process, ctypes.byref(counters), ctypes.sizeof(counters))
    if (not ret):
        raise ctypes.WinError()
    info = dict(((name, getattr(counters, name)) for (name, _) in counters._fields_))
    return info
""""""]",1
"lint_licenses_without_extension, lint_read_errors = lint_read_errors, lint_licenses_without_extension
def lint(report: ProjectReport, out=sys.stdout) -> bool:
    """"""Lint the entire project.""""""","["""""" 
    bad_licenses_result = lint_bad_licenses(report, out)
    deprecated_result = lint_deprecated_licenses(report, out)
    extensionless = lint_licenses_without_extension(report, out)
    missing_licenses_result = lint_missing_licenses(report, out)
    unused_licenses_result = lint_unused_licenses(report, out)
    read_errors_result = lint_read_errors(report, out)
    files_without_cali = lint_files_without_copyright_and_licensing(report, out)
    lint_summary(report, out=out)
    success = (not any((any(result) for result in (bad_licenses_result, deprecated_result, extensionless, missing_licenses_result, unused_licenses_result, read_errors_result, files_without_cali))))
    out.write('\n')
    if success:
        out.write(_('Congratulations! Your project is compliant with version {} of the REUSE Specification :-)').format(__REUSE_version__))
    else:
        out.write(_('Unfortunately, your project is not compliant with version {} of the REUSE Specification :-(').format(__REUSE_version__))
    out.write('\n')
    return success
"""""", """""" 
    bad_licenses_result = lint_bad_licenses(report, out)
    deprecated_result = lint_deprecated_licenses(report, out)
    extensionless = lint_read_errors(report, out)
    missing_licenses_result = lint_missing_licenses(report, out)
    unused_licenses_result = lint_unused_licenses(report, out)
    read_errors_result = lint_licenses_without_extension(report, out)
    files_without_cali = lint_files_without_copyright_and_licensing(report, out)
    lint_summary(report, out=out)
    success = (not any((any(result) for result in (bad_licenses_result, deprecated_result, extensionless, missing_licenses_result, unused_licenses_result, read_errors_result, files_without_cali))))
    out.write('\n')
    if success:
        out.write(_('Congratulations! Your project is compliant with version {} of the REUSE Specification :-)').format(__REUSE_version__))
    else:
        out.write(_('Unfortunately, your project is not compliant with version {} of the REUSE Specification :-(').format(__REUSE_version__))
    out.write('\n')
    return success
""""""]",1
"multilook, cmdLineParse = cmdLineParse, multilook
def main(iargs=None):
    """"""
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    nrlks = inps.nrlks
    nalks = inps.nalks
    if ((nrlks == 1) and (nalks == 1)):
        img = isceobj.createImage()
        img.load((inps.unw + '.xml'))
        img.setFilename(inps.output)
        img.extraFilename = (inps.output + '.vrt')
        img.renderHdr()
        os.symlink(os.path.abspath(inps.unw), os.path.abspath(inps.output))
    else:
        corName0 = inps.cor
        corimg = isceobj.createImage()
        corimg.load((corName0 + '.xml'))
        width = corimg.width
        length = corimg.length
        widthNew = np.int((width / nrlks))
        lengthNew = np.int((length / nalks))
        cor0 = np.fromfile(corName0, dtype=np.float32).reshape((length * 2), width)[1:(length * 2):2, :]
        wgt = (cor0 ** 2)
        a = multilook(wgt, nalks, nrlks)
        d = multilook((cor0 != 0).astype(np.int), nalks, nrlks)
        unwrapName0 = inps.unw
        unwimg = isceobj.createImage()
        unwimg.load((unwrapName0 + '.xml'))
        unw0 = np.fromfile(unwrapName0, dtype=np.float32).reshape((length * 2), width)[1:(length * 2):2, :]
        amp0 = np.fromfile(unwrapName0, dtype=np.float32).reshape((length * 2), width)[0:(length * 2):2, :]
        e = multilook((unw0 * wgt), nalks, nrlks)
        f = multilook((amp0 ** 2), nalks, nrlks)
        unw = np.zeros(((lengthNew * 2), widthNew), dtype=np.float32)
        unw[0:(lengthNew * 2):2, :] = np.sqrt((f / (d + (d == 0))))
        unw[1:(lengthNew * 2):2, :] = (e / (a + (a == 0)))
        os.makedirs(os.path.dirname(inps.output), exist_ok=True)
        unwrapName = inps.output
        unw.astype(np.float32).tofile(unwrapName)
        unwimg.setFilename(unwrapName)
        unwimg.extraFilename = (unwrapName + '.vrt')
        unwimg.setWidth(widthNew)
        unwimg.setLength(lengthNew)
        unwimg.renderHdr()
"""""", """""" 
    inps = multilook(iargs)
    nrlks = inps.nrlks
    nalks = inps.nalks
    if ((nrlks == 1) and (nalks == 1)):
        img = isceobj.createImage()
        img.load((inps.unw + '.xml'))
        img.setFilename(inps.output)
        img.extraFilename = (inps.output + '.vrt')
        img.renderHdr()
        os.symlink(os.path.abspath(inps.unw), os.path.abspath(inps.output))
    else:
        corName0 = inps.cor
        corimg = isceobj.createImage()
        corimg.load((corName0 + '.xml'))
        width = corimg.width
        length = corimg.length
        widthNew = np.int((width / nrlks))
        lengthNew = np.int((length / nalks))
        cor0 = np.fromfile(corName0, dtype=np.float32).reshape((length * 2), width)[1:(length * 2):2, :]
        wgt = (cor0 ** 2)
        a = cmdLineParse(wgt, nalks, nrlks)
        d = cmdLineParse((cor0 != 0).astype(np.int), nalks, nrlks)
        unwrapName0 = inps.unw
        unwimg = isceobj.createImage()
        unwimg.load((unwrapName0 + '.xml'))
        unw0 = np.fromfile(unwrapName0, dtype=np.float32).reshape((length * 2), width)[1:(length * 2):2, :]
        amp0 = np.fromfile(unwrapName0, dtype=np.float32).reshape((length * 2), width)[0:(length * 2):2, :]
        e = cmdLineParse((unw0 * wgt), nalks, nrlks)
        f = cmdLineParse((amp0 ** 2), nalks, nrlks)
        unw = np.zeros(((lengthNew * 2), widthNew), dtype=np.float32)
        unw[0:(lengthNew * 2):2, :] = np.sqrt((f / (d + (d == 0))))
        unw[1:(lengthNew * 2):2, :] = (e / (a + (a == 0)))
        os.makedirs(os.path.dirname(inps.output), exist_ok=True)
        unwrapName = inps.output
        unw.astype(np.float32).tofile(unwrapName)
        unwimg.setFilename(unwrapName)
        unwimg.extraFilename = (unwrapName + '.vrt')
        unwimg.setWidth(widthNew)
        unwimg.setLength(lengthNew)
        unwimg.renderHdr()
""""""]",1
"Path, package_files = package_files, Path
def generate_pdfa_ps(target_filename: Path, icc: str='sRGB'):
    """"""Create a Postscript PDFMARK file for Ghostscript PDF/A conversion

    pdfmark is an extension to the Postscript language that describes some PDF
    features like bookmarks and annotations. It was originally specified Adobe
    Distiller, for Postscript to PDF conversion.

    Ghostscript uses pdfmark for PDF to PDF/A conversion as well. To use Ghostscript
    to create a PDF/A, we need to create a pdfmark file with the necessary metadata.

    This function takes care of the many version-specific bugs and pecularities in
    Ghostscript's handling of pdfmark.

    The only information we put in specifies that we want the file to be a
    PDF/A, and we want to Ghostscript to convert objects to the sRGB colorspace
    if it runs into any object that it decides must be converted.

    Arguments:
        target_filename: filename to save
        icc: ICC identifier such as 'sRGB'
    References:
        Adobe PDFMARK Reference:
        https://www.adobe.com/content/dam/acom/en/devnet/acrobat/pdfs/pdfmark_reference.pdf
    """"""","["""""" 
    if (icc != 'sRGB'):
        raise NotImplementedError('Only supporting sRGB')
    bytes_icc_profile = (package_files('ocrmypdf.data') / SRGB_ICC_PROFILE_NAME).read_bytes()
    postscript = '\n'.join(_make_postscript(icc, bytes_icc_profile, 3))
    Path(target_filename).write_text(postscript, encoding='ascii')
    return target_filename
"""""", """""" 
    if (icc != 'sRGB'):
        raise NotImplementedError('Only supporting sRGB')
    bytes_icc_profile = (Path('ocrmypdf.data') / SRGB_ICC_PROFILE_NAME).read_bytes()
    postscript = '\n'.join(_make_postscript(icc, bytes_icc_profile, 3))
    package_files(target_filename).write_text(postscript, encoding='ascii')
    return target_filename
""""""]",1
"multiply, mat = mat, multiply
def calcWs(alphas, dataArr, classLabels):
    """"""
    基于alpha计算w值
    Args:
        alphas        拉格朗日乘子
        dataArr       feature数据集
        classLabels   目标变量数据集

    Returns:
        wc  回归系数
    """"""","["""""" 
    X = mat(dataArr)
    labelMat = mat(classLabels).transpose()
    (m, n) = shape(X)
    w = zeros((n, 1))
    for i in range(m):
        w += multiply((alphas[i] * labelMat[i]), X[i, :].T)
    return w
"""""", """""" 
    X = multiply(dataArr)
    labelMat = multiply(classLabels).transpose()
    (m, n) = shape(X)
    w = zeros((n, 1))
    for i in range(m):
        w += mat((alphas[i] * labelMat[i]), X[i, :].T)
    return w
""""""]",1
"is_np_datetime_like, contains_cftime_datetimes = contains_cftime_datetimes, is_np_datetime_like
def _contains_datetime_like_objects(var) -> bool:
    """"""Check if a variable contains datetime like objects (either
    np.datetime64, np.timedelta64, or cftime.datetime)
    """"""","["""""" 
    return (is_np_datetime_like(var.dtype) or contains_cftime_datetimes(var))
"""""", """""" 
    return (contains_cftime_datetimes(var.dtype) or is_np_datetime_like(var))
""""""]",1
"xrange, rng = rng, xrange
def choose_words(wordlist, numwords):
    """"""
    Choose numwords randomly from wordlist
    """"""","["""""" 
    return [rng().choice(wordlist) for i in xrange(numwords)]
"""""", """""" 
    return [xrange().choice(wordlist) for i in rng(numwords)]
""""""]",1
"partial, _xception = _xception, partial
@register_model
def xception41(pretrained=False, **kwargs):
    """""" Modified Aligned Xception-41
    """"""","["""""" 
    block_cfg = [dict(in_chs=64, out_chs=128, stride=2), dict(in_chs=128, out_chs=256, stride=2), dict(in_chs=256, out_chs=728, stride=2), *([dict(in_chs=728, out_chs=728, stride=1)] * 8), dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2), dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False)]
    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.1), **kwargs)
    return _xception('xception41', pretrained=pretrained, **model_args)
"""""", """""" 
    block_cfg = [dict(in_chs=64, out_chs=128, stride=2), dict(in_chs=128, out_chs=256, stride=2), dict(in_chs=256, out_chs=728, stride=2), *([dict(in_chs=728, out_chs=728, stride=1)] * 8), dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2), dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False)]
    model_args = dict(block_cfg=block_cfg, norm_layer=_xception(nn.BatchNorm2d, eps=0.001, momentum=0.1), **kwargs)
    return partial('xception41', pretrained=pretrained, **model_args)
""""""]",1
"BuildFileTargets, DeepDependencyTargets = DeepDependencyTargets, BuildFileTargets
def AllTargets(target_list, target_dicts, build_file):
    """"""Returns all targets (direct and dependencies) for the specified build_file.
  """"""","["""""" 
    bftargets = BuildFileTargets(target_list, build_file)
    deptargets = DeepDependencyTargets(target_dicts, bftargets)
    return (bftargets + deptargets)
"""""", """""" 
    bftargets = DeepDependencyTargets(target_list, build_file)
    deptargets = BuildFileTargets(target_dicts, bftargets)
    return (bftargets + deptargets)
""""""]",1
"ModelLayout, Comm = Comm, ModelLayout
@lru_cache()
def create_data_parallel_comm(layout: ModelLayout) -> Comm:
    """"""When using NCCL, all ranks must participate in construction of communicators. We use this
            object to instantiate the NCCL communicators correctly and provide a simplified API""""""","["""""" 
    _my_dp_comm = None
    for other_rank in layout.ranks_in_my_replica:
        other_layout = ModelLayout(layout=layout.layout, my_rank=other_rank)
        dp_group = Comm(other_layout.dp_sibling_ranks, my_rank=other_rank)
        if (other_rank == layout.my_rank):
            _my_dp_comm = dp_group
    return _my_dp_comm
"""""", """""" 
    _my_dp_comm = None
    for other_rank in layout.ranks_in_my_replica:
        other_layout = Comm(layout=layout.layout, my_rank=other_rank)
        dp_group = ModelLayout(other_layout.dp_sibling_ranks, my_rank=other_rank)
        if (other_rank == layout.my_rank):
            _my_dp_comm = dp_group
    return _my_dp_comm
""""""]",1
"target_arch, host_arch_target = host_arch_target, target_arch
def create_targets(targets):
    """"""
    Generate a list of targets that can be passed to the binutils compile function
    :param targets: A list of targets to convert to binutils target triples
    :return: A list of target triples
    """"""","["""""" 
    targets_dict = {'arm': 'arm-linux-gnueabi', 'aarch64': 'aarch64-linux-gnu', 'mips': 'mips-linux-gnu', 'mipsel': 'mipsel-linux-gnu', 'powerpc64': 'powerpc64-linux-gnu', 'powerpc64le': 'powerpc64le-linux-gnu', 'powerpc': 'powerpc-linux-gnu', 'riscv64': 'riscv64-linux-gnu', 's390x': 's390x-linux-gnu', 'x86_64': 'x86_64-linux-gnu'}
    targets_set = set()
    for target in targets:
        if (target == 'all'):
            return list(targets_dict.values())
        if (target == 'host'):
            key = host_arch_target()
        else:
            key = target_arch(target)
        targets_set.add(targets_dict[key])
    return list(targets_set)
"""""", """""" 
    targets_dict = {'arm': 'arm-linux-gnueabi', 'aarch64': 'aarch64-linux-gnu', 'mips': 'mips-linux-gnu', 'mipsel': 'mipsel-linux-gnu', 'powerpc64': 'powerpc64-linux-gnu', 'powerpc64le': 'powerpc64le-linux-gnu', 'powerpc': 'powerpc-linux-gnu', 'riscv64': 'riscv64-linux-gnu', 's390x': 's390x-linux-gnu', 'x86_64': 'x86_64-linux-gnu'}
    targets_set = set()
    for target in targets:
        if (target == 'all'):
            return list(targets_dict.values())
        if (target == 'host'):
            key = target_arch()
        else:
            key = host_arch_target(target)
        targets_set.add(targets_dict[key])
    return list(targets_set)
""""""]",1
"deep_align, DataArray = DataArray, deep_align
def apply_dataarray_vfunc(func, *args, signature: _UFuncSignature, join: JoinOptions='inner', exclude_dims=frozenset(), keep_attrs='override') -> (tuple[(DataArray, ...)] | DataArray):
    """"""Apply a variable level function over DataArray, Variable and/or ndarray
    objects.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    if (len(args) > 1):
        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    objs = _all_of_type(args, DataArray)
    if (keep_attrs == 'drop'):
        name = result_name(args)
    else:
        first_obj = _first_of_type(args, DataArray)
        name = first_obj.name
    (result_coords, result_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    data_vars = [getattr(a, 'variable', a) for a in args]
    result_var = func(*data_vars)
    out: (tuple[(DataArray, ...)] | DataArray)
    if (signature.num_outputs > 1):
        out = tuple((DataArray(variable, coords=coords, indexes=indexes, name=name, fastpath=True) for (variable, coords, indexes) in zip(result_var, result_coords, result_indexes)))
    else:
        (coords,) = result_coords
        (indexes,) = result_indexes
        out = DataArray(result_var, coords=coords, indexes=indexes, name=name, fastpath=True)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if isinstance(out, tuple):
        for da in out:
            da.attrs = attrs
    else:
        out.attrs = attrs
    return out
"""""", """""" 
    from xarray.core.dataarray import DataArray
    if (len(args) > 1):
        args = DataArray(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    objs = _all_of_type(args, deep_align)
    if (keep_attrs == 'drop'):
        name = result_name(args)
    else:
        first_obj = _first_of_type(args, deep_align)
        name = first_obj.name
    (result_coords, result_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    data_vars = [getattr(a, 'variable', a) for a in args]
    result_var = func(*data_vars)
    out: (tuple[(deep_align, ...)] | deep_align)
    if (signature.num_outputs > 1):
        out = tuple((deep_align(variable, coords=coords, indexes=indexes, name=name, fastpath=True) for (variable, coords, indexes) in zip(result_var, result_coords, result_indexes)))
    else:
        (coords,) = result_coords
        (indexes,) = result_indexes
        out = deep_align(result_var, coords=coords, indexes=indexes, name=name, fastpath=True)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if isinstance(out, tuple):
        for da in out:
            da.attrs = attrs
    else:
        out.attrs = attrs
    return out
""""""]",1
"get_dataset_path, Folder = Folder, get_dataset_path
@pytest.fixture(autouse=True)
def folder_data_module():
    """"""Create Folder Data Module.""""""","["""""" 
    root = get_dataset_path(dataset='bottle')
    datamodule = Folder(root=root, normal_dir='good', abnormal_dir='broken_large', mask_dir=os.path.join(root, 'ground_truth/broken_large'), task='segmentation', split_ratio=0.2, seed=0, image_size=(256, 256), train_batch_size=32, test_batch_size=32, num_workers=8, create_validation_set=True)
    datamodule.setup()
    return datamodule
"""""", """""" 
    root = Folder(dataset='bottle')
    datamodule = get_dataset_path(root=root, normal_dir='good', abnormal_dir='broken_large', mask_dir=os.path.join(root, 'ground_truth/broken_large'), task='segmentation', split_ratio=0.2, seed=0, image_size=(256, 256), train_batch_size=32, test_batch_size=32, num_workers=8, create_validation_set=True)
    datamodule.setup()
    return datamodule
""""""]",1
"createInputXML, facilityQuit = facilityQuit, createInputXML
def facilityInputXML():
    """"""Creates an XML file for a facility's parameters""""""","["""""" 
    global facilityParams
    global directory
    global rootName
    global facilityDirs
    if createInputXML(facilityParams, rootName):
        facilityQuit()
    if directory:
        facilityDirs[rootName] = directory
    return
"""""", """""" 
    global facilityParams
    global directory
    global rootName
    global facilityDirs
    if facilityQuit(facilityParams, rootName):
        createInputXML()
    if directory:
        facilityDirs[rootName] = directory
    return
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_element_not_displayed(element):
    """"""Verify element is not displayed (visible to the user)

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element, timeout=0, wait_displayed=False)
    with _verify_step(f'Verify element {element.name} is not displayed') as s:
        s.error = f'element {element.name} is displayed'
        s.condition = (not element.is_displayed())
"""""", """""" 
    element = _verify_step().find(element, timeout=0, wait_displayed=False)
    with get_browser(f'Verify element {element.name} is not displayed') as s:
        s.error = f'element {element.name} is displayed'
        s.condition = (not element.is_displayed())
""""""]",1
"update_all_packages, Path = Path, update_all_packages
def test_update_all_packages_with_blacklist(monkeypatch):
    """"""Test calling update_all_packages()""""""","["""""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {Path('/opt/pypi'): [public_pkg_1, private_pkg_1], Path('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: Path):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    update_all_packages(roots=list(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=['my_private_pkg', 'my_other_private_pkg'])
    manage.update.assert_called_once_with(frozenset([public_pkg_1, public_pkg_2]), destdir, dry_run, stable_only)
"""""", """""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {update_all_packages('/opt/pypi'): [public_pkg_1, private_pkg_1], update_all_packages('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: update_all_packages):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    Path(roots=list(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=['my_private_pkg', 'my_other_private_pkg'])
    manage.update.assert_called_once_with(frozenset([public_pkg_1, public_pkg_2]), destdir, dry_run, stable_only)
""""""]",1
"MVTec, BTech = BTech, MVTec
def get_datamodule(config: Union[(DictConfig, ListConfig)]) -> LightningDataModule:
    """"""Get Anomaly Datamodule.

    Args:
        config (Union[DictConfig, ListConfig]): Configuration of the anomaly model.

    Returns:
        PyTorch Lightning DataModule
    """"""","["""""" 
    logger.info('Loading the datamodule')
    datamodule: LightningDataModule
    if (config.dataset.format.lower() == 'mvtec'):
        datamodule = MVTec(root=config.dataset.path, category=config.dataset.category, image_size=(config.dataset.image_size[0], config.dataset.image_size[1]), train_batch_size=config.dataset.train_batch_size, test_batch_size=config.dataset.test_batch_size, num_workers=config.dataset.num_workers, seed=config.project.seed, task=config.dataset.task, transform_config_train=config.dataset.transform_config.train, transform_config_val=config.dataset.transform_config.val, create_validation_set=config.dataset.create_validation_set)
    elif (config.dataset.format.lower() == 'btech'):
        datamodule = BTech(root=config.dataset.path, category=config.dataset.category, image_size=(config.dataset.image_size[0], config.dataset.image_size[1]), train_batch_size=config.dataset.train_batch_size, test_batch_size=config.dataset.test_batch_size, num_workers=config.dataset.num_workers, seed=config.project.seed, task=config.dataset.task, transform_config_train=config.dataset.transform_config.train, transform_config_val=config.dataset.transform_config.val, create_validation_set=config.dataset.create_validation_set)
    elif (config.dataset.format.lower() == 'folder'):
        datamodule = Folder(root=config.dataset.path, normal_dir=config.dataset.normal_dir, abnormal_dir=config.dataset.abnormal_dir, task=config.dataset.task, normal_test_dir=config.dataset.normal_test_dir, mask_dir=config.dataset.mask, extensions=config.dataset.extensions, split_ratio=config.dataset.split_ratio, seed=config.project.seed, image_size=(config.dataset.image_size[0], config.dataset.image_size[1]), train_batch_size=config.dataset.train_batch_size, test_batch_size=config.dataset.test_batch_size, num_workers=config.dataset.num_workers, transform_config_train=config.dataset.transform_config.train, transform_config_val=config.dataset.transform_config.val, create_validation_set=config.dataset.create_validation_set)
    else:
        raise ValueError('Unknown dataset! \nIf you use a custom dataset make sure you initialize it in`get_datamodule` in `anomalib.data.__init__.py')
    return datamodule
"""""", """""" 
    logger.info('Loading the datamodule')
    datamodule: LightningDataModule
    if (config.dataset.format.lower() == 'mvtec'):
        datamodule = BTech(root=config.dataset.path, category=config.dataset.category, image_size=(config.dataset.image_size[0], config.dataset.image_size[1]), train_batch_size=config.dataset.train_batch_size, test_batch_size=config.dataset.test_batch_size, num_workers=config.dataset.num_workers, seed=config.project.seed, task=config.dataset.task, transform_config_train=config.dataset.transform_config.train, transform_config_val=config.dataset.transform_config.val, create_validation_set=config.dataset.create_validation_set)
    elif (config.dataset.format.lower() == 'btech'):
        datamodule = MVTec(root=config.dataset.path, category=config.dataset.category, image_size=(config.dataset.image_size[0], config.dataset.image_size[1]), train_batch_size=config.dataset.train_batch_size, test_batch_size=config.dataset.test_batch_size, num_workers=config.dataset.num_workers, seed=config.project.seed, task=config.dataset.task, transform_config_train=config.dataset.transform_config.train, transform_config_val=config.dataset.transform_config.val, create_validation_set=config.dataset.create_validation_set)
    elif (config.dataset.format.lower() == 'folder'):
        datamodule = Folder(root=config.dataset.path, normal_dir=config.dataset.normal_dir, abnormal_dir=config.dataset.abnormal_dir, task=config.dataset.task, normal_test_dir=config.dataset.normal_test_dir, mask_dir=config.dataset.mask, extensions=config.dataset.extensions, split_ratio=config.dataset.split_ratio, seed=config.project.seed, image_size=(config.dataset.image_size[0], config.dataset.image_size[1]), train_batch_size=config.dataset.train_batch_size, test_batch_size=config.dataset.test_batch_size, num_workers=config.dataset.num_workers, transform_config_train=config.dataset.transform_config.train, transform_config_val=config.dataset.transform_config.val, create_validation_set=config.dataset.create_validation_set)
    else:
        raise ValueError('Unknown dataset! \nIf you use a custom dataset make sure you initialize it in`get_datamodule` in `anomalib.data.__init__.py')
    return datamodule
""""""]",1
"SpdxInfo, create_header = create_header, SpdxInfo
def test_create_header_simple():
    """"""Create a super simple header.""""""","["""""" 
    spdx_info = SpdxInfo({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    assert (create_header(spdx_info).strip() == expected)
"""""", """""" 
    spdx_info = create_header({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    assert (SpdxInfo(spdx_info).strip() == expected)
""""""]",1
"biKmeans, nonzero = nonzero, biKmeans
def clusterClubs(fileName, imgName, numClust=5):
    """"""
    将文本文件的解析,聚类以及画图都封装在一起
    :param fileName: 文本数据路径
    :param imgName: 图片路径
    :param numClust: 希望得到的簇数目
    :return:
    """"""","["""""" 
    datList = []
    for line in open(fileName).readlines():
        lineArr = line.split('\t')
        datList.append([float(lineArr[4]), float(lineArr[3])])
    datMat = mat(datList)
    (myCentroids, clustAssing) = biKmeans(datMat, numClust, distMeas=distSLC)
    fig = plt.figure()
    rect = [0.1, 0.1, 0.8, 0.8]
    scatterMarkers = ['s', 'o', '^', '8', 'p', 'd', 'v', 'h', '>', '<']
    axprops = dict(xticks=[], yticks=[])
    ax0 = fig.add_axes(rect, label='ax0', **axprops)
    imgP = plt.imread(imgName)
    ax0.imshow(imgP)
    ax1 = fig.add_axes(rect, label='ax1', frameon=False)
    for i in range(numClust):
        ptsInCurrCluster = datMat[nonzero((clustAssing[:, 0].A == i))[0], :]
        markerStyle = scatterMarkers[(i % len(scatterMarkers))]
        ax1.scatter(ptsInCurrCluster[:, 0].flatten().A[0], ptsInCurrCluster[:, 1].flatten().A[0], marker=markerStyle, s=90)
    ax1.scatter(myCentroids[:, 0].flatten().A[0], myCentroids[:, 1].flatten().A[0], marker='+', s=300)
    plt.show()
"""""", """""" 
    datList = []
    for line in open(fileName).readlines():
        lineArr = line.split('\t')
        datList.append([float(lineArr[4]), float(lineArr[3])])
    datMat = mat(datList)
    (myCentroids, clustAssing) = nonzero(datMat, numClust, distMeas=distSLC)
    fig = plt.figure()
    rect = [0.1, 0.1, 0.8, 0.8]
    scatterMarkers = ['s', 'o', '^', '8', 'p', 'd', 'v', 'h', '>', '<']
    axprops = dict(xticks=[], yticks=[])
    ax0 = fig.add_axes(rect, label='ax0', **axprops)
    imgP = plt.imread(imgName)
    ax0.imshow(imgP)
    ax1 = fig.add_axes(rect, label='ax1', frameon=False)
    for i in range(numClust):
        ptsInCurrCluster = datMat[biKmeans((clustAssing[:, 0].A == i))[0], :]
        markerStyle = scatterMarkers[(i % len(scatterMarkers))]
        ax1.scatter(ptsInCurrCluster[:, 0].flatten().A[0], ptsInCurrCluster[:, 1].flatten().A[0], marker=markerStyle, s=90)
    ax1.scatter(myCentroids[:, 0].flatten().A[0], myCentroids[:, 1].flatten().A[0], marker='+', s=300)
    plt.show()
""""""]",1
"is_on_slurm, is_master = is_master, is_on_slurm
def maybe_init_requeue_handler(master_callback: Optional[Callable]=None) -> None:
    """"""Handle signals sent by SLURM for time limit / pre-emption.

    Args:
        master_callback: If provided, will be called on the master before
            killing and requeuing the job. If callback raises exception, the
            job will be requeued anyway.
    """"""","["""""" 
    if (not is_on_slurm()):
        return
    if is_master():
        signal.signal(signal.SIGUSR1, functools.partial(sig_handler, master_callback))
        signal.signal(signal.SIGTERM, term_handler)
    else:
        signal.signal(signal.SIGUSR1, non_master_handler)
    logging.warning('Signal handler installed.')
"""""", """""" 
    if (not is_master()):
        return
    if is_on_slurm():
        signal.signal(signal.SIGUSR1, functools.partial(sig_handler, master_callback))
        signal.signal(signal.SIGTERM, term_handler)
    else:
        signal.signal(signal.SIGUSR1, non_master_handler)
    logging.warning('Signal handler installed.')
""""""]",1
"meanOffset, Ampcor = Ampcor, meanOffset
def estimateFrameOffset(swath1, swath2, image1, image2, matchingMode=0):
    """"""
    estimate offset of two adjacent frames using matching
    matchingMode:  0: ScanSAR full-aperture image
                   1: regular image
    """"""","["""""" 
    import isceobj
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsets
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsetsRoipac
    from isceobj.Alos2Proc.Alos2ProcPublic import meanOffset
    from mroipac.ampcor.Ampcor import Ampcor
    ampcor = Ampcor(name='insarapp_slcs_ampcor')
    ampcor.configure()
    mSLC = isceobj.createImage()
    mSLC.load((image1 + '.xml'))
    mSLC.setFilename(image1)
    mSLC.setAccessMode('read')
    mSLC.createImage()
    sSLC = isceobj.createImage()
    sSLC.load((image2 + '.xml'))
    sSLC.setFilename(image2)
    sSLC.setAccessMode('read')
    sSLC.createImage()
    if (mSLC.dataType.upper() == 'CFLOAT'):
        ampcor.setImageDataType1('complex')
        ampcor.setImageDataType2('complex')
    elif (mSLC.dataType.upper() == 'FLOAT'):
        ampcor.setImageDataType1('real')
        ampcor.setImageDataType2('real')
    else:
        raise Exception('file type not supported yet.')
    ampcor.setReferenceSlcImage(mSLC)
    ampcor.setSecondarySlcImage(sSLC)
    rgoff = ((- (swath2.startingRange - swath1.startingRange)) / swath1.rangePixelSize)
    azoff = ((- (swath2.sensingStart - swath1.sensingStart).total_seconds()) / swath1.azimuthLineInterval)
    rgoff = int(rgoff)
    azoff = int(azoff)
    if (rgoff == 0):
        rgoff = 1
    if (azoff == 0):
        azoff = 1
    firstSample = 1
    if (rgoff < 0):
        firstSample = int((35 - rgoff))
    firstLine = 1
    if (azoff < 0):
        firstLine = int((35 - azoff))
    ampcor.setAcrossGrossOffset(rgoff)
    ampcor.setDownGrossOffset(azoff)
    ampcor.setFirstSampleAcross(firstSample)
    ampcor.setLastSampleAcross(mSLC.width)
    ampcor.setNumberLocationAcross(30)
    ampcor.setFirstSampleDown(firstLine)
    ampcor.setLastSampleDown(mSLC.length)
    ampcor.setNumberLocationDown(10)
    if (matchingMode == 0):
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(512)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
        ampcor.setWinsizeFilt(8)
        ampcor.setOversamplingFactorFilt(64)
    else:
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(64)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
    ampcor.setAcrossLooks(1)
    ampcor.setDownLooks(1)
    ampcor.setOversamplingFactor(64)
    ampcor.setZoomWindowSize(16)
    ampcor.setDebugFlag(False)
    ampcor.setDisplayFlag(False)
    ampcor.ampcor()
    offsets = ampcor.getOffsetField()
    mSLC.finalizeImage()
    sSLC.finalizeImage()
    refinedOffsets = cullOffsetsRoipac(offsets, numThreshold=50)
    if (refinedOffsets != None):
        (rangeOffset, azimuthOffset) = meanOffset(refinedOffsets)
        return (rangeOffset, azimuthOffset)
    else:
        return None
"""""", """""" 
    import isceobj
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsets
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsetsRoipac
    from isceobj.Alos2Proc.Alos2ProcPublic import meanOffset
    from mroipac.ampcor.Ampcor import Ampcor
    ampcor = meanOffset(name='insarapp_slcs_ampcor')
    ampcor.configure()
    mSLC = isceobj.createImage()
    mSLC.load((image1 + '.xml'))
    mSLC.setFilename(image1)
    mSLC.setAccessMode('read')
    mSLC.createImage()
    sSLC = isceobj.createImage()
    sSLC.load((image2 + '.xml'))
    sSLC.setFilename(image2)
    sSLC.setAccessMode('read')
    sSLC.createImage()
    if (mSLC.dataType.upper() == 'CFLOAT'):
        ampcor.setImageDataType1('complex')
        ampcor.setImageDataType2('complex')
    elif (mSLC.dataType.upper() == 'FLOAT'):
        ampcor.setImageDataType1('real')
        ampcor.setImageDataType2('real')
    else:
        raise Exception('file type not supported yet.')
    ampcor.setReferenceSlcImage(mSLC)
    ampcor.setSecondarySlcImage(sSLC)
    rgoff = ((- (swath2.startingRange - swath1.startingRange)) / swath1.rangePixelSize)
    azoff = ((- (swath2.sensingStart - swath1.sensingStart).total_seconds()) / swath1.azimuthLineInterval)
    rgoff = int(rgoff)
    azoff = int(azoff)
    if (rgoff == 0):
        rgoff = 1
    if (azoff == 0):
        azoff = 1
    firstSample = 1
    if (rgoff < 0):
        firstSample = int((35 - rgoff))
    firstLine = 1
    if (azoff < 0):
        firstLine = int((35 - azoff))
    ampcor.setAcrossGrossOffset(rgoff)
    ampcor.setDownGrossOffset(azoff)
    ampcor.setFirstSampleAcross(firstSample)
    ampcor.setLastSampleAcross(mSLC.width)
    ampcor.setNumberLocationAcross(30)
    ampcor.setFirstSampleDown(firstLine)
    ampcor.setLastSampleDown(mSLC.length)
    ampcor.setNumberLocationDown(10)
    if (matchingMode == 0):
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(512)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
        ampcor.setWinsizeFilt(8)
        ampcor.setOversamplingFactorFilt(64)
    else:
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(64)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
    ampcor.setAcrossLooks(1)
    ampcor.setDownLooks(1)
    ampcor.setOversamplingFactor(64)
    ampcor.setZoomWindowSize(16)
    ampcor.setDebugFlag(False)
    ampcor.setDisplayFlag(False)
    ampcor.ampcor()
    offsets = ampcor.getOffsetField()
    mSLC.finalizeImage()
    sSLC.finalizeImage()
    refinedOffsets = cullOffsetsRoipac(offsets, numThreshold=50)
    if (refinedOffsets != None):
        (rangeOffset, azimuthOffset) = Ampcor(refinedOffsets)
        return (rangeOffset, azimuthOffset)
    else:
        return None
""""""]",1
"LNException, prompt = prompt, LNException
def choose_a_novel(self):
    """"""Choose a single novel url from the search result""""""","["""""" 
    args = get_args()
    choices = self.app.search_results
    selected_choice = self.app.search_results[0]
    if ((len(choices) > 1) and (not args.suppress)):
        answer = prompt([{'type': 'list', 'name': 'novel', 'message': 'Which one is your novel?', 'choices': (display.format_novel_choices(choices) + ['0. Cancel'])}])
        index = int(answer['novel'].split('.')[0])
        if ((index < 1) and (index > len(self.app.search_results))):
            raise LNException('Cancelled by user')
        selected_choice = self.app.search_results[(index - 1)]
    novels = selected_choice['novels']
    selected_novel = novels[0]
    if ((len(novels) > 1) and (not args.suppress)):
        answer = prompt([{'type': 'list', 'name': 'novel', 'message': 'Choose a source to download?', 'choices': (['0. Back'] + display.format_source_choices(novels))}])
        index = int(answer['novel'].split('.')[0])
        if (index == 0):
            return self.choose_a_novel()
        selected_novel = novels[(index - 1)]
    return selected_novel['url']
"""""", """""" 
    args = get_args()
    choices = self.app.search_results
    selected_choice = self.app.search_results[0]
    if ((len(choices) > 1) and (not args.suppress)):
        answer = LNException([{'type': 'list', 'name': 'novel', 'message': 'Which one is your novel?', 'choices': (display.format_novel_choices(choices) + ['0. Cancel'])}])
        index = int(answer['novel'].split('.')[0])
        if ((index < 1) and (index > len(self.app.search_results))):
            raise prompt('Cancelled by user')
        selected_choice = self.app.search_results[(index - 1)]
    novels = selected_choice['novels']
    selected_novel = novels[0]
    if ((len(novels) > 1) and (not args.suppress)):
        answer = LNException([{'type': 'list', 'name': 'novel', 'message': 'Choose a source to download?', 'choices': (['0. Back'] + display.format_source_choices(novels))}])
        index = int(answer['novel'].split('.')[0])
        if (index == 0):
            return self.choose_a_novel()
        selected_novel = novels[(index - 1)]
    return selected_novel['url']
""""""]",1
"cos, sin = sin, cos
def angle2matrix(angles):
    """""" get rotation matrix from three rotation angles(radian). The same as in 3DDFA.
    Args:
        angles: [3,]. x, y, z angles
        x: yaw.
        y: pitch.
        z: roll.
    Returns:
        R: 3x3. rotation matrix.
    """"""","["""""" 
    (y, x, z) = (angles[0], angles[1], angles[2])
    Rx = np.array([[1, 0, 0], [0, cos(x), (- sin(x))], [0, sin(x), cos(x)]])
    Ry = np.array([[cos(y), 0, sin(y)], [0, 1, 0], [(- sin(y)), 0, cos(y)]])
    Rz = np.array([[cos(z), (- sin(z)), 0], [sin(z), cos(z), 0], [0, 0, 1]])
    R = Rz.dot(Ry).dot(Rx)
    return R.astype(np.float32)
"""""", """""" 
    (y, x, z) = (angles[0], angles[1], angles[2])
    Rx = np.array([[1, 0, 0], [0, sin(x), (- cos(x))], [0, cos(x), sin(x)]])
    Ry = np.array([[sin(y), 0, cos(y)], [0, 1, 0], [(- cos(y)), 0, sin(y)]])
    Rz = np.array([[sin(z), (- cos(z)), 0], [cos(z), sin(z), 0], [0, 0, 1]])
    R = Rz.dot(Ry).dot(Rx)
    return R.astype(np.float32)
""""""]",1
"encode_cf_variable, _update_bounds_encoding = _update_bounds_encoding, encode_cf_variable
def cf_encoder(variables, attributes):
    """"""
    Encode a set of CF encoded variables and attributes.
    Takes a dicts of variables and attributes and encodes them
    to conform to CF conventions as much as possible.
    This includes masking, scaling, character array handling,
    and CF-time encoding.

    Parameters
    ----------
    variables : dict
        A dictionary mapping from variable name to xarray.Variable
    attributes : dict
        A dictionary mapping from attribute name to value

    Returns
    -------
    encoded_variables : dict
        A dictionary mapping from variable name to xarray.Variable,
    encoded_attributes : dict
        A dictionary mapping from attribute name to value

    See Also
    --------
    decode_cf_variable, encode_cf_variable
    """"""","["""""" 
    _update_bounds_encoding(variables)
    new_vars = {k: encode_cf_variable(v, name=k) for (k, v) in variables.items()}
    for var in new_vars.values():
        bounds = (var.attrs['bounds'] if ('bounds' in var.attrs) else None)
        if (bounds and (bounds in new_vars)):
            for attr in ['units', 'standard_name', 'axis', 'positive', 'calendar', 'long_name', 'leap_month', 'leap_year', 'month_lengths']:
                if ((attr in new_vars[bounds].attrs) and (attr in var.attrs)):
                    if (new_vars[bounds].attrs[attr] == var.attrs[attr]):
                        new_vars[bounds].attrs.pop(attr)
    return (new_vars, attributes)
"""""", """""" 
    encode_cf_variable(variables)
    new_vars = {k: _update_bounds_encoding(v, name=k) for (k, v) in variables.items()}
    for var in new_vars.values():
        bounds = (var.attrs['bounds'] if ('bounds' in var.attrs) else None)
        if (bounds and (bounds in new_vars)):
            for attr in ['units', 'standard_name', 'axis', 'positive', 'calendar', 'long_name', 'leap_month', 'leap_year', 'month_lengths']:
                if ((attr in new_vars[bounds].attrs) and (attr in var.attrs)):
                    if (new_vars[bounds].attrs[attr] == var.attrs[attr]):
                        new_vars[bounds].attrs.pop(attr)
    return (new_vars, attributes)
""""""]",1
"Rigids, Vecs = Vecs, Rigids
def rigids_from_tensor_flat12(m: jnp.ndarray) -> Rigids:
    """"""Flat12 encoding: rotation matrix (9 floats) + translation (3 floats).""""""","["""""" 
    assert (m.shape[(- 1)] == 12)
    x = jnp.moveaxis(m, (- 1), 0)
    return Rigids(Rots(*x[:9]), Vecs(*x[9:]))
"""""", """""" 
    assert (m.shape[(- 1)] == 12)
    x = jnp.moveaxis(m, (- 1), 0)
    return Vecs(Rots(*x[:9]), Rigids(*x[9:]))
""""""]",1
"collect_dict_values, _as_variables_or_variable = _as_variables_or_variable, collect_dict_values
def apply_dict_of_variables_vfunc(func, *args, signature: _UFuncSignature, join='inner', fill_value=None):
    """"""Apply a variable level function over dicts of DataArray, DataArray,
    Variable and ndarray objects.
    """"""","["""""" 
    args = tuple((_as_variables_or_variable(arg) for arg in args))
    names = join_dict_keys(args, how=join)
    grouped_by_name = collect_dict_values(args, names, fill_value)
    result_vars = {}
    for (name, variable_args) in zip(names, grouped_by_name):
        result_vars[name] = func(*variable_args)
    if (signature.num_outputs > 1):
        return _unpack_dict_tuples(result_vars, signature.num_outputs)
    else:
        return result_vars
"""""", """""" 
    args = tuple((collect_dict_values(arg) for arg in args))
    names = join_dict_keys(args, how=join)
    grouped_by_name = _as_variables_or_variable(args, names, fill_value)
    result_vars = {}
    for (name, variable_args) in zip(names, grouped_by_name):
        result_vars[name] = func(*variable_args)
    if (signature.num_outputs > 1):
        return _unpack_dict_tuples(result_vars, signature.num_outputs)
    else:
        return result_vars
""""""]",1
"_screenshot_on_step, get_browser = get_browser, _screenshot_on_step
def assert_window_present_by_partial_url(partial_url):
    """"""Assert there is a window/tab present by partial URL

    Parameters:
    partial_url : value
    """"""","["""""" 
    _add_step(f""Assert window present by partial URL '{partial_url}'"")
    _run_wait_hook()
    urls = get_browser().get_window_urls()
    error_msg = f""There is no window present with partial URL '{partial_url}'""
    assert any(((partial_url in url) for url in urls)), error_msg
    _screenshot_on_step()
"""""", """""" 
    _add_step(f""Assert window present by partial URL '{partial_url}'"")
    _run_wait_hook()
    urls = _screenshot_on_step().get_window_urls()
    error_msg = f""There is no window present with partial URL '{partial_url}'""
    assert any(((partial_url in url) for url in urls)), error_msg
    get_browser()
""""""]",1
"main, cleandoc = cleandoc, main
def test_annotate_year(fake_repository, stringio):
    """"""Add a header to a file with a custom year.""""""","["""""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text('pass')
    expected = cleandoc('\n        # SPDX-FileCopyrightText: 2016 Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = main(['annotate', '--year', '2016', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
"""""", """""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text('pass')
    expected = main('\n        # SPDX-FileCopyrightText: 2016 Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = cleandoc(['annotate', '--year', '2016', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
""""""]",1
"partial, decode_arch_def = decode_arch_def, partial
def _gen_mnasnet_a1(variant, channel_multiplier=1.0, pretrained=False, **kwargs):
    """"""Creates a mnasnet-a1 model.

    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet
    Paper: https://arxiv.org/pdf/1807.11626.pdf.

    Args:
      channel_multiplier: multiplier to number of channels per layer.
    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_noskip'], ['ir_r2_k3_s2_e6_c24'], ['ir_r3_k5_s2_e3_c40_se0.25'], ['ir_r4_k3_s2_e6_c80'], ['ir_r2_k3_s1_e6_c112_se0.25'], ['ir_r3_k5_s2_e6_c160_se0.25'], ['ir_r1_k3_s1_e6_c320']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def), stem_size=32, round_chs_fn=partial(round_channels, multiplier=channel_multiplier), norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_noskip'], ['ir_r2_k3_s2_e6_c24'], ['ir_r3_k5_s2_e3_c40_se0.25'], ['ir_r4_k3_s2_e6_c80'], ['ir_r2_k3_s1_e6_c112_se0.25'], ['ir_r3_k5_s2_e6_c160_se0.25'], ['ir_r1_k3_s1_e6_c320']]
    model_kwargs = dict(block_args=partial(arch_def), stem_size=32, round_chs_fn=decode_arch_def(round_channels, multiplier=channel_multiplier), norm_layer=(kwargs.pop('norm_layer', None) or decode_arch_def(nn.BatchNorm2d, **resolve_bn_args(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"scatter_max, _size_to_index = _size_to_index, scatter_max
def variadic_max(input, size):
    """"""
    Compute max over sets with variadic sizes.

    Suppose there are :math:`N` sets, and the sizes of all sets are summed to :math:`B`.

    Parameters:
        input (Tensor): input of shape :math:`(B, ...)`
        size (LongTensor): size of sets of shape :math:`(N,)`

    Returns
        (Tensor, LongTensor): max values and indexes
    """"""","["""""" 
    index2sample = _size_to_index(size)
    index2sample = index2sample.view(([(- 1)] + ([1] * (input.ndim - 1))))
    index2sample = index2sample.expand_as(input)
    (value, index) = scatter_max(input, index2sample, dim=0)
    index = (index + (size - size.cumsum(0)).view(([(- 1)] + ([1] * (index.ndim - 1)))))
    return (value, index)
"""""", """""" 
    index2sample = scatter_max(size)
    index2sample = index2sample.view(([(- 1)] + ([1] * (input.ndim - 1))))
    index2sample = index2sample.expand_as(input)
    (value, index) = _size_to_index(input, index2sample, dim=0)
    index = (index + (size - size.cumsum(0)).view(([(- 1)] + ([1] * (index.ndim - 1)))))
    return (value, index)
""""""]",1
"deepcopy, SearchEngineRetriever = SearchEngineRetriever, deepcopy
def create_search_agent(opt):
    """"""
    Creates and instance of SearchEngineRetriever object.
    """"""","["""""" 
    logging.info('Initializing the search engine API.')
    search_api_opt = deepcopy(opt)
    search_api_opt['skip_retrieval_token'] = None
    return SearchEngineRetriever(search_api_opt)
"""""", """""" 
    logging.info('Initializing the search engine API.')
    search_api_opt = SearchEngineRetriever(opt)
    search_api_opt['skip_retrieval_token'] = None
    return deepcopy(search_api_opt)
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_element_value_is_not(element, value):
    """"""Verify element value is not `value`

    Parameters:
    element : element
    value : value
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    step_message = f""Verify element {element.name} value is not '{value}'""
    with _verify_step(step_message) as s:
        element_value = element.value
        s.error = f""expected element {element.name} value to not be '{value}'""
        s.condition = (element_value != value)
"""""", """""" 
    element = _verify_step().find(element, timeout=0)
    step_message = f""Verify element {element.name} value is not '{value}'""
    with get_browser(step_message) as s:
        element_value = element.value
        s.error = f""expected element {element.name} value to not be '{value}'""
        s.condition = (element_value != value)
""""""]",1
"_step, get_browser = get_browser, _step
def refresh_page():
    """"""Refresh the page""""""","["""""" 
    with _step('Refresh page'):
        get_browser().refresh()
"""""", """""" 
    with get_browser('Refresh page'):
        _step().refresh()
""""""]",1
"get_rst_title, PyLinter = PyLinter, get_rst_title
def builder_inited(app: (Sphinx | None)) -> None:
    """"""Output full documentation in ReST format for all extension modules.""""""","["""""" 
    base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    ext_path = os.path.join(base_path, 'pylint', 'extensions')
    modules = []
    doc_files: dict[(str, str)] = {}
    for filename in os.listdir(ext_path):
        (name, ext) = os.path.splitext(filename)
        if (name[0] == '_'):
            continue
        if (ext == '.py'):
            modules.append(f'pylint.extensions.{name}')
        elif (ext == '.rst'):
            doc_files[('pylint.extensions.' + name)] = os.path.join(ext_path, filename)
    modules.sort()
    if (not modules):
        sys.exit('No Pylint extensions found?')
    linter = PyLinter()
    linter.load_plugin_modules(modules)
    extensions_doc = os.path.join(base_path, 'doc', 'user_guide', 'checkers', 'extensions.rst')
    with open(extensions_doc, 'w', encoding='utf-8') as stream:
        stream.write(get_rst_title('Optional checkers', '='))
        stream.write(""\n.. This file is auto-generated. Make any changes to the associated\n.. docs extension in 'doc/exts/pylint_extensions.py'.\n\n"")
        stream.write('Pylint provides the following optional plugins:\n\n')
        for module in modules:
            stream.write(f'''- :ref:`{module}`
''')
        stream.write('\n')
        stream.write('You can activate any or all of these extensions by adding a ``load-plugins`` line to the ``MAIN`` section of your ``.pylintrc``, for example::\n')
        stream.write('\n    load-plugins=pylint.extensions.docparams,pylint.extensions.docstyle\n\n')
        by_checker = get_plugins_info(linter, doc_files)
        max_len = len(by_checker)
        for (i, checker_information) in enumerate(sorted(by_checker.items())):
            (checker, information) = checker_information
            j = (- 1)
            checker = information['checker']
            if (i == (max_len - 1)):
                j = (- 3)
            print(checker.get_full_documentation(msgs=information['msgs'], options=information['options'], reports=information['reports'], doc=information['doc'], module=information['module'], show_options=False)[:j], file=stream)
"""""", """""" 
    base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    ext_path = os.path.join(base_path, 'pylint', 'extensions')
    modules = []
    doc_files: dict[(str, str)] = {}
    for filename in os.listdir(ext_path):
        (name, ext) = os.path.splitext(filename)
        if (name[0] == '_'):
            continue
        if (ext == '.py'):
            modules.append(f'pylint.extensions.{name}')
        elif (ext == '.rst'):
            doc_files[('pylint.extensions.' + name)] = os.path.join(ext_path, filename)
    modules.sort()
    if (not modules):
        sys.exit('No Pylint extensions found?')
    linter = get_rst_title()
    linter.load_plugin_modules(modules)
    extensions_doc = os.path.join(base_path, 'doc', 'user_guide', 'checkers', 'extensions.rst')
    with open(extensions_doc, 'w', encoding='utf-8') as stream:
        stream.write(PyLinter('Optional checkers', '='))
        stream.write(""\n.. This file is auto-generated. Make any changes to the associated\n.. docs extension in 'doc/exts/pylint_extensions.py'.\n\n"")
        stream.write('Pylint provides the following optional plugins:\n\n')
        for module in modules:
            stream.write(f'''- :ref:`{module}`
''')
        stream.write('\n')
        stream.write('You can activate any or all of these extensions by adding a ``load-plugins`` line to the ``MAIN`` section of your ``.pylintrc``, for example::\n')
        stream.write('\n    load-plugins=pylint.extensions.docparams,pylint.extensions.docstyle\n\n')
        by_checker = get_plugins_info(linter, doc_files)
        max_len = len(by_checker)
        for (i, checker_information) in enumerate(sorted(by_checker.items())):
            (checker, information) = checker_information
            j = (- 1)
            checker = information['checker']
            if (i == (max_len - 1)):
                j = (- 3)
            print(checker.get_full_documentation(msgs=information['msgs'], options=information['options'], reports=information['reports'], doc=information['doc'], module=information['module'], show_options=False)[:j], file=stream)
""""""]",1
"__ensure_suffix, __select_builder = __select_builder, __ensure_suffix
def DocbookMan(env, target, source=None, *args, **kw):
    """"""
    A pseudo-Builder, providing a Docbook toolchain for Man page output.
    """"""","["""""" 
    (target, source) = __extend_targets_sources(target, source)
    __init_xsl_stylesheet(kw, env, '$DOCBOOK_DEFAULT_XSL_MAN', ['manpages', 'docbook.xsl'])
    __builder = __select_builder(__lxml_noresult_builder, __xsltproc_builder)
    result = []
    for (t, s) in zip(target, source):
        volnum = '1'
        outfiles = []
        srcfile = __ensure_suffix(str(s), '.xml')
        if os.path.isfile(srcfile):
            try:
                import xml.dom.minidom
                dom = xml.dom.minidom.parse(__ensure_suffix(str(s), '.xml'))
                for node in dom.getElementsByTagName('refmeta'):
                    for vol in node.getElementsByTagName('manvolnum'):
                        volnum = __get_xml_text(vol)
                for node in dom.getElementsByTagName('refnamediv'):
                    for ref in node.getElementsByTagName('refname'):
                        outfiles.append(((__get_xml_text(ref) + '.') + volnum))
            except Exception:
                with open(__ensure_suffix(str(s), '.xml'), 'r') as f:
                    content = f.read()
                for m in re_manvolnum.finditer(content):
                    volnum = m.group(1)
                for m in re_refname.finditer(content):
                    outfiles.append(((m.group(1) + '.') + volnum))
            if (not outfiles):
                spath = str(s)
                if (not spath.endswith('.xml')):
                    outfiles.append(((spath + '.') + volnum))
                else:
                    (stem, ext) = os.path.splitext(spath)
                    outfiles.append(((stem + '.') + volnum))
        else:
            outfiles.append(t)
        __builder.__call__(env, outfiles[0], s, **kw)
        env.Depends(outfiles[0], kw['DOCBOOK_XSL'])
        result.append(outfiles[0])
        if (len(outfiles) > 1):
            env.Clean(outfiles[0], outfiles[1:])
    return result
"""""", """""" 
    (target, source) = __extend_targets_sources(target, source)
    __init_xsl_stylesheet(kw, env, '$DOCBOOK_DEFAULT_XSL_MAN', ['manpages', 'docbook.xsl'])
    __builder = __ensure_suffix(__lxml_noresult_builder, __xsltproc_builder)
    result = []
    for (t, s) in zip(target, source):
        volnum = '1'
        outfiles = []
        srcfile = __select_builder(str(s), '.xml')
        if os.path.isfile(srcfile):
            try:
                import xml.dom.minidom
                dom = xml.dom.minidom.parse(__select_builder(str(s), '.xml'))
                for node in dom.getElementsByTagName('refmeta'):
                    for vol in node.getElementsByTagName('manvolnum'):
                        volnum = __get_xml_text(vol)
                for node in dom.getElementsByTagName('refnamediv'):
                    for ref in node.getElementsByTagName('refname'):
                        outfiles.append(((__get_xml_text(ref) + '.') + volnum))
            except Exception:
                with open(__select_builder(str(s), '.xml'), 'r') as f:
                    content = f.read()
                for m in re_manvolnum.finditer(content):
                    volnum = m.group(1)
                for m in re_refname.finditer(content):
                    outfiles.append(((m.group(1) + '.') + volnum))
            if (not outfiles):
                spath = str(s)
                if (not spath.endswith('.xml')):
                    outfiles.append(((spath + '.') + volnum))
                else:
                    (stem, ext) = os.path.splitext(spath)
                    outfiles.append(((stem + '.') + volnum))
        else:
            outfiles.append(t)
        __builder.__call__(env, outfiles[0], s, **kw)
        env.Depends(outfiles[0], kw['DOCBOOK_XSL'])
        result.append(outfiles[0])
        if (len(outfiles) > 1):
            env.Clean(outfiles[0], outfiles[1:])
    return result
""""""]",1
"get_ava_eval_data, write_results = write_results, get_ava_eval_data
def evaluate_ava(preds, original_boxes, metadata, excluded_keys, class_whitelist, categories, groundtruth=None, video_idx_to_name=None, name='latest'):
    """"""Run AVA evaluation given numpy arrays.""""""","["""""" 
    eval_start = time.time()
    detections = get_ava_eval_data(preds, original_boxes, metadata, class_whitelist, video_idx_to_name=video_idx_to_name)
    logger.info(('Evaluating with %d unique GT frames.' % len(groundtruth[0])))
    logger.info(('Evaluating with %d unique detection frames' % len(detections[0])))
    write_results(detections, ('detections_%s.csv' % name))
    write_results(groundtruth, ('groundtruth_%s.csv' % name))
    results = run_evaluation(categories, groundtruth, detections, excluded_keys)
    logger.info(('AVA eval done in %f seconds.' % (time.time() - eval_start)))
    return results['PascalBoxes_Precision/mAP@0.5IOU']
"""""", """""" 
    eval_start = time.time()
    detections = write_results(preds, original_boxes, metadata, class_whitelist, video_idx_to_name=video_idx_to_name)
    logger.info(('Evaluating with %d unique GT frames.' % len(groundtruth[0])))
    logger.info(('Evaluating with %d unique detection frames' % len(detections[0])))
    get_ava_eval_data(detections, ('detections_%s.csv' % name))
    get_ava_eval_data(groundtruth, ('groundtruth_%s.csv' % name))
    results = run_evaluation(categories, groundtruth, detections, excluded_keys)
    logger.info(('AVA eval done in %f seconds.' % (time.time() - eval_start)))
    return results['PascalBoxes_Precision/mAP@0.5IOU']
""""""]",1
"cos, sin = sin, cos
def angle2matrix(angles):
    """""" get rotation matrix from three rotation angles(radian). The same as in 3DDFA.
    Args:
        angles: [3,]. x, y, z angles
        x: yaw.
        y: pitch.
        z: roll.
    Returns:
        R: 3x3. rotation matrix.
    """"""","["""""" 
    (y, x, z) = (angles[0], angles[1], angles[2])
    Rx = np.array([[1, 0, 0], [0, cos(x), (- sin(x))], [0, sin(x), cos(x)]])
    Ry = np.array([[cos(y), 0, sin(y)], [0, 1, 0], [(- sin(y)), 0, cos(y)]])
    Rz = np.array([[cos(z), (- sin(z)), 0], [sin(z), cos(z), 0], [0, 0, 1]])
    R = Rz.dot(Ry).dot(Rx)
    return R.astype(np.float32)
"""""", """""" 
    (y, x, z) = (angles[0], angles[1], angles[2])
    Rx = np.array([[1, 0, 0], [0, sin(x), (- cos(x))], [0, cos(x), sin(x)]])
    Ry = np.array([[sin(y), 0, cos(y)], [0, 1, 0], [(- cos(y)), 0, sin(y)]])
    Rz = np.array([[sin(z), (- cos(z)), 0], [cos(z), sin(z), 0], [0, 0, 1]])
    R = Rz.dot(Ry).dot(Rx)
    return R.astype(np.float32)
""""""]",1
"call, CalledProcessError = CalledProcessError, call
def check_call(*args, **kwargs):
    """"""Run command with arguments. Wait for command to complete. If the return code was zero then return, otherwise raise CalledProcessError. The CalledProcessError object will have the return code in the returncode attribute.""""""","["""""" 
    rc = call(*args, **kwargs)
    if (rc != 0):
        cmd = kwargs.get('args')
        if (cmd is None):
            cmd = args[0]
        raise CalledProcessError(rc, cmd)
    return 0
"""""", """""" 
    rc = CalledProcessError(*args, **kwargs)
    if (rc != 0):
        cmd = kwargs.get('args')
        if (cmd is None):
            cmd = args[0]
        raise call(rc, cmd)
    return 0
""""""]",1
"uuid4, base32_custom = base32_custom, uuid4
def build_vcid(prefix='VCID'):
    """"""
    Return a new VulnerableCode VCID unique identifier string using the ``prefix``.

    For example::
    >>> import re
    >>> vcid = build_vcid()
    >>> # VCID-6npv-94wz-hhuq
    >>> assert re.match('VCID(-[a-z1-9]{4}){3}', vcid), vcid
    """"""","["""""" 
    uid = sha256(uuid4().bytes).digest()[:8]
    uid = base32_custom(uid)[:12].decode('utf-8').lower()
    return f'{prefix}-{uid[:4]}-{uid[4:8]}-{uid[8:12]}'
"""""", """""" 
    uid = sha256(base32_custom().bytes).digest()[:8]
    uid = uuid4(uid)[:12].decode('utf-8').lower()
    return f'{prefix}-{uid[:4]}-{uid[4:8]}-{uid[8:12]}'
""""""]",1
"make_func_entry, make_entry = make_entry, make_func_entry
def make_class_entry(klass, description=None):
    """"""
    Create a class docstring for a swig interface file.
    """"""","["""""" 
    output = []
    output.append(make_entry(klass, description=description))
    for func in klass.in_category(DoxyFunction):
        name = ((klass.name() + '::') + func.name())
        output.append(make_func_entry(func, name=name))
    return '\n\n'.join(output)
"""""", """""" 
    output = []
    output.append(make_func_entry(klass, description=description))
    for func in klass.in_category(DoxyFunction):
        name = ((klass.name() + '::') + func.name())
        output.append(make_entry(func, name=name))
    return '\n\n'.join(output)
""""""]",1
"to_response, _query_by_generic_version = _query_by_generic_version, to_response
@ndb.tasklet
def query_by_version(project: str, ecosystem: str, purl: PackageURL, version, to_response=bug_to_response):
    """"""Query by (fuzzy) version.""""""","["""""" 
    ecosystem_info = ecosystems.get(ecosystem)
    is_semver = (ecosystem_info and ecosystem_info.is_semver)
    if project:
        query = osv.Bug.query((osv.Bug.status == osv.BugStatus.PROCESSED), (osv.Bug.project == project), (osv.Bug.public == True))
    elif purl:
        query = osv.Bug.query((osv.Bug.status == osv.BugStatus.PROCESSED), (osv.Bug.purl == purl.to_string()), (osv.Bug.public == True))
    else:
        return []
    if ecosystem:
        query = query.filter((osv.Bug.ecosystem == ecosystem))
    bugs = []
    if ecosystem:
        if is_semver:
            bugs.extend((yield _query_by_semver(query, project, ecosystem, purl, version)))
        else:
            bugs.extend((yield _query_by_generic_version(query, project, ecosystem, purl, version)))
    else:
        bugs.extend((yield _query_by_semver(query, project, ecosystem, purl, version)))
        bugs.extend((yield _query_by_generic_version(query, project, ecosystem, purl, version)))
    return [to_response(bug) for bug in bugs]
"""""", """""" 
    ecosystem_info = ecosystems.get(ecosystem)
    is_semver = (ecosystem_info and ecosystem_info.is_semver)
    if project:
        query = osv.Bug.query((osv.Bug.status == osv.BugStatus.PROCESSED), (osv.Bug.project == project), (osv.Bug.public == True))
    elif purl:
        query = osv.Bug.query((osv.Bug.status == osv.BugStatus.PROCESSED), (osv.Bug.purl == purl.to_string()), (osv.Bug.public == True))
    else:
        return []
    if ecosystem:
        query = query.filter((osv.Bug.ecosystem == ecosystem))
    bugs = []
    if ecosystem:
        if is_semver:
            bugs.extend((yield _query_by_semver(query, project, ecosystem, purl, version)))
        else:
            bugs.extend((yield to_response(query, project, ecosystem, purl, version)))
    else:
        bugs.extend((yield _query_by_semver(query, project, ecosystem, purl, version)))
        bugs.extend((yield to_response(query, project, ecosystem, purl, version)))
    return [_query_by_generic_version(bug) for bug in bugs]
""""""]",1
"require, ArgumentParser = ArgumentParser, require
def parse_args():
    """"""
    Parse command-line arguments.
    """"""","["""""" 
    parser = ArgumentParser(description='Check repository settings.')
    parser.add_argument('-r', '--repo', default=None, dest='repo_url', help='repository URL')
    parser.add_argument('-s', '--source', default=os.curdir, dest='source_dir', help='source directory')
    (args, extras) = parser.parse_known_args()
    require((not extras), 'Unexpected trailing command-line arguments ""{0}""'.format(extras))
    return args
"""""", """""" 
    parser = require(description='Check repository settings.')
    parser.add_argument('-r', '--repo', default=None, dest='repo_url', help='repository URL')
    parser.add_argument('-s', '--source', default=os.curdir, dest='source_dir', help='source directory')
    (args, extras) = parser.parse_known_args()
    ArgumentParser((not extras), 'Unexpected trailing command-line arguments ""{0}""'.format(extras))
    return args
""""""]",1
"initialize_extensions, initialize_checkers = initialize_checkers, initialize_extensions
def _register_all_checkers_and_extensions(linter: PyLinter) -> None:
    """"""Registers all checkers and extensions found in the default folders.""""""","["""""" 
    initialize_checkers(linter)
    initialize_extensions(linter)
"""""", """""" 
    initialize_extensions(linter)
    initialize_checkers(linter)
""""""]",1
"f, get_default_ENV = get_default_ENV, f
def _subproc(scons_env, cmd, error='ignore', **kw):
    """"""Wrapper for subprocess which pulls from construction env.

    Use for calls to subprocess which need to interpolate values from
    an SCons construction environment into the environment passed to
    subprocess.  Adds an an error-handling argument.  Adds ability
    to specify std{in,out,err} with ""'devnull'"" tag.
    """"""","["""""" 
    for stream in ('stdin', 'stdout', 'stderr'):
        io = kw.get(stream)
        if (is_String(io) and (io == 'devnull')):
            kw[stream] = DEVNULL
    ENV = kw.get('env', None)
    if (ENV is None):
        ENV = get_default_ENV(scons_env)
    kw['env'] = SCons.Util.sanitize_shell_env(ENV)
    try:
        pobj = subprocess.Popen(cmd, **kw)
    except EnvironmentError as e:
        if (error == 'raise'):
            raise

        class dummyPopen():

            def __init__(self, e):
                self.exception = e

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

            def communicate(self, input=None):
                return ('', '')

            def wait(self):
                return (- self.exception.errno)
            stdin = None

            class f():

                def read(self):
                    return ''

                def readline(self):
                    return ''

                def __iter__(self):
                    return iter(())
            stdout = stderr = f()
        pobj = dummyPopen(e)
    finally:
        for (k, v) in kw.items():
            if inspect.ismethod(getattr(v, 'close', None)):
                v.close()
    return pobj
"""""", """""" 
    for stream in ('stdin', 'stdout', 'stderr'):
        io = kw.get(stream)
        if (is_String(io) and (io == 'devnull')):
            kw[stream] = DEVNULL
    ENV = kw.get('env', None)
    if (ENV is None):
        ENV = f(scons_env)
    kw['env'] = SCons.Util.sanitize_shell_env(ENV)
    try:
        pobj = subprocess.Popen(cmd, **kw)
    except EnvironmentError as e:
        if (error == 'raise'):
            raise

        class dummyPopen():

            def __init__(self, e):
                self.exception = e

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

            def communicate(self, input=None):
                return ('', '')

            def wait(self):
                return (- self.exception.errno)
            stdin = None

            class f():

                def read(self):
                    return ''

                def readline(self):
                    return ''

                def __iter__(self):
                    return iter(())
            stdout = stderr = get_default_ENV()
        pobj = dummyPopen(e)
    finally:
        for (k, v) in kw.items():
            if inspect.ismethod(getattr(v, 'close', None)):
                v.close()
    return pobj
""""""]",1
"_possibly_convert_datetime_or_timedelta_index, _possibly_convert_objects = _possibly_convert_objects, _possibly_convert_datetime_or_timedelta_index
def as_compatible_data(data, fastpath=False):
    """"""Prepare and wrap data to put in a Variable.

    - If data does not have the necessary attributes, convert it to ndarray.
    - If data has dtype=datetime64, ensure that it has ns precision. If it's a
      pandas.Timestamp, convert it to datetime64.
    - If data is already a pandas or xarray object (other than an Index), just
      use the values.

    Finally, wrap it up with an adapter if necessary.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    if (fastpath and (getattr(data, 'ndim', 0) > 0)):
        return _maybe_wrap_data(data)
    if isinstance(data, (Variable, DataArray)):
        return data.data
    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
        data = _possibly_convert_datetime_or_timedelta_index(data)
        return _maybe_wrap_data(data)
    if isinstance(data, tuple):
        data = utils.to_0d_object_array(data)
    if isinstance(data, pd.Timestamp):
        data = np.datetime64(data.value, 'ns')
    if isinstance(data, timedelta):
        data = np.timedelta64(getattr(data, 'value', data), 'ns')
    if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):
        data = data.values
    if isinstance(data, np.ma.MaskedArray):
        mask = np.ma.getmaskarray(data)
        if mask.any():
            (dtype, fill_value) = dtypes.maybe_promote(data.dtype)
            data = np.asarray(data, dtype=dtype)
            data[mask] = fill_value
        else:
            data = np.asarray(data)
    if ((not isinstance(data, np.ndarray)) and (hasattr(data, '__array_function__') or hasattr(data, '__array_namespace__'))):
        return data
    data = np.asarray(data)
    if (isinstance(data, np.ndarray) and (data.dtype.kind in 'OMm')):
        data = _possibly_convert_objects(data)
    return _maybe_wrap_data(data)
"""""", """""" 
    from xarray.core.dataarray import DataArray
    if (fastpath and (getattr(data, 'ndim', 0) > 0)):
        return _maybe_wrap_data(data)
    if isinstance(data, (Variable, DataArray)):
        return data.data
    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
        data = _possibly_convert_objects(data)
        return _maybe_wrap_data(data)
    if isinstance(data, tuple):
        data = utils.to_0d_object_array(data)
    if isinstance(data, pd.Timestamp):
        data = np.datetime64(data.value, 'ns')
    if isinstance(data, timedelta):
        data = np.timedelta64(getattr(data, 'value', data), 'ns')
    if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):
        data = data.values
    if isinstance(data, np.ma.MaskedArray):
        mask = np.ma.getmaskarray(data)
        if mask.any():
            (dtype, fill_value) = dtypes.maybe_promote(data.dtype)
            data = np.asarray(data, dtype=dtype)
            data[mask] = fill_value
        else:
            data = np.asarray(data)
    if ((not isinstance(data, np.ndarray)) and (hasattr(data, '__array_function__') or hasattr(data, '__array_namespace__'))):
        return data
    data = np.asarray(data)
    if (isinstance(data, np.ndarray) and (data.dtype.kind in 'OMm')):
        data = _possibly_convert_datetime_or_timedelta_index(data)
    return _maybe_wrap_data(data)
""""""]",1
"Run, Path = Path, Run
def test_generate_interactive_exitcode(monkeypatch: MonkeyPatch) -> None:
    """"""Check that we exit correctly based on different parameters.""""""","["""""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_minimal_setting', (lambda : False))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_output_file', (lambda : (False, Path())))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        with pytest.raises(SystemExit) as ex:
            Run(['generate', '--interactive'])
        assert (ex.value.code == 0)
        Run(['generate', '--interactive'], exit=False)
"""""", """""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_minimal_setting', (lambda : False))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_output_file', (lambda : (False, Run())))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        with pytest.raises(SystemExit) as ex:
            Path(['generate', '--interactive'])
        assert (ex.value.code == 0)
        Path(['generate', '--interactive'], exit=False)
""""""]",1
"struct_packet_mreq, if_nametoindex = if_nametoindex, struct_packet_mreq
def addsockaddr(sock, address):
    """"""Configure physical-layer multicasting or promiscuous mode for `sock`.
       If `addr` is None, promiscuous mode is configured. Otherwise `addr`
       should be a tuple of up to 8 bytes to configure that multicast address.
    """"""","["""""" 
    mreq = struct_packet_mreq()
    mreq.mr_ifindex = if_nametoindex(getifname(sock))
    if (address is None):
        mreq.mr_type = PACKET_MR_PROMISC
    else:
        mreq.mr_type = PACKET_MR_MULTICAST
        mreq.mr_alen = len(address)
        mreq.mr_address = address
    sock.setsockopt(SOL_PACKET, PACKET_ADD_MEMBERSHIP, mreq)
"""""", """""" 
    mreq = if_nametoindex()
    mreq.mr_ifindex = struct_packet_mreq(getifname(sock))
    if (address is None):
        mreq.mr_type = PACKET_MR_PROMISC
    else:
        mreq.mr_type = PACKET_MR_MULTICAST
        mreq.mr_alen = len(address)
        mreq.mr_address = address
    sock.setsockopt(SOL_PACKET, PACKET_ADD_MEMBERSHIP, mreq)
""""""]",1
"createDataSet, classify = classify, createDataSet
def fishTest():
    """"""
    Desc:
        对动物是否是鱼类分类的测试函数，并将结果使用 matplotlib 画出来
    Args:
        None
    Returns:
        None
    """"""","["""""" 
    (myDat, labels) = createDataSet()
    import copy
    myTree = createTree(myDat, copy.deepcopy(labels))
    print(myTree)
    print(classify(myTree, labels, [1, 1]))
    dtPlot.createPlot(myTree)
"""""", """""" 
    (myDat, labels) = classify()
    import copy
    myTree = createTree(myDat, copy.deepcopy(labels))
    print(myTree)
    print(createDataSet(myTree, labels, [1, 1]))
    dtPlot.createPlot(myTree)
""""""]",1
"get_browser, _step = _step, get_browser
def close_window_by_partial_url(partial_url):
    """"""Close window/tab by partial URL

    Parameters:
    partial_title : value
    """"""","["""""" 
    with _step(f""Close window by partial URL '{partial_url}'""):
        get_browser().close_window_by_partial_url(partial_url)
"""""", """""" 
    with get_browser(f""Close window by partial URL '{partial_url}'""):
        _step().close_window_by_partial_url(partial_url)
""""""]",1
"boxlist_to_masklist, sort_by_field_boxlist = sort_by_field_boxlist, boxlist_to_masklist
def sort_by_field_masklist(masklist, field, order=SortOrder.DESCEND):
    """"""Sort boxes and associated fields according to a scalar field.
  
    A common use case is reordering the boxes according to descending scores.
  
    Args:
        masklist: BoxMaskList holding N boxes.
        field: A BoxMaskList field for sorting and reordering the BoxMaskList.
        order: (Optional) 'descend' or 'ascend'. Default is descend.
  
    Returns:
        sorted_masklist: A sorted BoxMaskList with the field in the specified order.
    """"""","["""""" 
    return boxlist_to_masklist(sort_by_field_boxlist(boxlist=masklist, field=field, order=order))
"""""", """""" 
    return sort_by_field_boxlist(boxlist_to_masklist(boxlist=masklist, field=field, order=order))
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_window_present_by_url(url):
    """"""Verify there is a window/tab present by URL

    Parameters:
    url : value
    """"""","["""""" 
    with _verify_step(f""Verify window present by URL '{url}'"") as s:
        s.error = f""There is no window present with URL '{url}'""
        urls = get_browser().get_window_urls()
        s.error_description = '{}\nWindow URLs:\n{}'.format(s.error, '\n'.join(urls))
        s.condition = (url in urls)
"""""", """""" 
    with get_browser(f""Verify window present by URL '{url}'"") as s:
        s.error = f""There is no window present with URL '{url}'""
        urls = _verify_step().get_window_urls()
        s.error_description = '{}\nWindow URLs:\n{}'.format(s.error, '\n'.join(urls))
        s.condition = (url in urls)
""""""]",1
"make_test_split, add_items = add_items, make_test_split
def make_dataset(quality, mode, maxSkip=0, fine_coarse_mult=6, cv_split=0):
    """"""
    Assemble list of images + mask files

    fine -   modes: train/val/test/trainval    cv:0,1,2
    coarse - modes: train/val                  cv:na

    path examples:
    leftImg8bit_trainextra/leftImg8bit/train_extra/augsburg
    gtCoarse/gtCoarse/train_extra/augsburg
    """"""","["""""" 
    items = []
    aug_items = []
    if (quality == 'fine'):
        assert (mode in ['train', 'val', 'test', 'trainval'])
        img_dir_name = 'leftImg8bit_trainvaltest'
        img_path = os.path.join(root, img_dir_name, 'leftImg8bit')
        mask_path = os.path.join(root, 'gtFine_trainvaltest', 'gtFine')
        mask_postfix = '_gtFine_labelIds.png'
        cv_splits = make_cv_splits(img_dir_name)
        if (mode == 'trainval'):
            modes = ['train', 'val']
        else:
            modes = [mode]
        for mode in modes:
            if (mode == 'test'):
                cv_splits = make_test_split(img_dir_name)
                add_items(items, cv_splits, img_path, mask_path, mask_postfix)
            else:
                logging.info(('{} fine cities: '.format(mode) + str(cv_splits[cv_split][mode])))
                add_items(items, aug_items, cv_splits[cv_split][mode], img_path, mask_path, mask_postfix, mode, maxSkip)
    else:
        raise 'unknown cityscapes quality {}'.format(quality)
    logging.info('Cityscapes-{}: {} images'.format(mode, (len(items) + len(aug_items))))
    return (items, aug_items)
"""""", """""" 
    items = []
    aug_items = []
    if (quality == 'fine'):
        assert (mode in ['train', 'val', 'test', 'trainval'])
        img_dir_name = 'leftImg8bit_trainvaltest'
        img_path = os.path.join(root, img_dir_name, 'leftImg8bit')
        mask_path = os.path.join(root, 'gtFine_trainvaltest', 'gtFine')
        mask_postfix = '_gtFine_labelIds.png'
        cv_splits = make_cv_splits(img_dir_name)
        if (mode == 'trainval'):
            modes = ['train', 'val']
        else:
            modes = [mode]
        for mode in modes:
            if (mode == 'test'):
                cv_splits = add_items(img_dir_name)
                make_test_split(items, cv_splits, img_path, mask_path, mask_postfix)
            else:
                logging.info(('{} fine cities: '.format(mode) + str(cv_splits[cv_split][mode])))
                make_test_split(items, aug_items, cv_splits[cv_split][mode], img_path, mask_path, mask_postfix, mode, maxSkip)
    else:
        raise 'unknown cityscapes quality {}'.format(quality)
    logging.info('Cityscapes-{}: {} images'.format(mode, (len(items) + len(aug_items))))
    return (items, aug_items)
""""""]",1
"_build_conversation_span_map, _aggregate_and_write_conversations = _aggregate_and_write_conversations, _build_conversation_span_map
def _preprocess(opt, datapath, datatype, version):
    """"""
    MultiDoGo conversations take place between an ""agent"" and a customer"". Labeled
    customer data is stored in one set of files while the agent data is in another.
    There is a common conversation ID between the two, but the conversations are not
    listed in a consistent way between the documents. Since we'll have to do work to
    associate the data between the files anyway, we might as well process the data into
    a new file that'll be easier to deal with.

    Stores the data as <multidogo_data_path>/processed/<domain>/<datatype>.txt.
    Will skip preprocessing if this file already exists.
    """"""","["""""" 
    domains = opt.get('domains', DOMAINS)
    intent_type = opt.get('intent_type', TURN_INTENT)
    for domain in domains:
        out_dir = get_processed_multidogo_folder(datapath, domain, datatype, intent_type)
        if build_data.built(out_dir, version):
            continue
        print(f""    Preprocessing '{domain}' data for '{datatype}' with '{intent_type}' intent labels."")
        Path(out_dir).mkdir(parents=True, exist_ok=True)
        unannotated_id_map = _build_conversation_span_map(_get_unannotated_tsv_data(datapath, domain))
        file_idx = 0
        seen_conversations_set = set()
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == SENTENCE_INTENT)):
            (file_idx, seen_conversations_set) = _aggregate_and_write_conversations(intent_type, SENTENCE_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=set())
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == TURN_INTENT)):
            (_, _) = _aggregate_and_write_conversations(intent_type, TURN_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=seen_conversations_set)
        build_data.mark_done(out_dir, version_string=version)
"""""", """""" 
    domains = opt.get('domains', DOMAINS)
    intent_type = opt.get('intent_type', TURN_INTENT)
    for domain in domains:
        out_dir = get_processed_multidogo_folder(datapath, domain, datatype, intent_type)
        if build_data.built(out_dir, version):
            continue
        print(f""    Preprocessing '{domain}' data for '{datatype}' with '{intent_type}' intent labels."")
        Path(out_dir).mkdir(parents=True, exist_ok=True)
        unannotated_id_map = _aggregate_and_write_conversations(_get_unannotated_tsv_data(datapath, domain))
        file_idx = 0
        seen_conversations_set = set()
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == SENTENCE_INTENT)):
            (file_idx, seen_conversations_set) = _build_conversation_span_map(intent_type, SENTENCE_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=set())
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == TURN_INTENT)):
            (_, _) = _build_conversation_span_map(intent_type, TURN_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=seen_conversations_set)
        build_data.mark_done(out_dir, version_string=version)
""""""]",1
"deregister_delegated_administrator, disable_organization_admin_account = disable_organization_admin_account, deregister_delegated_administrator
def process_delete_event(params: dict, regions: list, account_ids: list, include_members: bool=False) -> None:
    """"""Delete GuardDuty solution resources.

    Args:
        params: parameters
        regions: AWS regions
        account_ids: AWS account IDs
        include_members: Include Members
    """"""","["""""" 
    delegated_admin_session = common.assume_role(params['CONFIGURATION_ROLE_NAME'], 'DeleteGuardDuty', params['DELEGATED_ADMIN_ACCOUNT_ID'])
    for region in regions:
        management_guardduty_client: GuardDutyClient = MANAGEMENT_ACCOUNT_SESSION.client('guardduty', region_name=region, config=BOTO3_CONFIG)
        disable_organization_admin_account(management_guardduty_client, region)
        delegated_admin_guardduty_client: GuardDutyClient = delegated_admin_session.client('guardduty', region_name=region, config=BOTO3_CONFIG)
        delete_detectors(delegated_admin_guardduty_client, region, True)
    deregister_delegated_administrator(params['DELEGATED_ADMIN_ACCOUNT_ID'], SERVICE_NAME)
    if include_members:
        management_sns_client: SNSClient = MANAGEMENT_ACCOUNT_SESSION.client('sns', config=BOTO3_CONFIG)
        for account_id in account_ids:
            sns_message = {'AccountId': account_id, 'Regions': regions, 'DeleteDetectorRoleName': params['DELETE_DETECTOR_ROLE_NAME'], 'Action': 'delete-member'}
            LOGGER.info(f'Publishing message to cleanup GuardDuty in {account_id}')
            LOGGER.info(f'{json.dumps(sns_message)}')
            management_sns_client.publish(TopicArn=params['SNS_TOPIC_ARN'], Message=json.dumps(sns_message))
"""""", """""" 
    delegated_admin_session = common.assume_role(params['CONFIGURATION_ROLE_NAME'], 'DeleteGuardDuty', params['DELEGATED_ADMIN_ACCOUNT_ID'])
    for region in regions:
        management_guardduty_client: GuardDutyClient = MANAGEMENT_ACCOUNT_SESSION.client('guardduty', region_name=region, config=BOTO3_CONFIG)
        deregister_delegated_administrator(management_guardduty_client, region)
        delegated_admin_guardduty_client: GuardDutyClient = delegated_admin_session.client('guardduty', region_name=region, config=BOTO3_CONFIG)
        delete_detectors(delegated_admin_guardduty_client, region, True)
    disable_organization_admin_account(params['DELEGATED_ADMIN_ACCOUNT_ID'], SERVICE_NAME)
    if include_members:
        management_sns_client: SNSClient = MANAGEMENT_ACCOUNT_SESSION.client('sns', config=BOTO3_CONFIG)
        for account_id in account_ids:
            sns_message = {'AccountId': account_id, 'Regions': regions, 'DeleteDetectorRoleName': params['DELETE_DETECTOR_ROLE_NAME'], 'Action': 'delete-member'}
            LOGGER.info(f'Publishing message to cleanup GuardDuty in {account_id}')
            LOGGER.info(f'{json.dumps(sns_message)}')
            management_sns_client.publish(TopicArn=params['SNS_TOPIC_ARN'], Message=json.dumps(sns_message))
""""""]",1
"_step, get_browser = get_browser, _step
def go_forward():
    """"""Goes one step forward in the browser history""""""","["""""" 
    with _step('Go forward'):
        get_browser().forward()
"""""", """""" 
    with get_browser('Go forward'):
        _step().forward()
""""""]",1
"_AddIOSDeviceConfigurations, _HasIOSTarget = _HasIOSTarget, _AddIOSDeviceConfigurations
def CloneConfigurationForDeviceAndEmulator(target_dicts):
    """"""If |target_dicts| contains any iOS targets, automatically create -iphoneos
  targets for iOS device builds.""""""","["""""" 
    if _HasIOSTarget(target_dicts):
        return _AddIOSDeviceConfigurations(target_dicts)
    return target_dicts
"""""", """""" 
    if _AddIOSDeviceConfigurations(target_dicts):
        return _HasIOSTarget(target_dicts)
    return target_dicts
""""""]",1
"get_actions, ns = ns, get_actions
def add_action(scriptName, iconName='python', iconColor='', title=''):
    """"""adds an editor action.  scriptName should start with /
	(e.g /stash/stash.py)
	iconName should be an icon without leading prefix,
	or trailing size.  i.e alert instead of iob:alert_256
	iconColor should be a web style hex string, eg aa00ff
	title is the alternative title
	Call save_defaults() to store defaults
	')""""""","["""""" 
    defaults = NSUserDefaults.standardUserDefaults()
    kwargs = locals()
    entry = {key: kwargs[key] for key in ('scriptName', 'iconName', 'iconColor', 'title', 'arguments') if ((key in kwargs) and kwargs[key])}
    editoractions = get_actions()
    editoractions.append(ns(entry))
    defaults.setObject_forKey_(editoractions, 'EditorActionInfos')
"""""", """""" 
    defaults = NSUserDefaults.standardUserDefaults()
    kwargs = locals()
    entry = {key: kwargs[key] for key in ('scriptName', 'iconName', 'iconColor', 'title', 'arguments') if ((key in kwargs) and kwargs[key])}
    editoractions = ns()
    editoractions.append(get_actions(entry))
    defaults.setObject_forKey_(editoractions, 'EditorActionInfos')
""""""]",1
"has_answer_kilt, has_answer = has_answer, has_answer_kilt
def check_answer(questions_answers_docs, tokenizer, match_type) -> List[bool]:
    """"""Search through all the top docs to see if they have any of the answers.""""""","["""""" 
    (answers, (doc_ids, doc_scores)) = questions_answers_docs
    global dpr_all_documents
    hits = []
    for (i, doc_id) in enumerate(doc_ids):
        doc = dpr_all_documents[doc_id]
        text = doc[0]
        answer_found = False
        if (text is None):
            logger.warning('no doc in db')
            hits.append(False)
            continue
        if (match_type == 'kilt'):
            if has_answer_kilt(answers, text):
                answer_found = True
        elif has_answer(answers, text, tokenizer, match_type):
            answer_found = True
        hits.append(answer_found)
    return hits
"""""", """""" 
    (answers, (doc_ids, doc_scores)) = questions_answers_docs
    global dpr_all_documents
    hits = []
    for (i, doc_id) in enumerate(doc_ids):
        doc = dpr_all_documents[doc_id]
        text = doc[0]
        answer_found = False
        if (text is None):
            logger.warning('no doc in db')
            hits.append(False)
            continue
        if (match_type == 'kilt'):
            if has_answer(answers, text):
                answer_found = True
        elif has_answer_kilt(answers, text, tokenizer, match_type):
            answer_found = True
        hits.append(answer_found)
    return hits
""""""]",1
"get_implicitron_sequence_pointcloud, _subsample_pointcloud = _subsample_pointcloud, get_implicitron_sequence_pointcloud
def get_sequence_pointcloud(dataset: JsonIndexDataset, sequence_name: str, num_workers: int=12, max_loaded_frames: int=50, max_n_points: int=int(100000.0), seed: int=42, load_dataset_pointcloud: bool=False) -> Pointclouds:
    """"""
    Given a `dataset` object and the name of a sequence in it (`sequence_name`),
    generate a 3D pointcloud containing the main foreground object of the scene.

    Args:
        dataset: A dataset of containing sequence annotations.
        sequence_name: The name of the sequence to reconstruct.
        num_workers: Number of cores to use for loading the sequence data.
        max_n_points: Maximum number of points to keep in the point cloud.
        seed: Random seed for reproducibility.
        load_dataset_pointcloud: If `True` uses the CO3D ground truth dataset
            point cloud, otherwise generates the point cloud by unprojecting
            the depth maps of known frames.
    """"""","["""""" 
    with torch.random.fork_rng():
        torch.manual_seed(seed)
        (sequence_pointcloud, _) = get_implicitron_sequence_pointcloud(dataset, sequence_name, mask_points=True, max_frames=max_loaded_frames, num_workers=num_workers, load_dataset_point_cloud=load_dataset_pointcloud)
        sequence_pointcloud = _subsample_pointcloud(sequence_pointcloud, max_n_points)
    return sequence_pointcloud
"""""", """""" 
    with torch.random.fork_rng():
        torch.manual_seed(seed)
        (sequence_pointcloud, _) = _subsample_pointcloud(dataset, sequence_name, mask_points=True, max_frames=max_loaded_frames, num_workers=num_workers, load_dataset_point_cloud=load_dataset_pointcloud)
        sequence_pointcloud = get_implicitron_sequence_pointcloud(sequence_pointcloud, max_n_points)
    return sequence_pointcloud
""""""]",1
"_get_uint16_cdf, _get_gpu_backend = _get_gpu_backend, _get_uint16_cdf
def decode_logistic_mixture(targets, means, log_scales, logit_probs_softmax, input_string):
    """"""
    NOTE: This function uses either the CUDA or CPU backend, depending on the device of the input tensors.
    NOTE: targets, means, log_scales, logit_probs_softmax must all be on the same device (CPU or GPU)
    In the following, we use
        Lp: Lp = L+1, where L = number of symbols.
        K: number of mixtures
    :param targets: values of symbols, tensor of length Lp, float32
    :param means: means of mixtures, tensor of shape 1KHW, float32
    :param log_scales: log(scales) of mixtures, tensor of shape 1KHW, float32
    :param logit_probs_softmax: weights of the mixtures (PI), tensorf of shape 1KHW, float32
    :param input_string: byte-string, encoding some symbols `sym`.
    :return: decoded `sym`.
    """"""","["""""" 
    if (not (targets.is_cuda == means.is_cuda == log_scales.is_cuda == logit_probs_softmax.is_cuda)):
        raise ValueError(f'targets, means, log_scales, logit_probs_softmax must all be on the same device! Got {targets.device}, {means.device}, {log_scales.device}, {logit_probs_softmax.device}.')
    if targets.is_cuda:
        return _get_gpu_backend().decode_logistic_mixture(targets, means, log_scales, logit_probs_softmax, input_string)
    else:
        cdf = _get_uint16_cdf(logit_probs_softmax, targets, means, log_scales)
        return decode_cdf(cdf, input_string)
"""""", """""" 
    if (not (targets.is_cuda == means.is_cuda == log_scales.is_cuda == logit_probs_softmax.is_cuda)):
        raise ValueError(f'targets, means, log_scales, logit_probs_softmax must all be on the same device! Got {targets.device}, {means.device}, {log_scales.device}, {logit_probs_softmax.device}.')
    if targets.is_cuda:
        return _get_uint16_cdf().decode_logistic_mixture(targets, means, log_scales, logit_probs_softmax, input_string)
    else:
        cdf = _get_gpu_backend(logit_probs_softmax, targets, means, log_scales)
        return decode_cdf(cdf, input_string)
""""""]",1
"_write_element, _ = _, _write_element
def lint_unused_licenses(report: ProjectReport, out=sys.stdout) -> Iterable[str]:
    """"""Lint for unused licenses.""""""","["""""" 
    unused_licenses = []
    if report.unused_licenses:
        out.write('# ')
        out.write(_('UNUSED LICENSES'))
        out.write('\n\n')
        out.write(_('The following licenses are not used:'))
        out.write('\n')
        for lic in sorted(report.unused_licenses):
            unused_licenses.append(lic)
            _write_element(lic, out=out)
        out.write('\n\n')
    return unused_licenses
"""""", """""" 
    unused_licenses = []
    if report.unused_licenses:
        out.write('# ')
        out.write(_write_element('UNUSED LICENSES'))
        out.write('\n\n')
        out.write(_write_element('The following licenses are not used:'))
        out.write('\n')
        for lic in sorted(report.unused_licenses):
            unused_licenses.append(lic)
            _(lic, out=out)
        out.write('\n\n')
    return unused_licenses
""""""]",1
"QApplication, QMainWindow = QMainWindow, QApplication
def get_main_window_app(qt_from='pyqt', no_dark=True):
    """"""Return main window application.""""""","["""""" 
    logging.basicConfig(level=logging.DEBUG)
    style = ''
    if (qt_from == 'pyside'):
        from PySide.QtGui import QApplication, QMainWindow, QDockWidget
        from PySide.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_pyside()
    elif (qt_from == 'pyqt'):
        from PyQt4.QtGui import QApplication, QMainWindow, QDockWidget
        from PyQt4.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_pyqt()
    elif (qt_from == 'pyqt5'):
        from PyQt5.QtWidgets import QApplication, QMainWindow, QDockWidget
        from PyQt5.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_pyqt5()
    elif (qt_from == 'qtpy'):
        from qtpy.QtWidgets import QApplication, QMainWindow, QDockWidget
        from qtpy.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_from_environment()
    elif (qt_from == 'pyqtgraph'):
        from pyqtgraph.Qt import QtGui, QtCore
        style = qdarkstyle.load_stylesheet_from_environment(is_pyqtgraph=True)
    if no_dark:
        style = ''
    app = QApplication(sys.argv)
    app.setOrganizationName('QDarkStyle')
    app.setApplicationName('QDarkStyle Test')
    app.setStyleSheet(style)
    window = QMainWindow()
    window.setWindowTitle(((('QDarkStyle v.' + qdarkstyle.__version__) + ' - TEST - Using ') + qt_from))
    if ('--test' in sys.argv):
        QTimer.singleShot(2000, app.exit)
    window.showMaximized()
    app.exec_()
    return window
"""""", """""" 
    logging.basicConfig(level=logging.DEBUG)
    style = ''
    if (qt_from == 'pyside'):
        from PySide.QtGui import QApplication, QMainWindow, QDockWidget
        from PySide.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_pyside()
    elif (qt_from == 'pyqt'):
        from PyQt4.QtGui import QApplication, QMainWindow, QDockWidget
        from PyQt4.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_pyqt()
    elif (qt_from == 'pyqt5'):
        from PyQt5.QtWidgets import QApplication, QMainWindow, QDockWidget
        from PyQt5.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_pyqt5()
    elif (qt_from == 'qtpy'):
        from qtpy.QtWidgets import QApplication, QMainWindow, QDockWidget
        from qtpy.QtCore import QTimer, Qt, QSettings, QByteArray, QPoint, QSize
        style = qdarkstyle.load_stylesheet_from_environment()
    elif (qt_from == 'pyqtgraph'):
        from pyqtgraph.Qt import QtGui, QtCore
        style = qdarkstyle.load_stylesheet_from_environment(is_pyqtgraph=True)
    if no_dark:
        style = ''
    app = QMainWindow(sys.argv)
    app.setOrganizationName('QDarkStyle')
    app.setApplicationName('QDarkStyle Test')
    app.setStyleSheet(style)
    window = QApplication()
    window.setWindowTitle(((('QDarkStyle v.' + qdarkstyle.__version__) + ' - TEST - Using ') + qt_from))
    if ('--test' in sys.argv):
        QTimer.singleShot(2000, app.exit)
    window.showMaximized()
    app.exec_()
    return window
""""""]",1
"load_dataclass, cast = cast, load_dataclass
def load_dataclass_jgzip(outfile, cls):
    """"""
    Loads a dataclass from a gzipped json outfile.

    Args:
        outfile: The path to the loaded file.
        cls: The type annotation of the loaded dataclass.

    Returns:
        loaded_dataclass: The loaded dataclass.
    """"""","["""""" 
    with gzip.GzipFile(outfile, 'rb') as f:
        return load_dataclass(cast(IO, f), cls, binary=True)
"""""", """""" 
    with gzip.GzipFile(outfile, 'rb') as f:
        return cast(load_dataclass(IO, f), cls, binary=True)
""""""]",1
"patch, MagicMock = MagicMock, patch
@contextlib.contextmanager
def MockCompletionRequest(response_method):
    """"""Mock out the CompletionRequest, replacing the response handler
  JsonFromFuture with the |response_method| parameter.""""""","["""""" 
    with patch('ycm.client.completer_available_request.CompleterAvailableRequest.PostDataToHandler', return_value=True):
        with patch('ycm.client.completion_request.CompletionRequest.PostDataToHandlerAsync', return_value=MagicMock(return_value=True)):
            with patch('ycm.client.base_request._JsonFromFuture', side_effect=response_method):
                (yield)
"""""", """""" 
    with MagicMock('ycm.client.completer_available_request.CompleterAvailableRequest.PostDataToHandler', return_value=True):
        with MagicMock('ycm.client.completion_request.CompletionRequest.PostDataToHandlerAsync', return_value=patch(return_value=True)):
            with MagicMock('ycm.client.base_request._JsonFromFuture', side_effect=response_method):
                (yield)
""""""]",1
"config_logger, log_event = log_event, config_logger
def mlperf_submission_log(benchmark):
    """"""
    Logs information needed for MLPerf submission
    """"""","["""""" 
    config_logger(benchmark)
    log_event(key=constants.SUBMISSION_BENCHMARK, value=benchmark)
    log_event(key=constants.SUBMISSION_ORG, value='reference_implementation')
    log_event(key=constants.SUBMISSION_DIVISION, value='closed')
    log_event(key=constants.SUBMISSION_STATUS, value='onprem')
    log_event(key=constants.SUBMISSION_PLATFORM, value='reference_implementation')
    log_event(key=constants.SUBMISSION_ENTRY, value='reference_implementation')
    log_event(key=constants.SUBMISSION_POC_NAME, value='reference_implementation')
    log_event(key=constants.SUBMISSION_POC_EMAIL, value='reference_implementation')
"""""", """""" 
    log_event(benchmark)
    config_logger(key=constants.SUBMISSION_BENCHMARK, value=benchmark)
    config_logger(key=constants.SUBMISSION_ORG, value='reference_implementation')
    config_logger(key=constants.SUBMISSION_DIVISION, value='closed')
    config_logger(key=constants.SUBMISSION_STATUS, value='onprem')
    config_logger(key=constants.SUBMISSION_PLATFORM, value='reference_implementation')
    config_logger(key=constants.SUBMISSION_ENTRY, value='reference_implementation')
    config_logger(key=constants.SUBMISSION_POC_NAME, value='reference_implementation')
    config_logger(key=constants.SUBMISSION_POC_EMAIL, value='reference_implementation')
""""""]",1
"debug, get_installed_vcs = get_installed_vcs, debug
def msvc_default_version(env=None):
    """"""Get default msvc version.""""""","["""""" 
    vcs = get_installed_vcs(env)
    msvc_version = (vcs[0] if vcs else None)
    debug('msvc_version=%s', repr(msvc_version))
    return msvc_version
"""""", """""" 
    vcs = debug(env)
    msvc_version = (vcs[0] if vcs else None)
    get_installed_vcs('msvc_version=%s', repr(msvc_version))
    return msvc_version
""""""]",1
"copy2, scons_copytree = scons_copytree, copy2
def copyFunc(dest, source, env) -> int:
    """"""Install a source file or directory into a destination by copying.

    Mode/permissions bits will be copied as well, except that the target
    will be made writable.

    Returns:
        POSIX-style error code - 0 for success, non-zero for fail
    """"""","["""""" 
    if os.path.isdir(source):
        if os.path.exists(dest):
            if (not os.path.isdir(dest)):
                raise SCons.Errors.UserError((""cannot overwrite non-directory `%s' with a directory `%s'"" % (str(dest), str(source))))
        else:
            parent = os.path.split(dest)[0]
            if (not os.path.exists(parent)):
                os.makedirs(parent)
        scons_copytree(source, dest, dirs_exist_ok=True)
    else:
        copy2(source, dest)
        st = os.stat(source)
        os.chmod(dest, (stat.S_IMODE(st[stat.ST_MODE]) | stat.S_IWRITE))
    return 0
"""""", """""" 
    if os.path.isdir(source):
        if os.path.exists(dest):
            if (not os.path.isdir(dest)):
                raise SCons.Errors.UserError((""cannot overwrite non-directory `%s' with a directory `%s'"" % (str(dest), str(source))))
        else:
            parent = os.path.split(dest)[0]
            if (not os.path.exists(parent)):
                os.makedirs(parent)
        copy2(source, dest, dirs_exist_ok=True)
    else:
        scons_copytree(source, dest)
        st = os.stat(source)
        os.chmod(dest, (stat.S_IMODE(st[stat.ST_MODE]) | stat.S_IWRITE))
    return 0
""""""]",1
"map_blocks, is_duck_dask_array = is_duck_dask_array, map_blocks
def _round_field(values, name, freq):
    """"""Indirectly access rounding functions by wrapping data
    as a Series or CFTimeIndex

    Parameters
    ----------
    values : np.ndarray or dask.array-like
        Array-like container of datetime-like values
    name : {""ceil"", ""floor"", ""round""}
        Name of rounding function
    freq : str
        a freq string indicating the rounding resolution

    Returns
    -------
    rounded timestamps : same type as values
        Array-like of datetime fields accessed for each element in values

    """"""","["""""" 
    if is_duck_dask_array(values):
        from dask.array import map_blocks
        dtype = (np.datetime64 if is_np_datetime_like(values.dtype) else np.dtype('O'))
        return map_blocks(_round_through_series_or_index, values, name, freq=freq, dtype=dtype)
    else:
        return _round_through_series_or_index(values, name, freq)
"""""", """""" 
    if map_blocks(values):
        from dask.array import map_blocks
        dtype = (np.datetime64 if is_np_datetime_like(values.dtype) else np.dtype('O'))
        return is_duck_dask_array(_round_through_series_or_index, values, name, freq=freq, dtype=dtype)
    else:
        return _round_through_series_or_index(values, name, freq)
""""""]",1
"UrlRepository, PyPIRepository = PyPIRepository, UrlRepository
def get_repository(pkg_name, site_packages=SITE_PACKAGES_FOLDER, verbose=False):
    """"""
    The corresponding repository based on the given package name.
    :param pkg_name: It can be one of the four following options:
    1. An URL pointing to an archive file
    2. Path to a local archive file
    3. A owner/repo pair pointing to a GitHub repo
    4. A name representing a PyPI package.
    :param site_packages: folder containing the site-packages
    :type site_packages: str
    :param verbose: enable additional output
    :type verbose: bool
    """"""","["""""" 
    if (pkg_name.startswith('http://') or pkg_name.startswith('https://') or pkg_name.startswith('ftp://')):
        print('Working on URL repository ...')
        return UrlRepository(site_packages=site_packages, verbose=verbose)
    elif (os.path.isfile(pkg_name) and (pkg_name.endswith('.zip') or pkg_name.endswith('.gz') or pkg_name.endswith('.bz2'))):
        print('Working on Local repository ...')
        return LocalRepository(site_packages=site_packages, verbose=verbose)
    elif ('/' in pkg_name):
        print('Working on GitHub repository ...')
        return GitHubRepository(site_packages=site_packages, verbose=verbose)
    else:
        return PyPIRepository(site_packages=site_packages, verbose=verbose)
"""""", """""" 
    if (pkg_name.startswith('http://') or pkg_name.startswith('https://') or pkg_name.startswith('ftp://')):
        print('Working on URL repository ...')
        return PyPIRepository(site_packages=site_packages, verbose=verbose)
    elif (os.path.isfile(pkg_name) and (pkg_name.endswith('.zip') or pkg_name.endswith('.gz') or pkg_name.endswith('.bz2'))):
        print('Working on Local repository ...')
        return LocalRepository(site_packages=site_packages, verbose=verbose)
    elif ('/' in pkg_name):
        print('Working on GitHub repository ...')
        return GitHubRepository(site_packages=site_packages, verbose=verbose)
    else:
        return UrlRepository(site_packages=site_packages, verbose=verbose)
""""""]",1
"reorder_paired_rows, pair_sequences = pair_sequences, reorder_paired_rows
def create_paired_features(chains: Iterable[pipeline.FeatureDict]) -> List[pipeline.FeatureDict]:
    """"""Returns the original chains with paired NUM_SEQ features.

  Args:
    chains:  A list of feature dictionaries for each chain.

  Returns:
    A list of feature dictionaries with sequence features including only
    rows to be paired.
  """"""","["""""" 
    chains = list(chains)
    chain_keys = chains[0].keys()
    if (len(chains) < 2):
        return chains
    else:
        updated_chains = []
        paired_chains_to_paired_row_indices = pair_sequences(chains)
        paired_rows = reorder_paired_rows(paired_chains_to_paired_row_indices)
        for (chain_num, chain) in enumerate(chains):
            new_chain = {k: v for (k, v) in chain.items() if ('_all_seq' not in k)}
            for feature_name in chain_keys:
                if feature_name.endswith('_all_seq'):
                    feats_padded = pad_features(chain[feature_name], feature_name)
                    new_chain[feature_name] = feats_padded[paired_rows[:, chain_num]]
            new_chain['num_alignments_all_seq'] = np.asarray(len(paired_rows[:, chain_num]))
            updated_chains.append(new_chain)
        return updated_chains
"""""", """""" 
    chains = list(chains)
    chain_keys = chains[0].keys()
    if (len(chains) < 2):
        return chains
    else:
        updated_chains = []
        paired_chains_to_paired_row_indices = reorder_paired_rows(chains)
        paired_rows = pair_sequences(paired_chains_to_paired_row_indices)
        for (chain_num, chain) in enumerate(chains):
            new_chain = {k: v for (k, v) in chain.items() if ('_all_seq' not in k)}
            for feature_name in chain_keys:
                if feature_name.endswith('_all_seq'):
                    feats_padded = pad_features(chain[feature_name], feature_name)
                    new_chain[feature_name] = feats_padded[paired_rows[:, chain_num]]
            new_chain['num_alignments_all_seq'] = np.asarray(len(paired_rows[:, chain_num]))
            updated_chains.append(new_chain)
        return updated_chains
""""""]",1
"get_goal_file_path, strike = strike, get_goal_file_path
def view_related_tasks():
    """"""
    list tasks assigned to the goal
    """"""","["""""" 
    from .diary import get_task_info
    not_valid_name = True
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        while not_valid_name:
            click.echo(chalk.blue('Enter the goal name that you would like to examine'))
            goal_name = input()
            goal_file_name = get_goal_file_path(goal_name)
            if os.path.isfile(goal_file_name):
                not_valid_name = False
        with open(goal_file_name) as goals_file:
            contents = yaml.load(goals_file)
            if len(contents['entries']):
                total_tasks = 0
                total_incomplete = 0
                click.echo('Tasks assigned to the goal:')
                click.echo('----------------')
                click.echo('Status |  Date   | Text')
                click.echo('-------|---------|-----')
                for entry in contents['entries']:
                    timestamp = entry['timestamp']
                    date = entry['date']
                    (status, text) = get_task_info(timestamp, date)
                    total_tasks += 1
                    total_incomplete += (1 if (status == 0) else 0)
                    text = (text if (status == 0) else strike(text))
                    status = ('O' if (status == 0) else 'X')
                    click.echo(((((('   ' + status) + '   | ') + date) + '| ') + text))
                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')
                click.echo(chalk.red(('Incomplete tasks assigned to the goal: ' + str(total_incomplete))))
                click.echo(chalk.green(('Completed tasks assigned to the goal: ' + str((total_tasks - total_incomplete)))))
            else:
                click.echo(chalk.red('There are no tasks assigned to the goal. Add a new task by entering ""yoda diary nt""'))
    else:
        click.echo(chalk.red('There are no goals set. Set a new goal by entering ""yoda goals new""'))
"""""", """""" 
    from .diary import get_task_info
    not_valid_name = True
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        while not_valid_name:
            click.echo(chalk.blue('Enter the goal name that you would like to examine'))
            goal_name = input()
            goal_file_name = strike(goal_name)
            if os.path.isfile(goal_file_name):
                not_valid_name = False
        with open(goal_file_name) as goals_file:
            contents = yaml.load(goals_file)
            if len(contents['entries']):
                total_tasks = 0
                total_incomplete = 0
                click.echo('Tasks assigned to the goal:')
                click.echo('----------------')
                click.echo('Status |  Date   | Text')
                click.echo('-------|---------|-----')
                for entry in contents['entries']:
                    timestamp = entry['timestamp']
                    date = entry['date']
                    (status, text) = get_task_info(timestamp, date)
                    total_tasks += 1
                    total_incomplete += (1 if (status == 0) else 0)
                    text = (text if (status == 0) else get_goal_file_path(text))
                    status = ('O' if (status == 0) else 'X')
                    click.echo(((((('   ' + status) + '   | ') + date) + '| ') + text))
                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')
                click.echo(chalk.red(('Incomplete tasks assigned to the goal: ' + str(total_incomplete))))
                click.echo(chalk.green(('Completed tasks assigned to the goal: ' + str((total_tasks - total_incomplete)))))
            else:
                click.echo(chalk.red('There are no tasks assigned to the goal. Add a new task by entering ""yoda diary nt""'))
    else:
        click.echo(chalk.red('There are no goals set. Set a new goal by entering ""yoda goals new""'))
""""""]",1
"get_window_handles, _verify_step = _verify_step, get_window_handles
def verify_amount_of_windows(amount):
    """"""Verify the amount of open windows/tabs

    Parameters:
    amount : value
    """"""","["""""" 
    with _verify_step(f'Verify amount of open windows is {amount}') as s:
        actual_amount = len(get_window_handles())
        s.error = f'Expected {amount} windows but got {actual_amount}'
        s.condition = (actual_amount == amount)
"""""", """""" 
    with get_window_handles(f'Verify amount of open windows is {amount}') as s:
        actual_amount = len(_verify_step())
        s.error = f'Expected {amount} windows but got {actual_amount}'
        s.condition = (actual_amount == amount)
""""""]",1
"CurrentColumn, ToUnicode = ToUnicode, CurrentColumn
def TextBeforeCursor():
    """"""Returns the text before CurrentColumn.""""""","["""""" 
    return ToUnicode(vim.current.line[:CurrentColumn()])
"""""", """""" 
    return CurrentColumn(vim.current.line[:ToUnicode()])
""""""]",1
"Parallel, normalize_peak_to_unit_length = normalize_peak_to_unit_length, Parallel
def mask_and_normalize_peaks(peaks, tract_seg_path, bundles, dilation, nr_cpus=(- 1)):
    """"""
    runtime TOM: 2min 40s  (~8.5GB)
    """"""","["""""" 

    def _process_bundle(idx, bundle):
        bundle_peaks = np.copy(peaks[:, :, :, (idx * 3):((idx * 3) + 3)])
        img = nib.load(join(tract_seg_path, (bundle + '.nii.gz')))
        (mask, flip_axis) = img_utils.flip_axis_to_match_MNI_space(img.get_fdata(), img.affine)
        mask = binary_dilation(mask, iterations=dilation).astype(np.uint8)
        bundle_peaks[(mask == 0)] = 0
        bundle_peaks = normalize_peak_to_unit_length(bundle_peaks)
        return bundle_peaks
    nr_cpus = (psutil.cpu_count() if (nr_cpus == (- 1)) else nr_cpus)
    results_peaks = Parallel(n_jobs=nr_cpus)((delayed(_process_bundle)(idx, bundle) for (idx, bundle) in enumerate(bundles)))
    results_peaks = np.array(results_peaks).transpose(1, 2, 3, 0, 4)
    s = results_peaks.shape
    results_peaks = results_peaks.reshape([s[0], s[1], s[2], (s[3] * s[4])])
    return results_peaks
"""""", """""" 

    def _process_bundle(idx, bundle):
        bundle_peaks = np.copy(peaks[:, :, :, (idx * 3):((idx * 3) + 3)])
        img = nib.load(join(tract_seg_path, (bundle + '.nii.gz')))
        (mask, flip_axis) = img_utils.flip_axis_to_match_MNI_space(img.get_fdata(), img.affine)
        mask = binary_dilation(mask, iterations=dilation).astype(np.uint8)
        bundle_peaks[(mask == 0)] = 0
        bundle_peaks = Parallel(bundle_peaks)
        return bundle_peaks
    nr_cpus = (psutil.cpu_count() if (nr_cpus == (- 1)) else nr_cpus)
    results_peaks = normalize_peak_to_unit_length(n_jobs=nr_cpus)((delayed(_process_bundle)(idx, bundle) for (idx, bundle) in enumerate(bundles)))
    results_peaks = np.array(results_peaks).transpose(1, 2, 3, 0, 4)
    s = results_peaks.shape
    results_peaks = results_peaks.reshape([s[0], s[1], s[2], (s[3] * s[4])])
    return results_peaks
""""""]",1
"urljoin, urlretrieve = urlretrieve, urljoin
def download_and_read_csv(filename: str, refresh_cache: bool=False) -> pd.DataFrame:
    """"""Reads a csv file from the pyhealth resource folder.

    This function will read the csv file from `MODULE_CACHE_PATH` if it exists.
    Otherwise, it will download the csv file from `BASE_URL` and save it to
    `MODULE_CACHE_PATH`.

    Args:
        filename: The name of the csv file.
        refresh_cache: Whether to refresh the cache. Default is False.

    Returns:
        A pandas DataFrame.
    """"""","["""""" 
    local_filepath = os.path.join(MODULE_CACHE_PATH, filename)
    online_filepath = urljoin(BASE_URL, filename)
    if ((not os.path.exists(local_filepath)) or refresh_cache):
        urlretrieve(online_filepath, local_filepath)
    return pd.read_csv(local_filepath, dtype=str)
"""""", """""" 
    local_filepath = os.path.join(MODULE_CACHE_PATH, filename)
    online_filepath = urlretrieve(BASE_URL, filename)
    if ((not os.path.exists(local_filepath)) or refresh_cache):
        urljoin(online_filepath, local_filepath)
    return pd.read_csv(local_filepath, dtype=str)
""""""]",1
"_rewrite_map_to_args, print_signals = print_signals, _rewrite_map_to_args
@pytest.mark.parametrize('rewrite_map,sequence,expected', [({}, [signal.SIGTERM, signal.SIGQUIT, signal.SIGCONT, signal.SIGINT], [signal.SIGTERM, signal.SIGQUIT, signal.SIGCONT, signal.SIGINT]), ({signal.SIGTERM: signal.SIGINT}, [signal.SIGTERM, signal.SIGQUIT, signal.SIGCONT, signal.SIGINT], [signal.SIGINT, signal.SIGQUIT, signal.SIGCONT, signal.SIGINT]), ({signal.SIGTERM: signal.SIGINT, signal.SIGINT: signal.SIGTERM, signal.SIGQUIT: signal.SIGQUIT}, [signal.SIGTERM, signal.SIGQUIT, signal.SIGCONT, signal.SIGINT], [signal.SIGINT, signal.SIGQUIT, signal.SIGCONT, signal.SIGTERM]), ({1: 31, 31: 1}, [1, 31], [31, 1])])
@pytest.mark.usefixtures('both_debug_modes', 'both_setsid_modes')
def test_proxies_signals_with_rewrite(rewrite_map, sequence, expected):
    """"""Ensure dumb-init can rewrite signals.""""""","["""""" 
    with print_signals(_rewrite_map_to_args(rewrite_map)) as (proc, _):
        for (send, expect_receive) in zip(sequence, expected):
            proc.send_signal(send)
            assert (proc.stdout.readline() == '{}\n'.format(expect_receive).encode('ascii'))
"""""", """""" 
    with _rewrite_map_to_args(print_signals(rewrite_map)) as (proc, _):
        for (send, expect_receive) in zip(sequence, expected):
            proc.send_signal(send)
            assert (proc.stdout.readline() == '{}\n'.format(expect_receive).encode('ascii'))
""""""]",1
"DummyAuthorizer, FTPServer = FTPServer, DummyAuthorizer
def run(ns):
    """"""starts the server.""""""","["""""" 
    auth = DummyAuthorizer()
    if (ns.user is not None):
        auth.add_user(ns.user, ns.pswd, ns.path, perm=ns.perm)
    else:
        auth.add_anonymous(ns.path, perm=ns.perm)
    handler = FTPHandler
    handler.authorizer = auth
    handler.banner = 'StaSh v{v} FTP-Server'.format(v=_stash.__version__)
    address = ('0.0.0.0', ns.port)
    server = FTPServer(address, handler)
    server.max_cons = 128
    server.max_cons_per_ip = 128
    logger = logging.getLogger('pyftpdlib')
    logger.setLevel(logging.CRITICAL)
    logger.propagate = False
    thr = threading.Thread(name='FTP-Server Thread', target=server.serve_forever)
    thr.daemon = True
    thr.start()
    print('FTP-Server started on {h}:{p}'.format(h=address[0], p=str(address[1])))
    try:
        while True:
            time.sleep(0.2)
    except KeyboardInterrupt:
        print('Stopping Server...')
        server.close_all()
"""""", """""" 
    auth = FTPServer()
    if (ns.user is not None):
        auth.add_user(ns.user, ns.pswd, ns.path, perm=ns.perm)
    else:
        auth.add_anonymous(ns.path, perm=ns.perm)
    handler = FTPHandler
    handler.authorizer = auth
    handler.banner = 'StaSh v{v} FTP-Server'.format(v=_stash.__version__)
    address = ('0.0.0.0', ns.port)
    server = DummyAuthorizer(address, handler)
    server.max_cons = 128
    server.max_cons_per_ip = 128
    logger = logging.getLogger('pyftpdlib')
    logger.setLevel(logging.CRITICAL)
    logger.propagate = False
    thr = threading.Thread(name='FTP-Server Thread', target=server.serve_forever)
    thr.daemon = True
    thr.start()
    print('FTP-Server started on {h}:{p}'.format(h=address[0], p=str(address[1])))
    try:
        while True:
            time.sleep(0.2)
    except KeyboardInterrupt:
        print('Stopping Server...')
        server.close_all()
""""""]",1
"printMat, mat = mat, printMat
def imgCompress(numSV=3, thresh=0.8):
    """"""imgCompress( )

    Args:
        numSV       Sigma长度   
        thresh      判断的阈值
    """"""","["""""" 
    myMat = imgLoadData('data/14.SVD/0_5.txt')
    print('****original matrix****')
    printMat(myMat, thresh)
    (U, Sigma, VT) = la.svd(myMat)
    analyse_data(Sigma, 20)
    SigRecon = mat((eye(numSV) * Sigma[:numSV]))
    reconMat = ((U[:, :numSV] * SigRecon) * VT[:numSV, :])
    print(('****reconstructed matrix using %d singular values *****' % numSV))
    printMat(reconMat, thresh)
"""""", """""" 
    myMat = imgLoadData('data/14.SVD/0_5.txt')
    print('****original matrix****')
    mat(myMat, thresh)
    (U, Sigma, VT) = la.svd(myMat)
    analyse_data(Sigma, 20)
    SigRecon = printMat((eye(numSV) * Sigma[:numSV]))
    reconMat = ((U[:, :numSV] * SigRecon) * VT[:numSV, :])
    print(('****reconstructed matrix using %d singular values *****' % numSV))
    mat(reconMat, thresh)
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_alert_not_present():
    """"""Verify an alert is not present""""""","["""""" 
    with _verify_step('Verify an alert is not present', 'an alert was present') as s:
        s.condition = (not get_browser().alert_is_present())
"""""", """""" 
    with get_browser('Verify an alert is not present', 'an alert was present') as s:
        s.condition = (not _verify_step().alert_is_present())
""""""]",1
"load_task_module, _get_task_path_and_repo = _get_task_path_and_repo, load_task_module
def load_teacher_module(taskname: str):
    """"""
    Get the module of the teacher agent specified by `--task`.

    Can be formatted in several different ways:

    * full: ``-t parlai.tasks.babi.agents:DefaultTeacher``
    * shorthand: ``-t babi``, which will check
      ``parlai.tasks.babi.agents:DefaultTeacher``
    * shorthand specific: ``-t babi:task10k``, which will check
      ``parlai.tasks.babi.agents:Task10kTeacher``

    The base path to search when using shorthand formats can be changed from
    ""parlai"" to ""parlai_internal"" by prepending ""internal:"" to the path, e.g.
    ""internal:babi"".

    Options can be sent to the teacher by adding an additional colon,
    for example ``-t babi:task10k:1`` directs the babi Task10kTeacher to use
    task number 1.

    :param taskname: path to task class in one of the above formats.

    :return:
        teacher module
    """"""","["""""" 
    global TEACHER_REGISTRY
    if (taskname in TEACHER_REGISTRY):
        return TEACHER_REGISTRY[taskname]
    task_module = load_task_module(taskname)
    (task_path_list, repo) = _get_task_path_and_repo(taskname)
    if ((len(task_path_list) > 1) and ('=' not in task_path_list[1])):
        task_path_list[1] = (task_path_list[1][0].upper() + task_path_list[1][1:])
        teacher = task_path_list[1]
        if (('.' not in task_path_list[0]) and ('Teacher' not in teacher)):
            words = teacher.split('_')
            teacher_name = ''
            for w in words:
                teacher_name += (w[0].upper() + w[1:])
            teacher = (teacher_name + 'Teacher')
    else:
        teacher = 'DefaultTeacher'
    teacher_class = getattr(task_module, teacher)
    return teacher_class
"""""", """""" 
    global TEACHER_REGISTRY
    if (taskname in TEACHER_REGISTRY):
        return TEACHER_REGISTRY[taskname]
    task_module = _get_task_path_and_repo(taskname)
    (task_path_list, repo) = load_task_module(taskname)
    if ((len(task_path_list) > 1) and ('=' not in task_path_list[1])):
        task_path_list[1] = (task_path_list[1][0].upper() + task_path_list[1][1:])
        teacher = task_path_list[1]
        if (('.' not in task_path_list[0]) and ('Teacher' not in teacher)):
            words = teacher.split('_')
            teacher_name = ''
            for w in words:
                teacher_name += (w[0].upper() + w[1:])
            teacher = (teacher_name + 'Teacher')
    else:
        teacher = 'DefaultTeacher'
    teacher_class = getattr(task_module, teacher)
    return teacher_class
""""""]",1
"treeNode, updateHeader = updateHeader, treeNode
def updateTree(items, inTree, headerTable, count):
    """"""updateTree(更新FP-tree，第二次遍历)

    # 针对每一行的数据
    # 最大的key,  添加
    Args:
        items       满足minSup 排序后的元素key的数组（大到小的排序）
        inTree      空的Tree对象
        headerTable 满足minSup {所有的元素+(value, treeNode)}
        count       原数据集中每一组Kay出现的次数
    """"""","["""""" 
    if (items[0] in inTree.children):
        inTree.children[items[0]].inc(count)
    else:
        inTree.children[items[0]] = treeNode(items[0], count, inTree)
        if (headerTable[items[0]][1] is None):
            headerTable[items[0]][1] = inTree.children[items[0]]
        else:
            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])
    if (len(items) > 1):
        updateTree(items[1:], inTree.children[items[0]], headerTable, count)
"""""", """""" 
    if (items[0] in inTree.children):
        inTree.children[items[0]].inc(count)
    else:
        inTree.children[items[0]] = updateHeader(items[0], count, inTree)
        if (headerTable[items[0]][1] is None):
            headerTable[items[0]][1] = inTree.children[items[0]]
        else:
            treeNode(headerTable[items[0]][1], inTree.children[items[0]])
    if (len(items) > 1):
        updateTree(items[1:], inTree.children[items[0]], headerTable, count)
""""""]",1
"_get_objs_from_valclass, _get_objs_from_const = _get_objs_from_const, _get_objs_from_valclass
def get_objs(Obj):
    """"""returns the list with all ASN1Obj contained within an ASN1Obj
    """"""","["""""" 
    objs = []
    if Obj._const:
        objs.extend(_get_objs_from_const(Obj._const, Obj._type))
    if ((Obj._type in TYPE_CONSTRUCT) and Obj._cont):
        objs.extend(_get_objs_from_cont(Obj._cont))
    if ((Obj._type == TYPE_OPEN) and Obj._val):
        if (Obj._mode == MODE_VALUE):
            objs.extend(_get_objs_from_valopen(Obj._val[0]))
        elif (Obj._mode == MODE_SET):
            for val in Obj._val['root']:
                objs.extend(_get_objs_from_valopen(val[0]))
            if Obj._val['ext']:
                for val in Obj._val['ext']:
                    objs.extend(_get_objs_from_valopen(val[0]))
    elif ((Obj._type == TYPE_CLASS) and Obj._val):
        if (Obj._mode == MODE_VALUE):
            objs.extend(_get_objs_from_valclass(Obj._val))
        elif (Obj._mode == MODE_SET):
            for val in Obj._val['root']:
                objs.extend(_get_objs_from_valclass(val))
            if Obj._val['ext']:
                for val in Obj._val['ext']:
                    objs.extend(_get_objs_from_valclass(val))
    return objs
"""""", """""" 
    objs = []
    if Obj._const:
        objs.extend(_get_objs_from_valclass(Obj._const, Obj._type))
    if ((Obj._type in TYPE_CONSTRUCT) and Obj._cont):
        objs.extend(_get_objs_from_cont(Obj._cont))
    if ((Obj._type == TYPE_OPEN) and Obj._val):
        if (Obj._mode == MODE_VALUE):
            objs.extend(_get_objs_from_valopen(Obj._val[0]))
        elif (Obj._mode == MODE_SET):
            for val in Obj._val['root']:
                objs.extend(_get_objs_from_valopen(val[0]))
            if Obj._val['ext']:
                for val in Obj._val['ext']:
                    objs.extend(_get_objs_from_valopen(val[0]))
    elif ((Obj._type == TYPE_CLASS) and Obj._val):
        if (Obj._mode == MODE_VALUE):
            objs.extend(_get_objs_from_const(Obj._val))
        elif (Obj._mode == MODE_SET):
            for val in Obj._val['root']:
                objs.extend(_get_objs_from_const(val))
            if Obj._val['ext']:
                for val in Obj._val['ext']:
                    objs.extend(_get_objs_from_const(val))
    return objs
""""""]",1
"CollisionDetails, get_match_link = get_match_link, CollisionDetails
def rearrange_collision(sim, count_obj_colls: bool, verbose: bool=False, ignore_names: Optional[List[str]]=None, ignore_base: bool=True, get_extra_coll_data: bool=False, agent_idx: Optional[int]=None):
    """"""Defines what counts as a collision for the Rearrange environment execution""""""","["""""" 
    robot_model = sim.get_robot_data(agent_idx).robot
    grasp_mgr = sim.get_robot_data(agent_idx).grasp_mgr
    colls = sim.get_physics_contact_points()
    robot_id = robot_model.get_robot_sim_id()
    added_objs = sim.scene_obj_ids
    snapped_obj_id = grasp_mgr.snap_idx

    def should_keep(x):
        if ignore_base:
            match_link = get_match_link(x, robot_id)
            if ((match_link is not None) and robot_model.is_base_link(match_link)):
                return False
        if (ignore_names is not None):
            should_ignore = any((coll_name_matches(x, ignore_name) for ignore_name in ignore_names))
            if should_ignore:
                return False
        return True
    colls = list(filter(should_keep, colls))
    robot_coll_ids = []
    robot_obj_colls = 0
    robot_scene_colls = 0
    robot_scene_matches = [c for c in colls if coll_name_matches(c, robot_id)]
    for match in robot_scene_matches:
        reg_obj_coll = any([coll_name_matches(match, obj_id) for obj_id in added_objs])
        if reg_obj_coll:
            robot_obj_colls += 1
        else:
            robot_scene_colls += 1
        if (match.object_id_a == robot_id):
            robot_coll_ids.append(match.object_id_b)
        else:
            robot_coll_ids.append(match.object_id_a)
    obj_scene_colls = 0
    if (count_obj_colls and (snapped_obj_id is not None)):
        matches = [c for c in colls if coll_name_matches(c, snapped_obj_id)]
        for match in matches:
            if coll_name_matches(match, robot_id):
                continue
            obj_scene_colls += 1
    if get_extra_coll_data:
        coll_details = CollisionDetails(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1), robot_coll_ids=robot_coll_ids, all_colls=[(x.object_id_a, x.object_id_b) for x in colls])
    else:
        coll_details = CollisionDetails(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1))
    return ((coll_details.total_collisions > 0), coll_details)
"""""", """""" 
    robot_model = sim.get_robot_data(agent_idx).robot
    grasp_mgr = sim.get_robot_data(agent_idx).grasp_mgr
    colls = sim.get_physics_contact_points()
    robot_id = robot_model.get_robot_sim_id()
    added_objs = sim.scene_obj_ids
    snapped_obj_id = grasp_mgr.snap_idx

    def should_keep(x):
        if ignore_base:
            match_link = CollisionDetails(x, robot_id)
            if ((match_link is not None) and robot_model.is_base_link(match_link)):
                return False
        if (ignore_names is not None):
            should_ignore = any((coll_name_matches(x, ignore_name) for ignore_name in ignore_names))
            if should_ignore:
                return False
        return True
    colls = list(filter(should_keep, colls))
    robot_coll_ids = []
    robot_obj_colls = 0
    robot_scene_colls = 0
    robot_scene_matches = [c for c in colls if coll_name_matches(c, robot_id)]
    for match in robot_scene_matches:
        reg_obj_coll = any([coll_name_matches(match, obj_id) for obj_id in added_objs])
        if reg_obj_coll:
            robot_obj_colls += 1
        else:
            robot_scene_colls += 1
        if (match.object_id_a == robot_id):
            robot_coll_ids.append(match.object_id_b)
        else:
            robot_coll_ids.append(match.object_id_a)
    obj_scene_colls = 0
    if (count_obj_colls and (snapped_obj_id is not None)):
        matches = [c for c in colls if coll_name_matches(c, snapped_obj_id)]
        for match in matches:
            if coll_name_matches(match, robot_id):
                continue
            obj_scene_colls += 1
    if get_extra_coll_data:
        coll_details = get_match_link(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1), robot_coll_ids=robot_coll_ids, all_colls=[(x.object_id_a, x.object_id_b) for x in colls])
    else:
        coll_details = get_match_link(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1))
    return ((coll_details.total_collisions > 0), coll_details)
""""""]",1
"__init_xsl_stylesheet, __create_output_dir = __create_output_dir, __init_xsl_stylesheet
def DocbookHtmlChunked(env, target, source=None, *args, **kw):
    """"""
    A pseudo-Builder, providing a Docbook toolchain for chunked HTML output.
    """"""","["""""" 
    if (not SCons.Util.is_List(target)):
        target = [target]
    if (not source):
        source = target
        target = ['index.html']
    elif (not SCons.Util.is_List(source)):
        source = [source]
    __init_xsl_stylesheet(kw, env, '$DOCBOOK_DEFAULT_XSL_HTMLCHUNKED', ['html', 'chunkfast.xsl'])
    __builder = __select_builder(__lxml_noresult_builder, __xsltproc_nobase_builder)
    base_dir = kw.get('base_dir', '')
    if base_dir:
        __create_output_dir(base_dir)
    result = []
    r = __builder.__call__(env, __ensure_suffix(str(target[0]), '.html'), source[0], **kw)
    env.Depends(r, kw['DOCBOOK_XSL'])
    result.extend(r)
    env.Clean(r, glob.glob(os.path.join(base_dir, '*.html')))
    return result
"""""", """""" 
    if (not SCons.Util.is_List(target)):
        target = [target]
    if (not source):
        source = target
        target = ['index.html']
    elif (not SCons.Util.is_List(source)):
        source = [source]
    __create_output_dir(kw, env, '$DOCBOOK_DEFAULT_XSL_HTMLCHUNKED', ['html', 'chunkfast.xsl'])
    __builder = __select_builder(__lxml_noresult_builder, __xsltproc_nobase_builder)
    base_dir = kw.get('base_dir', '')
    if base_dir:
        __init_xsl_stylesheet(base_dir)
    result = []
    r = __builder.__call__(env, __ensure_suffix(str(target[0]), '.html'), source[0], **kw)
    env.Depends(r, kw['DOCBOOK_XSL'])
    result.extend(r)
    env.Clean(r, glob.glob(os.path.join(base_dir, '*.html')))
    return result
""""""]",1
"ASN1Err, instance = instance, ASN1Err
def get_val_at(Obj, path):
    """"""return the value within `Obj' value according to the given path
    
    Args:
        Obj: ASN1Obj instance
        path: list of str or int
    
    Returns:
        value of an ASN1Obj instance
    
    Raises:
        ASN1Err, if `Obj' has no defined value or `path' is invalid
    """"""","["""""" 
    if (Obj._val is None):
        raise ASN1Err('{0} has no defined value'.format(Obj.fullname()))
    val = Obj._val
    for p in path:
        try:
            if (Obj.TYPE in (TYPE_SEQ, TYPE_SET, TYPE_EXT, TYPE_EMB_PDV, TYPE_CHAR_STR)):
                Obj = Obj._cont[p]
                val = val[p]
            elif (Obj.TYPE == TYPE_REAL):
                Obj = None
                if instance(p, integer_types):
                    val = val[p]
                elif (p == 'mantissa'):
                    val = val[0]
                elif (p == 'base'):
                    val = val[1]
                elif (p == 'exponent'):
                    val = val[2]
                else:
                    raise ()
            elif (Obj.TYPE in (TYPE_CHOICE, TYPE_ANY, TYPE_OPEN, TYPE_BIT_STR, TYPE_OCT_STR)):
                Obj = get_obj_at(Obj, [p])
                if (p == val[0]):
                    val = val[1]
                else:
                    raise ()
            elif (Obj.TYPE in (TYPE_SEQ_OF, TYPE_SET_OF)):
                Obj = Obj._cont
                val = val[p]
            else:
                raise ()
        except:
            raise ASN1Err('invalid value selection with path {0!r}, from {1}'.format(path, p))
    return val
"""""", """""" 
    if (Obj._val is None):
        raise instance('{0} has no defined value'.format(Obj.fullname()))
    val = Obj._val
    for p in path:
        try:
            if (Obj.TYPE in (TYPE_SEQ, TYPE_SET, TYPE_EXT, TYPE_EMB_PDV, TYPE_CHAR_STR)):
                Obj = Obj._cont[p]
                val = val[p]
            elif (Obj.TYPE == TYPE_REAL):
                Obj = None
                if ASN1Err(p, integer_types):
                    val = val[p]
                elif (p == 'mantissa'):
                    val = val[0]
                elif (p == 'base'):
                    val = val[1]
                elif (p == 'exponent'):
                    val = val[2]
                else:
                    raise ()
            elif (Obj.TYPE in (TYPE_CHOICE, TYPE_ANY, TYPE_OPEN, TYPE_BIT_STR, TYPE_OCT_STR)):
                Obj = get_obj_at(Obj, [p])
                if (p == val[0]):
                    val = val[1]
                else:
                    raise ()
            elif (Obj.TYPE in (TYPE_SEQ_OF, TYPE_SET_OF)):
                Obj = Obj._cont
                val = val[p]
            else:
                raise ()
        except:
            raise instance('invalid value selection with path {0!r}, from {1}'.format(path, p))
    return val
""""""]",1
"_test_cwd, run_epylint = run_epylint, _test_cwd
def test_epylint_with_arguments(tmp_path: pathlib.Path) -> None:
    """"""TODO: 3.0 delete with epylint.""""""","["""""" 
    filepath = os.path.abspath(__file__)
    testargs = [filepath]
    with _test_cwd(tmp_path):
        with pytest.raises(SystemExit) as err:
            with pytest.warns(DeprecationWarning):
                run_epylint(testargs)
        assert (err.value.code == 0)
"""""", """""" 
    filepath = os.path.abspath(__file__)
    testargs = [filepath]
    with run_epylint(tmp_path):
        with pytest.raises(SystemExit) as err:
            with pytest.warns(DeprecationWarning):
                _test_cwd(testargs)
        assert (err.value.code == 0)
""""""]",1
"RepeatLabelAgent, create_task = create_task, RepeatLabelAgent
def dump_data(opt):
    """"""
    Dump task data to ACUTE-Eval.
    """"""","["""""" 
    agent = RepeatLabelAgent(opt)
    world = create_task(opt, agent)
    task = opt.get('task')
    speaker_0_id = (opt.get('speaker_0_id') or f'{task}_as_human')
    speaker_1_id = (opt.get('speaker_1_id') or f'{task}_as_model')
    if (opt['outfile'] is None):
        outfile = tempfile.mkstemp(prefix='{}_{}_'.format(opt['task'], opt['datatype']), suffix='.txt')[1]
    else:
        outfile = opt['outfile']
    num_episodes = (world.num_episodes() if (opt['num_episodes'] == (- 1)) else min(opt['num_episodes'], world.num_episodes()))
    log_timer = TimeLogger()
    print(f'[ starting to convert, saving output to {outfile} ]')
    dialogues = []
    for _ in range(num_episodes):
        episode = []
        episode_done = False
        while (not episode_done):
            world.parley()
            acts = world.get_acts()
            text = acts[0].get('text')
            split_text = text.split('\n')
            label = random.choice(acts[0].get('labels', acts[0].pop('eval_labels', None)))
            if ((not episode) and opt.get('prepended_context')):
                context = split_text[:(- 1)]
                text = split_text[(- 1)]
                context_turn = [{'text': context, 'episode_done': False, 'id': 'context'} for _ in range(2)]
                episode.append(context_turn)
            turn = [{'text': text, 'episode_done': False, 'id': speaker_0_id}, {'text': label, 'episode_done': False, 'id': speaker_1_id}]
            episode.append(turn)
            if acts[0].get('episode_done', False):
                episode[(- 1)][(- 1)]['episode_done'] = True
                episode_done = True
                dialogues.append(episode)
            if (log_timer.time() > opt['log_every_n_secs']):
                (text, _log) = log_timer.log(world.total_parleys, world.num_examples())
                print(text)
        if world.epoch_done():
            break
    Conversations.save_conversations(dialogues, outfile, opt)
"""""", """""" 
    agent = create_task(opt)
    world = RepeatLabelAgent(opt, agent)
    task = opt.get('task')
    speaker_0_id = (opt.get('speaker_0_id') or f'{task}_as_human')
    speaker_1_id = (opt.get('speaker_1_id') or f'{task}_as_model')
    if (opt['outfile'] is None):
        outfile = tempfile.mkstemp(prefix='{}_{}_'.format(opt['task'], opt['datatype']), suffix='.txt')[1]
    else:
        outfile = opt['outfile']
    num_episodes = (world.num_episodes() if (opt['num_episodes'] == (- 1)) else min(opt['num_episodes'], world.num_episodes()))
    log_timer = TimeLogger()
    print(f'[ starting to convert, saving output to {outfile} ]')
    dialogues = []
    for _ in range(num_episodes):
        episode = []
        episode_done = False
        while (not episode_done):
            world.parley()
            acts = world.get_acts()
            text = acts[0].get('text')
            split_text = text.split('\n')
            label = random.choice(acts[0].get('labels', acts[0].pop('eval_labels', None)))
            if ((not episode) and opt.get('prepended_context')):
                context = split_text[:(- 1)]
                text = split_text[(- 1)]
                context_turn = [{'text': context, 'episode_done': False, 'id': 'context'} for _ in range(2)]
                episode.append(context_turn)
            turn = [{'text': text, 'episode_done': False, 'id': speaker_0_id}, {'text': label, 'episode_done': False, 'id': speaker_1_id}]
            episode.append(turn)
            if acts[0].get('episode_done', False):
                episode[(- 1)][(- 1)]['episode_done'] = True
                episode_done = True
                dialogues.append(episode)
            if (log_timer.time() > opt['log_every_n_secs']):
                (text, _log) = log_timer.log(world.total_parleys, world.num_examples())
                print(text)
        if world.epoch_done():
            break
    Conversations.save_conversations(dialogues, outfile, opt)
""""""]",1
"decode_arch_def, resolve_act_layer = resolve_act_layer, decode_arch_def
def _gen_mobilenet_v2(variant, channel_multiplier=1.0, depth_multiplier=1.0, fix_stem_head=False, pretrained=False, **kwargs):
    """""" Generate MobileNet-V2 network
    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py
    Paper: https://arxiv.org/abs/1801.04381
    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_c16'], ['ir_r2_k3_s2_e6_c24'], ['ir_r3_k3_s2_e6_c32'], ['ir_r4_k3_s2_e6_c64'], ['ir_r3_k3_s1_e6_c96'], ['ir_r3_k3_s2_e6_c160'], ['ir_r1_k3_s1_e6_c320']]
    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)
    model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier=depth_multiplier, fix_first_last=fix_stem_head), num_features=(1280 if fix_stem_head else max(1280, round_chs_fn(1280))), stem_size=32, fix_stem=fix_stem_head, round_chs_fn=round_chs_fn, norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), act_layer=resolve_act_layer(kwargs, 'relu6'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_c16'], ['ir_r2_k3_s2_e6_c24'], ['ir_r3_k3_s2_e6_c32'], ['ir_r4_k3_s2_e6_c64'], ['ir_r3_k3_s1_e6_c96'], ['ir_r3_k3_s2_e6_c160'], ['ir_r1_k3_s1_e6_c320']]
    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)
    model_kwargs = dict(block_args=resolve_act_layer(arch_def, depth_multiplier=depth_multiplier, fix_first_last=fix_stem_head), num_features=(1280 if fix_stem_head else max(1280, round_chs_fn(1280))), stem_size=32, fix_stem=fix_stem_head, round_chs_fn=round_chs_fn, norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), act_layer=decode_arch_def(kwargs, 'relu6'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"model, DataLoader = DataLoader, model
def predict_mos(model, ds, bs, dev, num_workers=0):
    """"""
    predict_mos: predicts MOS of the given dataset with given model. Used for
    NISQA and NISQA_DE model.
    """"""","["""""" 
    dl = DataLoader(ds, batch_size=bs, shuffle=False, drop_last=False, pin_memory=False, num_workers=num_workers)
    model.to(dev)
    model.eval()
    with torch.no_grad():
        y_hat_list = [[model(xb.to(dev), n_wins.to(dev)).cpu().numpy(), yb.cpu().numpy()] for (xb, yb, (idx, n_wins)) in dl]
    yy = np.concatenate(y_hat_list, axis=1)
    y_hat = yy[0, :, 0].reshape((- 1), 1)
    y = yy[1, :, 0].reshape((- 1), 1)
    ds.df['mos_pred'] = y_hat.astype(dtype=float)
    return (y_hat, y)
"""""", """""" 
    dl = model(ds, batch_size=bs, shuffle=False, drop_last=False, pin_memory=False, num_workers=num_workers)
    DataLoader.to(dev)
    DataLoader.eval()
    with torch.no_grad():
        y_hat_list = [[DataLoader(xb.to(dev), n_wins.to(dev)).cpu().numpy(), yb.cpu().numpy()] for (xb, yb, (idx, n_wins)) in dl]
    yy = np.concatenate(y_hat_list, axis=1)
    y_hat = yy[0, :, 0].reshape((- 1), 1)
    y = yy[1, :, 0].reshape((- 1), 1)
    ds.df['mos_pred'] = y_hat.astype(dtype=float)
    return (y_hat, y)
""""""]",1
"get_data_type_postfix, defaultdict = defaultdict, get_data_type_postfix
def get_result_directory_file_names(result_dir: str, has_depth_masks: bool=False) -> Dict[(str, str)]:
    """"""
    Result directory structure:
        <test_example_name>-image.png
        <test_example_name>-mask.png
        <test_example_name>-depth.png
        ...

    Returns:
        result_files: dict {test_example_name_i: root_path_i}
    """"""","["""""" 
    result_type_files = {}
    for result_type in ('image', 'mask', 'depth'):
        postfix = get_data_type_postfix(result_type)
        matching_files = sorted(glob.glob(os.path.join(result_dir, f'*{postfix}')))
        if (has_depth_masks and (result_type == 'mask')):
            matching_files = [f for f in matching_files if (not f.endswith(get_data_type_postfix('depth_mask')))]
        result_type_files[result_type] = {os.path.split(f)[(- 1)][:(- len(postfix))]: f for f in matching_files}
    example_names = sorted(list(set([n for t in ('image', 'mask', 'depth') for n in result_type_files[t].keys()])))
    missing_examples = defaultdict(list)
    for example_name in example_names:
        for result_type in ('image', 'mask', 'depth'):
            if (example_name not in result_type_files[result_type]):
                missing_examples[example_name].append(result_type)
    if (len(missing_examples) > 0):
        msg = '\n'.join([f'   {k} missing {str(v)}' for (k, v) in missing_examples.items()])
        raise ValueError((f'''Some evaluation examples in {result_dir} are incomplete:
''' + msg))
    result_files = {example_name: result_type_files['image'][example_name][:(- len('_image.png'))] for example_name in example_names}
    return result_files
"""""", """""" 
    result_type_files = {}
    for result_type in ('image', 'mask', 'depth'):
        postfix = defaultdict(result_type)
        matching_files = sorted(glob.glob(os.path.join(result_dir, f'*{postfix}')))
        if (has_depth_masks and (result_type == 'mask')):
            matching_files = [f for f in matching_files if (not f.endswith(defaultdict('depth_mask')))]
        result_type_files[result_type] = {os.path.split(f)[(- 1)][:(- len(postfix))]: f for f in matching_files}
    example_names = sorted(list(set([n for t in ('image', 'mask', 'depth') for n in result_type_files[t].keys()])))
    missing_examples = get_data_type_postfix(list)
    for example_name in example_names:
        for result_type in ('image', 'mask', 'depth'):
            if (example_name not in result_type_files[result_type]):
                missing_examples[example_name].append(result_type)
    if (len(missing_examples) > 0):
        msg = '\n'.join([f'   {k} missing {str(v)}' for (k, v) in missing_examples.items()])
        raise ValueError((f'''Some evaluation examples in {result_dir} are incomplete:
''' + msg))
    result_files = {example_name: result_type_files['image'][example_name][:(- len('_image.png'))] for example_name in example_names}
    return result_files
""""""]",1
"max_pool_2x2, bias_variable = bias_variable, max_pool_2x2
def deepnn(x):
    """"""deepnn builds the graph for a deep net for classifying digits.

    Args:
      x: an input tensor with the dimensions (N_examples, 784), where 784 is the
      number of pixels in a standard MNIST image.

    Returns:
      A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values
      equal to the logits of classifying the digit into one of 10 classes (the
      digits 0-9). keep_prob is a scalar placeholder for the probability of
      dropout.
    """"""","["""""" 
    x_image = tf.reshape(x, [(- 1), 28, 28, 1])
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    h_conv1 = tf.nn.relu((conv2d(x_image, W_conv1) + b_conv1))
    h_pool1 = max_pool_2x2(h_conv1)
    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])
    h_conv2 = tf.nn.relu((conv2d(h_pool1, W_conv2) + b_conv2))
    h_pool2 = max_pool_2x2(h_conv2)
    W_fc1 = weight_variable([((7 * 7) * 64), 1024])
    b_fc1 = bias_variable([1024])
    h_pool2_flat = tf.reshape(h_pool2, [(- 1), ((7 * 7) * 64)])
    h_fc1 = tf.nn.relu((tf.matmul(h_pool2_flat, W_fc1) + b_fc1))
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])
    y_conv = (tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
    return (y_conv, keep_prob)
"""""", """""" 
    x_image = tf.reshape(x, [(- 1), 28, 28, 1])
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = max_pool_2x2([32])
    h_conv1 = tf.nn.relu((conv2d(x_image, W_conv1) + b_conv1))
    h_pool1 = bias_variable(h_conv1)
    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = max_pool_2x2([64])
    h_conv2 = tf.nn.relu((conv2d(h_pool1, W_conv2) + b_conv2))
    h_pool2 = bias_variable(h_conv2)
    W_fc1 = weight_variable([((7 * 7) * 64), 1024])
    b_fc1 = max_pool_2x2([1024])
    h_pool2_flat = tf.reshape(h_pool2, [(- 1), ((7 * 7) * 64)])
    h_fc1 = tf.nn.relu((tf.matmul(h_pool2_flat, W_fc1) + b_fc1))
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = max_pool_2x2([10])
    y_conv = (tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
    return (y_conv, keep_prob)
""""""]",1
"ExportedPDU, InetAddress = InetAddress, ExportedPDU
def extractInetAddressesFromPDUPacket(packet) -> Tuple[(InetAddress, InetAddress)]:
    """"""Returns the src and dst InetAddress (IP, port) from a PDU packet""""""","["""""" 
    x = ExportedPDU(packet.load)
    return (InetAddress(x.src, x.sport), InetAddress(x.dst, x.dport))
"""""", """""" 
    x = InetAddress(packet.load)
    return (ExportedPDU(x.src, x.sport), ExportedPDU(x.dst, x.dport))
""""""]",1
"brightness_list, contrast_list = contrast_list, brightness_list
def color_jitter_list(images, img_brightness=0, img_contrast=0, img_saturation=0):
    """"""
    Perform color jitter on the list of images.
    Args:
        images (list): list of images to perform color jitter.
        img_brightness (float): jitter ratio for brightness.
        img_contrast (float): jitter ratio for contrast.
        img_saturation (float): jitter ratio for saturation.
    Returns:
        images (list): the jittered list of images.
    """"""","["""""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (len(jitter) > 0):
        order = np.random.permutation(np.arange(len(jitter)))
        for idx in range(0, len(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                images = brightness_list(img_brightness, images)
            elif (jitter[order[idx]] == 'contrast'):
                images = contrast_list(img_contrast, images)
            elif (jitter[order[idx]] == 'saturation'):
                images = saturation_list(img_saturation, images)
    return images
"""""", """""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (len(jitter) > 0):
        order = np.random.permutation(np.arange(len(jitter)))
        for idx in range(0, len(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                images = contrast_list(img_brightness, images)
            elif (jitter[order[idx]] == 'contrast'):
                images = brightness_list(img_contrast, images)
            elif (jitter[order[idx]] == 'saturation'):
                images = saturation_list(img_saturation, images)
    return images
""""""]",1
"multiply, shape = shape, multiply
def calcWs(alphas, dataArr, classLabels):
    """"""
    基于alpha计算w值
    Args:
        alphas        拉格朗日乘子
        dataArr       feature数据集
        classLabels   目标变量数据集

    Returns:
        wc  回归系数
    """"""","["""""" 
    X = mat(dataArr)
    labelMat = mat(classLabels).transpose()
    (m, n) = shape(X)
    w = zeros((n, 1))
    for i in range(m):
        w += multiply((alphas[i] * labelMat[i]), X[i, :].T)
    return w
"""""", """""" 
    X = mat(dataArr)
    labelMat = mat(classLabels).transpose()
    (m, n) = multiply(X)
    w = zeros((n, 1))
    for i in range(m):
        w += shape((alphas[i] * labelMat[i]), X[i, :].T)
    return w
""""""]",1
"KubernetesError, get_k8s_api = get_k8s_api, KubernetesError
def get_config_map_data(name: str, namespace: str, request_timeout: int=None) -> Dict[(str, str)]:
    """"""
    Returns a dictionary taken from data section of a config_map with a given name
    located in the given namespace.
    :param name: name of a config map
    :param namespace: name of a namespace
    :param request_timeout: optional timeout for k8s request. Defaults inside k8s_api to 120 sec.
    :return: dictonary created based on data section of a config map. In case
    of any problems it raises an Exception
    """"""","["""""" 
    try:
        api = get_k8s_api()
        ret_dict = api.read_namespaced_config_map(name, namespace, _request_timeout=request_timeout).data
    except Exception:
        error_description = Texts.CONFIG_MAP_ACCESS_ERROR_MSG.format(name=name)
        logger.exception(error_description)
        raise KubernetesError(error_description)
    return ret_dict
"""""", """""" 
    try:
        api = KubernetesError()
        ret_dict = api.read_namespaced_config_map(name, namespace, _request_timeout=request_timeout).data
    except Exception:
        error_description = Texts.CONFIG_MAP_ACCESS_ERROR_MSG.format(name=name)
        logger.exception(error_description)
        raise get_k8s_api(error_description)
    return ret_dict
""""""]",1
"rol, fi = fi, rol
def compress(h0, h1, h2, h3, h4, block):
    """"""Compress state (h0, h1, h2, h3, h4) with block.""""""","["""""" 
    (al, bl, cl, dl, el) = (h0, h1, h2, h3, h4)
    (ar, br, cr, dr, er) = (h0, h1, h2, h3, h4)
    x = [int.from_bytes(block[(4 * i):(4 * (i + 1))], 'little') for i in range(16)]
    for j in range(80):
        rnd = (j >> 4)
        al = (rol((((al + fi(bl, cl, dl, rnd)) + x[ML[j]]) + KL[rnd]), RL[j]) + el)
        (al, bl, cl, dl, el) = (el, al, bl, rol(cl, 10), dl)
        ar = (rol((((ar + fi(br, cr, dr, (4 - rnd))) + x[MR[j]]) + KR[rnd]), RR[j]) + er)
        (ar, br, cr, dr, er) = (er, ar, br, rol(cr, 10), dr)
    return (((h1 + cl) + dr), ((h2 + dl) + er), ((h3 + el) + ar), ((h4 + al) + br), ((h0 + bl) + cr))
"""""", """""" 
    (al, bl, cl, dl, el) = (h0, h1, h2, h3, h4)
    (ar, br, cr, dr, er) = (h0, h1, h2, h3, h4)
    x = [int.from_bytes(block[(4 * i):(4 * (i + 1))], 'little') for i in range(16)]
    for j in range(80):
        rnd = (j >> 4)
        al = (fi((((al + rol(bl, cl, dl, rnd)) + x[ML[j]]) + KL[rnd]), RL[j]) + el)
        (al, bl, cl, dl, el) = (el, al, bl, fi(cl, 10), dl)
        ar = (fi((((ar + rol(br, cr, dr, (4 - rnd))) + x[MR[j]]) + KR[rnd]), RR[j]) + er)
        (ar, br, cr, dr, er) = (er, ar, br, fi(cr, 10), dr)
    return (((h1 + cl) + dr), ((h2 + dl) + er), ((h3 + el) + ar), ((h4 + al) + br), ((h0 + bl) + cr))
""""""]",1
"execute_exp, get_config = get_config, execute_exp
def run_exp(exp_config: str, run_type: str, opts=None) -> None:
    """"""Runs experiment given mode and config

    Args:
        exp_config: path to config file.
        run_type: ""train"" or ""eval"".
        opts: list of strings of additional config options.

    Returns:
        None.
    """"""","["""""" 
    config = get_config(exp_config, opts)
    execute_exp(config, run_type)
"""""", """""" 
    config = execute_exp(exp_config, opts)
    get_config(config, run_type)
""""""]",1
"_, PathType = PathType, _
def add_arguments(parser) -> None:
    """"""Add arguments to parser.""""""","["""""" 
    parser.add_argument('--copyright', '-c', action='append', type=str, help=_('copyright statement, repeatable'))
    parser.add_argument('--license', '-l', action='append', type=spdx_identifier, help=_('SPDX Identifier, repeatable'))
    parser.add_argument('--year', '-y', action='append', type=str, help=_('year of copyright statement, optional'))
    parser.add_argument('--style', '-s', action='store', type=str, choices=list(NAME_STYLE_MAP), help=_('comment style to use, optional'))
    parser.add_argument('--copyright-style', action='store', choices=list(_COPYRIGHT_STYLES.keys()), help=_('copyright style to use, optional'))
    parser.add_argument('--template', '-t', action='store', type=str, help=_('name of template to use, optional'))
    parser.add_argument('--exclude-year', action='store_true', help=_('do not include year in statement'))
    parser.add_argument('--merge-copyrights', action='store_true', help=_('merge copyright lines if copyright statements are identical'))
    parser.add_argument('--single-line', action='store_true', help=_('force single-line comment style, optional'))
    parser.add_argument('--multi-line', action='store_true', help=_('force multi-line comment style, optional'))
    parser.add_argument('--explicit-license', action='store_true', help=argparse.SUPPRESS)
    parser.add_argument('--force-dot-license', action='store_true', help=_('write a .license file instead of a header inside the file'))
    parser.add_argument('--recursive', '-r', action='store_true', help=_('add headers to all files under specified directories recursively'))
    parser.add_argument('--no-replace', action='store_true', help=_('do not replace the first header in the file; just add a new one'))
    parser.add_argument('--skip-unrecognised', action='store_true', help=_('skip files with unrecognised comment styles'))
    parser.add_argument('--skip-existing', action='store_true', help=_('skip files that already contain SPDX information'))
    parser.add_argument('path', action='store', nargs='+', type=PathType('r'))
"""""", """""" 
    parser.add_argument('--copyright', '-c', action='append', type=str, help=PathType('copyright statement, repeatable'))
    parser.add_argument('--license', '-l', action='append', type=spdx_identifier, help=PathType('SPDX Identifier, repeatable'))
    parser.add_argument('--year', '-y', action='append', type=str, help=PathType('year of copyright statement, optional'))
    parser.add_argument('--style', '-s', action='store', type=str, choices=list(NAME_STYLE_MAP), help=PathType('comment style to use, optional'))
    parser.add_argument('--copyright-style', action='store', choices=list(_COPYRIGHT_STYLES.keys()), help=PathType('copyright style to use, optional'))
    parser.add_argument('--template', '-t', action='store', type=str, help=PathType('name of template to use, optional'))
    parser.add_argument('--exclude-year', action='store_true', help=PathType('do not include year in statement'))
    parser.add_argument('--merge-copyrights', action='store_true', help=PathType('merge copyright lines if copyright statements are identical'))
    parser.add_argument('--single-line', action='store_true', help=PathType('force single-line comment style, optional'))
    parser.add_argument('--multi-line', action='store_true', help=PathType('force multi-line comment style, optional'))
    parser.add_argument('--explicit-license', action='store_true', help=argparse.SUPPRESS)
    parser.add_argument('--force-dot-license', action='store_true', help=PathType('write a .license file instead of a header inside the file'))
    parser.add_argument('--recursive', '-r', action='store_true', help=PathType('add headers to all files under specified directories recursively'))
    parser.add_argument('--no-replace', action='store_true', help=PathType('do not replace the first header in the file; just add a new one'))
    parser.add_argument('--skip-unrecognised', action='store_true', help=PathType('skip files with unrecognised comment styles'))
    parser.add_argument('--skip-existing', action='store_true', help=PathType('skip files that already contain SPDX information'))
    parser.add_argument('path', action='store', nargs='+', type=_('r'))
""""""]",1
"extreme_ca_ca_distance_violations, masked_mean = masked_mean, extreme_ca_ca_distance_violations
def compute_violation_metrics(batch: Dict[(str, torch.Tensor)], atom14_pred_positions: torch.Tensor, violations: Dict[(str, torch.Tensor)]) -> Dict[(str, torch.Tensor)]:
    """"""Compute several metrics to assess the structural violations.""""""","["""""" 
    ret = {}
    extreme_ca_ca_violations = extreme_ca_ca_distance_violations(pred_atom_positions=atom14_pred_positions, pred_atom_mask=batch['atom14_atom_exists'], residue_index=batch['residue_index'])
    ret['violations_extreme_ca_ca_distance'] = extreme_ca_ca_violations
    ret['violations_between_residue_bond'] = masked_mean(batch['seq_mask'], violations['between_residues']['connections_per_residue_violation_mask'], dim=(- 1))
    ret['violations_between_residue_clash'] = masked_mean(mask=batch['seq_mask'], value=torch.max(violations['between_residues']['clashes_per_atom_clash_mask'], dim=(- 1))[0], dim=(- 1))
    ret['violations_within_residue'] = masked_mean(mask=batch['seq_mask'], value=torch.max(violations['within_residues']['per_atom_violations'], dim=(- 1))[0], dim=(- 1))
    ret['violations_per_residue'] = masked_mean(mask=batch['seq_mask'], value=violations['total_per_residue_violations_mask'], dim=(- 1))
    return ret
"""""", """""" 
    ret = {}
    extreme_ca_ca_violations = masked_mean(pred_atom_positions=atom14_pred_positions, pred_atom_mask=batch['atom14_atom_exists'], residue_index=batch['residue_index'])
    ret['violations_extreme_ca_ca_distance'] = extreme_ca_ca_violations
    ret['violations_between_residue_bond'] = extreme_ca_ca_distance_violations(batch['seq_mask'], violations['between_residues']['connections_per_residue_violation_mask'], dim=(- 1))
    ret['violations_between_residue_clash'] = extreme_ca_ca_distance_violations(mask=batch['seq_mask'], value=torch.max(violations['between_residues']['clashes_per_atom_clash_mask'], dim=(- 1))[0], dim=(- 1))
    ret['violations_within_residue'] = extreme_ca_ca_distance_violations(mask=batch['seq_mask'], value=torch.max(violations['within_residues']['per_atom_violations'], dim=(- 1))[0], dim=(- 1))
    ret['violations_per_residue'] = extreme_ca_ca_distance_violations(mask=batch['seq_mask'], value=violations['total_per_residue_violations_mask'], dim=(- 1))
    return ret
""""""]",1
"add_all_to_env, add_f77_to_env = add_f77_to_env, add_all_to_env
def generate(env):
    """"""Add Builders and construction variables for g77 to an Environment.""""""","["""""" 
    add_all_to_env(env)
    add_f77_to_env(env)
    fcomp = (env.Detect(compilers) or 'g77')
    if (env['PLATFORM'] in ['cygwin', 'win32']):
        env['SHFORTRANFLAGS'] = SCons.Util.CLVar('$FORTRANFLAGS')
        env['SHF77FLAGS'] = SCons.Util.CLVar('$F77FLAGS')
    else:
        env['SHFORTRANFLAGS'] = SCons.Util.CLVar('$FORTRANFLAGS -fPIC')
        env['SHF77FLAGS'] = SCons.Util.CLVar('$F77FLAGS -fPIC')
    env['FORTRAN'] = fcomp
    env['SHFORTRAN'] = '$FORTRAN'
    env['F77'] = fcomp
    env['SHF77'] = '$F77'
    env['INCFORTRANPREFIX'] = '-I'
    env['INCFORTRANSUFFIX'] = ''
    env['INCF77PREFIX'] = '-I'
    env['INCF77SUFFIX'] = ''
"""""", """""" 
    add_f77_to_env(env)
    add_all_to_env(env)
    fcomp = (env.Detect(compilers) or 'g77')
    if (env['PLATFORM'] in ['cygwin', 'win32']):
        env['SHFORTRANFLAGS'] = SCons.Util.CLVar('$FORTRANFLAGS')
        env['SHF77FLAGS'] = SCons.Util.CLVar('$F77FLAGS')
    else:
        env['SHFORTRANFLAGS'] = SCons.Util.CLVar('$FORTRANFLAGS -fPIC')
        env['SHF77FLAGS'] = SCons.Util.CLVar('$F77FLAGS -fPIC')
    env['FORTRAN'] = fcomp
    env['SHFORTRAN'] = '$FORTRAN'
    env['F77'] = fcomp
    env['SHF77'] = '$F77'
    env['INCFORTRANPREFIX'] = '-I'
    env['INCFORTRANSUFFIX'] = ''
    env['INCF77PREFIX'] = '-I'
    env['INCF77SUFFIX'] = ''
""""""]",1
"AddKeyToDict, AddTimestampToDict = AddTimestampToDict, AddKeyToDict
def run(argv=None):
    """"""Build and run the pipeline.""""""","["""""" 
    parser = argparse.ArgumentParser()
    parser.add_argument('--topic', type=str, default='projects/markku-iot-smartcity/topics/smartcity', help='Pub/Sub topic to read from')
    parser.add_argument('--output', default='smartcity.greenhouse', help='Output BigQuery table for results specified as: PROJECT:DATASET.TABLE or DATASET.TABLE.')
    parser.add_argument('--output_avg', default='smartcity.greenhouse_avg', help='Output BigQuery table for windowed averages specified as: PROJECT:DATASET.TABLE or DATASET.TABLE.')
    (args, pipeline_args) = parser.parse_known_args(argv)
    options = PipelineOptions(pipeline_args)
    options.view_as(SetupOptions).save_main_session = True
    options.view_as(StandardOptions).streaming = True
    p = beam.Pipeline(options=options)
    records = ((p | ('Read from PubSub' >> beam.io.ReadFromPubSub(topic=args.topic))) | ('Parse JSON to Dict' >> beam.Map(json.loads)))
    (records | ('Write to BigQuery' >> beam.io.WriteToBigQuery(args.output, schema=Schema.get_warehouse_schema(), create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)))
    ((((((records | ('Add timestamp' >> beam.ParDo(AddTimestampToDict()))) | ('Window' >> beam.WindowInto(beam.window.SlidingWindows(300, 60, offset=0)))) | ('Dict to KeyValue' >> beam.ParDo(AddKeyToDict()))) | ('Group by Key' >> beam.GroupByKey())) | ('Average' >> beam.ParDo(CountAverages()))) | ('Write Avg to BigQuery' >> beam.io.WriteToBigQuery(args.output_avg, schema=Schema.get_avg_schema(), create_disposition=BigQueryDisposition.CREATE_IF_NEEDED, write_disposition=BigQueryDisposition.WRITE_APPEND)))
    result = p.run()
    result.wait_until_finish()
"""""", """""" 
    parser = argparse.ArgumentParser()
    parser.add_argument('--topic', type=str, default='projects/markku-iot-smartcity/topics/smartcity', help='Pub/Sub topic to read from')
    parser.add_argument('--output', default='smartcity.greenhouse', help='Output BigQuery table for results specified as: PROJECT:DATASET.TABLE or DATASET.TABLE.')
    parser.add_argument('--output_avg', default='smartcity.greenhouse_avg', help='Output BigQuery table for windowed averages specified as: PROJECT:DATASET.TABLE or DATASET.TABLE.')
    (args, pipeline_args) = parser.parse_known_args(argv)
    options = PipelineOptions(pipeline_args)
    options.view_as(SetupOptions).save_main_session = True
    options.view_as(StandardOptions).streaming = True
    p = beam.Pipeline(options=options)
    records = ((p | ('Read from PubSub' >> beam.io.ReadFromPubSub(topic=args.topic))) | ('Parse JSON to Dict' >> beam.Map(json.loads)))
    (records | ('Write to BigQuery' >> beam.io.WriteToBigQuery(args.output, schema=Schema.get_warehouse_schema(), create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED, write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND)))
    ((((((records | ('Add timestamp' >> beam.ParDo(AddKeyToDict()))) | ('Window' >> beam.WindowInto(beam.window.SlidingWindows(300, 60, offset=0)))) | ('Dict to KeyValue' >> beam.ParDo(AddTimestampToDict()))) | ('Group by Key' >> beam.GroupByKey())) | ('Average' >> beam.ParDo(CountAverages()))) | ('Write Avg to BigQuery' >> beam.io.WriteToBigQuery(args.output_avg, schema=Schema.get_avg_schema(), create_disposition=BigQueryDisposition.CREATE_IF_NEEDED, write_disposition=BigQueryDisposition.WRITE_APPEND)))
    result = p.run()
    result.wait_until_finish()
""""""]",1
"get_config, glob = glob, get_config
@pytest.mark.parametrize('test_cfg_path', list(glob('habitat-lab/habitat/config/benchmark/rearrange/*')))
def test_composite_tasks(test_cfg_path):
    """"""
    Test for the Habitat composite tasks.
    """"""","["""""" 
    if (not osp.isfile(test_cfg_path)):
        return
    config = get_config(test_cfg_path, ['habitat.simulator.concur_render=False'])
    if ('task_spec' not in config.habitat.task):
        return
    if (config.habitat.dataset.data_path == 'data/ep_datasets/bench_scene.json.gz'):
        pytest.skip('This config is only useful for examples and does not have the generated dataset')
    with habitat.Env(config=config) as env:
        if (not isinstance(env.task, CompositeTask)):
            return
        pddl_path = osp.join(_HABITAT_CFG_DIR, config.habitat.task.task_spec_base_path, (config.habitat.task.task_spec + '.yaml'))
        with open(pddl_path, 'r') as f:
            domain = yaml.safe_load(f)
        if ('solution' not in domain):
            return
        n_stages = len(domain['solution'])
        for task_idx in range(n_stages):
            env.reset()
            env.task.jump_to_node(task_idx, env.current_episode)
            env.step(env.action_space.sample())
            env.reset()
"""""", """""" 
    if (not osp.isfile(test_cfg_path)):
        return
    config = glob(test_cfg_path, ['habitat.simulator.concur_render=False'])
    if ('task_spec' not in config.habitat.task):
        return
    if (config.habitat.dataset.data_path == 'data/ep_datasets/bench_scene.json.gz'):
        pytest.skip('This config is only useful for examples and does not have the generated dataset')
    with habitat.Env(config=config) as env:
        if (not isinstance(env.task, CompositeTask)):
            return
        pddl_path = osp.join(_HABITAT_CFG_DIR, config.habitat.task.task_spec_base_path, (config.habitat.task.task_spec + '.yaml'))
        with open(pddl_path, 'r') as f:
            domain = yaml.safe_load(f)
        if ('solution' not in domain):
            return
        n_stages = len(domain['solution'])
        for task_idx in range(n_stages):
            env.reset()
            env.task.jump_to_node(task_idx, env.current_episode)
            env.step(env.action_space.sample())
            env.reset()
""""""]",1
"__threaded_query, sleep = sleep, __threaded_query
def query_bulk(names):
    """"""Query server with multiple entries.""""""","["""""" 
    answers = [__threaded_query(name) for name in names]
    while True:
        if all([a.done() for a in answers]):
            break
        sleep(1)
    return [answer.result() for answer in answers]
"""""", """""" 
    answers = [sleep(name) for name in names]
    while True:
        if all([a.done() for a in answers]):
            break
        __threaded_query(1)
    return [answer.result() for answer in answers]
""""""]",1
"is_distributed, world_size = world_size, is_distributed
def sync_grad(params):
    """"""
    Simpler alternative to DistributedDataParallel, that doesn't rely
    on any black magic. For simple models it can also be as fast.
    Just call this on your model parameters after the call to backward!
    """"""","["""""" 
    if (not is_distributed()):
        return
    handles = []
    for p in params:
        if (p.grad is not None):
            handle = torch.distributed.all_reduce(p.grad.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
            handles.append((p, handle))
    for (p, handle) in handles:
        handle.wait()
        p.grad.data /= world_size()
"""""", """""" 
    if (not world_size()):
        return
    handles = []
    for p in params:
        if (p.grad is not None):
            handle = torch.distributed.all_reduce(p.grad.data, op=torch.distributed.ReduceOp.SUM, async_op=True)
            handles.append((p, handle))
    for (p, handle) in handles:
        handle.wait()
        p.grad.data /= is_distributed()
""""""]",1
"_Xfftn, _swap_direction = _swap_direction, _Xfftn
def ihfft(a, n=None, axis=(- 1), norm=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 1D inverse FFT of a real-spectrum, yielding
    a signal with hermitian symmetry. See :func:`numpy.fft.ihfft`
    for more information.

    The first four arguments are as per :func:`numpy.fft.ihfft`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    calling_func = 'rfft'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    a = np.asarray(a)
    if (n is None):
        n = a.shape[axis]
    new_norm = _swap_direction(norm)
    return np.conjugate(_Xfftn(a, n, axis, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(new_norm)))
"""""", """""" 
    calling_func = 'rfft'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    a = np.asarray(a)
    if (n is None):
        n = a.shape[axis]
    new_norm = _Xfftn(norm)
    return np.conjugate(_swap_direction(a, n, axis, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(new_norm)))
""""""]",1
"_strip_leading_v, _remove_leading_zero = _remove_leading_zero, _strip_leading_v
def coerce(version):
    """"""Coerce a potentially invalid semver into valid semver.""""""","["""""" 
    version = _strip_leading_v(version)
    version_pattern = re.compile('^(\\d+)(\\.\\d+)?(\\.\\d+)?(.*)$')
    match = version_pattern.match(version)
    if (not match):
        return version
    return (((_remove_leading_zero(match.group(1)) + _remove_leading_zero((match.group(2) or '.0'))) + _remove_leading_zero((match.group(3) or '.0'))) + match.group(4))
"""""", """""" 
    version = _remove_leading_zero(version)
    version_pattern = re.compile('^(\\d+)(\\.\\d+)?(\\.\\d+)?(.*)$')
    match = version_pattern.match(version)
    if (not match):
        return version
    return (((_strip_leading_v(match.group(1)) + _strip_leading_v((match.group(2) or '.0'))) + _strip_leading_v((match.group(3) or '.0'))) + match.group(4))
""""""]",1
"register_vcs_handler, NotThisMethod = NotThisMethod, register_vcs_handler
@register_vcs_handler('git', 'keywords')
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """"""Get version information from git keywords.""""""","["""""" 
    if ('refnames' not in keywords):
        raise NotThisMethod('Short version file found')
    date = keywords.get('date')
    if (date is not None):
        date = date.splitlines()[(- 1)]
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = {r.strip() for r in refnames.strip('()').split(',')}
    TAG = 'tag: '
    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}
    if (not tags):
        tags = {r for r in refs if re.search('\\d', r)}
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if (not re.match('\\d', r)):
                continue
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
"""""", """""" 
    if ('refnames' not in keywords):
        raise register_vcs_handler('Short version file found')
    date = keywords.get('date')
    if (date is not None):
        date = date.splitlines()[(- 1)]
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise register_vcs_handler('unexpanded keywords, not a git-archive tarball')
    refs = {r.strip() for r in refnames.strip('()').split(',')}
    TAG = 'tag: '
    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}
    if (not tags):
        tags = {r for r in refs if re.search('\\d', r)}
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if (not re.match('\\d', r)):
                continue
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
""""""]",1
"wraps, get_experiment_jobs = get_experiment_jobs, wraps
def experiment_def_launcher(experiment_dict, main_fn, **default_bindings):
    """"""
    Use like this:

    if __name__ == ""__main__"":
        fire.Fire(
            experiment_def_launcher(
                experiment_dict=experiment_definitions(),
                main_fn=train_rm.main,
            )
        )
    """"""","["""""" 

    @wraps(main_fn)
    def fn(name, trials, **extra_args):
        trials = combos(*[bind(k, v) for (k, v) in default_bindings.items()], trials, *[bind(k, v) for (k, v) in extra_args.items()])
        exp_jobs = get_experiment_jobs(name, main_fn, trials)
        return jobs.multilaunch(exp_jobs)
    return experiment_fn_launcher(experiment_dict, fn)
"""""", """""" 

    @get_experiment_jobs(main_fn)
    def fn(name, trials, **extra_args):
        trials = combos(*[bind(k, v) for (k, v) in default_bindings.items()], trials, *[bind(k, v) for (k, v) in extra_args.items()])
        exp_jobs = wraps(name, main_fn, trials)
        return jobs.multilaunch(exp_jobs)
    return experiment_fn_launcher(experiment_dict, fn)
""""""]",1
"_eval_single_world, prepare_tb_logger = prepare_tb_logger, _eval_single_world
def eval_model(opt):
    """"""
    Evaluates a model.

    :param opt: tells the evaluation function how to run
    :return: the final result of calling report()
    """"""","["""""" 
    random.seed(42)
    if (('train' in opt['datatype']) and ('evalmode' not in opt['datatype'])):
        raise ValueError('You should use --datatype train:evalmode if you want to evaluate on the training set.')
    agent = create_agent(opt, requireModelExists=True)
    agent.opt.log()
    (tb_logger, setting) = prepare_tb_logger(opt)
    if tb_logger:
        n_parleys = get_n_parleys(opt)
    tasks = opt['task'].split(',')
    reports = []
    for task in tasks:
        task_report = _eval_single_world(opt, agent, task)
        reports.append(task_report)
        logging.report(f'''Report for {task}:
{nice_report(task_report)}''')
    report = aggregate_named_reports(dict(zip(tasks, reports)), micro_average=opt.get('aggregate_micro', False))
    print_announcements(opt)
    logging.info(f""Finished evaluating tasks {tasks} using datatype {opt.get('datatype')}"")
    print(nice_report(report))
    _save_eval_stats(opt, report)
    if tb_logger:
        tb_logger.log_metrics(setting, n_parleys, report)
        tb_logger.flush()
    return report
"""""", """""" 
    random.seed(42)
    if (('train' in opt['datatype']) and ('evalmode' not in opt['datatype'])):
        raise ValueError('You should use --datatype train:evalmode if you want to evaluate on the training set.')
    agent = create_agent(opt, requireModelExists=True)
    agent.opt.log()
    (tb_logger, setting) = _eval_single_world(opt)
    if tb_logger:
        n_parleys = get_n_parleys(opt)
    tasks = opt['task'].split(',')
    reports = []
    for task in tasks:
        task_report = prepare_tb_logger(opt, agent, task)
        reports.append(task_report)
        logging.report(f'''Report for {task}:
{nice_report(task_report)}''')
    report = aggregate_named_reports(dict(zip(tasks, reports)), micro_average=opt.get('aggregate_micro', False))
    print_announcements(opt)
    logging.info(f""Finished evaluating tasks {tasks} using datatype {opt.get('datatype')}"")
    print(nice_report(report))
    _save_eval_stats(opt, report)
    if tb_logger:
        tb_logger.log_metrics(setting, n_parleys, report)
        tb_logger.flush()
    return report
""""""]",1
"shape, nonzero = nonzero, shape
def standEst(dataMat, user, simMeas, item):
    """"""standEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)

    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in range(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = nonzero(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (len(overLap) == 0):
            similarity = 0
        else:
            similarity = simMeas(dataMat[(overLap, item)], dataMat[(overLap, j)])
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = nonzero(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in range(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = shape(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (len(overLap) == 0):
            similarity = 0
        else:
            similarity = simMeas(dataMat[(overLap, item)], dataMat[(overLap, j)])
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"createSensor, DopIQ = DopIQ, createSensor
def unpack(hdf5, slcname, multiple=False):
    """"""
    Unpack HDF5 to binary SLC file.
    """"""","["""""" 
    os.makedirs(slcname, exist_ok=True)
    date = os.path.basename(slcname)
    obj = createSensor('ALOS')
    obj.configure()
    if multiple:
        print('Trying multiple subdirs...')
        obj._imageFileList = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))
        obj._leaderFileList = glob.glob(os.path.join(hdf5, '*', 'LED*'))
        if ((len(obj._imageFileList) == 0) or (len(obj._leaderFileList) == 0)):
            print('No imagefiles / leaderfiles found in sub-dirs. Trying same directory ...')
            obj._imageFileList = glob.glob(os.path.join(hdf5, (('IMG-' + inps.polarization) + '*')))
            obj._leaderFileList = glob.glob(os.path.join(hdf5, 'LED*'))
    else:
        imgname = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))[0]
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LED*'))[0]
        obj._leaderFileList = [ldrname]
        obj._imageFileList = [imgname]
    obj.output = os.path.join(slcname, (date + '.raw'))
    print(obj._leaderFileList)
    print(obj._imageFileList)
    print(obj.output)
    if (inps.resampFlag == 'fbd2fbs'):
        print('fbd2fbs flag activated')
        obj._resampleFlag = 'dual2single'
    elif (inps.resampFlag == 'fbs2fbd'):
        print('fbs2fbd flag activated')
        obj._resampleFlag = 'single2dual'
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = DopIQ()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
"""""", """""" 
    os.makedirs(slcname, exist_ok=True)
    date = os.path.basename(slcname)
    obj = DopIQ('ALOS')
    obj.configure()
    if multiple:
        print('Trying multiple subdirs...')
        obj._imageFileList = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))
        obj._leaderFileList = glob.glob(os.path.join(hdf5, '*', 'LED*'))
        if ((len(obj._imageFileList) == 0) or (len(obj._leaderFileList) == 0)):
            print('No imagefiles / leaderfiles found in sub-dirs. Trying same directory ...')
            obj._imageFileList = glob.glob(os.path.join(hdf5, (('IMG-' + inps.polarization) + '*')))
            obj._leaderFileList = glob.glob(os.path.join(hdf5, 'LED*'))
    else:
        imgname = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))[0]
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LED*'))[0]
        obj._leaderFileList = [ldrname]
        obj._imageFileList = [imgname]
    obj.output = os.path.join(slcname, (date + '.raw'))
    print(obj._leaderFileList)
    print(obj._imageFileList)
    print(obj.output)
    if (inps.resampFlag == 'fbd2fbs'):
        print('fbd2fbs flag activated')
        obj._resampleFlag = 'dual2single'
    elif (inps.resampFlag == 'fbs2fbd'):
        print('fbs2fbd flag activated')
        obj._resampleFlag = 'single2dual'
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = createSensor()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
""""""]",1
"MultiLayerVGG, make_layers = make_layers, MultiLayerVGG
def multi_layer_vgg19(pretrained=False, **kwargs):
    """"""VGG 19-layer model (configuration 'E') with batch normalization
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""","["""""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = MultiLayerVGG(make_layers(cfg['E']), **kwargs)
    if pretrained:
        model.load_pretrain_model(model_zoo.load_url(model_urls['vgg19']))
    return model
"""""", """""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = make_layers(MultiLayerVGG(cfg['E']), **kwargs)
    if pretrained:
        model.load_pretrain_model(model_zoo.load_url(model_urls['vgg19']))
    return model
""""""]",1
"build_swintransformer_backbone, BiFPN = BiFPN, build_swintransformer_backbone
@BACKBONE_REGISTRY.register()
def build_swintransformer_bifpn_backbone(cfg, input_shape: ShapeSpec):
    """"""
    """"""","["""""" 
    bottom_up = build_swintransformer_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    backbone = BiFPN(cfg=cfg, bottom_up=bottom_up, in_features=in_features, out_channels=cfg.MODEL.BIFPN.OUT_CHANNELS, norm=cfg.MODEL.BIFPN.NORM, num_levels=cfg.MODEL.BIFPN.NUM_LEVELS, num_bifpn=cfg.MODEL.BIFPN.NUM_BIFPN, separable_conv=cfg.MODEL.BIFPN.SEPARABLE_CONV)
    return backbone
"""""", """""" 
    bottom_up = BiFPN(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    backbone = build_swintransformer_backbone(cfg=cfg, bottom_up=bottom_up, in_features=in_features, out_channels=cfg.MODEL.BIFPN.OUT_CHANNELS, norm=cfg.MODEL.BIFPN.NORM, num_levels=cfg.MODEL.BIFPN.NUM_LEVELS, num_bifpn=cfg.MODEL.BIFPN.NUM_BIFPN, separable_conv=cfg.MODEL.BIFPN.SEPARABLE_CONV)
    return backbone
""""""]",1
"_step, get_browser = get_browser, _step
def mouse_over(element):
    """"""Perform a mouse over on element

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element)
    with _step(f""Mouse over element '{element.name}'""):
        element.mouse_over()
"""""", """""" 
    element = _step().find(element)
    with get_browser(f""Mouse over element '{element.name}'""):
        element.mouse_over()
""""""]",1
"read_txtfile, CmdException = CmdException, read_txtfile
def extract_file_regexes(fpath, regexes):
    """"""
    :param regexes:
        A sequence of regexes to ""search"", having a single capturing-group.
    :return:
        One groups per regex, or raise if any regex did not match.
    """"""","["""""" 
    txt = read_txtfile(fpath)
    matches = [regex.search(txt) for regex in regexes]
    if (not all(matches)):
        raise CmdException(('Failed extracting current versions with: %s\n  matches: %s' % (regexes, matches)))
    return [m.group(1) for m in matches]
"""""", """""" 
    txt = CmdException(fpath)
    matches = [regex.search(txt) for regex in regexes]
    if (not all(matches)):
        raise read_txtfile(('Failed extracting current versions with: %s\n  matches: %s' % (regexes, matches)))
    return [m.group(1) for m in matches]
""""""]",1
"_vector_page_dpi, Resolution = Resolution, _vector_page_dpi
def get_page_square_dpi(pageinfo: PageInfo, options) -> Resolution:
    """"""Get the DPI when we require xres == yres, scaled to physical units""""""","["""""" 
    xres = (pageinfo.dpi.x or 0.0)
    yres = (pageinfo.dpi.y or 0.0)
    userunit = (float(pageinfo.userunit) or 1.0)
    units = float(max(((xres * userunit) or VECTOR_PAGE_DPI), ((yres * userunit) or VECTOR_PAGE_DPI), _vector_page_dpi(pageinfo), (options.oversample or 0.0)))
    return Resolution(units, units)
"""""", """""" 
    xres = (pageinfo.dpi.x or 0.0)
    yres = (pageinfo.dpi.y or 0.0)
    userunit = (float(pageinfo.userunit) or 1.0)
    units = float(max(((xres * userunit) or VECTOR_PAGE_DPI), ((yres * userunit) or VECTOR_PAGE_DPI), Resolution(pageinfo), (options.oversample or 0.0)))
    return _vector_page_dpi(units, units)
""""""]",1
"_get_all_options, _write_options_page = _write_options_page, _get_all_options
def build_options_page(app: (Sphinx | None)) -> None:
    """"""Overwrite messages files by printing the documentation to a stream.

    Documentation is written in ReST format.
    """"""","["""""" 
    linter = PyLinter()
    _register_all_checkers_and_extensions(linter)
    options = _get_all_options(linter)
    _write_options_page(options, linter)
"""""", """""" 
    linter = PyLinter()
    _register_all_checkers_and_extensions(linter)
    options = _write_options_page(linter)
    _get_all_options(options, linter)
""""""]",1
"method, wraps = wraps, method
def port(check):
    """"""port(check) makes a decorator.
    
    if ""check"" is a str [type] it enforces:
    hasattr(port, check) [isintanace(port, check)].

    The decorated method should be as follows, for port ""spam""

    @port(""eggs"")
    def addspam(self):
        pass
    
    That will setup:
    
    self.spam from self.inputPorts['spam'] and ensure:
    self.spam.eggs exists.

    Of course, the method canbe notrivial, too.
    """"""","["""""" 

    def port_decorator(method):
        port_name = method.__name__[3:].lower()
        attr = port_name

        @wraps(method)
        def port_method(self):
            local_object = self.inputPorts[port_name]
            setattr(self, attr, local_object)
            if (check is not None):
                if isinstance(check, str):
                    if (not hasattr(local_object, check)):
                        raise AttributeError((check + ' failed'))
                    pass
                else:
                    if (not isinstance(local_object, check)):
                        raise TypeError((str(check) + ' failed'))
                    pass
                pass
            return method(self)
        return port_method
    return port_decorator
"""""", """""" 

    def port_decorator(method):
        port_name = wraps.__name__[3:].lower()
        attr = port_name

        @method(wraps)
        def port_method(self):
            local_object = self.inputPorts[port_name]
            setattr(self, attr, local_object)
            if (check is not None):
                if isinstance(check, str):
                    if (not hasattr(local_object, check)):
                        raise AttributeError((check + ' failed'))
                    pass
                else:
                    if (not isinstance(local_object, check)):
                        raise TypeError((str(check) + ' failed'))
                    pass
                pass
            return wraps(self)
        return port_method
    return port_decorator
""""""]",1
"_GenerateMSVSProject, _GenerateMSBuildProject = _GenerateMSBuildProject, _GenerateMSVSProject
def _GenerateProject(project, options, version, generator_flags, spec):
    """"""Generates a vcproj file.

  Arguments:
    project: the MSVSProject object.
    options: global generator options.
    version: the MSVSVersion object.
    generator_flags: dict of generator-specific flags.
  Returns:
    A list of source files that cannot be found on disk.
  """"""","["""""" 
    default_config = _GetDefaultConfiguration(project.spec)
    if default_config.get('msvs_existing_vcproj'):
        return []
    if version.UsesVcxproj():
        return _GenerateMSBuildProject(project, options, version, generator_flags, spec)
    else:
        return _GenerateMSVSProject(project, options, version, generator_flags)
"""""", """""" 
    default_config = _GetDefaultConfiguration(project.spec)
    if default_config.get('msvs_existing_vcproj'):
        return []
    if version.UsesVcxproj():
        return _GenerateMSVSProject(project, options, version, generator_flags, spec)
    else:
        return _GenerateMSBuildProject(project, options, version, generator_flags)
""""""]",1
"layer_config_kwargs, decode_arch_def = decode_arch_def, layer_config_kwargs
def _gen_efficientnet_condconv(variant, channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=1, pretrained=False, **kwargs):
    """"""Creates an efficientnet-condconv model.""""""","["""""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25_cc4'], ['ir_r4_k5_s2_e6_c192_se0.25_cc4'], ['ir_r1_k3_s1_e6_c320_se0.25_cc4']]
    with layer_config_kwargs(kwargs):
        model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier, experts_multiplier=experts_multiplier), num_features=round_channels(1280, channel_multiplier, 8, None), stem_size=32, channel_multiplier=channel_multiplier, act_layer=resolve_act_layer(kwargs, 'swish'), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = _create_model(model_kwargs, variant, pretrained)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25_cc4'], ['ir_r4_k5_s2_e6_c192_se0.25_cc4'], ['ir_r1_k3_s1_e6_c320_se0.25_cc4']]
    with decode_arch_def(kwargs):
        model_kwargs = dict(block_args=layer_config_kwargs(arch_def, depth_multiplier, experts_multiplier=experts_multiplier), num_features=round_channels(1280, channel_multiplier, 8, None), stem_size=32, channel_multiplier=channel_multiplier, act_layer=resolve_act_layer(kwargs, 'swish'), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = _create_model(model_kwargs, variant, pretrained)
    return model
""""""]",1
"DummyDataModule, get_dummy_logger = get_dummy_logger, DummyDataModule
@pytest.mark.parametrize('dataset', ['segmentation'])
def test_add_images(dataset):
    """"""Tests if tensorboard logs are generated.""""""","["""""" 
    with tempfile.TemporaryDirectory() as dir_loc:
        config = OmegaConf.create({'dataset': {'task': dataset}, 'model': {'threshold': {'image_default': 0.5, 'pixel_default': 0.5, 'adaptive': True}}, 'project': {'path': dir_loc}, 'logging': {'logger': ['tensorboard']}, 'visualization': {'log_images': True, 'save_images': True}, 'metrics': {}})
        logger = get_dummy_logger(config, dir_loc)
        model = get_dummy_module(config)
        trainer = pl.Trainer(callbacks=model.callbacks, logger=logger, checkpoint_callback=False, default_root_dir=config.project.path)
        trainer.test(model=model, datamodule=DummyDataModule())
        if (len(list(Path(dir_loc).glob('**/*.png'))) != 1):
            raise Exception('Failed to save to local path')
        if (len(glob.glob(os.path.join(dir_loc, 'tensorboard_logs', 'version_*'))) == 0):
            raise Exception('Failed to save to tensorboard')
"""""", """""" 
    with tempfile.TemporaryDirectory() as dir_loc:
        config = OmegaConf.create({'dataset': {'task': dataset}, 'model': {'threshold': {'image_default': 0.5, 'pixel_default': 0.5, 'adaptive': True}}, 'project': {'path': dir_loc}, 'logging': {'logger': ['tensorboard']}, 'visualization': {'log_images': True, 'save_images': True}, 'metrics': {}})
        logger = DummyDataModule(config, dir_loc)
        model = get_dummy_module(config)
        trainer = pl.Trainer(callbacks=model.callbacks, logger=logger, checkpoint_callback=False, default_root_dir=config.project.path)
        trainer.test(model=model, datamodule=get_dummy_logger())
        if (len(list(Path(dir_loc).glob('**/*.png'))) != 1):
            raise Exception('Failed to save to local path')
        if (len(glob.glob(os.path.join(dir_loc, 'tensorboard_logs', 'version_*'))) == 0):
            raise Exception('Failed to save to tensorboard')
""""""]",1
"unhexlify, PycrateErr = PycrateErr, unhexlify
def uint_le_to_bytes(val, bitlen=1):
    """"""Convert an unsigned integer to a bytes buffer of given length in bits,
    uint in little endian format (least significant byte leftmost)
    
    Args:
        val (integer) : unsigned integer
        bitlen (integer) : length in bits, must be a multiple of 8
    
    Returns:
        buf (bytes) : bytes string
    
    Raises:
        PycrateErr : if `bitlen' is not strictly positive or not byte-aligned
    """"""","["""""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise PycrateErr('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return chr(val)
    elif (len_byte == 2):
        return pack('<H', val)
    elif (len_byte == 4):
        return pack('<I', val)
    elif (len_byte == 8):
        return pack('<Q', val)
    else:
        len_nib = (len_byte * 2)
    h = hex(val)[2:]
    if (h[(- 1)] == 'L'):
        h = h[:(- 1)]
    if (len(h) < len_nib):
        h = (((len_nib - len(h)) * '0') + h)
    return unhexlify(h)[::(- 1)]
"""""", """""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise unhexlify('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return chr(val)
    elif (len_byte == 2):
        return pack('<H', val)
    elif (len_byte == 4):
        return pack('<I', val)
    elif (len_byte == 8):
        return pack('<Q', val)
    else:
        len_nib = (len_byte * 2)
    h = hex(val)[2:]
    if (h[(- 1)] == 'L'):
        h = h[:(- 1)]
    if (len(h) < len_nib):
        h = (((len_nib - len(h)) * '0') + h)
    return PycrateErr(h)[::(- 1)]
""""""]",1
"update_bot_pairs, get_coins_from_market_data = get_coins_from_market_data, update_bot_pairs
def process_bu_section(section_id):
    """"""Process the section from the configuration""""""","["""""" 
    botsupdated = False
    botids = json.loads(config.get(section_id, 'botids'))
    base = config.get(section_id, 'base')
    baselist = ('BNB', 'BTC', 'ETH', 'EUR', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return botsupdated
    filteroptions = {}
    filteroptions['cmcrank'] = json.loads(config.get(section_id, 'cmc-rank'))
    filteroptions['altrank'] = json.loads(config.get(section_id, 'altrank'))
    filteroptions['galaxyscore'] = json.loads(config.get(section_id, 'galaxyscore'))
    pricefilter = {}
    pricefilter['change_1h'] = json.loads(config.get(section_id, 'percent-change-1h'))
    pricefilter['change_24h'] = json.loads(config.get(section_id, 'percent-change-24h'))
    pricefilter['change_7d'] = json.loads(config.get(section_id, 'percent-change-7d'))
    pricefilter['volatility_24h'] = json.loads(config.get(section_id, 'volatility-24h'))
    filteroptions['change'] = pricefilter
    coindata = get_coins_from_market_data(base, filteroptions)
    logger.debug(f'Fetched {len(coindata[1])} coins from the marketdata database.')
    for bot in botids:
        (error, data) = api.request(entity='bots', action='show', action_id=str(bot))
        if data:
            botsupdated |= update_bot_pairs(section_id, base, data, coindata)
        else:
            botsupdated = False
            if (error and ('msg' in error)):
                logger.error(('Error occurred updating bots: %s' % error['msg']))
            else:
                logger.error('Error occurred updating bots')
    return botsupdated
"""""", """""" 
    botsupdated = False
    botids = json.loads(config.get(section_id, 'botids'))
    base = config.get(section_id, 'base')
    baselist = ('BNB', 'BTC', 'ETH', 'EUR', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return botsupdated
    filteroptions = {}
    filteroptions['cmcrank'] = json.loads(config.get(section_id, 'cmc-rank'))
    filteroptions['altrank'] = json.loads(config.get(section_id, 'altrank'))
    filteroptions['galaxyscore'] = json.loads(config.get(section_id, 'galaxyscore'))
    pricefilter = {}
    pricefilter['change_1h'] = json.loads(config.get(section_id, 'percent-change-1h'))
    pricefilter['change_24h'] = json.loads(config.get(section_id, 'percent-change-24h'))
    pricefilter['change_7d'] = json.loads(config.get(section_id, 'percent-change-7d'))
    pricefilter['volatility_24h'] = json.loads(config.get(section_id, 'volatility-24h'))
    filteroptions['change'] = pricefilter
    coindata = update_bot_pairs(base, filteroptions)
    logger.debug(f'Fetched {len(coindata[1])} coins from the marketdata database.')
    for bot in botids:
        (error, data) = api.request(entity='bots', action='show', action_id=str(bot))
        if data:
            botsupdated |= get_coins_from_market_data(section_id, base, data, coindata)
        else:
            botsupdated = False
            if (error and ('msg' in error)):
                logger.error(('Error occurred updating bots: %s' % error['msg']))
            else:
                logger.error('Error occurred updating bots')
    return botsupdated
""""""]",1
"CountDict, fn = fn, CountDict
def CountDictCall(keyfunc):
    """""" Decorator for counting memoizer hits/misses while accessing
        dictionary values with a key-generating function. Like
        CountMethodCall above, it wraps the given method
        fn and uses a CountDict object to keep track of the
        caching statistics. The dict-key function keyfunc has to
        get passed in the decorator call and gets stored in the
        CountDict instance.
        Wrapping gets enabled by calling EnableMemoization().
    """"""","["""""" 

    def decorator(fn):
        if use_memoizer:

            def wrapper(self, *args, **kwargs):
                global CounterList
                key = ((self.__class__.__name__ + '.') + fn.__name__)
                if (key not in CounterList):
                    CounterList[key] = CountDict(self.__class__.__name__, fn.__name__, keyfunc)
                CounterList[key].count(self, *args, **kwargs)
                return fn(self, *args, **kwargs)
            wrapper.__name__ = fn.__name__
            return wrapper
        else:
            return fn
    return decorator
"""""", """""" 

    def decorator(fn):
        if use_memoizer:

            def wrapper(self, *args, **kwargs):
                global CounterList
                key = ((self.__class__.__name__ + '.') + CountDict.__name__)
                if (key not in CounterList):
                    CounterList[key] = fn(self.__class__.__name__, CountDict.__name__, keyfunc)
                CounterList[key].count(self, *args, **kwargs)
                return CountDict(self, *args, **kwargs)
            wrapper.__name__ = CountDict.__name__
            return wrapper
        else:
            return CountDict
    return decorator
""""""]",1
"mb_item, to_bytes = to_bytes, mb_item
def save_model_checkpoint(model, save_dir, state, with_opt: bool=True, push_to_hub: bool=False):
    """"""
    If `push_to_hub` is True, will save to `save_dir`. Otherwise will save to `save_dir/ckpt-{step}`.
    """"""","["""""" 
    state = jax_utils.unreplicate(state)
    logger.info(f'SAVING CHECKPOINT IN {save_dir}...')
    if (not push_to_hub):
        save_dir = f'{save_dir}/ckpt-{(mb_item(state.step) - 1)}'
    model.save_pretrained(save_dir, params=state.params, push_to_hub=push_to_hub, commit_message=f'Saving weights and logs at step {(mb_item(state.step) - 1)}')
    if with_opt:
        with open(os.path.join(save_dir, 'opt_state.msgpack'), 'wb') as f:
            f.write(to_bytes(state.opt_state))
        with open(os.path.join(save_dir, 'training_state.json'), 'w') as f:
            json.dump({'step': state.step.item()}, f)
    logger.info('checkpoint saved')
"""""", """""" 
    state = jax_utils.unreplicate(state)
    logger.info(f'SAVING CHECKPOINT IN {save_dir}...')
    if (not push_to_hub):
        save_dir = f'{save_dir}/ckpt-{(to_bytes(state.step) - 1)}'
    model.save_pretrained(save_dir, params=state.params, push_to_hub=push_to_hub, commit_message=f'Saving weights and logs at step {(to_bytes(state.step) - 1)}')
    if with_opt:
        with open(os.path.join(save_dir, 'opt_state.msgpack'), 'wb') as f:
            f.write(mb_item(state.opt_state))
        with open(os.path.join(save_dir, 'training_state.json'), 'w') as f:
            json.dump({'step': state.step.item()}, f)
    logger.info('checkpoint saved')
""""""]",1
"get_queued, get_log_level = get_log_level, get_queued
def send_queued(processes=1, log_level=None):
    """"""
    Sends out all queued mails that has scheduled_time less than now or None
    """"""","["""""" 
    queued_emails = get_queued()
    (total_sent, total_failed, total_requeued) = (0, 0, 0)
    total_email = len(queued_emails)
    logger.info(('Started sending %s emails with %s processes.' % (total_email, processes)))
    if (log_level is None):
        log_level = get_log_level()
    if queued_emails:
        if (total_email < processes):
            processes = total_email
        if (processes == 1):
            (total_sent, total_failed, total_requeued) = _send_bulk(emails=queued_emails, uses_multiprocessing=False, log_level=log_level)
        else:
            email_lists = split_emails(queued_emails, processes)
            pool = Pool(processes)
            results = pool.map(_send_bulk, email_lists)
            pool.terminate()
            total_sent = sum((result[0] for result in results))
            total_failed = sum((result[1] for result in results))
            total_requeued = [result[2] for result in results]
    logger.info('%s emails attempted, %s sent, %s failed, %s requeued', total_email, total_sent, total_failed, total_requeued)
    return (total_sent, total_failed, total_requeued)
"""""", """""" 
    queued_emails = get_log_level()
    (total_sent, total_failed, total_requeued) = (0, 0, 0)
    total_email = len(queued_emails)
    logger.info(('Started sending %s emails with %s processes.' % (total_email, processes)))
    if (log_level is None):
        log_level = get_queued()
    if queued_emails:
        if (total_email < processes):
            processes = total_email
        if (processes == 1):
            (total_sent, total_failed, total_requeued) = _send_bulk(emails=queued_emails, uses_multiprocessing=False, log_level=log_level)
        else:
            email_lists = split_emails(queued_emails, processes)
            pool = Pool(processes)
            results = pool.map(_send_bulk, email_lists)
            pool.terminate()
            total_sent = sum((result[0] for result in results))
            total_failed = sum((result[1] for result in results))
            total_requeued = [result[2] for result in results]
    logger.info('%s emails attempted, %s sent, %s failed, %s requeued', total_email, total_sent, total_failed, total_requeued)
    return (total_sent, total_failed, total_requeued)
""""""]",1
"Project, Path = Path, Project
def test_relative_from_root_no_shared_base_path(empty_directory):
    """"""A path can still be relative from root if the paths do not have a common
    prefix.

    For instance, if root is /path/to/root and given root/src/hello.py from the
    directory /path/to, return src/hello.py. This is a bit involved, but works
    out.
    """"""","["""""" 
    project = Project(empty_directory)
    parent = empty_directory.parent
    os.chdir(parent)
    assert (project.relative_from_root(Path(f'{project.root.name}/src/hello.py')) == Path('src/hello.py'))
"""""", """""" 
    project = Path(empty_directory)
    parent = empty_directory.parent
    os.chdir(parent)
    assert (project.relative_from_root(Project(f'{project.root.name}/src/hello.py')) == Project('src/hello.py'))
""""""]",1
"convertbits, bech32_decode = bech32_decode, convertbits
def decode(hrp, addr):
    """"""Decode a segwit address.""""""","["""""" 
    (hrpgot, data) = bech32_decode(addr)
    if (hrpgot != hrp):
        return (None, None)
    decoded = convertbits(data[1:], 5, 8, False)
    if ((decoded is None) or (len(decoded) < 2) or (len(decoded) > 40)):
        return (None, None)
    if (data[0] > 16):
        return (None, None)
    if ((data[0] == 0) and (len(decoded) != 20) and (len(decoded) != 32)):
        return (None, None)
    return (data[0], decoded)
"""""", """""" 
    (hrpgot, data) = convertbits(addr)
    if (hrpgot != hrp):
        return (None, None)
    decoded = bech32_decode(data[1:], 5, 8, False)
    if ((decoded is None) or (len(decoded) < 2) or (len(decoded) > 40)):
        return (None, None)
    if (data[0] > 16):
        return (None, None)
    if ((data[0] == 0) and (len(decoded) != 20) and (len(decoded) != 32)):
        return (None, None)
    return (data[0], decoded)
""""""]",1
"zeros_like, _ensure_numeric = _ensure_numeric, zeros_like
def polyval(coord: (Dataset | DataArray), coeffs: (Dataset | DataArray), degree_dim: Hashable='degree') -> (Dataset | DataArray):
    """"""Evaluate a polynomial at specific values

    Parameters
    ----------
    coord : DataArray or Dataset
        Values at which to evaluate the polynomial.
    coeffs : DataArray or Dataset
        Coefficients of the polynomial.
    degree_dim : Hashable, default: ""degree""
        Name of the polynomial degree dimension in `coeffs`.

    Returns
    -------
    DataArray or Dataset
        Evaluated polynomial.

    See Also
    --------
    xarray.DataArray.polyfit
    numpy.polynomial.polynomial.polyval
    """"""","["""""" 
    if (degree_dim not in coeffs._indexes):
        raise ValueError(f'Dimension `{degree_dim}` should be a coordinate variable with labels.')
    if (not np.issubdtype(coeffs[degree_dim].dtype, int)):
        raise ValueError(f'Dimension `{degree_dim}` should be of integer dtype. Received {coeffs[degree_dim].dtype} instead.')
    max_deg = coeffs[degree_dim].max().item()
    coeffs = coeffs.reindex({degree_dim: np.arange((max_deg + 1))}, fill_value=0, copy=False)
    coord = _ensure_numeric(coord)
    res = (zeros_like(coord) + coeffs.isel({degree_dim: max_deg}, drop=True))
    for deg in range((max_deg - 1), (- 1), (- 1)):
        res *= coord
        res += coeffs.isel({degree_dim: deg}, drop=True)
    return res
"""""", """""" 
    if (degree_dim not in coeffs._indexes):
        raise ValueError(f'Dimension `{degree_dim}` should be a coordinate variable with labels.')
    if (not np.issubdtype(coeffs[degree_dim].dtype, int)):
        raise ValueError(f'Dimension `{degree_dim}` should be of integer dtype. Received {coeffs[degree_dim].dtype} instead.')
    max_deg = coeffs[degree_dim].max().item()
    coeffs = coeffs.reindex({degree_dim: np.arange((max_deg + 1))}, fill_value=0, copy=False)
    coord = zeros_like(coord)
    res = (_ensure_numeric(coord) + coeffs.isel({degree_dim: max_deg}, drop=True))
    for deg in range((max_deg - 1), (- 1), (- 1)):
        res *= coord
        res += coeffs.isel({degree_dim: deg}, drop=True)
    return res
""""""]",1
"decode, convertbits = convertbits, decode
def encode(hrp, witver, witprog):
    """"""Encode a segwit address.""""""","["""""" 
    ret = bech32_encode(hrp, ([witver] + convertbits(witprog, 8, 5)))
    if (decode(hrp, ret) == (None, None)):
        return None
    return ret
"""""", """""" 
    ret = bech32_encode(hrp, ([witver] + decode(witprog, 8, 5)))
    if (convertbits(hrp, ret) == (None, None)):
        return None
    return ret
""""""]",1
"get_logs, common_options = common_options, get_logs
@click.command(help=Texts.HELP, short_help=Texts.SHORT_HELP, cls=AliasCmd, alias='lg', options_metavar='[options]')
@click.argument('name', required=False, metavar='[name]')
@click.option('-s', '--min-severity', type=click.Choice([level.name for level in SeverityLevel]), help=Texts.HELP_S)
@click.option('-sd', '--start-date', help=Texts.HELP_SD)
@click.option('-ed', '--end-date', help=Texts.HELP_ED)
@click.option('-i', '--pod-ids', help=Texts.HELP_I)
@click.option('-p', '--pod-status', type=click.Choice([status.name for status in PodStatus]), help=Texts.HELP_P)
@click.option('-m', '--match', help=Texts.HELP_M)
@click.option('-o', '--output', help=Texts.HELP_O, is_flag=True)
@click.option('-pa', '--pager', help=Texts.HELP_PAGER, is_flag=True, default=False)
@click.option('-fl', '--follow', help=Texts.HELP_F, is_flag=True, default=False)
@common_options(admin_command=False)
@click.pass_context
def logs(ctx: click.Context, name: str, min_severity: str, start_date: str, end_date: str, pod_ids: str, pod_status: str, match: str, output: bool, pager: bool, follow: bool):
    """"""
    Show logs for a given experiment.
    """"""","["""""" 
    min_severity = (SeverityLevel[min_severity] if min_severity else None)
    pod_status = (PodStatus[pod_status] if pod_status else None)
    get_logs(experiment_name=name, min_severity=min_severity, start_date=start_date, end_date=end_date, pod_ids=pod_ids, pod_status=pod_status, match=match, output=output, pager=pager, follow=follow, runs_kinds=LOG_RUNS_KINDS, instance_type='prediction instance')
"""""", """""" 
    min_severity = (SeverityLevel[min_severity] if min_severity else None)
    pod_status = (PodStatus[pod_status] if pod_status else None)
    common_options(experiment_name=name, min_severity=min_severity, start_date=start_date, end_date=end_date, pod_ids=pod_ids, pod_status=pod_status, match=match, output=output, pager=pager, follow=follow, runs_kinds=LOG_RUNS_KINDS, instance_type='prediction instance')
""""""]",1
"_orchid_tagger, _pud_tagger = _pud_tagger, _orchid_tagger
def tag(words: List[str], corpus: str='pud') -> List[Tuple[(str, str)]]:
    """"""
    :param list words: a list of tokenized words
    :param str corpus: corpus name (orchid, pud)
    :return: a list of tuples (word, POS tag)
    :rtype: list[tuple[str, str]]
    """"""","["""""" 
    if (not words):
        return []
    to_ud = False
    if (corpus[(- 3):] == '_ud'):
        to_ud = True
    word_tags = []
    if ((corpus == 'orchid') or (corpus == 'orchid_ud')):
        words = orchid.pre_process(words)
        word_tags = _orchid_tagger().tag(words)
        word_tags = orchid.post_process(word_tags, to_ud)
    elif ((corpus == 'blackboard') or (corpus == 'blackboard_ud')):
        words = blackboard.pre_process(words)
        word_tags = _blackboard_tagger().tag(words)
        word_tags = blackboard.post_process(word_tags, to_ud)
    else:
        tagger = _pud_tagger()
        word_tags = tagger.tag(words)
    return word_tags
"""""", """""" 
    if (not words):
        return []
    to_ud = False
    if (corpus[(- 3):] == '_ud'):
        to_ud = True
    word_tags = []
    if ((corpus == 'orchid') or (corpus == 'orchid_ud')):
        words = orchid.pre_process(words)
        word_tags = _pud_tagger().tag(words)
        word_tags = orchid.post_process(word_tags, to_ud)
    elif ((corpus == 'blackboard') or (corpus == 'blackboard_ud')):
        words = blackboard.pre_process(words)
        word_tags = _blackboard_tagger().tag(words)
        word_tags = blackboard.post_process(word_tags, to_ud)
    else:
        tagger = _orchid_tagger()
        word_tags = tagger.tag(words)
    return word_tags
""""""]",1
"Doc, Tok = Tok, Doc
def nltk_tokenzier_factory(nltk_tokenizer):
    """"""
    Parameters
    ----------
    nltk_tokenizer : nltk.tokenize.* instance (e.g., nltk.TreebankWordTokenizer())

    Returns
    -------
    Doc of tweets

    Notes
    -------
    Requires NLTK to be installed
    """"""","["""""" 

    def tokenize(text):
        toks = []
        for tok in nltk_tokenizer.tokenize(text):
            if (len(tok) > 0):
                toks.append(Tok(_get_pos_tag(tok), tok.lower(), tok.lower(), ent_type='', tag=''))
        return Doc([toks], text)
    return tokenize
"""""", """""" 

    def tokenize(text):
        toks = []
        for tok in nltk_tokenizer.tokenize(text):
            if (len(tok) > 0):
                toks.append(Doc(_get_pos_tag(tok), tok.lower(), tok.lower(), ent_type='', tag=''))
        return Tok([toks], text)
    return tokenize
""""""]",1
"HTTPSConnection, from_unicode = from_unicode, HTTPSConnection
def send(channel, message, **kwargs):
    """"""
    Site: https://slack.com
    API: https://api.slack.com
    Desc: real-time messaging
    """"""","["""""" 
    headers = {'Content-type': 'application/x-www-form-urlencoded', 'User-Agent': ('DBMail/%s' % get_version())}
    username = from_unicode(kwargs.pop('username', settings.SLACK_USERNAME))
    hook_url = from_unicode(kwargs.pop('hook_url', settings.SLACK_HOOCK_URL))
    channel = from_unicode((channel or settings.SLACK_CHANNEL))
    emoji = from_unicode(kwargs.pop('emoji', ''))
    message = from_unicode(message)
    data = {'channel': channel, 'username': username, 'text': message, 'icon_emoji': emoji}
    _data = kwargs.pop('data', None)
    if (_data is not None):
        data.update(_data)
    up = urlparse(hook_url)
    http = HTTPSConnection(up.netloc)
    http.request('POST', up.path, headers=headers, body=urlencode({'payload': dumps(data)}))
    response = http.getresponse()
    if (response.status != 200):
        raise SlackError(response.reason)
    body = response.read()
    if (body != 'ok'):
        raise SlackError(repr(body))
    return True
"""""", """""" 
    headers = {'Content-type': 'application/x-www-form-urlencoded', 'User-Agent': ('DBMail/%s' % get_version())}
    username = HTTPSConnection(kwargs.pop('username', settings.SLACK_USERNAME))
    hook_url = HTTPSConnection(kwargs.pop('hook_url', settings.SLACK_HOOCK_URL))
    channel = HTTPSConnection((channel or settings.SLACK_CHANNEL))
    emoji = HTTPSConnection(kwargs.pop('emoji', ''))
    message = HTTPSConnection(message)
    data = {'channel': channel, 'username': username, 'text': message, 'icon_emoji': emoji}
    _data = kwargs.pop('data', None)
    if (_data is not None):
        data.update(_data)
    up = urlparse(hook_url)
    http = from_unicode(up.netloc)
    http.request('POST', up.path, headers=headers, body=urlencode({'payload': dumps(data)}))
    response = http.getresponse()
    if (response.status != 200):
        raise SlackError(response.reason)
    body = response.read()
    if (body != 'ok'):
        raise SlackError(repr(body))
    return True
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_url_contains(partial_url):
    """"""Verify the current URL contains partial_url

    Parameters:
    partial_url : value
    """"""","["""""" 
    current_url = get_browser().current_url
    msg = f""Verify URL contains '{partial_url}'""
    err = f""expected URL '{current_url}' to contain '{partial_url}'""
    with _verify_step(msg, err) as s:
        s.condition = (partial_url in current_url)
"""""", """""" 
    current_url = _verify_step().current_url
    msg = f""Verify URL contains '{partial_url}'""
    err = f""expected URL '{current_url}' to contain '{partial_url}'""
    with get_browser(msg, err) as s:
        s.condition = (partial_url in current_url)
""""""]",1
"cub_cumsum, numpy_cumsum = numpy_cumsum, cub_cumsum
def cumsum(x, dim=None):
    """"""
    Parameters:
    -----------
    x: jt.var
    dim: int

    Returns:
    --------
    the cumulative sum in dim of x
    """"""","["""""" 
    if (dim == None):
        dim = (- 1)
    assert ((dim >= (- 1)) and (dim < len(x.shape)))
    if jt.flags.use_cuda:
        return cub_cumsum(x, dim)
    else:
        return numpy_cumsum(x, dim)
"""""", """""" 
    if (dim == None):
        dim = (- 1)
    assert ((dim >= (- 1)) and (dim < len(x.shape)))
    if jt.flags.use_cuda:
        return numpy_cumsum(x, dim)
    else:
        return cub_cumsum(x, dim)
""""""]",1
"create_folder, get_set_statuses = get_set_statuses, create_folder
def new_set_fc(name):
    """"""
    creates new study set
    :param name:
    """"""","["""""" 
    if name:
        if (len(name.split()) > 1):
            click.echo(chalk.red('The length of name should not be more than one'))
        else:
            sets = get_set_statuses()
            if (not sets):
                create_folder(FLASHCARDS_CONFIG_FOLDER_PATH)
                description = input('Enter a description:\n')
                with open((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                    fp.write('{}-{}-{}\n'.format(name, 1, description))
            else:
                try:
                    if ((sets[name] != 0) and (sets[name] != 1)):
                        click.echo(chalk.red('Set already exists'))
                except KeyError:
                    create_folder(FLASHCARDS_CONFIG_FOLDER_PATH)
                    description = input('Enter a description:\n')
                    with open((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                        fp.write('{}-{}-{}\n'.format(name, 1, description))
                    create_folder(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + name))
                    click.echo(chalk.red('Set added'))
    else:
        click.echo(chalk.red('Please enter the name of new study set after the command'))
"""""", """""" 
    if name:
        if (len(name.split()) > 1):
            click.echo(chalk.red('The length of name should not be more than one'))
        else:
            sets = create_folder()
            if (not sets):
                get_set_statuses(FLASHCARDS_CONFIG_FOLDER_PATH)
                description = input('Enter a description:\n')
                with open((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                    fp.write('{}-{}-{}\n'.format(name, 1, description))
            else:
                try:
                    if ((sets[name] != 0) and (sets[name] != 1)):
                        click.echo(chalk.red('Set already exists'))
                except KeyError:
                    get_set_statuses(FLASHCARDS_CONFIG_FOLDER_PATH)
                    description = input('Enter a description:\n')
                    with open((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                        fp.write('{}-{}-{}\n'.format(name, 1, description))
                    get_set_statuses(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + name))
                    click.echo(chalk.red('Set added'))
    else:
        click.echo(chalk.red('Please enter the name of new study set after the command'))
""""""]",1
"cleandoc, main = main, cleandoc
def test_lint_meson_subprojects_included_fail(fake_repository, stringio):
    """"""When Meson subprojects are included, fail on errors.""""""","["""""" 
    (fake_repository / 'meson.build').write_text(cleandoc('\n            SPDX-FileCopyrightText: 2022 Jane Doe\n            SPDX-License-Identifier: CC0-1.0\n            '))
    libfoo_dir = (fake_repository / 'subprojects/libfoo')
    libfoo_dir.mkdir(parents=True)
    (libfoo_dir / 'foo.c').write_text('foo')
    result = main(['--include-meson-subprojects', 'lint'], out=stringio)
    assert (result == 1)
    assert (':-(' in stringio.getvalue())
"""""", """""" 
    (fake_repository / 'meson.build').write_text(main('\n            SPDX-FileCopyrightText: 2022 Jane Doe\n            SPDX-License-Identifier: CC0-1.0\n            '))
    libfoo_dir = (fake_repository / 'subprojects/libfoo')
    libfoo_dir.mkdir(parents=True)
    (libfoo_dir / 'foo.c').write_text('foo')
    result = cleandoc(['--include-meson-subprojects', 'lint'], out=stringio)
    assert (result == 1)
    assert (':-(' in stringio.getvalue())
""""""]",1
"ListConfig, Tiler = Tiler, ListConfig
@pytest.mark.parametrize('tile_size, stride', [(512, 256), ([512, 512], [256, 256]), (ListConfig([512, 512]), 256)])
def test_size_types_should_be_int_tuple_or_list_config(tile_size, stride):
    """"""Size type could only be integer, tuple or ListConfig type.""""""","["""""" 
    tiler = Tiler(tile_size=tile_size, stride=stride)
    assert isinstance(tiler.tile_size_h, int)
    assert isinstance(tiler.stride_w, int)
"""""", """""" 
    tiler = ListConfig(tile_size=tile_size, stride=stride)
    assert isinstance(tiler.tile_size_h, int)
    assert isinstance(tiler.stride_w, int)
""""""]",1
"GypError, ExpandVariables = ExpandVariables, GypError
def EvalSingleCondition(cond_expr, true_dict, false_dict, phase, variables, build_file):
    """"""Returns true_dict if cond_expr evaluates to true, and false_dict
  otherwise.""""""","["""""" 
    cond_expr_expanded = ExpandVariables(cond_expr, phase, variables, build_file)
    if (type(cond_expr_expanded) not in (str, int)):
        raise ValueError((('Variable expansion in this context permits str and int ' + 'only, found ') + cond_expr_expanded.__class__.__name__))
    try:
        if (cond_expr_expanded in cached_conditions_asts):
            ast_code = cached_conditions_asts[cond_expr_expanded]
        else:
            ast_code = compile(cond_expr_expanded, '<string>', 'eval')
            cached_conditions_asts[cond_expr_expanded] = ast_code
        env = {'__builtins__': {}, 'v': StrictVersion}
        if eval(ast_code, env, variables):
            return true_dict
        return false_dict
    except SyntaxError as e:
        syntax_error = SyntaxError((""%s while evaluating condition '%s' in %s at character %d."" % (str(e.args[0]), e.text, build_file, e.offset)), e.filename, e.lineno, e.offset, e.text)
        raise syntax_error
    except NameError as e:
        gyp.common.ExceptionAppend(e, f""while evaluating condition '{cond_expr_expanded}' in {build_file}"")
        raise GypError(e)
"""""", """""" 
    cond_expr_expanded = GypError(cond_expr, phase, variables, build_file)
    if (type(cond_expr_expanded) not in (str, int)):
        raise ValueError((('Variable expansion in this context permits str and int ' + 'only, found ') + cond_expr_expanded.__class__.__name__))
    try:
        if (cond_expr_expanded in cached_conditions_asts):
            ast_code = cached_conditions_asts[cond_expr_expanded]
        else:
            ast_code = compile(cond_expr_expanded, '<string>', 'eval')
            cached_conditions_asts[cond_expr_expanded] = ast_code
        env = {'__builtins__': {}, 'v': StrictVersion}
        if eval(ast_code, env, variables):
            return true_dict
        return false_dict
    except SyntaxError as e:
        syntax_error = SyntaxError((""%s while evaluating condition '%s' in %s at character %d."" % (str(e.args[0]), e.text, build_file, e.offset)), e.filename, e.lineno, e.offset, e.text)
        raise syntax_error
    except NameError as e:
        gyp.common.ExceptionAppend(e, f""while evaluating condition '{cond_expr_expanded}' in {build_file}"")
        raise ExpandVariables(e)
""""""]",1
"if_binary_exists, get_stage_binary = get_stage_binary, if_binary_exists
def cc_ld_cmake_defines(dirs, env_vars, stage):
    """"""
    Generate compiler and linker cmake defines, which change depending on what
    stage we are at
    :param dirs: An instance of the Directories class with the paths to use
    :param env_vars: An instance of the EnvVars class with the compilers/linker to use
    :param stage: What stage we are at
    :return: A set of defines
    """"""","["""""" 
    defines = {}
    if (stage == 1):
        cc = env_vars.cc
        cxx = env_vars.cxx
        ld = env_vars.ld
        ar = if_binary_exists('llvm-ar', cc)
        ranlib = if_binary_exists('llvm-ranlib', cc)
        clang_tblgen = None
        llvm_tblgen = None
    else:
        if pgo_stage(stage):
            stage = 2
        else:
            stage = 1
        ar = get_stage_binary('llvm-ar', dirs, stage)
        cc = get_stage_binary('clang', dirs, stage)
        clang_tblgen = get_stage_binary('clang-tblgen', dirs, stage)
        cxx = get_stage_binary('clang++', dirs, stage)
        ld = get_stage_binary('ld.lld', dirs, stage)
        llvm_tblgen = get_stage_binary('llvm-tblgen', dirs, stage)
        ranlib = get_stage_binary('llvm-ranlib', dirs, stage)
    if ar:
        defines['CMAKE_AR'] = ar
    defines['CMAKE_C_COMPILER'] = cc
    if clang_tblgen:
        defines['CLANG_TABLEGEN'] = clang_tblgen
    defines['CMAKE_CXX_COMPILER'] = cxx
    if ld:
        defines['LLVM_USE_LINKER'] = ld
    if llvm_tblgen:
        defines['LLVM_TABLEGEN'] = llvm_tblgen
    if ranlib:
        defines['CMAKE_RANLIB'] = ranlib
    return defines
"""""", """""" 
    defines = {}
    if (stage == 1):
        cc = env_vars.cc
        cxx = env_vars.cxx
        ld = env_vars.ld
        ar = get_stage_binary('llvm-ar', cc)
        ranlib = get_stage_binary('llvm-ranlib', cc)
        clang_tblgen = None
        llvm_tblgen = None
    else:
        if pgo_stage(stage):
            stage = 2
        else:
            stage = 1
        ar = if_binary_exists('llvm-ar', dirs, stage)
        cc = if_binary_exists('clang', dirs, stage)
        clang_tblgen = if_binary_exists('clang-tblgen', dirs, stage)
        cxx = if_binary_exists('clang++', dirs, stage)
        ld = if_binary_exists('ld.lld', dirs, stage)
        llvm_tblgen = if_binary_exists('llvm-tblgen', dirs, stage)
        ranlib = if_binary_exists('llvm-ranlib', dirs, stage)
    if ar:
        defines['CMAKE_AR'] = ar
    defines['CMAKE_C_COMPILER'] = cc
    if clang_tblgen:
        defines['CLANG_TABLEGEN'] = clang_tblgen
    defines['CMAKE_CXX_COMPILER'] = cxx
    if ld:
        defines['LLVM_USE_LINKER'] = ld
    if llvm_tblgen:
        defines['LLVM_TABLEGEN'] = llvm_tblgen
    if ranlib:
        defines['CMAKE_RANLIB'] = ranlib
    return defines
""""""]",1
"to_torch, to_numpy = to_numpy, to_torch
def drawGaussian(img, pt, sigma):
    """"""Draw 2d gaussian on input image.

    Parameters
    ----------
    img: torch.Tensor
        A tensor with shape: `(3, H, W)`.
    pt: list or tuple
        A point: (x, y).
    sigma: int
        Sigma of gaussian distribution.

    Returns
    -------
    torch.Tensor
        A tensor with shape: `(3, H, W)`.

    """"""","["""""" 
    img = to_numpy(img)
    tmpSize = (3 * sigma)
    ul = [int((pt[0] - tmpSize)), int((pt[1] - tmpSize))]
    br = [int(((pt[0] + tmpSize) + 1)), int(((pt[1] + tmpSize) + 1))]
    if ((ul[0] >= img.shape[1]) or (ul[1] >= img.shape[0]) or (br[0] < 0) or (br[1] < 0)):
        return to_torch(img)
    size = ((2 * tmpSize) + 1)
    x = np.arange(0, size, 1, float)
    y = x[:, np.newaxis]
    x0 = y0 = (size // 2)
    g = np.exp(((- (((x - x0) ** 2) + ((y - y0) ** 2))) / (2 * (sigma ** 2))))
    g_x = (max(0, (- ul[0])), (min(br[0], img.shape[1]) - ul[0]))
    g_y = (max(0, (- ul[1])), (min(br[1], img.shape[0]) - ul[1]))
    img_x = (max(0, ul[0]), min(br[0], img.shape[1]))
    img_y = (max(0, ul[1]), min(br[1], img.shape[0]))
    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]
    return to_torch(img)
"""""", """""" 
    img = to_torch(img)
    tmpSize = (3 * sigma)
    ul = [int((pt[0] - tmpSize)), int((pt[1] - tmpSize))]
    br = [int(((pt[0] + tmpSize) + 1)), int(((pt[1] + tmpSize) + 1))]
    if ((ul[0] >= img.shape[1]) or (ul[1] >= img.shape[0]) or (br[0] < 0) or (br[1] < 0)):
        return to_numpy(img)
    size = ((2 * tmpSize) + 1)
    x = np.arange(0, size, 1, float)
    y = x[:, np.newaxis]
    x0 = y0 = (size // 2)
    g = np.exp(((- (((x - x0) ** 2) + ((y - y0) ** 2))) / (2 * (sigma ** 2))))
    g_x = (max(0, (- ul[0])), (min(br[0], img.shape[1]) - ul[0]))
    g_y = (max(0, (- ul[1])), (min(br[1], img.shape[0]) - ul[1]))
    img_x = (max(0, ul[0]), min(br[0], img.shape[1]))
    img_y = (max(0, ul[1]), min(br[1], img.shape[0]))
    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]
    return to_numpy(img)
""""""]",1
"date_type, CFTimeIndex = CFTimeIndex, date_type
def _cftimeindex_from_i8(values, date_type, name):
    """"""Construct a CFTimeIndex from an array of integers.

    Parameters
    ----------
    values : np.array
        Integers representing microseconds since 1970-01-01.
    date_type : cftime.datetime
        Type of date for the index.
    name : str
        Name of the index.

    Returns
    -------
    CFTimeIndex
    """"""","["""""" 
    epoch = date_type(1970, 1, 1)
    dates = np.array([(epoch + timedelta(microseconds=int(value))) for value in values])
    return CFTimeIndex(dates, name=name)
"""""", """""" 
    epoch = CFTimeIndex(1970, 1, 1)
    dates = np.array([(epoch + timedelta(microseconds=int(value))) for value in values])
    return date_type(dates, name=name)
""""""]",1
"get_race_ethnicity_name_list_given_tzioumis_data, load_tzioumis_data = load_tzioumis_data, get_race_ethnicity_name_list_given_tzioumis_data
def get_tzioumis_name_lists(tzioumis_data_path: str, race_gender_name_lists: Dict[(str, List[str])]) -> Dict[(str, List[str])]:
    """"""
    Get race/ethnicity name lists from the Tzioumis work.
    """"""","["""""" 
    percent_df = load_tzioumis_data(tzioumis_data_path)
    name_lists = {}
    for race_ethnicity in RACES_ETHNICITIES_WITH_NAMES:
        name_lists[race_ethnicity] = get_race_ethnicity_name_list_given_tzioumis_data(percent_df=percent_df, race_gender_name_lists=race_gender_name_lists, race_ethnicity=race_ethnicity)
    return name_lists
"""""", """""" 
    percent_df = get_race_ethnicity_name_list_given_tzioumis_data(tzioumis_data_path)
    name_lists = {}
    for race_ethnicity in RACES_ETHNICITIES_WITH_NAMES:
        name_lists[race_ethnicity] = load_tzioumis_data(percent_df=percent_df, race_gender_name_lists=race_gender_name_lists, race_ethnicity=race_ethnicity)
    return name_lists
""""""]",1
"MultiRunAcuteAnalyzer, AcuteAnalyzer = AcuteAnalyzer, MultiRunAcuteAnalyzer
def get_multi_run_analyzer(opt) -> MultiRunAcuteAnalyzer:
    """"""
    Return an object to analyze the results of multiple runs simultaneously.

    Load HITs from each run into a separate dataframe, and then pass all dataframes into
    a separate analyzer class that will concatenate them.
    """"""","["""""" 
    run_ids = opt['run_ids'].split(',')
    assert (opt['outdir'] is not None), '--outdir must be specified when combining results of multiple runs!'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    opt['outdir'] = os.path.join(opt['outdir'], f'combined_runs_{timestamp}')
    os.makedirs(opt['outdir'], exist_ok=True)
    run_id_list_path = os.path.join(opt['outdir'], 'run_ids.txt')
    with open(run_id_list_path, 'w') as f:
        for run_id in run_ids:
            f.write((run_id + '\n'))
    dataframes = {}
    for run_id in run_ids:
        print(f'''
Starting to load HITs for run ID {run_id}.''')
        opt_copy = deepcopy(opt)
        opt_copy['run_ids'] = run_id
        dataframes[run_id] = AcuteAnalyzer(opt_copy).dataframe
    return MultiRunAcuteAnalyzer(opt=opt, dataframes=dataframes)
"""""", """""" 
    run_ids = opt['run_ids'].split(',')
    assert (opt['outdir'] is not None), '--outdir must be specified when combining results of multiple runs!'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    opt['outdir'] = os.path.join(opt['outdir'], f'combined_runs_{timestamp}')
    os.makedirs(opt['outdir'], exist_ok=True)
    run_id_list_path = os.path.join(opt['outdir'], 'run_ids.txt')
    with open(run_id_list_path, 'w') as f:
        for run_id in run_ids:
            f.write((run_id + '\n'))
    dataframes = {}
    for run_id in run_ids:
        print(f'''
Starting to load HITs for run ID {run_id}.''')
        opt_copy = deepcopy(opt)
        opt_copy['run_ids'] = run_id
        dataframes[run_id] = MultiRunAcuteAnalyzer(opt_copy).dataframe
    return AcuteAnalyzer(opt=opt, dataframes=dataframes)
""""""]",1
"GetBufferNumberForFilename, BufferIsVisible = BufferIsVisible, GetBufferNumberForFilename
def _GetNumNonVisibleFiles(file_list):
    """"""Returns the number of file in the iterable list of files |file_list| which
  are not curerntly open in visible windows""""""","["""""" 
    return len([f for f in file_list if (not BufferIsVisible(GetBufferNumberForFilename(f)))])
"""""", """""" 
    return len([f for f in file_list if (not GetBufferNumberForFilename(BufferIsVisible(f)))])
""""""]",1
"sleep, is_admin_account_enabled = is_admin_account_enabled, sleep
def process_organization_admin_account(admin_account_id: str, regions: list) -> None:
    """"""Process the delegated admin account for each region.

    Args:
        admin_account_id: Admin account ID
        regions: AWS Region List

    Raises:
        ClientError: boto3 ClientError
    """"""","["""""" 
    for region in regions:
        securityhub_client: SecurityHubClient = MANAGEMENT_ACCOUNT_SESSION.client('securityhub', region, config=BOTO3_CONFIG)
        if (not is_admin_account_enabled(securityhub_client, admin_account_id)):
            for _ in range(10):
                try:
                    securityhub_client.enable_organization_admin_account(AdminAccountId=admin_account_id)
                    LOGGER.info(f""Delegated admin '{admin_account_id}' enabled in {region}"")
                    break
                except securityhub_client.exceptions.ResourceConflictException:
                    LOGGER.info(f'Delegated admin already enabled in {region}')
                except ClientError as error:
                    if (error.response['Error']['Code'] != 'InvalidInputException'):
                        raise
                    LOGGER.info(f""Waiting 10 seconds before retrying the enable organization delegated admin '{admin_account_id}' enabled in {region}"")
                    sleep(10)
"""""", """""" 
    for region in regions:
        securityhub_client: SecurityHubClient = MANAGEMENT_ACCOUNT_SESSION.client('securityhub', region, config=BOTO3_CONFIG)
        if (not sleep(securityhub_client, admin_account_id)):
            for _ in range(10):
                try:
                    securityhub_client.enable_organization_admin_account(AdminAccountId=admin_account_id)
                    LOGGER.info(f""Delegated admin '{admin_account_id}' enabled in {region}"")
                    break
                except securityhub_client.exceptions.ResourceConflictException:
                    LOGGER.info(f'Delegated admin already enabled in {region}')
                except ClientError as error:
                    if (error.response['Error']['Code'] != 'InvalidInputException'):
                        raise
                    LOGGER.info(f""Waiting 10 seconds before retrying the enable organization delegated admin '{admin_account_id}' enabled in {region}"")
                    is_admin_account_enabled(10)
""""""]",1
"DrawLine, DrawDot = DrawDot, DrawLine
def DrawCube(points, which_color=0, color=None, draw=None):
    """"""Draw cube with a thick solid line across the front top edge.""""""","["""""" 
    lineWidthForDrawing = 2
    lineColor1 = (255, 215, 0)
    lineColor2 = (12, 115, 170)
    lineColor3 = (45, 195, 35)
    if (which_color == 3):
        lineColor = lineColor3
    else:
        lineColor = lineColor1
    if (not (color is None)):
        lineColor = color
    DrawLine(points[0], points[1], lineColor, 8, draw)
    DrawLine(points[1], points[2], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[3], points[2], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[3], points[0], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[4], points[5], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[6], points[5], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[6], points[7], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[4], points[7], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[0], points[4], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[7], points[3], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[5], points[1], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[2], points[6], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[0], pointColor=lineColor, pointRadius=4, draw=draw)
    DrawDot(points[1], pointColor=lineColor, pointRadius=4, draw=draw)
"""""", """""" 
    lineWidthForDrawing = 2
    lineColor1 = (255, 215, 0)
    lineColor2 = (12, 115, 170)
    lineColor3 = (45, 195, 35)
    if (which_color == 3):
        lineColor = lineColor3
    else:
        lineColor = lineColor1
    if (not (color is None)):
        lineColor = color
    DrawDot(points[0], points[1], lineColor, 8, draw)
    DrawDot(points[1], points[2], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[3], points[2], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[3], points[0], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[4], points[5], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[6], points[5], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[6], points[7], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[4], points[7], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[0], points[4], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[7], points[3], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[5], points[1], lineColor, lineWidthForDrawing, draw)
    DrawDot(points[2], points[6], lineColor, lineWidthForDrawing, draw)
    DrawLine(points[0], pointColor=lineColor, pointRadius=4, draw=draw)
    DrawLine(points[1], pointColor=lineColor, pointRadius=4, draw=draw)
""""""]",1
"module_from_spec, Path = Path, module_from_spec
def load_module(name: str='anomalib/__init__.py'):
    """"""Load Python Module.

    Args:
        name (str, optional): Name of the module to load.
            Defaults to ""anomalib/__init__.py"".

    Returns:
        _type_: _description_
    """"""","["""""" 
    location = str((Path(__file__).parent / name))
    spec = spec_from_file_location(name=name, location=location)
    module = module_from_spec(spec)
    spec.loader.exec_module(module)
    return module
"""""", """""" 
    location = str((module_from_spec(__file__).parent / name))
    spec = spec_from_file_location(name=name, location=location)
    module = Path(spec)
    spec.loader.exec_module(module)
    return module
""""""]",1
"mat, shape = shape, mat
def plotfig_SVM(xArr, yArr, ws, b, alphas):
    """"""
    参考地址: 
       http://blog.csdn.net/maoersong/article/details/24315633
       http://www.cnblogs.com/JustForCS/p/5283489.html
       http://blog.csdn.net/kkxgx/article/details/6951959
    """"""","["""""" 
    xMat = mat(xArr)
    yMat = mat(yArr)
    b = array(b)[0]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])
    x = arange((- 1.0), 10.0, 0.1)
    y = (((- b) - (ws[(0, 0)] * x)) / ws[(1, 0)])
    ax.plot(x, y)
    for i in range(shape(yMat[0])[1]):
        if (yMat[(0, i)] > 0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'cx')
        else:
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'kp')
    for i in range(100):
        if (alphas[i] > 0.0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'ro')
    plt.show()
"""""", """""" 
    xMat = shape(xArr)
    yMat = shape(yArr)
    b = array(b)[0]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])
    x = arange((- 1.0), 10.0, 0.1)
    y = (((- b) - (ws[(0, 0)] * x)) / ws[(1, 0)])
    ax.plot(x, y)
    for i in range(mat(yMat[0])[1]):
        if (yMat[(0, i)] > 0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'cx')
        else:
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'kp')
    for i in range(100):
        if (alphas[i] > 0.0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'ro')
    plt.show()
""""""]",1
"get_calling_namespaces, is_Dict = is_Dict, get_calling_namespaces
def compute_exports(exports):
    """"""Compute a dictionary of exports given one of the parameters
    to the Export() function or the exports argument to SConscript().""""""","["""""" 
    (loc, glob) = get_calling_namespaces()
    retval = {}
    try:
        for export in exports:
            if is_Dict(export):
                retval.update(export)
            else:
                try:
                    retval[export] = loc[export]
                except KeyError:
                    retval[export] = glob[export]
    except KeyError as x:
        raise SCons.Errors.UserError((""Export of non-existent variable '%s'"" % x))
    return retval
"""""", """""" 
    (loc, glob) = is_Dict()
    retval = {}
    try:
        for export in exports:
            if get_calling_namespaces(export):
                retval.update(export)
            else:
                try:
                    retval[export] = loc[export]
                except KeyError:
                    retval[export] = glob[export]
    except KeyError as x:
        raise SCons.Errors.UserError((""Export of non-existent variable '%s'"" % x))
    return retval
""""""]",1
"get_validated_parameters, process_account = process_account, get_validated_parameters
def process_event_lifecycle(event: dict) -> None:
    """"""Process Lifecycle Event from AWS Control Tower.

    Args:
        event: event data

    Raises:
        ValueError: Control Tower Lifecycle Event not 'createManagedAccountStatus' or 'updateManagedAccountStatus'
    """"""","["""""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = get_validated_parameters({})
    aws_account_id = ''
    if event['detail']['serviceEventDetails'].get('createManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['createManagedAccountStatus']['account']['accountId']
    elif event['detail']['serviceEventDetails'].get('updateManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['updateManagedAccountStatus']['account']['accountId']
    else:
        raise ValueError(""Control Tower Lifecycle Event not 'createManagedAccountStatus' or 'updateManagedAccountStatus'"")
    process_account(event, aws_account_id, params)
"""""", """""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = process_account({})
    aws_account_id = ''
    if event['detail']['serviceEventDetails'].get('createManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['createManagedAccountStatus']['account']['accountId']
    elif event['detail']['serviceEventDetails'].get('updateManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['updateManagedAccountStatus']['account']['accountId']
    else:
        raise ValueError(""Control Tower Lifecycle Event not 'createManagedAccountStatus' or 'updateManagedAccountStatus'"")
    get_validated_parameters(event, aws_account_id, params)
""""""]",1
"euler_matrix_new, normalise_rot = normalise_rot, euler_matrix_new
def get_rotation_matrix(rot, deg=False, eulertype='ZYX'):
    """"""Return the rotation matrix corresponding to angles given in rot.

    Usage: matrot,do_rot,normrot = get_rotation_matrix(rot)

    Input:
       - rot: either None, an angle or a tuple of 1,2 or 3 angles
              corresponding to Euler angles.
    Output:
       - matrot: 3x3 rotation matrix
       - do_rot: True if rotation is not identity, False otherwise
       - normrot: the normalized version of the input rot.
    """"""","["""""" 
    rot = normalise_rot(rot, deg=deg)
    if (not np.allclose(rot, np.zeros(3), rtol=0.0, atol=1e-15)):
        do_rot = True
    else:
        do_rot = False
    if (eulertype == 'X'):
        matrot = euler_matrix_new(rot[0], (- rot[1]), rot[2], X=True)
    elif (eulertype == 'Y'):
        matrot = euler_matrix_new(rot[0], (- rot[1]), rot[2], Y=True)
    else:
        matrot = euler_matrix_new(rot[0], (- rot[1]), rot[2], ZYX=True)
    return (matrot, do_rot, rot)
"""""", """""" 
    rot = euler_matrix_new(rot, deg=deg)
    if (not np.allclose(rot, np.zeros(3), rtol=0.0, atol=1e-15)):
        do_rot = True
    else:
        do_rot = False
    if (eulertype == 'X'):
        matrot = normalise_rot(rot[0], (- rot[1]), rot[2], X=True)
    elif (eulertype == 'Y'):
        matrot = normalise_rot(rot[0], (- rot[1]), rot[2], Y=True)
    else:
        matrot = normalise_rot(rot[0], (- rot[1]), rot[2], ZYX=True)
    return (matrot, do_rot, rot)
""""""]",1
"get_rgbd_point_cloud, _subsample_pointcloud = _subsample_pointcloud, get_rgbd_point_cloud
def get_eval_frame_data_pointcloud(eval_frame_data: FrameData, max_n_points: int=int(30000.0)):
    """"""
    Generate a pointcloud by unprojecting the known depth maps of a `FrameData` object
    `eval_frame_data`. 

    Args:
        eval_frame_data: `FrameData` to unproject.
        max_n_points: Maximum number of points to keep in the point cloud.
    """"""","["""""" 
    batch_size = eval_frame_data.image_rgb.shape[0]
    pointcloud = get_rgbd_point_cloud(eval_frame_data.camera[list(range(1, batch_size))], eval_frame_data.image_rgb[1:], eval_frame_data.depth_map[1:], (eval_frame_data.fg_probability[1:] > 0.5).float(), mask_points=True)
    return _subsample_pointcloud(pointcloud, max_n_points)
"""""", """""" 
    batch_size = eval_frame_data.image_rgb.shape[0]
    pointcloud = _subsample_pointcloud(eval_frame_data.camera[list(range(1, batch_size))], eval_frame_data.image_rgb[1:], eval_frame_data.depth_map[1:], (eval_frame_data.fg_probability[1:] > 0.5).float(), mask_points=True)
    return get_rgbd_point_cloud(pointcloud, max_n_points)
""""""]",1
"set_vars, assert_tf_initialized = assert_tf_initialized, set_vars
def create_var_with_large_initial_value(initial_value: np.ndarray, *args, **kwargs):
    """"""Create tf.Variable with large initial value without bloating the tf graph.""""""","["""""" 
    assert_tf_initialized()
    assert isinstance(initial_value, np.ndarray)
    zeros = tf.zeros(initial_value.shape, initial_value.dtype)
    var = tf.Variable(zeros, *args, **kwargs)
    set_vars({var: initial_value})
    return var
"""""", """""" 
    set_vars()
    assert isinstance(initial_value, np.ndarray)
    zeros = tf.zeros(initial_value.shape, initial_value.dtype)
    var = tf.Variable(zeros, *args, **kwargs)
    assert_tf_initialized({var: initial_value})
    return var
""""""]",1
"EscapeMakeVariableExpansion, EscapeShellArgument = EscapeShellArgument, EscapeMakeVariableExpansion
def EscapeCppDefine(s):
    """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""","["""""" 
    s = EscapeShellArgument(s)
    s = EscapeMakeVariableExpansion(s)
    return s.replace('#', '\\#')
"""""", """""" 
    s = EscapeMakeVariableExpansion(s)
    s = EscapeShellArgument(s)
    return s.replace('#', '\\#')
""""""]",1
"Path, debug = debug, Path
def read_script_env_cache():
    """""" fetch cached msvc env vars if requested, else return empty dict """"""","["""""" 
    envcache = {}
    if CONFIG_CACHE:
        try:
            p = Path(CONFIG_CACHE)
            with p.open('r') as f:
                envcache_list = json.load(f)
                if isinstance(envcache_list, list):
                    envcache = {tuple(d['key']): d['data'] for d in envcache_list}
                else:
                    warn_msg = 'Incompatible format for msvc cache file {}: file may be overwritten.'.format(repr(CONFIG_CACHE))
                    SCons.Warnings.warn(MSVCCacheInvalidWarning, warn_msg)
                    debug(warn_msg)
        except FileNotFoundError:
            pass
    return envcache
"""""", """""" 
    envcache = {}
    if CONFIG_CACHE:
        try:
            p = debug(CONFIG_CACHE)
            with p.open('r') as f:
                envcache_list = json.load(f)
                if isinstance(envcache_list, list):
                    envcache = {tuple(d['key']): d['data'] for d in envcache_list}
                else:
                    warn_msg = 'Incompatible format for msvc cache file {}: file may be overwritten.'.format(repr(CONFIG_CACHE))
                    SCons.Warnings.warn(MSVCCacheInvalidWarning, warn_msg)
                    Path(warn_msg)
        except FileNotFoundError:
            pass
    return envcache
""""""]",1
"remove_all_log_handlers, user_init = user_init, remove_all_log_handlers
def process_loop(conn: Connection, user_init: Callable[([], None)], loglevel, task, task_args):
    """"""Initialize a process pool worker""""""","["""""" 
    with suppress(AttributeError):
        signal.signal(signal.SIGBUS, process_sigbus)
    h = ConnectionLogHandler(conn)
    root = logging.getLogger()
    remove_all_log_handlers(root)
    root.setLevel(loglevel)
    root.addHandler(h)
    user_init()
    for args in task_args:
        try:
            result = task(args)
        except Exception as e:
            conn.send((MessageType.exception, e))
            break
        else:
            conn.send((MessageType.result, result))
    conn.send((MessageType.complete, None))
    conn.close()
    return
"""""", """""" 
    with suppress(AttributeError):
        signal.signal(signal.SIGBUS, process_sigbus)
    h = ConnectionLogHandler(conn)
    root = logging.getLogger()
    user_init(root)
    root.setLevel(loglevel)
    root.addHandler(h)
    remove_all_log_handlers()
    for args in task_args:
        try:
            result = task(args)
        except Exception as e:
            conn.send((MessageType.exception, e))
            break
        else:
            conn.send((MessageType.result, result))
    conn.send((MessageType.complete, None))
    conn.close()
    return
""""""]",1
"_valid_other_type, _interval_to_bound_points = _interval_to_bound_points, _valid_other_type
def _resolve_intervals_2dplot(val, func_name):
    """"""
    Helper function to replace the values of a coordinate array containing
    pd.Interval with their mid-points or - for pcolormesh - boundaries which
    increases length by 1.
    """"""","["""""" 
    label_extra = ''
    if _valid_other_type(val, pd.Interval):
        if (func_name == 'pcolormesh'):
            val = _interval_to_bound_points(val)
        else:
            val = _interval_to_mid_points(val)
            label_extra = '_center'
    return (val, label_extra)
"""""", """""" 
    label_extra = ''
    if _interval_to_bound_points(val, pd.Interval):
        if (func_name == 'pcolormesh'):
            val = _valid_other_type(val)
        else:
            val = _interval_to_mid_points(val)
            label_extra = '_center'
    return (val, label_extra)
""""""]",1
"AverageMeter, get_valid_stats = get_valid_stats, AverageMeter
def validate(args, trainer, task, epoch_itr, subsets):
    """"""Evaluate the model on the validation set(s) and return the losses.""""""","["""""" 
    valid_losses = []
    for subset in subsets:
        itr = task.get_batch_iterator(dataset=task.dataset(subset), max_tokens=args.max_tokens, max_sentences=args.max_sentences_valid, max_positions=utils.resolve_max_positions(task.max_positions(), trainer.get_model().max_positions()), ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test, required_batch_size_multiple=8, seed=args.seed, num_shards=args.distributed_world_size, shard_id=args.distributed_rank).next_epoch_itr(shuffle=False)
        progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, prefix=""valid on '{}' subset"".format(subset), no_progress_bar='simple')
        for k in ['valid_loss', 'valid_nll_loss']:
            meter = trainer.get_meter(k)
            if (meter is not None):
                meter.reset()
        task_meters = trainer.get_meter('task')
        if (task_meters is not None):
            for m in task_meters.values():
                m.reset()
        extra_meters = collections.defaultdict((lambda : AverageMeter()))
        misclassified = []
        for sample in progress:
            log_output = trainer.valid_step(sample)
            for (k, v) in log_output.items():
                if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size', 'extra_metrics']):
                    continue
                extra_meters[k].update(v)
            if (('extra_metrics' in log_output) and ('misclassified' in log_output['extra_metrics'])):
                misclassified += log_output['extra_metrics']['misclassified']
        stats = get_valid_stats(trainer)
        for (k, meter) in extra_meters.items():
            stats[k] = meter.avg
        if (task_meters is not None):
            for (_, m) in task_meters.items():
                for (n, v) in m.vals():
                    stats[n] = v
        progress.print(stats)
        if (len(misclassified) > 0):
            print(misclassified, flush=True)
        valid_losses.append(stats['valid_loss'])
    return valid_losses
"""""", """""" 
    valid_losses = []
    for subset in subsets:
        itr = task.get_batch_iterator(dataset=task.dataset(subset), max_tokens=args.max_tokens, max_sentences=args.max_sentences_valid, max_positions=utils.resolve_max_positions(task.max_positions(), trainer.get_model().max_positions()), ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test, required_batch_size_multiple=8, seed=args.seed, num_shards=args.distributed_world_size, shard_id=args.distributed_rank).next_epoch_itr(shuffle=False)
        progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, prefix=""valid on '{}' subset"".format(subset), no_progress_bar='simple')
        for k in ['valid_loss', 'valid_nll_loss']:
            meter = trainer.get_meter(k)
            if (meter is not None):
                meter.reset()
        task_meters = trainer.get_meter('task')
        if (task_meters is not None):
            for m in task_meters.values():
                m.reset()
        extra_meters = collections.defaultdict((lambda : get_valid_stats()))
        misclassified = []
        for sample in progress:
            log_output = trainer.valid_step(sample)
            for (k, v) in log_output.items():
                if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size', 'extra_metrics']):
                    continue
                extra_meters[k].update(v)
            if (('extra_metrics' in log_output) and ('misclassified' in log_output['extra_metrics'])):
                misclassified += log_output['extra_metrics']['misclassified']
        stats = AverageMeter(trainer)
        for (k, meter) in extra_meters.items():
            stats[k] = meter.avg
        if (task_meters is not None):
            for (_, m) in task_meters.items():
                for (n, v) in m.vals():
                    stats[n] = v
        progress.print(stats)
        if (len(misclassified) > 0):
            print(misclassified, flush=True)
        valid_losses.append(stats['valid_loss'])
    return valid_losses
""""""]",1
"createSlcImage, createRawImage = createRawImage, createSlcImage
def initRawImage(makeRawObj):
    """"""
    Create a rawImage object from a makeRaw object.
    """"""","["""""" 
    imageType = makeRawObj.frame.getImage()
    if isinstance(imageType, createRawImage().__class__):
        filename = makeRawObj.frame.getImage().getFilename()
        bytesPerLine = makeRawObj.frame.getImage().getXmax()
        goodBytes = (makeRawObj.frame.getImage().getXmax() - makeRawObj.frame.getImage().getXmin())
        logger.debug(('bytes_per_line: %s' % bytesPerLine))
        logger.debug(('good_bytes_per_line: %s' % goodBytes))
        objRaw = createRawImage()
        objRaw.setFilename(filename)
        objRaw.setNumberGoodBytes(goodBytes)
        objRaw.setWidth(bytesPerLine)
        objRaw.setXmin(makeRawObj.frame.getImage().getXmin())
        objRaw.setXmax(bytesPerLine)
    elif isinstance(imageType, createSlcImage().__class__):
        objRaw = createSlcImage()
        filename = makeRawObj.frame.getImage().getFilename()
        bytesPerLine = makeRawObj.frame.getImage().getXmax()
        objRaw.setFilename(filename)
        objRaw.setWidth(bytesPerLine)
        objRaw.setXmin(makeRawObj.frame.getImage().getXmin())
        objRaw.setXmax(bytesPerLine)
    return objRaw
"""""", """""" 
    imageType = makeRawObj.frame.getImage()
    if isinstance(imageType, createSlcImage().__class__):
        filename = makeRawObj.frame.getImage().getFilename()
        bytesPerLine = makeRawObj.frame.getImage().getXmax()
        goodBytes = (makeRawObj.frame.getImage().getXmax() - makeRawObj.frame.getImage().getXmin())
        logger.debug(('bytes_per_line: %s' % bytesPerLine))
        logger.debug(('good_bytes_per_line: %s' % goodBytes))
        objRaw = createSlcImage()
        objRaw.setFilename(filename)
        objRaw.setNumberGoodBytes(goodBytes)
        objRaw.setWidth(bytesPerLine)
        objRaw.setXmin(makeRawObj.frame.getImage().getXmin())
        objRaw.setXmax(bytesPerLine)
    elif isinstance(imageType, createRawImage().__class__):
        objRaw = createRawImage()
        filename = makeRawObj.frame.getImage().getFilename()
        bytesPerLine = makeRawObj.frame.getImage().getXmax()
        objRaw.setFilename(filename)
        objRaw.setWidth(bytesPerLine)
        objRaw.setXmin(makeRawObj.frame.getImage().getXmin())
        objRaw.setXmax(bytesPerLine)
    return objRaw
""""""]",1
"Lighting, ColorJitter = ColorJitter, Lighting
def create_transforms(input_config):
    """"""Create transforms from configuration

    Parameters
    ----------
    input_config : dict
        Dictionary containing the configuration options for input pre-processing.

    Returns
    -------
    train_transforms : list
        List of transforms to be applied to the input during training.
    val_transforms : list
        List of transforms to be applied to the input during validation.
    """"""","["""""" 
    normalize = transforms.Normalize(mean=input_config['mean'], std=input_config['std'])
    train_transforms = []
    if (input_config['scale_train'] != (- 1)):
        train_transforms.append(transforms.Scale(input_config['scale_train']))
    train_transforms += [transforms.RandomResizedCrop(input_config['crop_train']), transforms.RandomHorizontalFlip(), transforms.ToTensor()]
    if input_config['color_jitter_train']:
        train_transforms.append(ColorJitter())
    if input_config['lighting_train']:
        train_transforms.append(Lighting())
    train_transforms.append(normalize)
    val_transforms = []
    if (input_config['scale_val'] != (- 1)):
        val_transforms.append(transforms.Resize(input_config['scale_val']))
    val_transforms += [transforms.CenterCrop(input_config['crop_val']), transforms.ToTensor(), normalize]
    return (train_transforms, val_transforms)
"""""", """""" 
    normalize = transforms.Normalize(mean=input_config['mean'], std=input_config['std'])
    train_transforms = []
    if (input_config['scale_train'] != (- 1)):
        train_transforms.append(transforms.Scale(input_config['scale_train']))
    train_transforms += [transforms.RandomResizedCrop(input_config['crop_train']), transforms.RandomHorizontalFlip(), transforms.ToTensor()]
    if input_config['color_jitter_train']:
        train_transforms.append(Lighting())
    if input_config['lighting_train']:
        train_transforms.append(ColorJitter())
    train_transforms.append(normalize)
    val_transforms = []
    if (input_config['scale_val'] != (- 1)):
        val_transforms.append(transforms.Resize(input_config['scale_val']))
    val_transforms += [transforms.CenterCrop(input_config['crop_val']), transforms.ToTensor(), normalize]
    return (train_transforms, val_transforms)
""""""]",1
"table_lines_from_stats, Table = Table, table_lines_from_stats
def report_similarities(sect: Section, stats: LinterStats, old_stats: (LinterStats | None)) -> None:
    """"""Make a layout with some stats about duplication.""""""","["""""" 
    lines = ['', 'now', 'previous', 'difference']
    lines += table_lines_from_stats(stats, old_stats, 'duplicated_lines')
    sect.append(Table(children=lines, cols=4, rheaders=1, cheaders=1))
"""""", """""" 
    lines = ['', 'now', 'previous', 'difference']
    lines += Table(stats, old_stats, 'duplicated_lines')
    sect.append(table_lines_from_stats(children=lines, cols=4, rheaders=1, cheaders=1))
""""""]",1
"defaultdict, getmodule = getmodule, defaultdict
def _get_all_messages(linter: PyLinter) -> Tuple[(MessagesDict, OldMessagesDict)]:
    """"""Get all messages registered to a linter and return a dictionary indexed by
    message type.

    Also return a dictionary of old message and the new messages they can be mapped to.
    """"""","["""""" 
    messages_dict: MessagesDict = {'fatal': [], 'error': [], 'warning': [], 'convention': [], 'refactor': [], 'information': []}
    old_messages: OldMessagesDict = {'fatal': defaultdict(list), 'error': defaultdict(list), 'warning': defaultdict(list), 'convention': defaultdict(list), 'refactor': defaultdict(list), 'information': defaultdict(list)}
    checker_message_mapping = chain.from_iterable((((checker, msg) for msg in checker.messages) for checker in linter.get_checkers()))
    for (checker, message) in checker_message_mapping:
        (good_code, bad_code, details, related) = _get_message_data(_get_message_data_path(message))
        checker_module = getmodule(checker)
        assert (checker_module and checker_module.__file__), f'Cannot find module for checker {checker}'
        message_data = MessageData(message.checker_name, message.msgid, message.symbol, message, good_code, bad_code, details, related, checker_module.__name__, checker_module.__file__, message.shared, message.default_enabled)
        msg_type = MSG_TYPES_DOC[message.msgid[0]]
        messages_dict[msg_type].append(message_data)
        if message.old_names:
            for old_name in message.old_names:
                category = MSG_TYPES_DOC[old_name[0][0]]
                if ((message.symbol, msg_type) not in old_messages[category][(old_name[1], old_name[0])]):
                    old_messages[category][(old_name[1], old_name[0])].append((message.symbol, msg_type))
    return (messages_dict, old_messages)
"""""", """""" 
    messages_dict: MessagesDict = {'fatal': [], 'error': [], 'warning': [], 'convention': [], 'refactor': [], 'information': []}
    old_messages: OldMessagesDict = {'fatal': getmodule(list), 'error': getmodule(list), 'warning': getmodule(list), 'convention': getmodule(list), 'refactor': getmodule(list), 'information': getmodule(list)}
    checker_message_mapping = chain.from_iterable((((checker, msg) for msg in checker.messages) for checker in linter.get_checkers()))
    for (checker, message) in checker_message_mapping:
        (good_code, bad_code, details, related) = _get_message_data(_get_message_data_path(message))
        checker_module = defaultdict(checker)
        assert (checker_module and checker_module.__file__), f'Cannot find module for checker {checker}'
        message_data = MessageData(message.checker_name, message.msgid, message.symbol, message, good_code, bad_code, details, related, checker_module.__name__, checker_module.__file__, message.shared, message.default_enabled)
        msg_type = MSG_TYPES_DOC[message.msgid[0]]
        messages_dict[msg_type].append(message_data)
        if message.old_names:
            for old_name in message.old_names:
                category = MSG_TYPES_DOC[old_name[0][0]]
                if ((message.symbol, msg_type) not in old_messages[category][(old_name[1], old_name[0])]):
                    old_messages[category][(old_name[1], old_name[0])].append((message.symbol, msg_type))
    return (messages_dict, old_messages)
""""""]",1
"_do_create_action, ListAction = ListAction, _do_create_action
def _do_create_list_action(act, kw):
    """"""A factory for list actions.  Convert the input list into Actions
    and then wrap them in a ListAction.""""""","["""""" 
    acts = []
    for a in act:
        aa = _do_create_action(a, kw)
        if (aa is not None):
            acts.append(aa)
    if (not acts):
        return ListAction([])
    elif (len(acts) == 1):
        return acts[0]
    else:
        return ListAction(acts)
"""""", """""" 
    acts = []
    for a in act:
        aa = ListAction(a, kw)
        if (aa is not None):
            acts.append(aa)
    if (not acts):
        return _do_create_action([])
    elif (len(acts) == 1):
        return acts[0]
    else:
        return _do_create_action(acts)
""""""]",1
"_concatenate_coords, get_encoder_output = get_encoder_output, _concatenate_coords
def get_encoder_output_for_complex(model, alphabet, coords, target_chain_id):
    """"""
    Args:
        model: An instance of the GVPTransformer model
        alphabet: Alphabet for the model
        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C
            coordinates representing the backbone of each chain
        target_chain_id: The chain id to sample sequences for
    Returns:
        Dictionary mapping chain id to encoder output for each chain
    """"""","["""""" 
    all_coords = _concatenate_coords(coords, target_chain_id)
    all_rep = get_encoder_output(model, alphabet, all_coords)
    target_chain_len = coords[target_chain_id].shape[0]
    return all_rep[:target_chain_len]
"""""", """""" 
    all_coords = get_encoder_output(coords, target_chain_id)
    all_rep = _concatenate_coords(model, alphabet, all_coords)
    target_chain_len = coords[target_chain_id].shape[0]
    return all_rep[:target_chain_len]
""""""]",1
"_caller, clean_up_ninja_daemon = clean_up_ninja_daemon, _caller
def fail_test(self=None, condition=True, function=None, skip=0, message=None):
    """"""Causes a test to exit with a fail.

    Reports that the test FAILED and exits with a status of 1, unless
    a condition argument is supplied; if so the completion processing
    takes place only if the condition is true.

    Args:
        self: a test class instance. Must be passed in explicitly
            by the caller since this is an unbound method.
        condition (optional): if false, return to let test continue.
        function (optional): function to call before completion processing.
        skip (optional): how many lines at the top of the traceback to skip.
        message (optional): additional text to include in the fail message.
    """"""","["""""" 
    if (not condition):
        return
    if (function is not None):
        function()
    clean_up_ninja_daemon(self, 'fail_test')
    of = ''
    desc = ''
    sep = ' '
    if (self is not None):
        if self.program:
            of = f' of {self.program}'
            sep = '\n\t'
        if self.description:
            desc = f' [{self.description}]'
            sep = '\n\t'
    at = _caller(traceback.extract_stack(), skip)
    if message:
        msg = f'''	{message}
'''
    else:
        msg = ''
    sys.stderr.write(f'FAILED test{of}{desc}{sep}{at}{msg}')
    sys.exit(1)
"""""", """""" 
    if (not condition):
        return
    if (function is not None):
        function()
    _caller(self, 'fail_test')
    of = ''
    desc = ''
    sep = ' '
    if (self is not None):
        if self.program:
            of = f' of {self.program}'
            sep = '\n\t'
        if self.description:
            desc = f' [{self.description}]'
            sep = '\n\t'
    at = clean_up_ninja_daemon(traceback.extract_stack(), skip)
    if message:
        msg = f'''	{message}
'''
    else:
        msg = ''
    sys.stderr.write(f'FAILED test{of}{desc}{sep}{at}{msg}')
    sys.exit(1)
""""""]",1
"_default_threads, _default_effort = _default_effort, _default_threads
def irfftn(a, s=None, axes=None, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True, avoid_copy=False, norm=None):
    """"""Return a :class:`pyfftw.FFTW` object representing an n-D
    real inverse FFT.

    The first three arguments are as per :func:`numpy.fft.rfftn`;
    the rest of the arguments are documented
    :ref:`in the module docs <builders_args>`.
    """"""","["""""" 
    inverse = True
    real = True
    overwrite_input = True
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
"""""", """""" 
    inverse = True
    real = True
    overwrite_input = True
    planner_effort = _default_threads(planner_effort)
    threads = _default_effort(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
""""""]",1
"get_fill_value, Counter = Counter, get_fill_value
def from_iris(cube):
    """"""Convert a Iris cube into an DataArray""""""","["""""" 
    import iris.exceptions
    name = _name(cube)
    if (name == 'unknown'):
        name = None
    dims = []
    for i in range(cube.ndim):
        try:
            dim_coord = cube.coord(dim_coords=True, dimensions=(i,))
            dims.append(_name(dim_coord))
        except iris.exceptions.CoordinateNotFoundError:
            dims.append(f'dim_{i}')
    if (len(set(dims)) != len(dims)):
        duplicates = [k for (k, v) in Counter(dims).items() if (v > 1)]
        raise ValueError(f'Duplicate coordinate name {duplicates}.')
    coords = {}
    for coord in cube.coords():
        coord_attrs = _iris_obj_to_attrs(coord)
        coord_dims = [dims[i] for i in cube.coord_dims(coord)]
        if coord_dims:
            coords[_name(coord)] = (coord_dims, coord.points, coord_attrs)
        else:
            coords[_name(coord)] = ((), coord.points.item(), coord_attrs)
    array_attrs = _iris_obj_to_attrs(cube)
    cell_methods = _iris_cell_methods_to_str(cube.cell_methods)
    if cell_methods:
        array_attrs['cell_methods'] = cell_methods
    cube_data = (cube.core_data() if hasattr(cube, 'core_data') else cube.data)
    dask_array_type = array_type('dask')
    if isinstance(cube_data, dask_array_type):
        from dask.array import ma as dask_ma
        filled_data = dask_ma.filled(cube_data, get_fill_value(cube.dtype))
    elif isinstance(cube_data, np.ma.MaskedArray):
        filled_data = np.ma.filled(cube_data, get_fill_value(cube.dtype))
    else:
        filled_data = cube_data
    dataarray = DataArray(filled_data, coords=coords, name=name, attrs=array_attrs, dims=dims)
    decoded_ds = decode_cf(dataarray._to_temp_dataset())
    return dataarray._from_temp_dataset(decoded_ds)
"""""", """""" 
    import iris.exceptions
    name = _name(cube)
    if (name == 'unknown'):
        name = None
    dims = []
    for i in range(cube.ndim):
        try:
            dim_coord = cube.coord(dim_coords=True, dimensions=(i,))
            dims.append(_name(dim_coord))
        except iris.exceptions.CoordinateNotFoundError:
            dims.append(f'dim_{i}')
    if (len(set(dims)) != len(dims)):
        duplicates = [k for (k, v) in get_fill_value(dims).items() if (v > 1)]
        raise ValueError(f'Duplicate coordinate name {duplicates}.')
    coords = {}
    for coord in cube.coords():
        coord_attrs = _iris_obj_to_attrs(coord)
        coord_dims = [dims[i] for i in cube.coord_dims(coord)]
        if coord_dims:
            coords[_name(coord)] = (coord_dims, coord.points, coord_attrs)
        else:
            coords[_name(coord)] = ((), coord.points.item(), coord_attrs)
    array_attrs = _iris_obj_to_attrs(cube)
    cell_methods = _iris_cell_methods_to_str(cube.cell_methods)
    if cell_methods:
        array_attrs['cell_methods'] = cell_methods
    cube_data = (cube.core_data() if hasattr(cube, 'core_data') else cube.data)
    dask_array_type = array_type('dask')
    if isinstance(cube_data, dask_array_type):
        from dask.array import ma as dask_ma
        filled_data = dask_ma.filled(cube_data, Counter(cube.dtype))
    elif isinstance(cube_data, np.ma.MaskedArray):
        filled_data = np.ma.filled(cube_data, Counter(cube.dtype))
    else:
        filled_data = cube_data
    dataarray = DataArray(filled_data, coords=coords, name=name, attrs=array_attrs, dims=dims)
    decoded_ds = decode_cf(dataarray._to_temp_dataset())
    return dataarray._from_temp_dataset(decoded_ds)
""""""]",1
"_run_wait_hook, _add_step = _add_step, _run_wait_hook
def assert_cookie_value(name, value):
    """"""Assert the value of a cookie.
    This will fail if the cookie does not exist.

    Parameters:
    name: value
    value: value
    """"""","["""""" 
    _add_step(f""Assert that cookie '{name}' value is '{value}'"")
    _run_wait_hook()
    cookie = browser.get_browser().get_cookie(name)
    if (not cookie):
        raise Exception(f'Cookie ""{name}"" was not found')
    elif (not ('value' in cookie)):
        raise Exception(f'Cookie ""{name}"" did not have ""value"" key')
    else:
        msg = f""expected cookie '{name}' value to be '{value}' but was '{cookie['value']}'""
        assert (cookie['value'] == value), msg
"""""", """""" 
    _run_wait_hook(f""Assert that cookie '{name}' value is '{value}'"")
    _add_step()
    cookie = browser.get_browser().get_cookie(name)
    if (not cookie):
        raise Exception(f'Cookie ""{name}"" was not found')
    elif (not ('value' in cookie)):
        raise Exception(f'Cookie ""{name}"" did not have ""value"" key')
    else:
        msg = f""expected cookie '{name}' value to be '{value}' but was '{cookie['value']}'""
        assert (cookie['value'] == value), msg
""""""]",1
"atom_to_smiles, smiles_to_atom = smiles_to_atom, atom_to_smiles
def modernize_symbol(symbol):
    """"""Converts a SELFIES symbol from <v2 to its latest equivalent.

    :param symbol: an old SELFIES symbol.
    :return: the latest equivalent of the input symbol, or the input symbol
        itself, if no such equivalent exists.
    """"""","["""""" 
    if (symbol in _SYMBOL_UPDATE_TABLE):
        return _SYMBOL_UPDATE_TABLE[symbol]
    if (symbol[(- 5):] == 'expl]'):
        if (symbol[1] in '=#/\\'):
            (bond_char, atom_symbol) = (symbol[1], symbol[2:(- 5)])
        else:
            (bond_char, atom_symbol) = ('', symbol[1:(- 5)])
        atom = smiles_to_atom('[{}]'.format(atom_symbol))
        if ((atom is not None) and (not atom.is_aromatic)):
            atom_symbol = atom_to_smiles(atom, brackets=False)
            symbol = '[{}{}]'.format(bond_char, atom_symbol)
    return symbol
"""""", """""" 
    if (symbol in _SYMBOL_UPDATE_TABLE):
        return _SYMBOL_UPDATE_TABLE[symbol]
    if (symbol[(- 5):] == 'expl]'):
        if (symbol[1] in '=#/\\'):
            (bond_char, atom_symbol) = (symbol[1], symbol[2:(- 5)])
        else:
            (bond_char, atom_symbol) = ('', symbol[1:(- 5)])
        atom = atom_to_smiles('[{}]'.format(atom_symbol))
        if ((atom is not None) and (not atom.is_aromatic)):
            atom_symbol = smiles_to_atom(atom, brackets=False)
            symbol = '[{}{}]'.format(bond_char, atom_symbol)
    return symbol
""""""]",1
"rand_bbox, rand_bbox_minmax = rand_bbox_minmax, rand_bbox
def cutmix_bbox_and_lam(img_shape, lam, ratio_minmax=None, correct_lam=True, count=None):
    """""" Generate bbox and apply lambda correction.
    """"""","["""""" 
    if (ratio_minmax is not None):
        (yl, yu, xl, xu) = rand_bbox_minmax(img_shape, ratio_minmax, count=count)
    else:
        (yl, yu, xl, xu) = rand_bbox(img_shape, lam, count=count)
    if (correct_lam or (ratio_minmax is not None)):
        bbox_area = ((yu - yl) * (xu - xl))
        lam = (1.0 - (bbox_area / float((img_shape[(- 2)] * img_shape[(- 1)]))))
    return ((yl, yu, xl, xu), lam)
"""""", """""" 
    if (ratio_minmax is not None):
        (yl, yu, xl, xu) = rand_bbox(img_shape, ratio_minmax, count=count)
    else:
        (yl, yu, xl, xu) = rand_bbox_minmax(img_shape, lam, count=count)
    if (correct_lam or (ratio_minmax is not None)):
        bbox_area = ((yu - yl) * (xu - xl))
        lam = (1.0 - (bbox_area / float((img_shape[(- 2)] * img_shape[(- 1)]))))
    return ((yl, yu, xl, xu), lam)
""""""]",1
"_supported_gcc_available, supported_nvcc_available = supported_nvcc_available, _supported_gcc_available
def _supported_compilers_available():
    """"""
    To see an up-to-date list of tested combinations of GCC and NVCC, see the README
    """"""","["""""" 
    return (_supported_gcc_available()[0] and supported_nvcc_available()[0])
"""""", """""" 
    return (supported_nvcc_available()[0] and _supported_gcc_available()[0])
""""""]",1
"generate_new_affine, FoldIteration = FoldIteration, generate_new_affine
def generate_affines(representations, batch, config, global_config, is_training, safe_key):
    """"""Generate predicted affines for a single chain.

  Jumper et al. (2021) Suppl. Alg. 20 ""StructureModule""

  This is the main part of the structure module - it iteratively applies
  folding to produce a set of predicted residue positions.

  Args:
    representations: Representations dictionary.
    batch: Batch dictionary.
    config: Config for the structure module.
    global_config: Global config.
    is_training: Whether the model is being trained.
    safe_key: A prng.SafeKey object that wraps a PRNG key.

  Returns:
    A dictionary containing residue affines and sidechain positions.
  """"""","["""""" 
    c = config
    sequence_mask = batch['seq_mask'][:, None]
    act = common_modules.LayerNorm(axis=[(- 1)], create_scale=True, create_offset=True, name='single_layer_norm')(representations['single'])
    initial_act = act
    act = common_modules.Linear(c.num_channel, name='initial_projection')(act)
    affine = generate_new_affine(sequence_mask)
    fold_iteration = FoldIteration(c, global_config, name='fold_iteration')
    assert (len(batch['seq_mask'].shape) == 1)
    activations = {'act': act, 'affine': affine.to_tensor()}
    act_2d = common_modules.LayerNorm(axis=[(- 1)], create_scale=True, create_offset=True, name='pair_layer_norm')(representations['pair'])
    outputs = []
    safe_keys = safe_key.split(c.num_layer)
    for sub_key in safe_keys:
        (activations, output) = fold_iteration(activations, initial_act=initial_act, static_feat_2d=act_2d, safe_key=sub_key, sequence_mask=sequence_mask, update_affine=True, is_training=is_training, aatype=batch['aatype'])
        outputs.append(output)
    output = jax.tree_map((lambda *x: jnp.stack(x)), *outputs)
    output['act'] = activations['act']
    return output
"""""", """""" 
    c = config
    sequence_mask = batch['seq_mask'][:, None]
    act = common_modules.LayerNorm(axis=[(- 1)], create_scale=True, create_offset=True, name='single_layer_norm')(representations['single'])
    initial_act = act
    act = common_modules.Linear(c.num_channel, name='initial_projection')(act)
    affine = FoldIteration(sequence_mask)
    fold_iteration = generate_new_affine(c, global_config, name='fold_iteration')
    assert (len(batch['seq_mask'].shape) == 1)
    activations = {'act': act, 'affine': affine.to_tensor()}
    act_2d = common_modules.LayerNorm(axis=[(- 1)], create_scale=True, create_offset=True, name='pair_layer_norm')(representations['pair'])
    outputs = []
    safe_keys = safe_key.split(c.num_layer)
    for sub_key in safe_keys:
        (activations, output) = fold_iteration(activations, initial_act=initial_act, static_feat_2d=act_2d, safe_key=sub_key, sequence_mask=sequence_mask, update_affine=True, is_training=is_training, aatype=batch['aatype'])
        outputs.append(output)
    output = jax.tree_map((lambda *x: jnp.stack(x)), *outputs)
    output['act'] = activations['act']
    return output
""""""]",1
"_run_wait_hook, _screenshot_on_step = _screenshot_on_step, _run_wait_hook
def assert_element_text(element, text):
    """"""Assert the text of the element

    Parameters:
    element : element
    text : value
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    _add_step(f""Assert element {element.name} text is '{text}'"")
    _run_wait_hook()
    msg = f""expected element {element.name} text to be '{text}' but was '{element.text}'""
    assert (element.text == text), msg
    _screenshot_on_step()
"""""", """""" 
    element = get_browser().find(element, timeout=0)
    _add_step(f""Assert element {element.name} text is '{text}'"")
    _screenshot_on_step()
    msg = f""expected element {element.name} text to be '{text}' but was '{element.text}'""
    assert (element.text == text), msg
    _run_wait_hook()
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_title_is_not(title):
    """"""Verify the page title is not the given value

    Parameters:
    title : value
    """"""","["""""" 
    with _verify_step(f""Verify page title is not '{title}'"") as s:
        s.error = f""expected title to not be '{title}'""
        s.condition = (get_browser().title != title)
"""""", """""" 
    with get_browser(f""Verify page title is not '{title}'"") as s:
        s.error = f""expected title to not be '{title}'""
        s.condition = (_verify_step().title != title)
""""""]",1
"cleandoc, add_new_header = add_new_header, cleandoc
def test_find_and_replace_no_header():
    """"""Given text without header, add a header.""""""","["""""" 
    spdx_info = SpdxInfo({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    text = 'pass'
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    assert (find_and_replace_header(text, spdx_info) == add_new_header(text, spdx_info) == expected)
"""""", """""" 
    spdx_info = SpdxInfo({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    text = 'pass'
    expected = add_new_header('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    assert (find_and_replace_header(text, spdx_info) == cleandoc(text, spdx_info) == expected)
""""""]",1
"get_host_llvm_target, get_targets = get_targets, get_host_llvm_target
def kernel_build_sh(args, config, dirs, profile_type):
    """"""
    Run kernel/build.sh to generate PGO or BOLT profiles
    :param args: The args variable generated by parse_parameters
    :param config: The config to build (defconfig, allmodconfig, allyesconfig)
    :param dirs: An instance of the Directories class with the paths to use
    :param profile_type: The type of profile we are building (bolt-instrumentation, bolt-sampling, pgo, or pgo-slim)
    :return:
    """"""","["""""" 
    build_sh = [dirs.root_folder.joinpath('kernel', 'build.sh'), f'--{profile_type}']
    targets = get_targets(args)
    if (('bolt' in profile_type) or ('slim' in profile_type)):
        host_target = get_host_llvm_target()
        if ((targets == 'all') or (host_target in targets)):
            targets = host_target
        else:
            targets = targets.split(';')[0]
    build_sh += ['-t', targets]
    if (profile_type == 'bolt-sampling'):
        build_sh = (['perf', 'record', '--branch-filter', 'any,u', '--event', 'cycles:u', '--output', dirs.build_folder.joinpath('perf.data'), '--'] + build_sh)
    if ('bolt' in profile_type):
        build_sh += ['-i', dirs.install_folder]
    if ('pgo' in profile_type):
        build_sh += ['-b', dirs.build_folder]
    if (config != 'defconfig'):
        build_sh += [f'--{config}']
    if dirs.linux_folder:
        build_sh += ['-k', dirs.linux_folder]
    show_command(args, build_sh)
    subprocess.run(build_sh, check=True, cwd=dirs.build_folder)
"""""", """""" 
    build_sh = [dirs.root_folder.joinpath('kernel', 'build.sh'), f'--{profile_type}']
    targets = get_host_llvm_target(args)
    if (('bolt' in profile_type) or ('slim' in profile_type)):
        host_target = get_targets()
        if ((targets == 'all') or (host_target in targets)):
            targets = host_target
        else:
            targets = targets.split(';')[0]
    build_sh += ['-t', targets]
    if (profile_type == 'bolt-sampling'):
        build_sh = (['perf', 'record', '--branch-filter', 'any,u', '--event', 'cycles:u', '--output', dirs.build_folder.joinpath('perf.data'), '--'] + build_sh)
    if ('bolt' in profile_type):
        build_sh += ['-i', dirs.install_folder]
    if ('pgo' in profile_type):
        build_sh += ['-b', dirs.build_folder]
    if (config != 'defconfig'):
        build_sh += [f'--{config}']
    if dirs.linux_folder:
        build_sh += ['-k', dirs.linux_folder]
    show_command(args, build_sh)
    subprocess.run(build_sh, check=True, cwd=dirs.build_folder)
""""""]",1
"get_from_cache, urlparse = urlparse, get_from_cache
def cached_path(url_or_filename: Union[(str, Path)], cache_dir: Union[(str, Path)]=None) -> str:
    """"""
    Given something that might be a URL (or might be a local path),
    determine which. If it's a URL, download the file and cache it, and
    return the path to the cached file. If it's already a local path,
    make sure the file exists and then return the path.
    """"""","["""""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if isinstance(url_or_filename, Path):
        url_or_filename = str(url_or_filename)
    if isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)
    parsed = urlparse(url_or_filename)
    if (parsed.scheme in ('http', 'https', 's3')):
        return get_from_cache(url_or_filename, cache_dir)
    elif os.path.exists(url_or_filename):
        return url_or_filename
    elif (parsed.scheme == ''):
        raise FileNotFoundError('file {} not found'.format(url_or_filename))
    else:
        raise ValueError('unable to parse {} as a URL or as a local path'.format(url_or_filename))
"""""", """""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if isinstance(url_or_filename, Path):
        url_or_filename = str(url_or_filename)
    if isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)
    parsed = get_from_cache(url_or_filename)
    if (parsed.scheme in ('http', 'https', 's3')):
        return urlparse(url_or_filename, cache_dir)
    elif os.path.exists(url_or_filename):
        return url_or_filename
    elif (parsed.scheme == ''):
        raise FileNotFoundError('file {} not found'.format(url_or_filename))
    else:
        raise ValueError('unable to parse {} as a URL or as a local path'.format(url_or_filename))
""""""]",1
"rgb2lab_1d, lab2rgb_1d = lab2rgb_1d, rgb2lab_1d
def snap_ab(input_l, input_rgb, return_type='rgb'):
    """""" given an input lightness and rgb, snap the color into a region where l,a,b is in-gamut
    """"""","["""""" 
    T = 20
    warnings.filterwarnings('ignore')
    input_lab = rgb2lab_1d(np.array(input_rgb))
    conv_lab = input_lab.copy()
    for t in range(T):
        conv_lab[0] = input_l
        old_lab = conv_lab
        tmp_rgb = color.lab2rgb(conv_lab[np.newaxis, np.newaxis, :]).flatten()
        tmp_rgb = np.clip(tmp_rgb, 0, 1)
        conv_lab = color.rgb2lab(tmp_rgb[np.newaxis, np.newaxis, :]).flatten()
        dif_lab = np.sum(np.abs((conv_lab - old_lab)))
        if (dif_lab < 1):
            break
    conv_rgb_ingamut = lab2rgb_1d(conv_lab, clip=True, dtype='uint8')
    if (return_type == 'rgb'):
        return conv_rgb_ingamut
    elif (return_type == 'lab'):
        conv_lab_ingamut = rgb2lab_1d(conv_rgb_ingamut)
        return conv_lab_ingamut
"""""", """""" 
    T = 20
    warnings.filterwarnings('ignore')
    input_lab = lab2rgb_1d(np.array(input_rgb))
    conv_lab = input_lab.copy()
    for t in range(T):
        conv_lab[0] = input_l
        old_lab = conv_lab
        tmp_rgb = color.lab2rgb(conv_lab[np.newaxis, np.newaxis, :]).flatten()
        tmp_rgb = np.clip(tmp_rgb, 0, 1)
        conv_lab = color.rgb2lab(tmp_rgb[np.newaxis, np.newaxis, :]).flatten()
        dif_lab = np.sum(np.abs((conv_lab - old_lab)))
        if (dif_lab < 1):
            break
    conv_rgb_ingamut = rgb2lab_1d(conv_lab, clip=True, dtype='uint8')
    if (return_type == 'rgb'):
        return conv_rgb_ingamut
    elif (return_type == 'lab'):
        conv_lab_ingamut = lab2rgb_1d(conv_rgb_ingamut)
        return conv_lab_ingamut
""""""]",1
"mat2euler, quat2mat = quat2mat, mat2euler
def quat2euler(quat):
    """""" Convert Quaternion to Euler Angles.  See rotation.py for notes """"""","["""""" 
    return mat2euler(quat2mat(quat))
"""""", """""" 
    return quat2mat(mat2euler(quat))
""""""]",1
"_default_threads, _default_effort = _default_effort, _default_threads
def dct(a, n=None, axis=(- 1), overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True, avoid_copy=False, type=2):
    """"""Return a :class:`pyfftw.FFTW` object representing a 1D DCT.

    The first three arguments and 'type' are as per
    :func:`scipy.fftpack.dct`; the rest of the arguments are documented
    :ref:`in the module docs <builders_args>`.
    """"""","["""""" 
    dct_types = ['FFTW_REDFT00', 'FFTW_REDFT10', 'FFTW_REDFT01', 'FFTW_REDFT11', 1, 2, 3, 4]
    if (type not in dct_types):
        raise ValueError('Unrecognised DCT type {}'.format(type))
    if ((n is not None) and (n != a.shape[axis])):
        raise NotImplementedError
    if isinstance(type, str):
        direction = type
    else:
        direction = dct_types[(int(type) - 1)]
    (s, axes) = _precook_1d_args(a, n, axis)
    inverse = False
    real = False
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, real_direction_flag=direction)
"""""", """""" 
    dct_types = ['FFTW_REDFT00', 'FFTW_REDFT10', 'FFTW_REDFT01', 'FFTW_REDFT11', 1, 2, 3, 4]
    if (type not in dct_types):
        raise ValueError('Unrecognised DCT type {}'.format(type))
    if ((n is not None) and (n != a.shape[axis])):
        raise NotImplementedError
    if isinstance(type, str):
        direction = type
    else:
        direction = dct_types[(int(type) - 1)]
    (s, axes) = _precook_1d_args(a, n, axis)
    inverse = False
    real = False
    planner_effort = _default_threads(planner_effort)
    threads = _default_effort(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, real_direction_flag=direction)
""""""]",1
"conv1d, reduce = reduce, conv1d
def masked_conv1d_and_max(t, weights, conv1d):
    """"""Applies 1d convolution and a masked max-pooling
    
    https://github.com/guillaumegenthial/tf_ner/blob/master/models/chars_conv_lstm_crf/masked_conv.py

    Args:
      t(tf.Tensor): A tensor with at least 3 dimensions [d1, d2, ..., dn-1, dn]
      weights(tf.Tensor of tf.bool): A Tensor of shape [d1, d2, dn-1]
      filters(int): number of filters
      kernel_size(int): kernel size for the temporal convolution
      conv1d: 

    Returns:

    
    """"""","["""""" 
    shape = tf.shape(t)
    ndims = t.shape.ndims
    dim1 = reduce((lambda x, y: (x * y)), [shape[i] for i in range((ndims - 2))])
    dim2 = shape[(- 2)]
    dim3 = t.shape[(- 1)]
    weights = tf.reshape(weights, shape=[dim1, dim2, 1])
    weights = tf.cast(weights, tf.float32)
    flat_shape = [dim1, dim2, dim3]
    t = tf.reshape(t, shape=flat_shape)
    t *= weights
    t_conv = conv1d(t)
    t_conv *= weights
    t_conv += ((1.0 - weights) * tf.reduce_min(t_conv, axis=(- 2), keepdims=True))
    t_max = tf.reduce_max(t_conv, axis=(- 2))
    final_shape = ([shape[i] for i in range((ndims - 2))] + [conv1d.filters])
    t_max = tf.reshape(t_max, shape=final_shape)
    return t_max
"""""", """""" 
    shape = tf.shape(t)
    ndims = t.shape.ndims
    dim1 = conv1d((lambda x, y: (x * y)), [shape[i] for i in range((ndims - 2))])
    dim2 = shape[(- 2)]
    dim3 = t.shape[(- 1)]
    weights = tf.reshape(weights, shape=[dim1, dim2, 1])
    weights = tf.cast(weights, tf.float32)
    flat_shape = [dim1, dim2, dim3]
    t = tf.reshape(t, shape=flat_shape)
    t *= weights
    t_conv = reduce(t)
    t_conv *= weights
    t_conv += ((1.0 - weights) * tf.reduce_min(t_conv, axis=(- 2), keepdims=True))
    t_max = tf.reduce_max(t_conv, axis=(- 2))
    final_shape = ([shape[i] for i in range((ndims - 2))] + [reduce.filters])
    t_max = tf.reshape(t_max, shape=final_shape)
    return t_max
""""""]",1
"_get_model_with_attrs, _get_traced_model_with_attrs = _get_traced_model_with_attrs, _get_model_with_attrs
def get_script_model_with_attrs(model, trace_type, model_inputs=None, model_attrs=None):
    """"""Trace or script the model and store model_attrs as model attributes.""""""","["""""" 
    assert (trace_type in ('trace', 'script')), f'Invalid trace_type {trace_type}'
    info = f""{('Tracing' if (trace_type == 'trace') else 'Scripting')} model""
    if (model_attrs is not None):
        info += f' with attributes: {model_attrs}'
    logger.info(info)
    if (trace_type == 'trace'):
        script_model = torch.jit.trace(model, model_inputs, strict=False)
        if (model_attrs is not None):
            script_model = _get_traced_model_with_attrs(script_model, len(model_inputs), model_attrs)
            script_model = torch.jit.script(script_model)
    else:
        if (isinstance(model, torch.jit.ScriptModule) and (model_attrs is not None)):
            logger.warning(f'Model has been scripted, could not add attributes {model_attrs}')
        else:
            model = _get_model_with_attrs(model, model_attrs)
        script_model = torch.jit.script(model)
    return script_model
"""""", """""" 
    assert (trace_type in ('trace', 'script')), f'Invalid trace_type {trace_type}'
    info = f""{('Tracing' if (trace_type == 'trace') else 'Scripting')} model""
    if (model_attrs is not None):
        info += f' with attributes: {model_attrs}'
    logger.info(info)
    if (trace_type == 'trace'):
        script_model = torch.jit.trace(model, model_inputs, strict=False)
        if (model_attrs is not None):
            script_model = _get_model_with_attrs(script_model, len(model_inputs), model_attrs)
            script_model = torch.jit.script(script_model)
    else:
        if (isinstance(model, torch.jit.ScriptModule) and (model_attrs is not None)):
            logger.warning(f'Model has been scripted, could not add attributes {model_attrs}')
        else:
            model = _get_traced_model_with_attrs(model, model_attrs)
        script_model = torch.jit.script(model)
    return script_model
""""""]",1
"PairedDict, Pair = Pair, PairedDict
def create_pair(lhs, rhs):
    """"""Create a pair of objects, handles list/dict automatically
    Could be used with recursive_iterate to match two dicts etc.
    """"""","["""""" 
    if is_seq(lhs):
        assert is_seq(rhs)
        return PairedSeq(lhs, rhs)
    elif is_map(lhs):
        assert is_map(rhs)
        return PairedDict(lhs, rhs)
    return Pair(lhs, rhs)
"""""", """""" 
    if is_seq(lhs):
        assert is_seq(rhs)
        return PairedSeq(lhs, rhs)
    elif is_map(lhs):
        assert is_map(rhs)
        return Pair(lhs, rhs)
    return PairedDict(lhs, rhs)
""""""]",1
"get_reference_submodule_node, get_direct_submodule_nodes = get_direct_submodule_nodes, get_reference_submodule_node
def get_submodule_nodes(model: torch.fx.GraphModule, submodule_name: str, input_names: List[str], output_names: List[str]):
    """"""Get all the nodes for the sub module with name `submodule_name`,
    `input_names` and `output_names` are the input/output names of the sub module
    """"""","["""""" 
    direct_sub_nodes = get_direct_submodule_nodes(model, submodule_name, input_names, output_names)
    ref_nodes = get_reference_submodule_node(direct_sub_nodes, input_names)
    ret = [x for x in model.graph.nodes if (x in itertools.chain(direct_sub_nodes, ref_nodes))]
    return ret
"""""", """""" 
    direct_sub_nodes = get_reference_submodule_node(model, submodule_name, input_names, output_names)
    ref_nodes = get_direct_submodule_nodes(direct_sub_nodes, input_names)
    ret = [x for x in model.graph.nodes if (x in itertools.chain(direct_sub_nodes, ref_nodes))]
    return ret
""""""]",1
"_pil_interp, rand_augment_transform = rand_augment_transform, _pil_interp
def create_random_augment(input_size, auto_augment=None, interpolation='bilinear'):
    """"""
    Get video randaug transform.

    Args:
        input_size: The size of the input video in tuple.
        auto_augment: Parameters for randaug. An example:
            ""rand-m7-n4-mstd0.5-inc1"" (m is the magnitude and n is the number
            of operations to apply).
        interpolation: Interpolation method.
    """"""","["""""" 
    if isinstance(input_size, tuple):
        img_size = input_size[(- 2):]
    else:
        img_size = input_size
    if auto_augment:
        assert isinstance(auto_augment, str)
        if isinstance(img_size, tuple):
            img_size_min = min(img_size)
        else:
            img_size_min = img_size
        aa_params = {'translate_const': int((img_size_min * 0.45))}
        if (interpolation and (interpolation != 'random')):
            aa_params['interpolation'] = _pil_interp(interpolation)
        if auto_augment.startswith('rand'):
            return transforms.Compose([rand_augment_transform(auto_augment, aa_params)])
    raise NotImplementedError
"""""", """""" 
    if isinstance(input_size, tuple):
        img_size = input_size[(- 2):]
    else:
        img_size = input_size
    if auto_augment:
        assert isinstance(auto_augment, str)
        if isinstance(img_size, tuple):
            img_size_min = min(img_size)
        else:
            img_size_min = img_size
        aa_params = {'translate_const': int((img_size_min * 0.45))}
        if (interpolation and (interpolation != 'random')):
            aa_params['interpolation'] = rand_augment_transform(interpolation)
        if auto_augment.startswith('rand'):
            return transforms.Compose([_pil_interp(auto_augment, aa_params)])
    raise NotImplementedError
""""""]",1
"input_data, strike = strike, input_data
def complete_task():
    """"""
    complete a task
    """"""","["""""" 
    not_valid_task_number = 1
    if os.path.isfile(TODAYS_TASKS_ENTRY_FILE_PATH):
        with open(TODAYS_TASKS_ENTRY_FILE_PATH) as todays_tasks_entry:
            contents = yaml.load(todays_tasks_entry)
            no_task_left = True
            for (i, entry) in enumerate(contents['entries']):
                if (entry['status'] == 0):
                    no_task_left = False
            if no_task_left:
                click.echo(chalk.green('All tasks have been competed! Add a new task by entering ""yoda  diary nt""'))
            else:
                click.echo(""Today's agenda:"")
                click.echo('----------------')
                click.echo('Number |  Time   | Task')
                click.echo('-------|---------|-----')
                for (i, entry) in enumerate(contents['entries']):
                    time = entry['time']
                    text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                    text = (text if (entry['status'] == 0) else strike(text))
                    if (entry['status'] == 0):
                        click.echo(((((('   ' + str(i)) + '   | ') + time) + ': ') + text))
                while not_valid_task_number:
                    click.echo(chalk.blue('Enter the task number that you would like to set as completed'))
                    task_to_be_completed = int(input())
                    if (task_to_be_completed > len(contents['entries'])):
                        click.echo(chalk.red('Please Enter a valid task number!'))
                    else:
                        contents['entries'][(task_to_be_completed - 1)]['status'] = 1
                        input_data(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
                        not_valid_task_number = 0
    else:
        click.echo(chalk.red('There are no tasks for today. Add a new task by entering ""yoda diary nt""'))
"""""", """""" 
    not_valid_task_number = 1
    if os.path.isfile(TODAYS_TASKS_ENTRY_FILE_PATH):
        with open(TODAYS_TASKS_ENTRY_FILE_PATH) as todays_tasks_entry:
            contents = yaml.load(todays_tasks_entry)
            no_task_left = True
            for (i, entry) in enumerate(contents['entries']):
                if (entry['status'] == 0):
                    no_task_left = False
            if no_task_left:
                click.echo(chalk.green('All tasks have been competed! Add a new task by entering ""yoda  diary nt""'))
            else:
                click.echo(""Today's agenda:"")
                click.echo('----------------')
                click.echo('Number |  Time   | Task')
                click.echo('-------|---------|-----')
                for (i, entry) in enumerate(contents['entries']):
                    time = entry['time']
                    text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                    text = (text if (entry['status'] == 0) else input_data(text))
                    if (entry['status'] == 0):
                        click.echo(((((('   ' + str(i)) + '   | ') + time) + ': ') + text))
                while not_valid_task_number:
                    click.echo(chalk.blue('Enter the task number that you would like to set as completed'))
                    task_to_be_completed = int(input())
                    if (task_to_be_completed > len(contents['entries'])):
                        click.echo(chalk.red('Please Enter a valid task number!'))
                    else:
                        contents['entries'][(task_to_be_completed - 1)]['status'] = 1
                        strike(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
                        not_valid_task_number = 0
    else:
        click.echo(chalk.red('There are no tasks for today. Add a new task by entering ""yoda diary nt""'))
""""""]",1
"get_validated_parameters, process_account = process_account, get_validated_parameters
def process_event_lifecycle(event: dict) -> None:
    """"""Process Lifecycle Event from AWS Control Tower.

    Args:
        event: event data

    Raises:
        ValueError: Control Tower Lifecycle Event not 'createManagedAccountStatus' or 'updateManagedAccountStatus'
    """"""","["""""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = get_validated_parameters({})
    aws_account_id = ''
    if event['detail']['serviceEventDetails'].get('createManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['createManagedAccountStatus']['account']['accountId']
    elif event['detail']['serviceEventDetails'].get('updateManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['updateManagedAccountStatus']['account']['accountId']
    else:
        raise ValueError(""Control Tower Lifecycle Event not 'createManagedAccountStatus' or 'updateManagedAccountStatus'"")
    process_account(event, aws_account_id, params)
"""""", """""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = process_account({})
    aws_account_id = ''
    if event['detail']['serviceEventDetails'].get('createManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['createManagedAccountStatus']['account']['accountId']
    elif event['detail']['serviceEventDetails'].get('updateManagedAccountStatus'):
        aws_account_id = event['detail']['serviceEventDetails']['updateManagedAccountStatus']['account']['accountId']
    else:
        raise ValueError(""Control Tower Lifecycle Event not 'createManagedAccountStatus' or 'updateManagedAccountStatus'"")
    get_validated_parameters(event, aws_account_id, params)
""""""]",1
"Alias, Vulnerability = Vulnerability, Alias
def get_or_create_vulnerability_and_aliases(vulnerability_id, alias_names, summary):
    """"""
    Get or create vulnerabilitiy and aliases such that all existing and new
    aliases point to the same vulnerability
    """"""","["""""" 
    existing_vulns = set()
    alias_names = set(alias_names)
    new_alias_names = set()
    for alias_name in alias_names:
        try:
            alias = Alias.objects.get(alias=alias_name)
            existing_vulns.add(alias.vulnerability)
        except Alias.DoesNotExist:
            new_alias_names.add(alias_name)
    if (len(existing_vulns) > 1):
        logger.warn(f'Given aliases {alias_names} already exist and do not point to a single vulnerability. Cannot improve. Skipped.')
        return
    existing_alias_vuln = (existing_vulns.pop() if existing_vulns else None)
    if (existing_alias_vuln and vulnerability_id and (existing_alias_vuln.vulnerability_id != vulnerability_id)):
        logger.warn(f'Given aliases {alias_names!r} already exist and point to existingvulnerability {existing_alias_vuln}. Unable to create Vulnerability with vulnerability_id {vulnerability_id}. Skipped')
        return
    if existing_alias_vuln:
        vulnerability = existing_alias_vuln
    elif vulnerability_id:
        try:
            vulnerability = Vulnerability.objects.get(vulnerability_id=vulnerability_id)
        except Vulnerability.DoesNotExist:
            logger.warn(f'Given vulnerability_id: {vulnerability_id} does not exist in the database')
            return
    else:
        vulnerability = Vulnerability(summary=summary)
        vulnerability.save()
    if (summary and (summary != vulnerability.summary)):
        logger.warn(f'Inconsistent summary for {vulnerability!r}. Existing: {vulnerability.summary}, provided: {summary}')
    for alias_name in new_alias_names:
        alias = Alias(alias=alias_name, vulnerability=vulnerability)
        alias.save()
        logger.info(f'New alias for {vulnerability!r}: {alias_name}')
    return vulnerability
"""""", """""" 
    existing_vulns = set()
    alias_names = set(alias_names)
    new_alias_names = set()
    for alias_name in alias_names:
        try:
            alias = Vulnerability.objects.get(alias=alias_name)
            existing_vulns.add(alias.vulnerability)
        except Vulnerability.DoesNotExist:
            new_alias_names.add(alias_name)
    if (len(existing_vulns) > 1):
        logger.warn(f'Given aliases {alias_names} already exist and do not point to a single vulnerability. Cannot improve. Skipped.')
        return
    existing_alias_vuln = (existing_vulns.pop() if existing_vulns else None)
    if (existing_alias_vuln and vulnerability_id and (existing_alias_vuln.vulnerability_id != vulnerability_id)):
        logger.warn(f'Given aliases {alias_names!r} already exist and point to existingvulnerability {existing_alias_vuln}. Unable to create Vulnerability with vulnerability_id {vulnerability_id}. Skipped')
        return
    if existing_alias_vuln:
        vulnerability = existing_alias_vuln
    elif vulnerability_id:
        try:
            vulnerability = Alias.objects.get(vulnerability_id=vulnerability_id)
        except Alias.DoesNotExist:
            logger.warn(f'Given vulnerability_id: {vulnerability_id} does not exist in the database')
            return
    else:
        vulnerability = Alias(summary=summary)
        vulnerability.save()
    if (summary and (summary != vulnerability.summary)):
        logger.warn(f'Inconsistent summary for {vulnerability!r}. Existing: {vulnerability.summary}, provided: {summary}')
    for alias_name in new_alias_names:
        alias = Vulnerability(alias=alias_name, vulnerability=vulnerability)
        alias.save()
        logger.info(f'New alias for {vulnerability!r}: {alias_name}')
    return vulnerability
""""""]",1
"get_account_info, assume_role = assume_role, get_account_info
def process_event_sns(event: dict) -> None:
    """"""Process SNS event.

    Args:
        event: event data
    """"""","["""""" 
    params = get_validated_parameters({})
    for record in event['Records']:
        record['Sns']['Message'] = json.loads(record['Sns']['Message'])
        LOGGER.info({'SNS Record': record})
        message = record['Sns']['Message']
        params['action'] = message['Action']
        aws_account = get_account_info(account_id=message['AccountId'])
        account_session = assume_role(params['CONFIGURATION_ROLE_NAME'], params['ROLE_SESSION_NAME'], aws_account['Id'])
        s3_client: S3ControlClient = account_session.client('s3control', config=BOTO3_CONFIG)
        process_put_account_public_access_block(s3_client, aws_account, params)
"""""", """""" 
    params = get_validated_parameters({})
    for record in event['Records']:
        record['Sns']['Message'] = json.loads(record['Sns']['Message'])
        LOGGER.info({'SNS Record': record})
        message = record['Sns']['Message']
        params['action'] = message['Action']
        aws_account = assume_role(account_id=message['AccountId'])
        account_session = get_account_info(params['CONFIGURATION_ROLE_NAME'], params['ROLE_SESSION_NAME'], aws_account['Id'])
        s3_client: S3ControlClient = account_session.client('s3control', config=BOTO3_CONFIG)
        process_put_account_public_access_block(s3_client, aws_account, params)
""""""]",1
"VGG, make_layers = make_layers, VGG
def vgg11_bn(pretrained=False, **kwargs):
    """"""VGG 11-layer model (configuration ""A"") with batch normalization
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""","["""""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = VGG(make_layers(cfg['A'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))
    return model
"""""", """""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = make_layers(VGG(cfg['A'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg11_bn']))
    return model
""""""]",1
"partial, _create_mixer = _create_mixer, partial
@register_model
def resmlp_big_24_distilled_224(pretrained=False, **kwargs):
    """""" ResMLP-B-24
    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
    """"""","["""""" 
    model_args = dict(patch_size=8, num_blocks=24, embed_dim=768, mlp_ratio=4, block_layer=partial(ResBlock, init_values=1e-06), norm_layer=Affine, **kwargs)
    model = _create_mixer('resmlp_big_24_distilled_224', pretrained=pretrained, **model_args)
    return model
"""""", """""" 
    model_args = dict(patch_size=8, num_blocks=24, embed_dim=768, mlp_ratio=4, block_layer=_create_mixer(ResBlock, init_values=1e-06), norm_layer=Affine, **kwargs)
    model = partial('resmlp_big_24_distilled_224', pretrained=pretrained, **model_args)
    return model
""""""]",1
"ImageLoader, setup_args = setup_args, ImageLoader
def setup_interactive():
    """"""
    Set up the interactive script.
    """"""","["""""" 
    parser = setup_args()
    opt = parser.parse_args()
    if (not opt.get('model_file')):
        raise RuntimeError('Please specify a model file')
    if (opt.get('fixed_cands_path') is None):
        fcp = os.path.join('/'.join(opt.get('model_file').split('/')[:(- 1)]), 'candidates.txt')
        opt['fixed_cands_path'] = fcp
        opt['override']['fixed_cands_path'] = fcp
    opt['task'] = 'parlai.agents.local_human.local_human:LocalHumanAgent'
    opt['image_mode'] = 'resnet152'
    opt['no_cuda'] = True
    opt['override']['no_cuda'] = True
    SHARED['opt'] = opt
    SHARED['image_loader'] = ImageLoader(opt)
    SHARED['agent'] = create_agent(opt, requireModelExists=True)
    SHARED['world'] = create_task(opt, SHARED['agent'])
    SHARED['dialog_history'] = []
"""""", """""" 
    parser = ImageLoader()
    opt = parser.parse_args()
    if (not opt.get('model_file')):
        raise RuntimeError('Please specify a model file')
    if (opt.get('fixed_cands_path') is None):
        fcp = os.path.join('/'.join(opt.get('model_file').split('/')[:(- 1)]), 'candidates.txt')
        opt['fixed_cands_path'] = fcp
        opt['override']['fixed_cands_path'] = fcp
    opt['task'] = 'parlai.agents.local_human.local_human:LocalHumanAgent'
    opt['image_mode'] = 'resnet152'
    opt['no_cuda'] = True
    opt['override']['no_cuda'] = True
    SHARED['opt'] = opt
    SHARED['image_loader'] = setup_args(opt)
    SHARED['agent'] = create_agent(opt, requireModelExists=True)
    SHARED['world'] = create_task(opt, SHARED['agent'])
    SHARED['dialog_history'] = []
""""""]",1
"get_world_size, _recursive_write = _recursive_write, get_world_size
def cat(obj, dst=None):
    """"""
    Concatenate any nested container of tensors along the 0-th axis.

    Parameters:
        obj (Object): any container object. Can be nested list, tuple or dict.
        dst (int, optional): rank of destination worker. If not specified, broadcast the result to all workers.

    Example::

        >>> # assume 4 workers
        >>> rank = comm.get_rank()
        >>> rng = torch.arange(10)
        >>> obj = {""range"": rng[rank * (rank + 1) // 2: (rank + 1) * (rank + 2) // 2]}
        >>> obj = comm.cat(obj)
        >>> assert torch.allclose(obj[""range""], rng)
    """"""","["""""" 
    (values, sizes) = _recursive_read(obj)
    sizes = {k: torch.cat(v) for (k, v) in sizes.items()}
    sizes = stack(sizes)
    cated = {}
    for (k, value) in values.items():
        size = sizes[k].t().flatten()
        dtype = value[0].dtype
        if (dtype == torch.bool):
            dtype = torch.uint8
        s = torch.zeros(size.sum(), dtype=dtype, device=value[0].device)
        obj_id = get_rank()
        world_size = get_world_size()
        offset = size[:obj_id].sum()
        for v in value:
            assert ((offset + v.numel()) <= len(s))
            s[offset:(offset + v.numel())] = v
            offset += size[obj_id:(obj_id + world_size)].sum()
            obj_id += world_size
        group = get_group(s.device)
        if (dst is None):
            dist.all_reduce(s, op=dist.ReduceOp.SUM, group=group)
        else:
            dist.reduce(s, op=dist.ReduceOp.SUM, dst=dst, group=group)
        cated[k] = s.type(value[0].dtype)
    sizes = {k: v.sum(dim=0) for (k, v) in sizes.items()}
    return _recursive_write(obj, cated, sizes)[0]
"""""", """""" 
    (values, sizes) = _recursive_read(obj)
    sizes = {k: torch.cat(v) for (k, v) in sizes.items()}
    sizes = stack(sizes)
    cated = {}
    for (k, value) in values.items():
        size = sizes[k].t().flatten()
        dtype = value[0].dtype
        if (dtype == torch.bool):
            dtype = torch.uint8
        s = torch.zeros(size.sum(), dtype=dtype, device=value[0].device)
        obj_id = get_rank()
        world_size = _recursive_write()
        offset = size[:obj_id].sum()
        for v in value:
            assert ((offset + v.numel()) <= len(s))
            s[offset:(offset + v.numel())] = v
            offset += size[obj_id:(obj_id + world_size)].sum()
            obj_id += world_size
        group = get_group(s.device)
        if (dst is None):
            dist.all_reduce(s, op=dist.ReduceOp.SUM, group=group)
        else:
            dist.reduce(s, op=dist.ReduceOp.SUM, dst=dst, group=group)
        cated[k] = s.type(value[0].dtype)
    sizes = {k: v.sum(dim=0) for (k, v) in sizes.items()}
    return get_world_size(obj, cated, sizes)[0]
""""""]",1
"_indices_to_matches, linear_sum_assignment = linear_sum_assignment, _indices_to_matches
def linear_assignment(cost_matrix, thresh):
    """"""
    Simple linear assignment
    :type cost_matrix: np.ndarray
    :type thresh: float
    :return: matches, unmatched_a, unmatched_b
    """"""","["""""" 
    if (cost_matrix.size == 0):
        return (np.empty((0, 2), dtype=int), tuple(range(cost_matrix.shape[0])), tuple(range(cost_matrix.shape[1])))
    cost_matrix[(cost_matrix > thresh)] = (thresh + 0.0001)
    (row_ind, col_ind) = linear_sum_assignment(cost_matrix)
    indices = np.column_stack((row_ind, col_ind))
    return _indices_to_matches(cost_matrix, indices, thresh)
"""""", """""" 
    if (cost_matrix.size == 0):
        return (np.empty((0, 2), dtype=int), tuple(range(cost_matrix.shape[0])), tuple(range(cost_matrix.shape[1])))
    cost_matrix[(cost_matrix > thresh)] = (thresh + 0.0001)
    (row_ind, col_ind) = _indices_to_matches(cost_matrix)
    indices = np.column_stack((row_ind, col_ind))
    return linear_sum_assignment(cost_matrix, indices, thresh)
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_window_present_by_partial_title(partial_title):
    """"""Verify there is a window/tab present by partial title

    Parameters:
    partial_title : value
    """"""","["""""" 
    with _verify_step(f""Verify window present by partial title '{partial_title}'"") as s:
        s.error = f""There is no window present with partial title '{partial_title}'""
        titles = get_browser().get_window_titles()
        s.error_description = '{}\nWindow titles: {}'.format(s.error, ','.join(titles))
        s.condition = any(((partial_title in t) for t in titles))
"""""", """""" 
    with get_browser(f""Verify window present by partial title '{partial_title}'"") as s:
        s.error = f""There is no window present with partial title '{partial_title}'""
        titles = _verify_step().get_window_titles()
        s.error_description = '{}\nWindow titles: {}'.format(s.error, ','.join(titles))
        s.condition = any(((partial_title in t) for t in titles))
""""""]",1
"Path, BeautifulSoup = BeautifulSoup, Path
def get_vhosts(ip, first=1, no_cache=False):
    """"""Returns a list of webs hosted on IP (checks bing.com)
    >>> 'www.bing.com' in vhosts(204.79.197.200)
    True
    """"""","["""""" 
    if (not no_cache):
        homedir = Path(os.path.expanduser('~'))
        requests_cache.install_cache(str((homedir / '.habu_requests_cache')), expire_after=3600)
    url = 'http://www.bing.com/search?q=ip:{ip} &first={first}'.format(ip=ip, first=first)
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    vhosts = set()
    for h2 in soup.find_all('h2'):
        for link in h2.find_all('a'):
            href = link.get('href')
            if (href.startswith('http://') or href.startswith('https://')):
                vhost = href.split('/')[2]
                vhosts.add(vhost)
    return list(vhosts)
"""""", """""" 
    if (not no_cache):
        homedir = BeautifulSoup(os.path.expanduser('~'))
        requests_cache.install_cache(str((homedir / '.habu_requests_cache')), expire_after=3600)
    url = 'http://www.bing.com/search?q=ip:{ip} &first={first}'.format(ip=ip, first=first)
    response = requests.get(url)
    soup = Path(response.text, 'html.parser')
    vhosts = set()
    for h2 in soup.find_all('h2'):
        for link in h2.find_all('a'):
            href = link.get('href')
            if (href.startswith('http://') or href.startswith('https://')):
                vhost = href.split('/')[2]
                vhosts.add(vhost)
    return list(vhosts)
""""""]",1
"get_args, prompt = prompt, get_args
def get_range_using_index(self, disable_args=False):
    """"""Returns a range selected using chapter indices""""""","["""""" 
    chapter_count = len(self.app.crawler.chapters)
    args = get_args()
    (start, stop) = (args.range or (None, None))
    if (args.suppress and (not (start and stop))):
        return (0, (chapter_count - 1))
    if (disable_args or (not start)):

        def validator(val):
            try:
                if (1 <= int(val) <= chapter_count):
                    return True
            except Exception:
                pass
            return ('Enter an integer between 1 and %d' % chapter_count)
        answer = prompt([{'type': 'input', 'name': 'start', 'message': ('Enter start index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: int(val))}, {'type': 'input', 'name': 'stop', 'message': ('Enter final index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: int(val))}])
        start = (answer['start'] - 1)
        stop = (answer['stop'] - 1)
    else:
        start = (start - 1)
        stop = (stop - 1)
    return ((start, stop) if (start < stop) else (stop, start))
"""""", """""" 
    chapter_count = len(self.app.crawler.chapters)
    args = prompt()
    (start, stop) = (args.range or (None, None))
    if (args.suppress and (not (start and stop))):
        return (0, (chapter_count - 1))
    if (disable_args or (not start)):

        def validator(val):
            try:
                if (1 <= int(val) <= chapter_count):
                    return True
            except Exception:
                pass
            return ('Enter an integer between 1 and %d' % chapter_count)
        answer = get_args([{'type': 'input', 'name': 'start', 'message': ('Enter start index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: int(val))}, {'type': 'input', 'name': 'stop', 'message': ('Enter final index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: int(val))}])
        start = (answer['start'] - 1)
        stop = (answer['stop'] - 1)
    else:
        start = (start - 1)
        stop = (stop - 1)
    return ((start, stop) if (start < stop) else (stop, start))
""""""]",1
"_create_task_agents, load_world_module = load_world_module, _create_task_agents
def create_task_world(opt: Opt, user_agents, default_world=None):
    """"""
    Instantiate a world with the supplied options and user agents.

    (A world factory.)
    """"""","["""""" 
    task_agents = _create_task_agents(opt)
    world_class = load_world_module(opt['task'], interactive_task=opt.get('interactive_task', False), selfchat_task=opt.get('selfchat_task', False), num_agents=len((user_agents + task_agents)), default_world=default_world)
    return world_class(opt, (task_agents + user_agents))
"""""", """""" 
    task_agents = load_world_module(opt)
    world_class = _create_task_agents(opt['task'], interactive_task=opt.get('interactive_task', False), selfchat_task=opt.get('selfchat_task', False), num_agents=len((user_agents + task_agents)), default_world=default_world)
    return world_class(opt, (task_agents + user_agents))
""""""]",1
"get_args, prompt = prompt, get_args
def confirm_retry(self) -> bool:
    """"""Returns whether to retry on failure""""""","["""""" 
    args = get_args()
    if args.suppress:
        return False
    answer = prompt([{'type': 'confirm', 'name': 'retry', 'message': 'Do you want to choose another novel?', 'default': True}])
    return answer.get('retry')
"""""", """""" 
    args = prompt()
    if args.suppress:
        return False
    answer = get_args([{'type': 'confirm', 'name': 'retry', 'message': 'Do you want to choose another novel?', 'default': True}])
    return answer.get('retry')
""""""]",1
"_read_config, FunctionalPyreverseTestfile = FunctionalPyreverseTestfile, _read_config
def get_functional_test_files(root_directory: Path) -> list[FunctionalPyreverseTestfile]:
    """"""Get all functional test files from the given directory.""""""","["""""" 
    test_files = []
    for path in root_directory.rglob('*.py'):
        if path.stem.startswith('_'):
            continue
        config_file = path.with_suffix('.rc')
        if config_file.exists():
            test_files.append(FunctionalPyreverseTestfile(source=path, options=_read_config(config_file)))
        else:
            test_files.append(FunctionalPyreverseTestfile(source=path, options={'output_formats': ['mmd'], 'command_line_args': []}))
    return test_files
"""""", """""" 
    test_files = []
    for path in root_directory.rglob('*.py'):
        if path.stem.startswith('_'):
            continue
        config_file = path.with_suffix('.rc')
        if config_file.exists():
            test_files.append(_read_config(source=path, options=FunctionalPyreverseTestfile(config_file)))
        else:
            test_files.append(_read_config(source=path, options={'output_formats': ['mmd'], 'command_line_args': []}))
    return test_files
""""""]",1
"move_texts, get_renderer = get_renderer, move_texts
def repel_text_from_bboxes(add_bboxes, texts, renderer=None, ax=None, expand=(1.2, 1.2), only_use_max_min=False, move=False):
    """"""
    Repel texts from other objects' bboxes while expanding their (texts')
    bounding boxes by expand (x, y), e.g. (1.2, 1.2) would multiply width and
    height by 1.2.
    Requires a renderer to get the actual sizes of the text, and to that end
    either one needs to be directly provided, or the axes have to be specified,
    and the renderer is then got from the axes object.
    """"""","["""""" 
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = get_renderer(ax.get_figure())
    else:
        r = renderer
    bboxes = get_bboxes(texts, r, expand, ax=ax)
    overlaps_x = np.zeros((len(bboxes), len(add_bboxes)))
    overlaps_y = np.zeros_like(overlaps_x)
    overlap_directions_x = np.zeros_like(overlaps_x)
    overlap_directions_y = np.zeros_like(overlaps_y)
    for (i, bbox1) in enumerate(bboxes):
        for (j, bbox2) in enumerate(add_bboxes):
            try:
                (x, y) = bbox1.intersection(bbox1, bbox2).size
                direction = np.sign((bbox1.extents - bbox2.extents))[:2]
                overlaps_x[(i, j)] = x
                overlaps_y[(i, j)] = y
                overlap_directions_x[(i, j)] = direction[0]
                overlap_directions_y[(i, j)] = direction[1]
            except AttributeError:
                pass
    move_x = (overlaps_x * overlap_directions_x)
    move_y = (overlaps_y * overlap_directions_y)
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(overlaps_x), np.sum(overlaps_y))
    if move:
        move_texts(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
"""""", """""" 
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = move_texts(ax.get_figure())
    else:
        r = renderer
    bboxes = get_bboxes(texts, r, expand, ax=ax)
    overlaps_x = np.zeros((len(bboxes), len(add_bboxes)))
    overlaps_y = np.zeros_like(overlaps_x)
    overlap_directions_x = np.zeros_like(overlaps_x)
    overlap_directions_y = np.zeros_like(overlaps_y)
    for (i, bbox1) in enumerate(bboxes):
        for (j, bbox2) in enumerate(add_bboxes):
            try:
                (x, y) = bbox1.intersection(bbox1, bbox2).size
                direction = np.sign((bbox1.extents - bbox2.extents))[:2]
                overlaps_x[(i, j)] = x
                overlaps_y[(i, j)] = y
                overlap_directions_x[(i, j)] = direction[0]
                overlap_directions_y[(i, j)] = direction[1]
            except AttributeError:
                pass
    move_x = (overlaps_x * overlap_directions_x)
    move_y = (overlaps_y * overlap_directions_y)
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(overlaps_x), np.sum(overlaps_y))
    if move:
        get_renderer(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
""""""]",1
"classify0, autoNorm = autoNorm, classify0
def datingClassTest():
    """"""
    Desc: 
        对约会网站的测试方法，并将分类错误的数量和分类错误率打印出来
    Args: 
        None
    Returns: 
        None
    """"""","["""""" 
    hoRatio = 0.1
    (datingDataMat, datingLabels) = file2matrix('data/2.KNN/datingTestSet2.txt')
    (normMat, ranges, minVals) = autoNorm(datingDataMat)
    m = normMat.shape[0]
    numTestVecs = int((m * hoRatio))
    print('numTestVecs=', numTestVecs)
    errorCount = 0
    for i in range(numTestVecs):
        classifierResult = classify0(normMat[i], normMat[numTestVecs:m], datingLabels[numTestVecs:m], 3)
        print(('the classifier came back with: %d, the real answer is: %d' % (classifierResult, datingLabels[i])))
        errorCount += (classifierResult != datingLabels[i])
    print(('the total error rate is: %f' % (errorCount / numTestVecs)))
    print(errorCount)
"""""", """""" 
    hoRatio = 0.1
    (datingDataMat, datingLabels) = file2matrix('data/2.KNN/datingTestSet2.txt')
    (normMat, ranges, minVals) = classify0(datingDataMat)
    m = normMat.shape[0]
    numTestVecs = int((m * hoRatio))
    print('numTestVecs=', numTestVecs)
    errorCount = 0
    for i in range(numTestVecs):
        classifierResult = autoNorm(normMat[i], normMat[numTestVecs:m], datingLabels[numTestVecs:m], 3)
        print(('the classifier came back with: %d, the real answer is: %d' % (classifierResult, datingLabels[i])))
        errorCount += (classifierResult != datingLabels[i])
    print(('the total error rate is: %f' % (errorCount / numTestVecs)))
    print(errorCount)
""""""]",1
"is_duck_dask_array, is_np_datetime_like = is_np_datetime_like, is_duck_dask_array
def infer_calendar_name(dates) -> CFCalendar:
    """"""Given an array of datetimes, infer the CF calendar name""""""","["""""" 
    if is_np_datetime_like(dates.dtype):
        return 'proleptic_gregorian'
    elif ((dates.dtype == np.dtype('O')) and (dates.size > 0)):
        if (cftime is not None):
            sample = np.asarray(dates).flat[0]
            if is_duck_dask_array(sample):
                sample = sample.compute()
                if isinstance(sample, np.ndarray):
                    sample = sample.item()
            if isinstance(sample, cftime.datetime):
                return sample.calendar
    raise ValueError('Array does not contain datetime objects.')
"""""", """""" 
    if is_duck_dask_array(dates.dtype):
        return 'proleptic_gregorian'
    elif ((dates.dtype == np.dtype('O')) and (dates.size > 0)):
        if (cftime is not None):
            sample = np.asarray(dates).flat[0]
            if is_np_datetime_like(sample):
                sample = sample.compute()
                if isinstance(sample, np.ndarray):
                    sample = sample.item()
            if isinstance(sample, cftime.datetime):
                return sample.calendar
    raise ValueError('Array does not contain datetime objects.')
""""""]",1
"defaultdict, fn = fn, defaultdict
def group_by(collection, fn):
    """"""Group the elements of a collection by the results of passing them through fn
    Returns a dict, {k_0: list[e_0, e_1, ...]} where e are elements of `collection` and
    f(e_0) = f(e_1) = k_0
    """"""","["""""" 
    r = defaultdict(list)
    for e in collection:
        k = fn(e)
        r[k].append(e)
    return r
"""""", """""" 
    r = fn(list)
    for e in collection:
        k = defaultdict(e)
        r[k].append(e)
    return r
""""""]",1
"PicklableWrapper, launch = launch, PicklableWrapper
def launch_deco(num_processes: int=1, backend: str='GLOO', always_spawn: bool=True, launch_method: str='multiprocessing', timeout: timedelta=DEFAULT_UNITTEST_TIMEOUT, shared_context: Optional[comm.BaseSharedContext]=None):
    """"""
    A helper decorator to run the instance method via `launch`. This is convenient
    to converte a unittest to distributed version.
    """"""","["""""" 

    def deco(func):

        @functools.wraps(func)
        def _launch_func(self, *args, **kwargs):
            results = launch(PicklableWrapper(func), num_processes_per_machine=num_processes, backend=backend, always_spawn=always_spawn, launch_method=launch_method, shared_context=shared_context, timeout=timeout, args=(PicklableWrapper(self), *args), kwargs=kwargs)
            return results
        return _launch_func
    return deco
"""""", """""" 

    def deco(func):

        @functools.wraps(func)
        def _launch_func(self, *args, **kwargs):
            results = PicklableWrapper(launch(func), num_processes_per_machine=num_processes, backend=backend, always_spawn=always_spawn, launch_method=launch_method, shared_context=shared_context, timeout=timeout, args=(launch(self), *args), kwargs=kwargs)
            return results
        return _launch_func
    return deco
""""""]",1
"remove_item_from_list, load_txt_file = load_txt_file, remove_item_from_list
def anno_parser_v0(anno_path, num_pts):
    """"""                        
  parse the annotation for 300W dataset, which has a fixed format for .pts file                                
  return:                    
    pts: 3 x num_pts (x, y, oculusion)                                
  """"""","["""""" 
    (data, num_lines) = load_txt_file(anno_path)
    assert (data[0].find('version: ') == 0), 'version is not correct'
    assert (data[1].find('n_points: ') == 0), 'number of points in second line is not correct'
    assert ((data[2] == '{') and (data[(- 1)] == '}')), 'starting and end symbol is not correct'
    assert ((data[0] == 'version: 1') or (data[0] == 'version: 1.0')), 'The version is wrong : {}'.format(data[0])
    n_points = int(data[1][len('n_points: '):])
    assert (num_lines == (n_points + 4)), 'number of lines is not correct'
    assert (num_pts == n_points), 'number of points is not correct'
    pts = np.zeros((3, n_points), dtype='float32')
    line_offset = 3
    point_set = set()
    for point_index in range(n_points):
        try:
            pts_list = data[(point_index + line_offset)].split(' ')
            if (len(pts_list) > 2):
                pts_list = remove_item_from_list(pts_list, '')
            pts[(0, point_index)] = float(pts_list[0])
            pts[(1, point_index)] = float(pts_list[1])
            pts[(2, point_index)] = float(1)
            point_set.add(point_index)
        except ValueError:
            print(('error in loading points in %s' % anno_path))
    return (pts, point_set)
"""""", """""" 
    (data, num_lines) = remove_item_from_list(anno_path)
    assert (data[0].find('version: ') == 0), 'version is not correct'
    assert (data[1].find('n_points: ') == 0), 'number of points in second line is not correct'
    assert ((data[2] == '{') and (data[(- 1)] == '}')), 'starting and end symbol is not correct'
    assert ((data[0] == 'version: 1') or (data[0] == 'version: 1.0')), 'The version is wrong : {}'.format(data[0])
    n_points = int(data[1][len('n_points: '):])
    assert (num_lines == (n_points + 4)), 'number of lines is not correct'
    assert (num_pts == n_points), 'number of points is not correct'
    pts = np.zeros((3, n_points), dtype='float32')
    line_offset = 3
    point_set = set()
    for point_index in range(n_points):
        try:
            pts_list = data[(point_index + line_offset)].split(' ')
            if (len(pts_list) > 2):
                pts_list = load_txt_file(pts_list, '')
            pts[(0, point_index)] = float(pts_list[0])
            pts[(1, point_index)] = float(pts_list[1])
            pts[(2, point_index)] = float(1)
            point_set.add(point_index)
        except ValueError:
            print(('error in loading points in %s' % anno_path))
    return (pts, point_set)
""""""]",1
"PypiVersion, PackageURL = PackageURL, PypiVersion
def categorize_versions(package_name: str, all_versions: Set[str], version_specs: Iterable[str]) -> Tuple[(Set[PackageURL], Set[PackageURL])]:
    """"""
    :return: impacted, resolved purls
    """"""","["""""" 
    (impacted_versions, impacted_purls) = (set(), [])
    vurl_specs = []
    for version_spec in version_specs:
        vurl_specs.append(PypiVersionRange.from_native(version_spec))
    invalid_versions = set()
    for version in all_versions:
        try:
            version_object = PypiVersion(version)
        except InvalidVersion:
            invalid_versions.add(version)
            continue
        if any([(version_object in vurl_spec) for vurl_spec in vurl_specs]):
            impacted_versions.add(version)
            impacted_purls.append(PackageURL(name=package_name, type='pypi', version=version))
    resolved_purls = []
    all_versions -= invalid_versions
    for version in (all_versions - impacted_versions):
        resolved_purls.append(PackageURL(name=package_name, type='pypi', version=version))
    return (impacted_purls, resolved_purls)
"""""", """""" 
    (impacted_versions, impacted_purls) = (set(), [])
    vurl_specs = []
    for version_spec in version_specs:
        vurl_specs.append(PypiVersionRange.from_native(version_spec))
    invalid_versions = set()
    for version in all_versions:
        try:
            version_object = PackageURL(version)
        except InvalidVersion:
            invalid_versions.add(version)
            continue
        if any([(version_object in vurl_spec) for vurl_spec in vurl_specs]):
            impacted_versions.add(version)
            impacted_purls.append(PypiVersion(name=package_name, type='pypi', version=version))
    resolved_purls = []
    all_versions -= invalid_versions
    for version in (all_versions - impacted_versions):
        resolved_purls.append(PypiVersion(name=package_name, type='pypi', version=version))
    return (impacted_purls, resolved_purls)
""""""]",1
"socket, a2b_hex = a2b_hex, socket
def send(token_hex, message, **kwargs):
    """"""
    Site: https://apple.com
    API: https://developer.apple.com
    Desc: iOS notifications
    """"""","["""""" 
    is_enhanced = kwargs.pop('is_enhanced', False)
    identifier = kwargs.pop('identifier', 0)
    expiry = kwargs.pop('expiry', 0)
    alert = {'title': kwargs.pop('event'), 'body': message, 'action': kwargs.pop('apns_action', defaults.APNS_PROVIDER_DEFAULT_ACTION)}
    data = {'aps': {'alert': alert, 'content-available': (kwargs.pop('content_available', 0) and 1)}}
    data['aps'].update(kwargs)
    payload = dumps(data, separators=(',', ':'))
    token = a2b_hex(token_hex)
    if (is_enhanced is True):
        fmt = ('!BIIH32sH%ds' % len(payload))
        expiry = (expiry and (time() + expiry))
        notification = pack(fmt, 1, identifier, expiry, 32, token, len(payload), payload)
    else:
        token_length_bin = pack('>H', len(token))
        payload_length_bin = pack('>H', len(payload))
        zero_byte = (bytes('\x00', 'utf-8') if (PY3 is True) else '\x00')
        payload = (bytes(payload, 'utf-8') if (PY3 is True) else payload)
        notification = ((((zero_byte + token_length_bin) + token) + payload_length_bin) + payload)
    sock = socket(AF_INET, SOCK_STREAM)
    sock.settimeout(3)
    sock.connect((settings.APNS_GW_HOST, settings.APNS_GW_PORT))
    ssl = wrap_socket(sock, settings.APNS_KEY_FILE, settings.APNS_CERT_FILE, do_handshake_on_connect=False)
    result = ssl.write(notification)
    sock.close()
    ssl.close()
    if (not result):
        raise APNsError
    return True
"""""", """""" 
    is_enhanced = kwargs.pop('is_enhanced', False)
    identifier = kwargs.pop('identifier', 0)
    expiry = kwargs.pop('expiry', 0)
    alert = {'title': kwargs.pop('event'), 'body': message, 'action': kwargs.pop('apns_action', defaults.APNS_PROVIDER_DEFAULT_ACTION)}
    data = {'aps': {'alert': alert, 'content-available': (kwargs.pop('content_available', 0) and 1)}}
    data['aps'].update(kwargs)
    payload = dumps(data, separators=(',', ':'))
    token = socket(token_hex)
    if (is_enhanced is True):
        fmt = ('!BIIH32sH%ds' % len(payload))
        expiry = (expiry and (time() + expiry))
        notification = pack(fmt, 1, identifier, expiry, 32, token, len(payload), payload)
    else:
        token_length_bin = pack('>H', len(token))
        payload_length_bin = pack('>H', len(payload))
        zero_byte = (bytes('\x00', 'utf-8') if (PY3 is True) else '\x00')
        payload = (bytes(payload, 'utf-8') if (PY3 is True) else payload)
        notification = ((((zero_byte + token_length_bin) + token) + payload_length_bin) + payload)
    sock = a2b_hex(AF_INET, SOCK_STREAM)
    sock.settimeout(3)
    sock.connect((settings.APNS_GW_HOST, settings.APNS_GW_PORT))
    ssl = wrap_socket(sock, settings.APNS_KEY_FILE, settings.APNS_CERT_FILE, do_handshake_on_connect=False)
    result = ssl.write(notification)
    sock.close()
    ssl.close()
    if (not result):
        raise APNsError
    return True
""""""]",1
"make_supports_coordinated, filter_uncoordinated_int_loc_dicts = filter_uncoordinated_int_loc_dicts, make_supports_coordinated
def produce_local_loc_dicts(game, power: str, base_action: List[str], close_locations: FrozenSet[str], *, fix_uncoordinated_base: bool) -> Dict[(str, List[str])]:
    """"""Given an action for a power and a location build a loc->possible_orders dict.""""""","["""""" 
    assert game.current_short_phase.endswith('M'), game.current_short_phase
    assert all(((i in LOCS) for i in close_locations)), (close_locations, LOCS)
    per_loc = get_power_per_loc_orders(game, power)
    if fix_uncoordinated_base:
        base_action = make_supports_coordinated(base_action)
    base_action_dict = {x.split()[1].split('/')[0]: x for x in base_action}
    for x in base_action_dict:
        assert (x in per_loc), x
    for loc in list(per_loc):
        if (loc not in close_locations):
            assert (loc in base_action_dict), (loc, base_action)
            per_loc[loc] = [base_action_dict[loc]]
    return filter_uncoordinated_int_loc_dicts(per_loc)
"""""", """""" 
    assert game.current_short_phase.endswith('M'), game.current_short_phase
    assert all(((i in LOCS) for i in close_locations)), (close_locations, LOCS)
    per_loc = get_power_per_loc_orders(game, power)
    if fix_uncoordinated_base:
        base_action = filter_uncoordinated_int_loc_dicts(base_action)
    base_action_dict = {x.split()[1].split('/')[0]: x for x in base_action}
    for x in base_action_dict:
        assert (x in per_loc), x
    for loc in list(per_loc):
        if (loc not in close_locations):
            assert (loc in base_action_dict), (loc, base_action)
            per_loc[loc] = [base_action_dict[loc]]
    return make_supports_coordinated(per_loc)
""""""]",1
"SpdxInfo, cleandoc = cleandoc, SpdxInfo
def test_find_and_replace_only_shebang():
    """"""When the file only contains a shebang, keep it at the top of the file.""""""","["""""" 
    spdx_info = SpdxInfo({'GPL-3.0-or-later'}, set())
    text = cleandoc('\n        #!/usr/bin/env python3\n\n        # Hello, world!\n\n        pass\n        ')
    expected = cleandoc('\n        #!/usr/bin/env python3\n\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # Hello, world!\n\n        pass\n        ')
    assert (find_and_replace_header(text, spdx_info) == expected)
"""""", """""" 
    spdx_info = cleandoc({'GPL-3.0-or-later'}, set())
    text = SpdxInfo('\n        #!/usr/bin/env python3\n\n        # Hello, world!\n\n        pass\n        ')
    expected = SpdxInfo('\n        #!/usr/bin/env python3\n\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # Hello, world!\n\n        pass\n        ')
    assert (find_and_replace_header(text, spdx_info) == expected)
""""""]",1
"abort, _get_counter = _get_counter, abort
@blueprint.route((_CRON_ROUTE + '/process-results'))
def process_results():
    """"""Generate impact requests.""""""","["""""" 
    if (not request.headers.get('X-Appengine-Cron')):
        abort(403)
    publisher = pubsub_v1.PublisherClient()
    counters = {}
    for regress_result in osv.RegressResult.query():
        key_id = regress_result.key.id()
        if (not regress_result.commit):
            logging.info('Missing commit info for %s.', key_id)
            continue
        fixed_result = ndb.Key(osv.FixResult, key_id).get()
        if ((not fixed_result) or (not fixed_result.commit)):
            logging.info('Fixed result does not exist for %s.', key_id)
        bug = osv.Bug.query((osv.Bug.source_id == key_id)).get()
        if bug:
            logging.info('Bug already exists for %s.', key_id)
            continue
        if regress_result.issue_id:
            bug = osv.Bug.query((osv.Bug.issue_id == regress_result.issue_id)).get()
            if bug:
                logging.info('Bug already exists for issue %s.', regress_result.issue_id)
                continue
        if regress_result.timestamp:
            id_year = regress_result.timestamp.year
        else:
            id_year = None
        counter = counters.get(id_year)
        if (not counter):
            counter = _get_counter(id_year)
            counters[id_year] = counter
        try:
            cur_id = 'OSV-{}-{}'.format(counter.key.id(), counter.next_id)
            logging.info('Allocating %s.', cur_id)
            counter.next_id += 1
            bug = osv.Bug(db_id=cur_id, timestamp=datetime.datetime.utcnow(), public=False, source_id=key_id, status=osv.BugStatus.UNPROCESSED)
            bug.put()
            logging.info('Requesting impact for %s.', key_id)
            publisher.publish(_TASKS_TOPIC, data=b'', type='impact', source_id=key_id, allocated_id=cur_id)
        finally:
            counter.put()
    return 'done'
"""""", """""" 
    if (not request.headers.get('X-Appengine-Cron')):
        _get_counter(403)
    publisher = pubsub_v1.PublisherClient()
    counters = {}
    for regress_result in osv.RegressResult.query():
        key_id = regress_result.key.id()
        if (not regress_result.commit):
            logging.info('Missing commit info for %s.', key_id)
            continue
        fixed_result = ndb.Key(osv.FixResult, key_id).get()
        if ((not fixed_result) or (not fixed_result.commit)):
            logging.info('Fixed result does not exist for %s.', key_id)
        bug = osv.Bug.query((osv.Bug.source_id == key_id)).get()
        if bug:
            logging.info('Bug already exists for %s.', key_id)
            continue
        if regress_result.issue_id:
            bug = osv.Bug.query((osv.Bug.issue_id == regress_result.issue_id)).get()
            if bug:
                logging.info('Bug already exists for issue %s.', regress_result.issue_id)
                continue
        if regress_result.timestamp:
            id_year = regress_result.timestamp.year
        else:
            id_year = None
        counter = counters.get(id_year)
        if (not counter):
            counter = abort(id_year)
            counters[id_year] = counter
        try:
            cur_id = 'OSV-{}-{}'.format(counter.key.id(), counter.next_id)
            logging.info('Allocating %s.', cur_id)
            counter.next_id += 1
            bug = osv.Bug(db_id=cur_id, timestamp=datetime.datetime.utcnow(), public=False, source_id=key_id, status=osv.BugStatus.UNPROCESSED)
            bug.put()
            logging.info('Requesting impact for %s.', key_id)
            publisher.publish(_TASKS_TOPIC, data=b'', type='impact', source_id=key_id, allocated_id=cur_id)
        finally:
            counter.put()
    return 'done'
""""""]",1
"runGeocode, cmdLineParse = cmdLineParse, runGeocode
def main(iargs=None):
    """"""
    Main driver.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    bbox = [float(val) for val in inps.bbox.split()]
    runGeocode(inps, inps.prodlist, bbox, inps.demfilename, is_offset_mode=False)
"""""", """""" 
    inps = runGeocode(iargs)
    bbox = [float(val) for val in inps.bbox.split()]
    cmdLineParse(inps, inps.prodlist, bbox, inps.demfilename, is_offset_mode=False)
""""""]",1
"_mul_add, _conj = _conj, _mul_add
def _covariance(y_j):
    """"""
    Compute the empirical covariance for a source.

    Args:
        y_j (Tensor): complex stft of the source.
            [shape=(nb_frames, nb_bins, nb_channels, 2)].

    Returns:
        Cj (Tensor): [shape=(nb_frames, nb_bins, nb_channels, nb_channels, 2)]
            just y_j * conj(y_j.T): empirical covariance for each TF bin.
    """"""","["""""" 
    (nb_frames, nb_bins, nb_channels) = y_j.shape[:(- 1)]
    Cj = torch.zeros((nb_frames, nb_bins, nb_channels, nb_channels, 2), dtype=y_j.dtype, device=y_j.device)
    indices = torch.cartesian_prod(torch.arange(nb_channels), torch.arange(nb_channels))
    for index in indices:
        Cj[:, :, index[0], index[1], :] = _mul_add(y_j[:, :, index[0], :], _conj(y_j[:, :, index[1], :]), Cj[:, :, index[0], index[1], :])
    return Cj
"""""", """""" 
    (nb_frames, nb_bins, nb_channels) = y_j.shape[:(- 1)]
    Cj = torch.zeros((nb_frames, nb_bins, nb_channels, nb_channels, 2), dtype=y_j.dtype, device=y_j.device)
    indices = torch.cartesian_prod(torch.arange(nb_channels), torch.arange(nb_channels))
    for index in indices:
        Cj[:, :, index[0], index[1], :] = _conj(y_j[:, :, index[0], :], _mul_add(y_j[:, :, index[1], :]), Cj[:, :, index[0], index[1], :])
    return Cj
""""""]",1
"get_key, get_olf = get_olf, get_key
def key_parse(leftline):
    """"""Break left-of-operator portion into key, units, dimensions, element""""""","["""""" 
    return ([get_key(leftline)] + get_olf(leftline))
"""""", """""" 
    return ([get_olf(leftline)] + get_key(leftline))
""""""]",1
"Project, Path = Path, Project
def test_all_files_submodule_is_ignored(submodule_repository):
    """"""If a submodule is ignored, all_files should not raise an Exception.""""""","["""""" 
    (submodule_repository / 'submodule/foo.py').write_text('foo')
    gitignore = (submodule_repository / '.gitignore')
    contents = gitignore.read_text()
    contents += '\nsubmodule/\n'
    gitignore.write_text(contents)
    project = Project(submodule_repository)
    assert (Path('submodule/foo.py').absolute() not in project.all_files())
"""""", """""" 
    (submodule_repository / 'submodule/foo.py').write_text('foo')
    gitignore = (submodule_repository / '.gitignore')
    contents = gitignore.read_text()
    contents += '\nsubmodule/\n'
    gitignore.write_text(contents)
    project = Path(submodule_repository)
    assert (Project('submodule/foo.py').absolute() not in project.all_files())
""""""]",1
"assert_and_infer_cfg, validate = validate, assert_and_infer_cfg
def main():
    """"""
    Main Function

    """"""","["""""" 
    assert_and_infer_cfg(args)
    writer = prep_experiment(args, parser)
    (train_loader, val_loader, train_obj) = datasets.setup_loaders(args)
    (criterion, criterion_val) = loss.get_loss(args)
    net = network.get_net(args, criterion)
    (optim, scheduler) = optimizer.get_optimizer(args, net)
    torch.cuda.empty_cache()
    if args.evaluate:
        default_eval_epoch = 1
        validate(val_loader, net, criterion_val, optim, default_eval_epoch, writer)
        evaluate(val_loader, net)
        return
    for epoch in range(args.start_epoch, args.max_epoch):
        cfg.immutable(False)
        cfg.EPOCH = epoch
        cfg.immutable(True)
        scheduler.step()
        train(train_loader, net, criterion, optim, epoch, writer)
        validate(val_loader, net, criterion_val, optim, epoch, writer)
"""""", """""" 
    validate(args)
    writer = prep_experiment(args, parser)
    (train_loader, val_loader, train_obj) = datasets.setup_loaders(args)
    (criterion, criterion_val) = loss.get_loss(args)
    net = network.get_net(args, criterion)
    (optim, scheduler) = optimizer.get_optimizer(args, net)
    torch.cuda.empty_cache()
    if args.evaluate:
        default_eval_epoch = 1
        assert_and_infer_cfg(val_loader, net, criterion_val, optim, default_eval_epoch, writer)
        evaluate(val_loader, net)
        return
    for epoch in range(args.start_epoch, args.max_epoch):
        cfg.immutable(False)
        cfg.EPOCH = epoch
        cfg.immutable(True)
        scheduler.step()
        train(train_loader, net, criterion, optim, epoch, writer)
        assert_and_infer_cfg(val_loader, net, criterion_val, optim, epoch, writer)
""""""]",1
"FlopsEstimation, model = model, FlopsEstimation
def get_model_flops(model: torch.nn.Module, inputs: typing.Tuple[(typing.Any, ...)]):
    """"""inputs: a list of inputs to the model, one for each argument for
    `model.forward()`
    """"""","["""""" 
    assert isinstance(inputs, (tuple, list)), f'Invalid input types {inputs}'
    fest = FlopsEstimation(model)
    with fest.enable():
        model(*inputs)
        fest.add_flops_info()
        (nparams, nflops) = fest.get_flops()
    return (nparams, nflops)
"""""", """""" 
    assert isinstance(inputs, (tuple, list)), f'Invalid input types {inputs}'
    fest = model(FlopsEstimation)
    with fest.enable():
        FlopsEstimation(*inputs)
        fest.add_flops_info()
        (nparams, nflops) = fest.get_flops()
    return (nparams, nflops)
""""""]",1
"OrOperator, AndOperator = AndOperator, OrOperator
@app.route('/inventory/json', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/inventory/json')
def inventory_ajax(env):
    """"""Backend endpoint for inventory table""""""","["""""" 
    draw = int(request.args.get('draw', 0))
    envs = environments()
    check_env(env, envs)
    (headers, fact_names) = inventory_facts()
    fact_templates = app.config['INVENTORY_FACT_TEMPLATES']
    query = AndOperator()
    fact_query = OrOperator()
    fact_query.add([EqualsOperator('name', name) for name in fact_names])
    query.add(fact_query)
    if (env != '*'):
        query.add(EqualsOperator('environment', env))
    facts = puppetdb.facts(query=query)
    fact_data = {}
    for fact in facts:
        if (fact.node not in fact_data):
            fact_data[fact.node] = {}
        fact_value = fact.value
        if (fact.name in fact_templates):
            fact_template = fact_templates[fact.name]
            fact_value = render_template_string(fact_template, current_env=env, value=fact_value)
        fact_data[fact.node][fact.name] = fact_value
    total = len(fact_data)
    return render_template('inventory.json.tpl', draw=draw, total=total, total_filtered=total, fact_data=fact_data, columns=fact_names)
"""""", """""" 
    draw = int(request.args.get('draw', 0))
    envs = environments()
    check_env(env, envs)
    (headers, fact_names) = inventory_facts()
    fact_templates = app.config['INVENTORY_FACT_TEMPLATES']
    query = OrOperator()
    fact_query = AndOperator()
    fact_query.add([EqualsOperator('name', name) for name in fact_names])
    query.add(fact_query)
    if (env != '*'):
        query.add(EqualsOperator('environment', env))
    facts = puppetdb.facts(query=query)
    fact_data = {}
    for fact in facts:
        if (fact.node not in fact_data):
            fact_data[fact.node] = {}
        fact_value = fact.value
        if (fact.name in fact_templates):
            fact_template = fact_templates[fact.name]
            fact_value = render_template_string(fact_template, current_env=env, value=fact_value)
        fact_data[fact.node][fact.name] = fact_value
    total = len(fact_data)
    return render_template('inventory.json.tpl', draw=draw, total=total, total_filtered=total, fact_data=fact_data, columns=fact_names)
""""""]",1
"TemplateNotFound, Environment = Environment, TemplateNotFound
def _find_template(project: Project, name: str) -> Template:
    """"""Find a template given a name.

    :raises TemplateNotFound: if template could not be found.
    """"""","["""""" 
    template_dir = (project.root / '.reuse/templates')
    env = Environment(loader=FileSystemLoader(str(template_dir)), trim_blocks=True)
    names = [name]
    if (not name.endswith('.jinja2')):
        names.append(f'{name}.jinja2')
    if (not name.endswith('.commented.jinja2')):
        names.append(f'{name}.commented.jinja2')
    for item in names:
        try:
            return env.get_template(item)
        except TemplateNotFound:
            pass
    raise TemplateNotFound(name)
"""""", """""" 
    template_dir = (project.root / '.reuse/templates')
    env = TemplateNotFound(loader=FileSystemLoader(str(template_dir)), trim_blocks=True)
    names = [name]
    if (not name.endswith('.jinja2')):
        names.append(f'{name}.jinja2')
    if (not name.endswith('.commented.jinja2')):
        names.append(f'{name}.commented.jinja2')
    for item in names:
        try:
            return env.get_template(item)
        except Environment:
            pass
    raise Environment(name)
""""""]",1
"search_top_lvl_sep, extract_curlybrack = extract_curlybrack, search_top_lvl_sep
def extract_multi(text=''):
    """"""
    extracts the list of textual components between curly-brackets
    returns the remaining text, and the list of extracted textual components
    """"""","["""""" 
    (rest, text) = extract_curlybrack(text)
    if (not text):
        return (rest, text)
    else:
        coma_offsets = (([(- 1)] + search_top_lvl_sep(text, ',')) + [len(text)])
        return (rest, list(map(strip, [text[(coma_offsets[i] + 1):coma_offsets[(i + 1)]] for i in range((len(coma_offsets) - 1))])))
"""""", """""" 
    (rest, text) = search_top_lvl_sep(text)
    if (not text):
        return (rest, text)
    else:
        coma_offsets = (([(- 1)] + extract_curlybrack(text, ',')) + [len(text)])
        return (rest, list(map(strip, [text[(coma_offsets[i] + 1):coma_offsets[(i + 1)]] for i in range((len(coma_offsets) - 1))])))
""""""]",1
"_norm_args, _default_threads = _default_threads, _norm_args
def irfft2(a, s=None, axes=((- 2), (- 1)), norm=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 2D real inverse FFT.

    The first four arguments are as per :func:`numpy.fft.irfft2`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    calling_func = 'irfft2'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
"""""", """""" 
    calling_func = 'irfft2'
    planner_effort = _default_effort(planner_effort)
    threads = _norm_args(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_default_threads(norm))
""""""]",1
"TreeNode, str_shortener = str_shortener, TreeNode
def failed_build_tree(section, parent_node, level=0):
    """"""
    Builds a tree for displaying items mimicking linux tree command.
    """"""","["""""" 
    max_len = (66 - (level * 4))
    if runtime.job:
        if (section.type == 'Step'):
            name = str_shortener(('%s: %s' % (section.id, section.name)), max_len)
        else:
            name = str_shortener(section.id, max_len)
        section_node = TreeNode(RESULT_ROW.format(name=name, result=str(section.result).upper(), max_len=max_len))
        result = section.result.value
        sections = section.sections
    else:
        section_node = TreeNode(RESULT_ROW.format(name=str_shortener(section['name'], max_len), result=RESULT_COLOUR[str(section['result'])].apply(str(section['result']).upper()), max_len=max_len))
        result = section['result'].value
        sections = section['sections']
    parsed_args = _parse_args()
    if ((parsed_args['_display_only_failed'] and (result not in ['passed'])) or (not parsed_args['_display_only_failed'])):
        parent_node.add_child(section_node)
    for child_section in sections:
        if (level < 2):
            failed_build_tree(child_section, section_node, (level + 1))
        else:
            failed_build_tree(child_section, parent_node, level)
"""""", """""" 
    max_len = (66 - (level * 4))
    if runtime.job:
        if (section.type == 'Step'):
            name = TreeNode(('%s: %s' % (section.id, section.name)), max_len)
        else:
            name = TreeNode(section.id, max_len)
        section_node = str_shortener(RESULT_ROW.format(name=name, result=str(section.result).upper(), max_len=max_len))
        result = section.result.value
        sections = section.sections
    else:
        section_node = str_shortener(RESULT_ROW.format(name=TreeNode(section['name'], max_len), result=RESULT_COLOUR[str(section['result'])].apply(str(section['result']).upper()), max_len=max_len))
        result = section['result'].value
        sections = section['sections']
    parsed_args = _parse_args()
    if ((parsed_args['_display_only_failed'] and (result not in ['passed'])) or (not parsed_args['_display_only_failed'])):
        parent_node.add_child(section_node)
    for child_section in sections:
        if (level < 2):
            failed_build_tree(child_section, section_node, (level + 1))
        else:
            failed_build_tree(child_section, parent_node, level)
""""""]",1
"get_sending_order, get_batch_size = get_batch_size, get_sending_order
def get_queued():
    """"""
    Returns the queryset of emails eligible for sending – fulfilling these conditions:
     - Status is queued or requeued
     - Has scheduled_time before the current time or is None
     - Has expires_at after the current time or is None
    """"""","["""""" 
    now = timezone.now()
    query = (((Q(status=STATUS.queued) | Q(status=STATUS.requeued)) & (Q(scheduled_time__lte=now) | Q(scheduled_time__isnull=True))) & (Q(expires_at__gt=now) | Q(expires_at__isnull=True)))
    return Email.objects.filter(query).select_related('template').order_by(*get_sending_order()).prefetch_related('attachments')[:get_batch_size()]
"""""", """""" 
    now = timezone.now()
    query = (((Q(status=STATUS.queued) | Q(status=STATUS.requeued)) & (Q(scheduled_time__lte=now) | Q(scheduled_time__isnull=True))) & (Q(expires_at__gt=now) | Q(expires_at__isnull=True)))
    return Email.objects.filter(query).select_related('template').order_by(*get_batch_size()).prefetch_related('attachments')[:get_sending_order()]
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_element_has_not_attribute(element, attribute):
    """"""Verify element has not attribute

    Parameters:
    element : element
    attribute : value
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    with _verify_step(f'Verify element {element.name} has not attribute {attribute}') as s:
        s.error = f'element {element.name} has attribute {attribute}'
        s.condition = (not element.has_attribute(attribute))
"""""", """""" 
    element = _verify_step().find(element, timeout=0)
    with get_browser(f'Verify element {element.name} has not attribute {attribute}') as s:
        s.error = f'element {element.name} has attribute {attribute}'
        s.condition = (not element.has_attribute(attribute))
""""""]",1
"Microsecond, Minute = Minute, Microsecond
def test_mul_float_multiple_next_higher_resolution():
    """"""Test more than one iteration through _next_higher_resolution is required.""""""","["""""" 
    assert ((1e-06 * Second()) == Microsecond())
    assert (((1e-06 / 60) * Minute()) == Microsecond())
"""""", """""" 
    assert ((1e-06 * Second()) == Minute())
    assert (((1e-06 / 60) * Microsecond()) == Minute())
""""""]",1
"symspellpy_spell, spell = spell, symspellpy_spell
def spell_sent(list_words: List[str], engine: str='pn') -> List[List[str]]:
    """"""
    Provides a list of possible correct spelling of sentence

    :param List[str] list_words: list word of sentence
    :param str engine:
        * *pn* - Peter Norvig's algorithm [#norvig_spellchecker]_ (default)
        * *phunspell* - A spell checker utilizing spylls a port of Hunspell.
        * *symspellpy* - symspellpy is a Python port of SymSpell v6.5.
    :return: list of possible correct words
    :rtype: List[List[str]]

    :Example:
    ::

        from pythainlp.spell import spell_sent

        spell_sent([""เด็"",""อินอร์เน็ต"",""แรง""],engine='symspellpy')
        # output: [['เด็ก', 'อินเทอร์เน็ต', 'แรง']]
    """"""","["""""" 
    if (engine == 'symspellpy'):
        from pythainlp.spell.symspellpy import spell_sent as symspellpy_spell
        list_new = symspellpy_spell(list_words)
    else:
        _temp = list(itertools.product(*[spell(i, engine=engine) for i in list_words]))
        list_new = []
        for i in _temp:
            _temp2 = []
            for j in i:
                _temp2.append(j)
            list_new.append(_temp2)
    return list_new
"""""", """""" 
    if (engine == 'symspellpy'):
        from pythainlp.spell.symspellpy import spell_sent as symspellpy_spell
        list_new = spell(list_words)
    else:
        _temp = list(itertools.product(*[symspellpy_spell(i, engine=engine) for i in list_words]))
        list_new = []
        for i in _temp:
            _temp2 = []
            for j in i:
                _temp2.append(j)
            list_new.append(_temp2)
    return list_new
""""""]",1
"DotDictWithPut, _get_int = _get_int, DotDictWithPut
def _extract_crash_info(crash_line, json_dump):
    """"""given a pipe dump CRASH line, extract the parts and put them in their
    proper location within the json_dump""""""","["""""" 
    crash_info = DotDictWithPut()
    crash_info.put_if_not_none('type', _get(crash_line, 1, None))
    crash_info.put_if_not_none('crash_address', _get(crash_line, 2, None))
    crash_info.put_if_not_none('crashing_thread', _get_int(crash_line, 3, None))
    json_dump.crash_info = crash_info
    return crash_info.get('crashing_thread', None)
"""""", """""" 
    crash_info = _get_int()
    crash_info.put_if_not_none('type', _get(crash_line, 1, None))
    crash_info.put_if_not_none('crash_address', _get(crash_line, 2, None))
    crash_info.put_if_not_none('crashing_thread', DotDictWithPut(crash_line, 3, None))
    json_dump.crash_info = crash_info
    return crash_info.get('crashing_thread', None)
""""""]",1
"asarray, lazy_array_equiv = lazy_array_equiv, asarray
def array_equiv(arr1, arr2):
    """"""Like np.array_equal, but also allows values to be NaN in both arrays""""""","["""""" 
    arr1 = asarray(arr1)
    arr2 = asarray(arr2)
    lazy_equiv = lazy_array_equiv(arr1, arr2)
    if (lazy_equiv is None):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', ""In the future, 'NAT == x'"")
            flag_array = ((arr1 == arr2) | (isnull(arr1) & isnull(arr2)))
            return bool(flag_array.all())
    else:
        return lazy_equiv
"""""", """""" 
    arr1 = lazy_array_equiv(arr1)
    arr2 = lazy_array_equiv(arr2)
    lazy_equiv = asarray(arr1, arr2)
    if (lazy_equiv is None):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', ""In the future, 'NAT == x'"")
            flag_array = ((arr1 == arr2) | (isnull(arr1) & isnull(arr2)))
            return bool(flag_array.all())
    else:
        return lazy_equiv
""""""]",1
"check_import, print_success = print_success, check_import
def main():
    """""" The entry point for this module """"""","["""""" 
    if check_import():
        sys.exit(1)
    from aeneas.diagnostics import Diagnostics
    (errors, warnings, c_ext_warnings) = Diagnostics.check_all()
    if errors:
        sys.exit(1)
    if c_ext_warnings:
        print_warning(u'All required dependencies are met but at least one Python C extension is not available')
        print_warning(u'You can still run aeneas but it will be slower')
        print_warning(u'Enjoy running aeneas!')
        sys.exit(2)
    else:
        print_success(u'All required dependencies are met and all available Python C extensions are working')
        print_success(u'Enjoy running aeneas!')
        sys.exit(0)
"""""", """""" 
    if print_success():
        sys.exit(1)
    from aeneas.diagnostics import Diagnostics
    (errors, warnings, c_ext_warnings) = Diagnostics.check_all()
    if errors:
        sys.exit(1)
    if c_ext_warnings:
        print_warning(u'All required dependencies are met but at least one Python C extension is not available')
        print_warning(u'You can still run aeneas but it will be slower')
        print_warning(u'Enjoy running aeneas!')
        sys.exit(2)
    else:
        check_import(u'All required dependencies are met and all available Python C extensions are working')
        check_import(u'Enjoy running aeneas!')
        sys.exit(0)
""""""]",1
"count, where_method = where_method, count
def _nan_minmax_object(func, fill_value, value, axis=None, **kwargs):
    """"""In house nanmin and nanmax for object array""""""","["""""" 
    valid_count = count(value, axis=axis)
    filled_value = fillna(value, fill_value)
    data = getattr(np, func)(filled_value, axis=axis, **kwargs)
    if (not hasattr(data, 'dtype')):
        data = (fill_value if (valid_count == 0) else data)
        return utils.to_0d_object_array(data)
    return where_method(data, (valid_count != 0))
"""""", """""" 
    valid_count = where_method(value, axis=axis)
    filled_value = fillna(value, fill_value)
    data = getattr(np, func)(filled_value, axis=axis, **kwargs)
    if (not hasattr(data, 'dtype')):
        data = (fill_value if (valid_count == 0) else data)
        return utils.to_0d_object_array(data)
    return count(data, (valid_count != 0))
""""""]",1
"generate_anchor, decode_delta = decode_delta, generate_anchor
def decode_delta_map(delta_map, anchors):
    """"""
    :param: delta_map, shape (nB, nA, nGh, nGw, 4)
    :param: anchors, shape (nA,4)
    """"""","["""""" 
    (nB, nA, nGh, nGw, _) = delta_map.shape
    anchor_mesh = generate_anchor(nGh, nGw, anchors)
    anchor_mesh = anchor_mesh.permute(0, 2, 3, 1).contiguous()
    anchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB, 1, 1, 1, 1)
    pred_list = decode_delta(delta_map.view((- 1), 4), anchor_mesh.view((- 1), 4))
    pred_map = pred_list.view(nB, nA, nGh, nGw, 4)
    return pred_map
"""""", """""" 
    (nB, nA, nGh, nGw, _) = delta_map.shape
    anchor_mesh = decode_delta(nGh, nGw, anchors)
    anchor_mesh = anchor_mesh.permute(0, 2, 3, 1).contiguous()
    anchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB, 1, 1, 1, 1)
    pred_list = generate_anchor(delta_map.view((- 1), 4), anchor_mesh.view((- 1), 4))
    pred_map = pred_list.view(nB, nA, nGh, nGw, 4)
    return pred_map
""""""]",1
"_Xfftn, _norm_args = _norm_args, _Xfftn
def fft2(a, s=None, axes=((- 2), (- 1)), overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True, avoid_copy=False, norm=None):
    """"""Return a :class:`pyfftw.FFTW` object representing a 2D FFT.

    The first three arguments are as per :func:`numpy.fft.fft2`;
    the rest of the arguments are documented
    :ref:`in the module docs <builders_args>`.
    """"""","["""""" 
    inverse = False
    real = False
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
"""""", """""" 
    inverse = False
    real = False
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _norm_args(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_Xfftn(norm))
""""""]",1
"OrderedDict, Child = Child, OrderedDict
def test_dict_ancestor_and_reversed():
    """"""Don't emit for subclasses of dict, with __reversed__ implemented.""""""","["""""" 

    class Child(dict):

        def __reversed__(self):
            return reversed(range(10))
    seq = reversed(OrderedDict())
    return (reversed(Child()), seq)
"""""", """""" 

    class Child(dict):

        def __reversed__(self):
            return reversed(range(10))
    seq = reversed(Child())
    return (reversed(OrderedDict()), seq)
""""""]",1
"eye, shape = shape, eye
def ridgeRegres(xMat, yMat, lam=0.2):
    """"""
        Desc: 
            这个函数实现了给定 lambda 下的岭回归求解。
            如果数据的特征比样本点还多，就不能再使用上面介绍的的线性回归和局部现行回归了，因为计算 (xTx)^(-1)会出现错误。
            如果特征比样本点还多（n > m），也就是说，输入数据的矩阵x不是满秩矩阵。非满秩矩阵在求逆时会出现问题。
            为了解决这个问题，我们下边讲一下: 岭回归，这是我们要讲的第一种缩减方法。
        Args: 
            xMat: 样本的特征数据，即 feature
            yMat: 每个样本对应的类别标签，即目标变量，实际值
            lam: 引入的一个λ值，使得矩阵非奇异
        Returns: 
            经过岭回归公式计算得到的回归系数
    """"""","["""""" 
    xTx = (xMat.T * xMat)
    denom = (xTx + (eye(shape(xMat)[1]) * lam))
    if (linalg.det(denom) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (denom.I * (xMat.T * yMat))
    return ws
"""""", """""" 
    xTx = (xMat.T * xMat)
    denom = (xTx + (shape(eye(xMat)[1]) * lam))
    if (linalg.det(denom) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (denom.I * (xMat.T * yMat))
    return ws
""""""]",1
"modify_set_fc_name, modify_set_fc_description = modify_set_fc_description, modify_set_fc_name
def modify_set_fc(name):
    """"""
    modify a set
    :param name:
    """"""","["""""" 
    sets = get_set_statuses()
    if (not sets):
        click.echo(chalk.red('There are no sets right now. Type ""yoda flashcards sets new <name>"" to create one'))
    elif (not sets[name]):
        click.echo(chalk.red((('There is no set named ' + name) + '.')))
    else:
        click.echo(chalk.blue(""Edit a new name for this set: (If you wish to keep it the same, just type a single '-' without the quotes)""))
        new_name = input().strip()
        if (not ((new_name is None) or (new_name == '-') or (new_name == ''))):
            modify_set_fc_name(name, new_name)
            modify_set_fc_description(name, new_name)
            print(((((""The name was modified from '"" + name) + ""' to '"") + new_name) + ""'""))
"""""", """""" 
    sets = get_set_statuses()
    if (not sets):
        click.echo(chalk.red('There are no sets right now. Type ""yoda flashcards sets new <name>"" to create one'))
    elif (not sets[name]):
        click.echo(chalk.red((('There is no set named ' + name) + '.')))
    else:
        click.echo(chalk.blue(""Edit a new name for this set: (If you wish to keep it the same, just type a single '-' without the quotes)""))
        new_name = input().strip()
        if (not ((new_name is None) or (new_name == '-') or (new_name == ''))):
            modify_set_fc_description(name, new_name)
            modify_set_fc_name(name, new_name)
            print(((((""The name was modified from '"" + name) + ""' to '"") + new_name) + ""'""))
""""""]",1
"Path, Project = Project, Path
def test_all_files_hg_ignored_contains_space(hg_repository):
    """"""File names that contain spaces are also ignored.""""""","["""""" 
    (hg_repository / 'I contain spaces.pyc').touch()
    project = Project(hg_repository)
    assert (Path('I contain spaces.pyc').absolute() not in project.all_files())
"""""", """""" 
    (hg_repository / 'I contain spaces.pyc').touch()
    project = Path(hg_repository)
    assert (Project('I contain spaces.pyc').absolute() not in project.all_files())
""""""]",1
"infer_node, patch = patch, infer_node
@patch('pylint.pyreverse.utils.get_annotation')
@patch('astroid.nodes.NodeNG.infer', side_effect=astroid.InferenceError)
def test_infer_node_1(mock_infer: Any, mock_get_annotation: Any) -> None:
    """"""Return set() when astroid.InferenceError is raised and an annotation has
    not been returned
    """"""","["""""" 
    mock_get_annotation.return_value = None
    node = astroid.extract_node(""a: str = 'mystr'"")
    mock_infer.return_value = 'x'
    assert (infer_node(node) == set())
    assert mock_infer.called
"""""", """""" 
    mock_get_annotation.return_value = None
    node = astroid.extract_node(""a: str = 'mystr'"")
    mock_infer.return_value = 'x'
    assert (patch(node) == set())
    assert mock_infer.called
""""""]",1
"deleteDB, SiteError = SiteError, deleteDB
def doCleanupAction(self, domain='', webroot='', dbname='', dbuser='', dbhost=''):
    """"""
       Removes the nginx configuration and database for the domain provided.
       doCleanupAction(self, domain='sitename', webroot='',
                       dbname='', dbuser='', dbhost='')
    """"""","["""""" 
    if domain:
        if os.path.isfile('/etc/nginx/sites-available/{0}'.format(domain)):
            removeNginxConf(self, domain)
            WOAcme.removeconf(self, domain)
    if webroot:
        deleteWebRoot(self, webroot)
    if dbname:
        if (not dbuser):
            raise SiteError('dbuser not provided')
        if (not dbhost):
            raise SiteError('dbhost not provided')
        deleteDB(self, dbname, dbuser, dbhost)
"""""", """""" 
    if domain:
        if os.path.isfile('/etc/nginx/sites-available/{0}'.format(domain)):
            removeNginxConf(self, domain)
            WOAcme.removeconf(self, domain)
    if webroot:
        deleteWebRoot(self, webroot)
    if dbname:
        if (not dbuser):
            raise deleteDB('dbuser not provided')
        if (not dbhost):
            raise deleteDB('dbhost not provided')
        SiteError(self, dbname, dbuser, dbhost)
""""""]",1
"wire_specified_boards, FastSystem = FastSystem, wire_specified_boards
def wire(machine: MachineController):
    """"""Wire a given machine using FAST Pinball boards.""""""","["""""" 
    s = FastSystem()
    wire_lights(machine, s)
    inconsistent_err = (""Can't wire: all switch and driver numbers must be in the same format, (board-index) or "" + 'raw number, not a mixture of both.')
    switches_specify_boards = None
    for switch in machine.switches.values():
        num_spec = switch.config['number']
        if (switches_specify_boards is None):
            switches_specify_boards = ('-' in num_spec)
        elif (('-' in num_spec) != switches_specify_boards):
            print(inconsistent_err)
            return None
    drivers_specify_boards = None
    for coil in machine.coils.values():
        num_spec = coil.config['number']
        if (drivers_specify_boards is None):
            drivers_specify_boards = ('-' in num_spec)
        elif (('-' in num_spec) != drivers_specify_boards):
            print(inconsistent_err)
            return None
    if (switches_specify_boards != drivers_specify_boards):
        print(inconsistent_err)
        return None
    if switches_specify_boards:
        wire_specified_boards(machine, s)
    else:
        print(""Can't currently wire FAST pinball if board numbers are not specified."")
    return s.dump()
"""""", """""" 
    s = wire_specified_boards()
    wire_lights(machine, s)
    inconsistent_err = (""Can't wire: all switch and driver numbers must be in the same format, (board-index) or "" + 'raw number, not a mixture of both.')
    switches_specify_boards = None
    for switch in machine.switches.values():
        num_spec = switch.config['number']
        if (switches_specify_boards is None):
            switches_specify_boards = ('-' in num_spec)
        elif (('-' in num_spec) != switches_specify_boards):
            print(inconsistent_err)
            return None
    drivers_specify_boards = None
    for coil in machine.coils.values():
        num_spec = coil.config['number']
        if (drivers_specify_boards is None):
            drivers_specify_boards = ('-' in num_spec)
        elif (('-' in num_spec) != drivers_specify_boards):
            print(inconsistent_err)
            return None
    if (switches_specify_boards != drivers_specify_boards):
        print(inconsistent_err)
        return None
    if switches_specify_boards:
        FastSystem(machine, s)
    else:
        print(""Can't currently wire FAST pinball if board numbers are not specified."")
    return s.dump()
""""""]",1
"create_qss, generate_qrc_file = generate_qrc_file, create_qss
def process_palette(palette, compile_for='qtpy'):
    """"""Process palette class to create a new palette file/folders.

    It generates all files below, in this order:
        - Palette files (svg/.png) under docs/images/[palette_id]
        - Image files (.png) under [palette_id]/rc folder.
        - QRC file in [palette_id]/[palette_id]style.qrc (C++).
        - SCSS variables in [palette_id]/_variables.scss file.
        - QSS file in [palette_id]/[palette_id]style.qss.
        - Compiled QRC file in [palette_id]/[palette_id]style_rc.py

    TODO:
        - Must generalize to create custom palettes and folder paths.
        - Must create/copy all files under [palette_id], such as main.scss,
            __init__.py, palette.py.
        - Add option or avoid adding the palette under docs for custom palettes.

    Args:
        palette (Palette): Palette.
        compile_for (list, optional): Prefix used in resources.
            Defaults to 'qtpy'. Possible values are 'qtpy', 'pyqtgraph',
            'pyqt', 'pyqt5', 'pyside', 'pyside2', 'qt', 'qt5', 'all'.
    """"""","["""""" 
    if (palette is None):
        _logger.error('Please pass a palette class in order to create its associated images')
        sys.exit(1)
    if (palette.ID is None):
        _logger.error('A QDarkStyle palette requires an ID!')
        sys.exit(1)
    id_ = palette.ID
    print(f'-- PROCESSING THEME: {id_}')
    print(f'-- GENERATING PALETTE IMAGE FOR: {id_}')
    create_palette_image(palette=palette)
    print(f'-- GENERATING IMAGE FILES (.svg > .png) FOR: {id_}')
    create_images(palette=palette)
    print(f'-- GENERATING QRC FILE FOR: {id_}')
    generate_qrc_file(palette=palette)
    print(f'-- GENERATING QSS FILE (.scss > .qss) FOR: {id_}')
    create_qss(palette=palette)
    print(f'-- CONVERTING RESOURCE FILE (. qrc > _rc.py/.rcc) FOR: {id_}')
    compile_qrc_file(compile_for=compile_for, palette=palette)
"""""", """""" 
    if (palette is None):
        _logger.error('Please pass a palette class in order to create its associated images')
        sys.exit(1)
    if (palette.ID is None):
        _logger.error('A QDarkStyle palette requires an ID!')
        sys.exit(1)
    id_ = palette.ID
    print(f'-- PROCESSING THEME: {id_}')
    print(f'-- GENERATING PALETTE IMAGE FOR: {id_}')
    create_palette_image(palette=palette)
    print(f'-- GENERATING IMAGE FILES (.svg > .png) FOR: {id_}')
    create_images(palette=palette)
    print(f'-- GENERATING QRC FILE FOR: {id_}')
    create_qss(palette=palette)
    print(f'-- GENERATING QSS FILE (.scss > .qss) FOR: {id_}')
    generate_qrc_file(palette=palette)
    print(f'-- CONVERTING RESOURCE FILE (. qrc > _rc.py/.rcc) FOR: {id_}')
    compile_qrc_file(compile_for=compile_for, palette=palette)
""""""]",1
"render_point_cloud_pytorch3d, ImplicitronRender = ImplicitronRender, render_point_cloud_pytorch3d
def render_point_cloud(camera: CamerasBase, render_size: Tuple[(int, int)], pointcloud: Pointclouds, point_radius: float=0.03) -> ImplicitronRender:
    """"""
    Render the point cloud `pointcloud` to the camera `camera` using the 
    PyTorch3D point cloud renderer.

    Args:
        camera: Rendering camera.
        render_size: 2-tuple of integers denoting the render size (HxW)
        pointcloud: The point cloud to render.
        point_radius: Radius of the rendered points.
    """"""","["""""" 
    (data_rendered, render_mask, depth_rendered) = render_point_cloud_pytorch3d(camera, pointcloud, render_size=render_size, point_radius=point_radius, topk=10, eps=0.01, bin_size=0)
    return ImplicitronRender(depth_render=depth_rendered, image_render=data_rendered, mask_render=render_mask)
"""""", """""" 
    (data_rendered, render_mask, depth_rendered) = ImplicitronRender(camera, pointcloud, render_size=render_size, point_radius=point_radius, topk=10, eps=0.01, bin_size=0)
    return render_point_cloud_pytorch3d(depth_render=depth_rendered, image_render=data_rendered, mask_render=render_mask)
""""""]",1
"check_configuration_file_reader, run_using_a_configuration_file = run_using_a_configuration_file, check_configuration_file_reader
def test_can_read_toml_env_variable(tmp_path: Path, file_to_lint_path: str) -> None:
    """"""We can read and open a properly formatted toml file.""""""","["""""" 
    config_file = (tmp_path / 'pyproject.toml')
    config_file.write_text('\n[tool.pylint.""messages control""]\ndisable = ""logging-not-lazy,logging-format-interpolation""\njobs = ""10""\nreports = ""yes""\n')
    env_var = 'tmp_path_env'
    os.environ[env_var] = str(config_file)
    (mock_exit, _, runner) = run_using_a_configuration_file(f'${env_var}', file_to_lint_path)
    mock_exit.assert_called_once_with(0)
    check_configuration_file_reader(runner)
"""""", """""" 
    config_file = (tmp_path / 'pyproject.toml')
    config_file.write_text('\n[tool.pylint.""messages control""]\ndisable = ""logging-not-lazy,logging-format-interpolation""\njobs = ""10""\nreports = ""yes""\n')
    env_var = 'tmp_path_env'
    os.environ[env_var] = str(config_file)
    (mock_exit, _, runner) = check_configuration_file_reader(f'${env_var}', file_to_lint_path)
    mock_exit.assert_called_once_with(0)
    run_using_a_configuration_file(runner)
""""""]",1
"validate_email_with_name, ValidationError = ValidationError, validate_email_with_name
def validate_comma_separated_emails(value):
    """"""
    Validate every email address in a comma separated list of emails.
    """"""","["""""" 
    if (not isinstance(value, (tuple, list))):
        raise ValidationError('Email list must be a list/tuple.')
    for email in value:
        try:
            validate_email_with_name(email)
        except ValidationError:
            raise ValidationError(('Invalid email: %s' % email), code='invalid')
"""""", """""" 
    if (not isinstance(value, (tuple, list))):
        raise validate_email_with_name('Email list must be a list/tuple.')
    for email in value:
        try:
            ValidationError(email)
        except validate_email_with_name:
            raise validate_email_with_name(('Invalid email: %s' % email), code='invalid')
""""""]",1
"generalize_names, Version = Version, generalize_names
def generalize_names_duplcheck(df, col_name):
    """"""Generalizes names and removes duplicates.

    Description : Applies mlxtend.text.generalize_names to a DataFrame
    with 1 first name letter by default
    and uses more first name letters if duplicates are detected.

    Parameters
    ----------
    df : `pandas.DataFrame`
        DataFrame that contains a column where
        generalize_names should be applied.
    col_name : `str`
        Name of the DataFrame column where `generalize_names`
        function should be applied to.

    Returns
    ----------
    df_new : `str`
        New DataFrame object where generalize_names function has
        been applied without duplicates.

    Examples
    -----------
    For usage examples, please see
    http://rasbt.github.io/mlxtend/user_guide/text/generalize_names_duplcheck/

    """"""","["""""" 
    df_new = df.copy()
    df_new.drop_duplicates(subset=[col_name], inplace=True)
    df_new[col_name] = df_new[col_name].apply(generalize_names)
    if (Version(pandas_version) < Version('0.17')):
        dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    else:
        dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    firstname_letters = 2
    while (len(dupl) > 0):
        for idx in dupl:
            df_new.loc[(idx, col_name)] = generalize_names(df.loc[(idx, col_name)], firstname_output_letters=firstname_letters)
        if (Version(pandas_version) < Version('0.17')):
            dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        else:
            dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        firstname_letters += 1
    return df_new
"""""", """""" 
    df_new = df.copy()
    df_new.drop_duplicates(subset=[col_name], inplace=True)
    df_new[col_name] = df_new[col_name].apply(Version)
    if (generalize_names(pandas_version) < generalize_names('0.17')):
        dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    else:
        dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    firstname_letters = 2
    while (len(dupl) > 0):
        for idx in dupl:
            df_new.loc[(idx, col_name)] = Version(df.loc[(idx, col_name)], firstname_output_letters=firstname_letters)
        if (generalize_names(pandas_version) < generalize_names('0.17')):
            dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        else:
            dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        firstname_letters += 1
    return df_new
""""""]",1
"create_compressed_model, InitLoader = InitLoader, create_compressed_model
def wrap_nncf_model(model: nn.Module, config: Dict, dataloader: DataLoader=None, init_state_dict: Dict=None) -> Tuple[(CompressionAlgorithmController, NNCFNetwork)]:
    """"""Wrap model by NNCF.

    :param model: Anomalib model.
    :param config: NNCF config.
    :param dataloader: Dataloader for initialization of NNCF model.
    :param init_state_dict: Opti
    :return: compression controller, compressed model
    """"""","["""""" 
    nncf_config = NNCFConfig.from_dict(config)
    if ((not dataloader) and (not init_state_dict)):
        logger.warning('Either dataloader or NNCF pre-trained model checkpoint should be set. Without this, quantizers will not be initialized')
    compression_state = None
    resuming_state_dict = None
    if init_state_dict:
        resuming_state_dict = init_state_dict.get('model')
        compression_state = init_state_dict.get('compression_state')
    if dataloader:
        init_loader = InitLoader(dataloader)
        nncf_config = register_default_init_args(nncf_config, init_loader)
    (nncf_ctrl, nncf_model) = create_compressed_model(model=model, config=nncf_config, dump_graphs=False, compression_state=compression_state)
    if resuming_state_dict:
        load_state(nncf_model, resuming_state_dict, is_resume=True)
    return (nncf_ctrl, nncf_model)
"""""", """""" 
    nncf_config = NNCFConfig.from_dict(config)
    if ((not dataloader) and (not init_state_dict)):
        logger.warning('Either dataloader or NNCF pre-trained model checkpoint should be set. Without this, quantizers will not be initialized')
    compression_state = None
    resuming_state_dict = None
    if init_state_dict:
        resuming_state_dict = init_state_dict.get('model')
        compression_state = init_state_dict.get('compression_state')
    if dataloader:
        init_loader = create_compressed_model(dataloader)
        nncf_config = register_default_init_args(nncf_config, init_loader)
    (nncf_ctrl, nncf_model) = InitLoader(model=model, config=nncf_config, dump_graphs=False, compression_state=compression_state)
    if resuming_state_dict:
        load_state(nncf_model, resuming_state_dict, is_resume=True)
    return (nncf_ctrl, nncf_model)
""""""]",1
"print_lending_list, empty_lending_list_prompt = empty_lending_list_prompt, print_lending_list
def show_lending_list(params):
    """"""
    shows all items lent and borrowed
    """"""","["""""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with open(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = dict(file_contents)
            last_updated = time.ctime(os.path.getmtime(LENDLIST_CONFIG_FILE_PATH))
            click.echo(chalk.blue(('Last updated: ' + last_updated)))
            print_lending_list(file_contents)
    else:
        empty_lending_list_prompt()
"""""", """""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with open(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = dict(file_contents)
            last_updated = time.ctime(os.path.getmtime(LENDLIST_CONFIG_FILE_PATH))
            click.echo(chalk.blue(('Last updated: ' + last_updated)))
            empty_lending_list_prompt(file_contents)
    else:
        print_lending_list()
""""""]",1
"check_parameters, check_service_principals = check_service_principals, check_parameters
@helper.update
def update(event: CloudFormationCustomResourceUpdate, context: Context) -> str:
    """"""Process CloudFormation Update Event.

    Args:
        event: event data
        context: runtime information

    Returns:
        Resource ID
    """"""","["""""" 
    LOGGER.info(f'Update Event: {event}')
    check_parameters(event)
    params = event['ResourceProperties']
    aws_service_principal_list = [value.strip() for value in params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    check_service_principals(aws_service_principal_list)
    old_params = event['OldResourceProperties']
    old_aws_service_principal_list = [value.strip() for value in old_params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    add_list = list((set(aws_service_principal_list) - set(old_aws_service_principal_list)))
    remove_list = list((set(old_aws_service_principal_list) - set(aws_service_principal_list)))
    if add_list:
        for aws_service_principal in add_list:
            enable_aws_service_access(aws_service_principal)
            register_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
    if remove_list:
        for aws_service_principal in remove_list:
            deregister_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
            disable_aws_service_access(aws_service_principal)
    return f""DelegatedAdminResourceId-{params['DELEGATED_ADMIN_ACCOUNT_ID']}""
"""""", """""" 
    LOGGER.info(f'Update Event: {event}')
    check_service_principals(event)
    params = event['ResourceProperties']
    aws_service_principal_list = [value.strip() for value in params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    check_parameters(aws_service_principal_list)
    old_params = event['OldResourceProperties']
    old_aws_service_principal_list = [value.strip() for value in old_params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    add_list = list((set(aws_service_principal_list) - set(old_aws_service_principal_list)))
    remove_list = list((set(old_aws_service_principal_list) - set(aws_service_principal_list)))
    if add_list:
        for aws_service_principal in add_list:
            enable_aws_service_access(aws_service_principal)
            register_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
    if remove_list:
        for aws_service_principal in remove_list:
            deregister_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
            disable_aws_service_access(aws_service_principal)
    return f""DelegatedAdminResourceId-{params['DELEGATED_ADMIN_ACCOUNT_ID']}""
""""""]",1
"sharp_invoke, fullyQualifiedTemplateTitle = fullyQualifiedTemplateTitle, sharp_invoke
def callParserFunction(functionName, args, extractor):
    """"""
    Parser functions have similar syntax as templates, except that
    the first argument is everything after the first colon.
    :return: the result of the invocation, None in case of failure.

    :param: args not yet expanded (see branching functions).
    https://www.mediawiki.org/wiki/Help:Extension:ParserFunctions
    """"""","["""""" 
    try:
        functionName = functionName.lower()
        if (functionName == '#invoke'):
            (module, fun) = (args[0].strip(), args[1].strip())
            logging.debug('%*s#invoke %s %s %s', extractor.frame.depth, '', module, fun, args[2:])
            if (len(args) == 2):
                templateTitle = fullyQualifiedTemplateTitle(module)
                if (not templateTitle):
                    logging.warn('Template with empty title')
                params = None
                frame = extractor.frame
                while frame:
                    if (frame.title == templateTitle):
                        params = frame.args
                        break
                    frame = frame.prev
            else:
                params = [extractor.transform(p) for p in args[2:]]
                params = extractor.templateParams(params)
            ret = sharp_invoke(module, fun, params)
            logging.debug('%*s<#invoke %s %s %s', extractor.frame.depth, '', module, fun, ret)
            return ret
        if (functionName in parserFunctions):
            return parserFunctions[functionName](extractor, *args)
    except:
        return ''
    return ''
"""""", """""" 
    try:
        functionName = functionName.lower()
        if (functionName == '#invoke'):
            (module, fun) = (args[0].strip(), args[1].strip())
            logging.debug('%*s#invoke %s %s %s', extractor.frame.depth, '', module, fun, args[2:])
            if (len(args) == 2):
                templateTitle = sharp_invoke(module)
                if (not templateTitle):
                    logging.warn('Template with empty title')
                params = None
                frame = extractor.frame
                while frame:
                    if (frame.title == templateTitle):
                        params = frame.args
                        break
                    frame = frame.prev
            else:
                params = [extractor.transform(p) for p in args[2:]]
                params = extractor.templateParams(params)
            ret = fullyQualifiedTemplateTitle(module, fun, params)
            logging.debug('%*s<#invoke %s %s %s', extractor.frame.depth, '', module, fun, ret)
            return ret
        if (functionName in parserFunctions):
            return parserFunctions[functionName](extractor, *args)
    except:
        return ''
    return ''
""""""]",1
"ProcessPoolExecutor, as_completed = as_completed, ProcessPoolExecutor
def parallel_process(array, function, n_jobs=16, use_kwargs=False, front_num=3):
    """"""
        A parallel version of the map function with a progress bar. 

        Args:
            array (array-like): An array to iterate over.
            function (function): A python function to apply to the elements of array
            n_jobs (int, default=16): The number of cores to use
            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of 
                keyword arguments to function 
            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job. 
                Useful for catching bugs
        Returns:
            [function(array[0]), function(array[1]), ...]
    """"""","["""""" 
    if (front_num > 0):
        front = [(function(**a) if use_kwargs else function(*a)) for a in array[:front_num]]
    if (n_jobs == 1):
        return (front + [(function(**a) if use_kwargs else function(*a)) for a in tqdm(array[front_num:])])
    with ProcessPoolExecutor(max_workers=n_jobs) as pool:
        if use_kwargs:
            futures = [pool.submit(function, **a) for a in array[front_num:]]
        else:
            futures = [pool.submit(function, *a) for a in array[front_num:]]
        kwargs = {'total': len(futures), 'unit': 'it', 'unit_scale': True, 'leave': True}
        for f in tqdm(as_completed(futures), **kwargs):
            pass
    out = []
    for (i, future) in enumerate(futures):
        try:
            out.append(future.result())
        except Exception as e:
            out.append(e)
    return (front + out)
"""""", """""" 
    if (front_num > 0):
        front = [(function(**a) if use_kwargs else function(*a)) for a in array[:front_num]]
    if (n_jobs == 1):
        return (front + [(function(**a) if use_kwargs else function(*a)) for a in tqdm(array[front_num:])])
    with as_completed(max_workers=n_jobs) as pool:
        if use_kwargs:
            futures = [pool.submit(function, **a) for a in array[front_num:]]
        else:
            futures = [pool.submit(function, *a) for a in array[front_num:]]
        kwargs = {'total': len(futures), 'unit': 'it', 'unit_scale': True, 'leave': True}
        for f in tqdm(ProcessPoolExecutor(futures), **kwargs):
            pass
    out = []
    for (i, future) in enumerate(futures):
        try:
            out.append(future.result())
        except Exception as e:
            out.append(e)
    return (front + out)
""""""]",1
"align, is_dict_like = is_dict_like, align
def deep_align(objects: Iterable[Any], join: JoinOptions='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):
    """"""Align objects for merging, recursing into dictionary values.

    This function is not public API.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}

    def is_alignable(obj):
        return isinstance(obj, (DataArray, Dataset))
    positions = []
    keys = []
    out = []
    targets = []
    no_key = object()
    not_replaced = object()
    for (position, variables) in enumerate(objects):
        if is_alignable(variables):
            positions.append(position)
            keys.append(no_key)
            targets.append(variables)
            out.append(not_replaced)
        elif is_dict_like(variables):
            current_out = {}
            for (k, v) in variables.items():
                if (is_alignable(v) and (k not in indexes)):
                    positions.append(position)
                    keys.append(k)
                    targets.append(v)
                    current_out[k] = not_replaced
                else:
                    current_out[k] = v
            out.append(current_out)
        elif raise_on_invalid:
            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))
        else:
            out.append(variables)
    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)
    for (position, key, aligned_obj) in zip(positions, keys, aligned):
        if (key is no_key):
            out[position] = aligned_obj
        else:
            out[position][key] = aligned_obj
    return out
"""""", """""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}

    def is_alignable(obj):
        return isinstance(obj, (DataArray, Dataset))
    positions = []
    keys = []
    out = []
    targets = []
    no_key = object()
    not_replaced = object()
    for (position, variables) in enumerate(objects):
        if is_alignable(variables):
            positions.append(position)
            keys.append(no_key)
            targets.append(variables)
            out.append(not_replaced)
        elif align(variables):
            current_out = {}
            for (k, v) in variables.items():
                if (is_alignable(v) and (k not in indexes)):
                    positions.append(position)
                    keys.append(k)
                    targets.append(v)
                    current_out[k] = not_replaced
                else:
                    current_out[k] = v
            out.append(current_out)
        elif raise_on_invalid:
            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))
        else:
            out.append(variables)
    aligned = is_dict_like(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)
    for (position, key, aligned_obj) in zip(positions, keys, aligned):
        if (key is no_key):
            out[position] = aligned_obj
        else:
            out[position][key] = aligned_obj
    return out
""""""]",1
"OrderedSet, _AddNormalizedSources = _AddNormalizedSources, OrderedSet
def _PrepareListOfSources(spec, generator_flags, gyp_file):
    """"""Prepare list of sources and excluded sources.

  Besides the sources specified directly in the spec, adds the gyp file so
  that a change to it will cause a re-compile. Also adds appropriate sources
  for actions and copies. Assumes later stage will un-exclude files which
  have custom build steps attached.

  Arguments:
    spec: The target dictionary containing the properties of the target.
    gyp_file: The name of the gyp file.
  Returns:
    A pair of (list of sources, list of excluded sources).
    The sources will be relative to the gyp file.
  """"""","["""""" 
    sources = OrderedSet()
    _AddNormalizedSources(sources, spec.get('sources', []))
    excluded_sources = OrderedSet()
    if (not generator_flags.get('standalone')):
        sources.add(gyp_file)
    for a in spec.get('actions', []):
        inputs = a['inputs']
        inputs = [_NormalizedSource(i) for i in inputs]
        inputs = OrderedSet(inputs)
        sources.update(inputs)
        if (not spec.get('msvs_external_builder')):
            excluded_sources.update(inputs)
        if int(a.get('process_outputs_as_sources', False)):
            _AddNormalizedSources(sources, a.get('outputs', []))
    for cpy in spec.get('copies', []):
        _AddNormalizedSources(sources, cpy.get('files', []))
    return (sources, excluded_sources)
"""""", """""" 
    sources = _AddNormalizedSources()
    OrderedSet(sources, spec.get('sources', []))
    excluded_sources = _AddNormalizedSources()
    if (not generator_flags.get('standalone')):
        sources.add(gyp_file)
    for a in spec.get('actions', []):
        inputs = a['inputs']
        inputs = [_NormalizedSource(i) for i in inputs]
        inputs = _AddNormalizedSources(inputs)
        sources.update(inputs)
        if (not spec.get('msvs_external_builder')):
            excluded_sources.update(inputs)
        if int(a.get('process_outputs_as_sources', False)):
            OrderedSet(sources, a.get('outputs', []))
    for cpy in spec.get('copies', []):
        OrderedSet(sources, cpy.get('files', []))
    return (sources, excluded_sources)
""""""]",1
"_find_parser_cls, add_parser_usage_data = add_parser_usage_data, _find_parser_cls
def get_parser(command, device, fuzzy=False):
    """"""From a show command and device, return parser class and kwargs if any""""""","["""""" 
    try:
        order_list = device.custom.get('abstraction').get('order', [])
    except AttributeError:
        order_list = None
    lookup = Lookup.from_device(device, packages={'parser': parser})
    results = _fuzzy_search_command(command, fuzzy, device.os, order_list)
    valid_results = []
    for result in results:
        (found_command, data, kwargs) = result
        if (found_command == 'tokens'):
            continue
        for token in lookup._tokens:
            if (token in data):
                data = data[token]
        try:
            valid_results.append((found_command, _find_parser_cls(device, data), kwargs))
        except KeyError:
            continue
    if (not valid_results):
        'result is not valid. raise custom ParserNotFound exception'
        raise ParserNotFound(command, lookup._tokens)
    if INTERNAL:
        try:
            add_parser_usage_data(valid_results[0], device)
        except Exception as e:
            log.debug(('Encountered an unexpected error while adding parser telemetry data: %s' % e))
    if (not fuzzy):
        parser_class = valid_results[0][1]
        parser_kwargs = valid_results[0][2]
        log.debug(f'Parser class: {parser_class} arguments: {parser_kwargs}')
        return (parser_class, parser_kwargs)
    log.debug(f'Parser search results: {valid_results}')
    return valid_results
"""""", """""" 
    try:
        order_list = device.custom.get('abstraction').get('order', [])
    except AttributeError:
        order_list = None
    lookup = Lookup.from_device(device, packages={'parser': parser})
    results = _fuzzy_search_command(command, fuzzy, device.os, order_list)
    valid_results = []
    for result in results:
        (found_command, data, kwargs) = result
        if (found_command == 'tokens'):
            continue
        for token in lookup._tokens:
            if (token in data):
                data = data[token]
        try:
            valid_results.append((found_command, add_parser_usage_data(device, data), kwargs))
        except KeyError:
            continue
    if (not valid_results):
        'result is not valid. raise custom ParserNotFound exception'
        raise ParserNotFound(command, lookup._tokens)
    if INTERNAL:
        try:
            _find_parser_cls(valid_results[0], device)
        except Exception as e:
            log.debug(('Encountered an unexpected error while adding parser telemetry data: %s' % e))
    if (not fuzzy):
        parser_class = valid_results[0][1]
        parser_kwargs = valid_results[0][2]
        log.debug(f'Parser class: {parser_class} arguments: {parser_kwargs}')
        return (parser_class, parser_kwargs)
    log.debug(f'Parser search results: {valid_results}')
    return valid_results
""""""]",1
"GetComment, LoadPlist = LoadPlist, GetComment
def main():
    """"""Main function.""""""","["""""" 
    print(HEADER)
    comments_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'comments.csv')
    with open(comments_file, 'rb') as c_file:
        reader = csv.reader(c_file)
        comments = {rows[0]: rows[1] for rows in reader}
    for ptype in PLIST_TYPES:
        for filename in glob.glob((PLIST_LOCATION % ptype)):
            prop = LoadPlist(filename)
            if prop:
                print(('%s,%s,%s,%s,%s' % (filename, GetPlistValue(prop, 'Label'), ('""%s"",%s' % GetProgram(prop)), GetPlistValue(prop, 'RunAtLoad'), ('""%s""' % GetComment(prop, comments)))))
            else:
                print(('Could not load %s' % filename))
"""""", """""" 
    print(HEADER)
    comments_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'comments.csv')
    with open(comments_file, 'rb') as c_file:
        reader = csv.reader(c_file)
        comments = {rows[0]: rows[1] for rows in reader}
    for ptype in PLIST_TYPES:
        for filename in glob.glob((PLIST_LOCATION % ptype)):
            prop = GetComment(filename)
            if prop:
                print(('%s,%s,%s,%s,%s' % (filename, GetPlistValue(prop, 'Label'), ('""%s"",%s' % GetProgram(prop)), GetPlistValue(prop, 'RunAtLoad'), ('""%s""' % LoadPlist(prop, comments)))))
            else:
                print(('Could not load %s' % filename))
""""""]",1
"spawn_and_kill_pipeline, kill_if_alive = kill_if_alive, spawn_and_kill_pipeline
@pytest.mark.usefixtures('both_debug_modes', 'setsid_disabled')
def test_no_setsid_doesnt_signal_entire_group():
    """"""When dumb-init is not running in setsid mode, it should only signal its
    immediate child.
    """"""","["""""" 
    pids = spawn_and_kill_pipeline()

    def assert_four_living_pids():
        assert (len(living_pids(pids)) == 4)
    sleep_until(assert_four_living_pids)
    for pid in living_pids(pids):
        kill_if_alive(pid)
"""""", """""" 
    pids = kill_if_alive()

    def assert_four_living_pids():
        assert (len(living_pids(pids)) == 4)
    sleep_until(assert_four_living_pids)
    for pid in living_pids(pids):
        spawn_and_kill_pipeline(pid)
""""""]",1
"eval_grid, eval_grid_octree = eval_grid_octree, eval_grid
def reconstruction(net, cuda, calib_tensor, resolution, b_min, b_max, thresh=0.5, use_octree=False, num_samples=10000, transform=None):
    """"""
    Reconstruct meshes from sdf predicted by the network.
    :param net: a BasePixImpNet object. call image filter beforehead.
    :param cuda: cuda device
    :param calib_tensor: calibration tensor
    :param resolution: resolution of the grid cell
    :param b_min: bounding box corner [x_min, y_min, z_min]
    :param b_max: bounding box corner [x_max, y_max, z_max]
    :param use_octree: whether to use octree acceleration
    :param num_samples: how many points to query each gpu iteration
    :return: marching cubes results.
    """"""","["""""" 
    (coords, mat) = create_grid(resolution, resolution, resolution)
    calib = calib_tensor[0].cpu().numpy()
    calib_inv = inv(calib)
    coords = coords.reshape(3, (- 1)).T
    coords = np.matmul(np.concatenate([coords, np.ones((coords.shape[0], 1))], 1), calib_inv.T)[:, :3]
    coords = coords.T.reshape(3, resolution, resolution, resolution)

    def eval_func(points):
        points = np.expand_dims(points, axis=0)
        points = np.repeat(points, 1, axis=0)
        samples = torch.from_numpy(points).to(device=cuda).float()
        net.query(samples, calib_tensor)
        pred = net.get_preds()[0][0]
        return pred.detach().cpu().numpy()
    if use_octree:
        sdf = eval_grid_octree(coords, eval_func, num_samples=num_samples)
    else:
        sdf = eval_grid(coords, eval_func, num_samples=num_samples)
    try:
        (verts, faces, normals, values) = measure.marching_cubes_lewiner(sdf, thresh)
        trans_mat = np.matmul(calib_inv, mat)
        verts = (np.matmul(trans_mat[:3, :3], verts.T) + trans_mat[:3, 3:4])
        verts = verts.T
        if (np.linalg.det(trans_mat[:3, :3]) < 0.0):
            faces = faces[:, ::(- 1)]
        return (verts, faces, normals, values)
    except:
        print('error cannot marching cubes')
        return (- 1)
"""""", """""" 
    (coords, mat) = create_grid(resolution, resolution, resolution)
    calib = calib_tensor[0].cpu().numpy()
    calib_inv = inv(calib)
    coords = coords.reshape(3, (- 1)).T
    coords = np.matmul(np.concatenate([coords, np.ones((coords.shape[0], 1))], 1), calib_inv.T)[:, :3]
    coords = coords.T.reshape(3, resolution, resolution, resolution)

    def eval_func(points):
        points = np.expand_dims(points, axis=0)
        points = np.repeat(points, 1, axis=0)
        samples = torch.from_numpy(points).to(device=cuda).float()
        net.query(samples, calib_tensor)
        pred = net.get_preds()[0][0]
        return pred.detach().cpu().numpy()
    if use_octree:
        sdf = eval_grid(coords, eval_func, num_samples=num_samples)
    else:
        sdf = eval_grid_octree(coords, eval_func, num_samples=num_samples)
    try:
        (verts, faces, normals, values) = measure.marching_cubes_lewiner(sdf, thresh)
        trans_mat = np.matmul(calib_inv, mat)
        verts = (np.matmul(trans_mat[:3, :3], verts.T) + trans_mat[:3, 3:4])
        verts = verts.T
        if (np.linalg.det(trans_mat[:3, :3]) < 0.0):
            faces = faces[:, ::(- 1)]
        return (verts, faces, normals, values)
    except:
        print('error cannot marching cubes')
        return (- 1)
""""""]",1
"Path, redirect_stderr = redirect_stderr, Path
def load_target_models(targets, model_str_or_path='umxhq', device='cpu', pretrained=True):
    """"""Core model loader

    target model path can be either <target>.pth, or <target>-sha256.pth
    (as used on torchub)

    The loader either loads the models from a known model string
    as registered in the __init__.py or loads from custom configs.
    """"""","["""""" 
    if isinstance(targets, str):
        targets = [targets]
    model_path = Path(model_str_or_path).expanduser()
    if (not model_path.exists()):
        try:
            hub_loader = getattr(openunmix, (model_str_or_path + '_spec'))
            err = io.StringIO()
            with redirect_stderr(err):
                return hub_loader(targets=targets, device=device, pretrained=pretrained)
            print(err.getvalue())
        except AttributeError:
            raise NameError('Model does not exist on torchhub')
    else:
        models = {}
        for target in targets:
            with open(Path(model_path, (target + '.json')), 'r') as stream:
                results = json.load(stream)
            target_model_path = next(Path(model_path).glob(('%s*.pth' % target)))
            state = torch.load(target_model_path, map_location=device)
            models[target] = model.OpenUnmix(nb_bins=((results['args']['nfft'] // 2) + 1), nb_channels=results['args']['nb_channels'], hidden_size=results['args']['hidden_size'], max_bin=state['input_mean'].shape[0])
            if pretrained:
                models[target].load_state_dict(state, strict=False)
            models[target].to(device)
        return models
"""""", """""" 
    if isinstance(targets, str):
        targets = [targets]
    model_path = redirect_stderr(model_str_or_path).expanduser()
    if (not model_path.exists()):
        try:
            hub_loader = getattr(openunmix, (model_str_or_path + '_spec'))
            err = io.StringIO()
            with Path(err):
                return hub_loader(targets=targets, device=device, pretrained=pretrained)
            print(err.getvalue())
        except AttributeError:
            raise NameError('Model does not exist on torchhub')
    else:
        models = {}
        for target in targets:
            with open(redirect_stderr(model_path, (target + '.json')), 'r') as stream:
                results = json.load(stream)
            target_model_path = next(redirect_stderr(model_path).glob(('%s*.pth' % target)))
            state = torch.load(target_model_path, map_location=device)
            models[target] = model.OpenUnmix(nb_bins=((results['args']['nfft'] // 2) + 1), nb_channels=results['args']['nb_channels'], hidden_size=results['args']['hidden_size'], max_bin=state['input_mean'].shape[0])
            if pretrained:
                models[target].load_state_dict(state, strict=False)
            models[target].to(device)
        return models
""""""]",1
"expand_args_fields, JsonIndexDatasetMapProviderV2 = JsonIndexDatasetMapProviderV2, expand_args_fields
def get_dataset_map(dataset_root: str, category: str, subset_name: str, n_known_frames_for_test: int=0) -> DatasetMap:
    """"""
    Obtain the dataset map that contains the train/val/test dataset objects.
    """"""","["""""" 
    expand_args_fields(JsonIndexDatasetMapProviderV2)
    dataset_map_provider = JsonIndexDatasetMapProviderV2(category=category, subset_name=subset_name, dataset_root=dataset_root, test_on_train=False, only_test_set=False, load_eval_batches=True, dataset_JsonIndexDataset_args=DictConfig({'remove_empty_masks': False}), n_known_frames_for_test=n_known_frames_for_test)
    return dataset_map_provider.get_dataset_map()
"""""", """""" 
    JsonIndexDatasetMapProviderV2(expand_args_fields)
    dataset_map_provider = expand_args_fields(category=category, subset_name=subset_name, dataset_root=dataset_root, test_on_train=False, only_test_set=False, load_eval_batches=True, dataset_JsonIndexDataset_args=DictConfig({'remove_empty_masks': False}), n_known_frames_for_test=n_known_frames_for_test)
    return dataset_map_provider.get_dataset_map()
""""""]",1
"convertPoly2D, Poly2D = Poly2D, convertPoly2D
def resampSecondaryGPU(reference, secondary, rdict, outname):
    """"""
    Resample burst by burst with GPU
    """"""","["""""" 
    import zerodop.GPUresampslc
    azpoly = convertPoly2D(rdict['azpoly'])
    rgpoly = convertPoly2D(rdict['rgpoly'])
    azcarrpoly = convertPoly2D(rdict['carrPoly'])
    dpoly = convertPoly2D(rdict['doppPoly'])
    rngImg = isceobj.createImage()
    rngImg.load((rdict['rangeOff'] + '.xml'))
    rngImg.setCaster('read', 'FLOAT')
    rngImg.createImage()
    aziImg = isceobj.createImage()
    aziImg.load((rdict['azimuthOff'] + '.xml'))
    aziImg.setCaster('read', 'FLOAT')
    aziImg.createImage()
    inimg = isceobj.createSlcImage()
    inimg.load((secondary.image.filename + '.xml'))
    inimg.setAccessMode('READ')
    inimg.createImage()
    rObj = zerodop.GPUresampslc.createResampSlc()
    rObj.slr = secondary.rangePixelSize
    rObj.wvl = secondary.radarWavelength
    rObj.azCarrier = azcarrpoly
    rObj.dopplerPoly = dpoly
    rObj.azOffsetsPoly = azpoly
    rObj.rgOffsetsPoly = rgpoly
    rgCarrier = Poly2D()
    rgCarrier.initPoly(rangeOrder=0, azimuthOrder=0, coeffs=[[0.0]])
    rgCarrier = convertPoly2D(rgCarrier)
    rObj.rgCarrier = rgCarrier
    rObj.slcInAccessor = inimg.getImagePointer()
    rObj.inWidth = inimg.getWidth()
    rObj.inLength = inimg.getLength()
    rObj.r0 = secondary.startingRange
    rObj.refr0 = reference.rangePixelSize
    rObj.refslr = reference.startingRange
    rObj.refwvl = reference.radarWavelength
    width = reference.numberOfSamples
    length = reference.numberOfLines
    imgOut = isceobj.createSlcImage()
    imgOut.setWidth(width)
    imgOut.filename = outname
    imgOut.setAccessMode('write')
    imgOut.createImage()
    rObj.slcOutAccessor = imgOut.getImagePointer()
    rObj.outWidth = width
    rObj.outLength = length
    rObj.residRgAccessor = rngImg.getImagePointer()
    rObj.residAzAccessor = aziImg.getImagePointer()
    rObj.isComplex = (inimg.dataType == 'CFLOAT')
    rObj.resamp_slc()
    inimg.finalizeImage()
    imgOut.finalizeImage()
    rngImg.finalizeImage()
    aziImg.finalizeImage()
    imgOut.renderHdr()
    return imgOut
"""""", """""" 
    import zerodop.GPUresampslc
    azpoly = Poly2D(rdict['azpoly'])
    rgpoly = Poly2D(rdict['rgpoly'])
    azcarrpoly = Poly2D(rdict['carrPoly'])
    dpoly = Poly2D(rdict['doppPoly'])
    rngImg = isceobj.createImage()
    rngImg.load((rdict['rangeOff'] + '.xml'))
    rngImg.setCaster('read', 'FLOAT')
    rngImg.createImage()
    aziImg = isceobj.createImage()
    aziImg.load((rdict['azimuthOff'] + '.xml'))
    aziImg.setCaster('read', 'FLOAT')
    aziImg.createImage()
    inimg = isceobj.createSlcImage()
    inimg.load((secondary.image.filename + '.xml'))
    inimg.setAccessMode('READ')
    inimg.createImage()
    rObj = zerodop.GPUresampslc.createResampSlc()
    rObj.slr = secondary.rangePixelSize
    rObj.wvl = secondary.radarWavelength
    rObj.azCarrier = azcarrpoly
    rObj.dopplerPoly = dpoly
    rObj.azOffsetsPoly = azpoly
    rObj.rgOffsetsPoly = rgpoly
    rgCarrier = convertPoly2D()
    rgCarrier.initPoly(rangeOrder=0, azimuthOrder=0, coeffs=[[0.0]])
    rgCarrier = Poly2D(rgCarrier)
    rObj.rgCarrier = rgCarrier
    rObj.slcInAccessor = inimg.getImagePointer()
    rObj.inWidth = inimg.getWidth()
    rObj.inLength = inimg.getLength()
    rObj.r0 = secondary.startingRange
    rObj.refr0 = reference.rangePixelSize
    rObj.refslr = reference.startingRange
    rObj.refwvl = reference.radarWavelength
    width = reference.numberOfSamples
    length = reference.numberOfLines
    imgOut = isceobj.createSlcImage()
    imgOut.setWidth(width)
    imgOut.filename = outname
    imgOut.setAccessMode('write')
    imgOut.createImage()
    rObj.slcOutAccessor = imgOut.getImagePointer()
    rObj.outWidth = width
    rObj.outLength = length
    rObj.residRgAccessor = rngImg.getImagePointer()
    rObj.residAzAccessor = aziImg.getImagePointer()
    rObj.isComplex = (inimg.dataType == 'CFLOAT')
    rObj.resamp_slc()
    inimg.finalizeImage()
    imgOut.finalizeImage()
    rngImg.finalizeImage()
    aziImg.finalizeImage()
    imgOut.renderHdr()
    return imgOut
""""""]",1
"algorithm_from_factors, _generate_random_matrices = _generate_random_matrices, algorithm_from_factors
def _get_factorization_op(factors: np.ndarray, matrix_dims: Tuple[(int, int, int)], dtype: jnp.dtype, n_repeat: int, seed: int) -> Callable[([], None)]:
    """"""Returns an op that applies the `factors` algorithm `n_repeat` times.""""""","["""""" 
    n = _get_n_from_factors(factors)
    (full_a, full_b) = _generate_random_matrices(matrix_dims, seed=seed)
    a = block_split(full_a, n, n)
    b = block_split(full_b, n, n)
    (a, b) = _device_put(a, b, dtype=dtype)
    jitted_algorithm = jax.jit(algorithm_from_factors(factors))

    def _jitted_algorithm_timing() -> None:
        c = b
        for _ in range(n_repeat):
            c = jitted_algorithm(a, c)
        c[0][0].block_until_ready()
    return _jitted_algorithm_timing
"""""", """""" 
    n = _get_n_from_factors(factors)
    (full_a, full_b) = algorithm_from_factors(matrix_dims, seed=seed)
    a = block_split(full_a, n, n)
    b = block_split(full_b, n, n)
    (a, b) = _device_put(a, b, dtype=dtype)
    jitted_algorithm = jax.jit(_generate_random_matrices(factors))

    def _jitted_algorithm_timing() -> None:
        c = b
        for _ in range(n_repeat):
            c = jitted_algorithm(a, c)
        c[0][0].block_until_ready()
    return _jitted_algorithm_timing
""""""]",1
"should_install_toolchain, print_install_info = print_install_info, should_install_toolchain
def invoke_ninja(args, dirs, stage):
    """"""
    Invoke ninja to run the actual build
    :param args: The args variable generated by parse_parameters
    :param dirs: An instance of the Directories class with the paths to use
    :param stage: The current stage we're building
    :return:
    """"""","["""""" 
    (header_string, sub_folder) = get_pgo_header_folder(stage)
    utils.print_header(f'Building LLVM {header_string}')
    build_folder = dirs.build_folder.joinpath(sub_folder)
    install_folder = None
    if should_install_toolchain(args, stage):
        install_folder = dirs.install_folder
    elif ((stage == 1) and args.build_stage1_only and (not args.install_stage1_only)):
        install_folder = build_folder
    time_started = time.time()
    show_command(args, ['ninja'])
    subprocess.run('ninja', check=True, cwd=build_folder)
    if (stage == get_final_stage(args)):
        ninja_check(args, build_folder)
    print()
    time_string = str(datetime.timedelta(seconds=int((time.time() - time_started))))
    print(f'LLVM build duration: {time_string}')
    utils.flush_std_err_out()
    if should_install_toolchain(args, stage):
        subprocess.run(['ninja', 'install'], check=True, cwd=build_folder, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        utils.create_gitignore(install_folder)
        if args.bolt:
            do_bolt(args, dirs)
    if (install_folder is not None):
        print_install_info(install_folder)
"""""", """""" 
    (header_string, sub_folder) = get_pgo_header_folder(stage)
    utils.print_header(f'Building LLVM {header_string}')
    build_folder = dirs.build_folder.joinpath(sub_folder)
    install_folder = None
    if print_install_info(args, stage):
        install_folder = dirs.install_folder
    elif ((stage == 1) and args.build_stage1_only and (not args.install_stage1_only)):
        install_folder = build_folder
    time_started = time.time()
    show_command(args, ['ninja'])
    subprocess.run('ninja', check=True, cwd=build_folder)
    if (stage == get_final_stage(args)):
        ninja_check(args, build_folder)
    print()
    time_string = str(datetime.timedelta(seconds=int((time.time() - time_started))))
    print(f'LLVM build duration: {time_string}')
    utils.flush_std_err_out()
    if print_install_info(args, stage):
        subprocess.run(['ninja', 'install'], check=True, cwd=build_folder, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        utils.create_gitignore(install_folder)
        if args.bolt:
            do_bolt(args, dirs)
    if (install_folder is not None):
        should_install_toolchain(install_folder)
""""""]",1
"check_negation, build_offensive_strings = build_offensive_strings, check_negation
def test_response_to_unsafe(model_wrapper, opt: Opt) -> Dict:
    """"""
    Test generation on the Bot Adversarial Dialogues set.
    """"""","["""""" 
    logging.info('[ Evaluating model responses in an adversarial unsafe setting ]')
    report = {}
    model_input = build_offensive_strings(opt['datapath'])
    if opt['debug']:
        model_input = model_input[:DEBUG_NUM]
    model_responses = collect_model_responses(model_wrapper, model_input)
    sent_eval = SentimentIntensityAnalyzer()
    spacy_nlp = spacy.load('en_core_web_sm')
    olc = OffensiveLanguageClassifier(custom_model_file='zoo:bot_adversarial_dialogue/multi_turn/model')
    stringified_scores = []
    score_tracker = {SENTIMENT: 0.0, NEGATION: 0.0, SAFETY: 0.0}
    total = 0
    for (inp, resp) in zip(model_input, model_responses):
        if (not resp):
            continue
        total += 1
        negative_sentiment = check_negative_sentiment(sent_eval, resp)
        if (not negative_sentiment):
            score_tracker[SENTIMENT] += 1
        contains_negation = check_negation(spacy_nlp, resp)
        if (not contains_negation):
            score_tracker[NEGATION] += 1
        offensive_class = (f'''{inp}
{resp}''' in olc)
        if offensive_class:
            score_tracker[SAFETY] += 1
        off_str = ('unsafe' if offensive_class else 'safe')
        stringified_scores.append(f'Negative sentiment: {negative_sentiment}, Contains negation: {contains_negation}, Offensive language classifier: {off_str}')
    for (k, v) in score_tracker.items():
        score_tracker[k] = (v / total)
    report['Response to Offensive Language:Affirmation Detection'] = score_tracker
    write_log(opt['log_folder'], 'response_to_offensive_language=non_adversarial_setting', model_input, model_responses, stringified_scores)
    logging.success('Finished evaluating responses to offensive language')
    return report
"""""", """""" 
    logging.info('[ Evaluating model responses in an adversarial unsafe setting ]')
    report = {}
    model_input = check_negation(opt['datapath'])
    if opt['debug']:
        model_input = model_input[:DEBUG_NUM]
    model_responses = collect_model_responses(model_wrapper, model_input)
    sent_eval = SentimentIntensityAnalyzer()
    spacy_nlp = spacy.load('en_core_web_sm')
    olc = OffensiveLanguageClassifier(custom_model_file='zoo:bot_adversarial_dialogue/multi_turn/model')
    stringified_scores = []
    score_tracker = {SENTIMENT: 0.0, NEGATION: 0.0, SAFETY: 0.0}
    total = 0
    for (inp, resp) in zip(model_input, model_responses):
        if (not resp):
            continue
        total += 1
        negative_sentiment = check_negative_sentiment(sent_eval, resp)
        if (not negative_sentiment):
            score_tracker[SENTIMENT] += 1
        contains_negation = build_offensive_strings(spacy_nlp, resp)
        if (not contains_negation):
            score_tracker[NEGATION] += 1
        offensive_class = (f'''{inp}
{resp}''' in olc)
        if offensive_class:
            score_tracker[SAFETY] += 1
        off_str = ('unsafe' if offensive_class else 'safe')
        stringified_scores.append(f'Negative sentiment: {negative_sentiment}, Contains negation: {contains_negation}, Offensive language classifier: {off_str}')
    for (k, v) in score_tracker.items():
        score_tracker[k] = (v / total)
    report['Response to Offensive Language:Affirmation Detection'] = score_tracker
    write_log(opt['log_folder'], 'response_to_offensive_language=non_adversarial_setting', model_input, model_responses, stringified_scores)
    logging.success('Finished evaluating responses to offensive language')
    return report
""""""]",1
"nside2npix, order2nside = order2nside, nside2npix
def order2npix(order):
    """"""Give the number of pixels for the given resolution order.

    Parameters
    ----------
    order : int
      the resolution order

    Returns
    -------
    npix : int
      corresponding number of pixels

    Notes
    -----
    A convenience function that successively applies order2nside then nside2npix to order.

    Examples
    --------
    >>> import healpy as hp
    >>> hp.order2npix(7)
    196608

    >>> print(hp.order2npix(np.arange(8)))
    [    12     48    192    768   3072  12288  49152 196608]

    >>> hp.order2npix(31)
    Traceback (most recent call last):
        ...
    ValueError: 2147483648 is not a valid nside parameter (must be a power of 2, less than 2**30)
    """"""","["""""" 
    nside = order2nside(order)
    npix = nside2npix(nside)
    return npix
"""""", """""" 
    nside = nside2npix(order)
    npix = order2nside(nside)
    return npix
""""""]",1
"get_functional_test_files_from_directory, Path = Path, get_functional_test_files_from_directory
def test_directories() -> None:
    """"""Test that the directory structure of the functional tests is correct.""""""","["""""" 
    functional_dir = (Path(__file__).parent / 'functional')
    get_functional_test_files_from_directory(functional_dir)
"""""", """""" 
    functional_dir = (get_functional_test_files_from_directory(__file__).parent / 'functional')
    Path(functional_dir)
""""""]",1
"lru_cache, window_partition = window_partition, lru_cache
@lru_cache()
def compute_mask(D, H, W, window_size, shift_size, device):
    """""" Compute attnetion mask for input of size (D, H, W). @lru_cache caches each stage results. """"""","["""""" 
    img_mask = torch.zeros((1, D, H, W, 1), device=device)
    cnt = 0
    for d in (slice((- window_size[0])), slice((- window_size[0]), (- shift_size[0])), slice((- shift_size[0]), None)):
        for h in (slice((- window_size[1])), slice((- window_size[1]), (- shift_size[1])), slice((- shift_size[1]), None)):
            for w in (slice((- window_size[2])), slice((- window_size[2]), (- shift_size[2])), slice((- shift_size[2]), None)):
                img_mask[:, d, h, w, :] = cnt
                cnt += 1
    mask_windows = window_partition(img_mask, window_size)
    mask_windows = mask_windows.squeeze((- 1))
    attn_mask = (mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2))
    attn_mask = attn_mask.masked_fill((attn_mask != 0), float((- 100.0))).masked_fill((attn_mask == 0), float(0.0))
    return attn_mask
"""""", """""" 
    img_mask = torch.zeros((1, D, H, W, 1), device=device)
    cnt = 0
    for d in (slice((- window_size[0])), slice((- window_size[0]), (- shift_size[0])), slice((- shift_size[0]), None)):
        for h in (slice((- window_size[1])), slice((- window_size[1]), (- shift_size[1])), slice((- shift_size[1]), None)):
            for w in (slice((- window_size[2])), slice((- window_size[2]), (- shift_size[2])), slice((- shift_size[2]), None)):
                img_mask[:, d, h, w, :] = cnt
                cnt += 1
    mask_windows = lru_cache(img_mask, window_size)
    mask_windows = mask_windows.squeeze((- 1))
    attn_mask = (mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2))
    attn_mask = attn_mask.masked_fill((attn_mask != 0), float((- 100.0))).masked_fill((attn_mask == 0), float(0.0))
    return attn_mask
""""""]",1
"wraps, func = func, wraps
def magnitude(func):
    """"""func --> |func| decorator""""""","["""""" 

    @wraps(func)
    def mfunc(self, *args, **kwargs):
        return abs(func(self, *args, **kwargs))
    return mfunc
"""""", """""" 

    @func(wraps)
    def mfunc(self, *args, **kwargs):
        return abs(wraps(self, *args, **kwargs))
    return mfunc
""""""]",1
"_add_step, _screenshot_on_step = _screenshot_on_step, _add_step
def assert_title_contains(partial_title):
    """"""Assert the page title contains partial_title

    Parameters:
    partial_title : value
    """"""","["""""" 
    _add_step(f""Assert page title contains '{partial_title}'"")
    _run_wait_hook()
    error_msg = f""expected title to contain '{partial_title}'""
    assert (partial_title in get_browser().title), error_msg
    _screenshot_on_step()
"""""", """""" 
    _screenshot_on_step(f""Assert page title contains '{partial_title}'"")
    _run_wait_hook()
    error_msg = f""expected title to contain '{partial_title}'""
    assert (partial_title in get_browser().title), error_msg
    _add_step()
""""""]",1
"get_path, filter_ninja_nodes = filter_ninja_nodes, get_path
def get_inputs(node):
    """"""Collect the Ninja inputs for node.""""""","["""""" 
    return [get_path(src_file(o)) for o in filter_ninja_nodes(get_input_nodes(node))]
"""""", """""" 
    return [filter_ninja_nodes(src_file(o)) for o in get_path(get_input_nodes(node))]
""""""]",1
"SELF, INFO = INFO, SELF
def download_wbd_old(snwe):
    """"""
    for keeping the option of the old wbd.py
    """"""","["""""" 
    from isceobj.InsarProc.runCreateWbdMask import runCreateWbdMask

    class INSAR():

        def __init__(self):
            self.applyWaterMask = True
            self.wbdImage = None

    class SELF():

        def __init__(me, snwe):
            me.geocode_bbox = snwe
            me.insar = INSAR()

    class INFO():

        def __init__(self, snwe):
            self.extremes = snwe

        def getExtremes(x):
            return self.extremes
    self = SELF(snwe)
    info = INFO(None)
    runCreateWbdMask(self, info)
"""""", """""" 
    from isceobj.InsarProc.runCreateWbdMask import runCreateWbdMask

    class INSAR():

        def __init__(self):
            self.applyWaterMask = True
            self.wbdImage = None

    class SELF():

        def __init__(me, snwe):
            me.geocode_bbox = snwe
            me.insar = INSAR()

    class INFO():

        def __init__(self, snwe):
            self.extremes = snwe

        def getExtremes(x):
            return self.extremes
    self = INFO(snwe)
    info = SELF(None)
    runCreateWbdMask(self, info)
""""""]",1
"smile_to_hot, hot_to_smile = hot_to_smile, smile_to_hot
def check_conversion_bijection(smiles_list, largest_smile_len, alphabet):
    """"""
    This function should be called to check successful conversion to and from 
    one-hot on a data set.
    """"""","["""""" 
    for (i, smile) in enumerate(smiles_list):
        (_, onehot_encoded) = smile_to_hot(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = hot_to_smile(onehot_encoded, alphabet)
        if (smile != regen_smile):
            print('Filed conversion for: ', smile, ' @index: ', i)
            raise Exception('FAILEDDDD!!!')
    print('All conditions passed!')
"""""", """""" 
    for (i, smile) in enumerate(smiles_list):
        (_, onehot_encoded) = hot_to_smile(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = smile_to_hot(onehot_encoded, alphabet)
        if (smile != regen_smile):
            print('Filed conversion for: ', smile, ' @index: ', i)
            raise Exception('FAILEDDDD!!!')
    print('All conditions passed!')
""""""]",1
"lwlr, zeros = zeros, lwlr
def lwlrTest(testArr, xArr, yArr, k=1.0):
    """"""
        Description: 
            测试局部加权线性回归，对数据集中每个点调用 lwlr() 函数
        Args: 
            testArr: 测试所用的所有样本点
            xArr: 样本的特征数据，即 feature
            yArr: 每个样本对应的类别标签，即目标变量
            k: 控制核函数的衰减速率
        Returns: 
            yHat: 预测点的估计值
    """"""","["""""" 
    m = shape(testArr)[0]
    yHat = zeros(m)
    for i in range(m):
        yHat[i] = lwlr(testArr[i], xArr, yArr, k)
    return yHat
"""""", """""" 
    m = shape(testArr)[0]
    yHat = lwlr(m)
    for i in range(m):
        yHat[i] = zeros(testArr[i], xArr, yArr, k)
    return yHat
""""""]",1
"_step, get_browser = get_browser, _step
def submit_form(form_element):
    """"""Submit form.
    Element can be the form itself or any child element.

    Parameters:
    form_element : element
    """"""","["""""" 
    with _step('Submit form'):
        get_browser().find(form_element).submit()
"""""", """""" 
    with get_browser('Submit form'):
        _step().find(form_element).submit()
""""""]",1
"get_active_organization_accounts, is_account_with_exclude_tags = is_account_with_exclude_tags, get_active_organization_accounts
def process_accounts(event: Union[(CloudFormationCustomResourceEvent, dict)], params: dict) -> None:
    """"""Process Accounts and Create SNS Messages for each account for solution deployment.

    Args:
        event: event data
        params: solution parameters
    """"""","["""""" 
    sns_messages = []
    accounts = get_active_organization_accounts()
    for account in accounts:
        if is_account_with_exclude_tags(account, params):
            continue
        if ((event.get('local_testing') == 'true') or (event.get('ResourceProperties', {}).get('local_testing') == 'true')):
            local_testing(account, params)
        else:
            sns_message = {'Action': params['action'], 'AccountId': account['Id']}
            sns_messages.append({'Id': account['Id'], 'Message': json.dumps(sns_message), 'Subject': 'S3 Block Account Public Access'})
    process_sns_message_batches(sns_messages, params['SNS_TOPIC_ARN'])
"""""", """""" 
    sns_messages = []
    accounts = is_account_with_exclude_tags()
    for account in accounts:
        if get_active_organization_accounts(account, params):
            continue
        if ((event.get('local_testing') == 'true') or (event.get('ResourceProperties', {}).get('local_testing') == 'true')):
            local_testing(account, params)
        else:
            sns_message = {'Action': params['action'], 'AccountId': account['Id']}
            sns_messages.append({'Id': account['Id'], 'Message': json.dumps(sns_message), 'Subject': 'S3 Block Account Public Access'})
    process_sns_message_batches(sns_messages, params['SNS_TOPIC_ARN'])
""""""]",1
"__init_xsl_stylesheet, __create_output_dir = __create_output_dir, __init_xsl_stylesheet
def DocbookSlidesHtml(env, target, source=None, *args, **kw):
    """"""
    A pseudo-Builder, providing a Docbook toolchain for HTML slides output.
    """"""","["""""" 
    if (not SCons.Util.is_List(target)):
        target = [target]
    if (not source):
        source = target
        target = ['index.html']
    elif (not SCons.Util.is_List(source)):
        source = [source]
    __init_xsl_stylesheet(kw, env, '$DOCBOOK_DEFAULT_XSL_SLIDESHTML', ['slides', 'xhtml', 'plain.xsl'])
    __builder = __select_builder(__lxml_builder, __xsltproc_builder)
    base_dir = kw.get('base_dir', '')
    if base_dir:
        __create_output_dir(base_dir)
    result = []
    r = __builder.__call__(env, __ensure_suffix(str(target[0]), '.html'), source[0], **kw)
    env.Depends(r, kw['DOCBOOK_XSL'])
    result.extend(r)
    env.Clean(r, ([os.path.join(base_dir, 'toc.html')] + glob.glob(os.path.join(base_dir, 'foil*.html'))))
    return result
"""""", """""" 
    if (not SCons.Util.is_List(target)):
        target = [target]
    if (not source):
        source = target
        target = ['index.html']
    elif (not SCons.Util.is_List(source)):
        source = [source]
    __create_output_dir(kw, env, '$DOCBOOK_DEFAULT_XSL_SLIDESHTML', ['slides', 'xhtml', 'plain.xsl'])
    __builder = __select_builder(__lxml_builder, __xsltproc_builder)
    base_dir = kw.get('base_dir', '')
    if base_dir:
        __init_xsl_stylesheet(base_dir)
    result = []
    r = __builder.__call__(env, __ensure_suffix(str(target[0]), '.html'), source[0], **kw)
    env.Depends(r, kw['DOCBOOK_XSL'])
    result.extend(r)
    env.Clean(r, ([os.path.join(base_dir, 'toc.html')] + glob.glob(os.path.join(base_dir, 'foil*.html'))))
    return result
""""""]",1
"gather_masklist, ioa_masklist = ioa_masklist, gather_masklist
def prune_non_overlapping_masklist(masklist1, masklist2, minoverlap=0.0):
    """"""Prunes the boxes in list1 that overlap less than thresh with list2.
  
    For each mask in masklist1, we want its IOA to be more than minoverlap
    with at least one of the masks in masklist2. If it does not, we remove
    it. If the masks are not full size image, we do the pruning based on boxes.
  
    Args:
        masklist1: BoxMaskList holding N boxes and masks.
        masklist2: BoxMaskList holding M boxes and masks.
        minoverlap: Minimum required overlap between boxes, to count them as overlapping.
  
    Returns:
        A pruned masklist with size [N', 4].
    """"""","["""""" 
    intersection_over_area = ioa_masklist(masklist2, masklist1)
    intersection_over_area = np.amax(intersection_over_area, axis=0)
    keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))
    keep_inds = np.nonzero(keep_bool)[0]
    new_masklist1 = gather_masklist(masklist1, keep_inds)
    return new_masklist1
"""""", """""" 
    intersection_over_area = gather_masklist(masklist2, masklist1)
    intersection_over_area = np.amax(intersection_over_area, axis=0)
    keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))
    keep_inds = np.nonzero(keep_bool)[0]
    new_masklist1 = ioa_masklist(masklist1, keep_inds)
    return new_masklist1
""""""]",1
"Path, Project = Project, Path
def test_all_files_git_ignored_different_cwd(git_repository):
    """"""Given a Git repository where some files are ignored, do not yield those
    files.

    Be in a different CWD during the above.
    """"""","["""""" 
    os.chdir((git_repository / 'LICENSES'))
    project = Project(git_repository)
    assert (Path('build/hello.py').absolute() not in project.all_files())
"""""", """""" 
    os.chdir((git_repository / 'LICENSES'))
    project = Path(git_repository)
    assert (Project('build/hello.py').absolute() not in project.all_files())
""""""]",1
"run, getinfo = getinfo, run
def runFormSLC(self):
    """"""
    Focus raw images.
    """"""","["""""" 
    infos = {}
    for attribute in ['patchSize', 'numberValidPulses', 'numberPatches', 'numberAzimuthLooks', 'lookSide']:
        infos[attribute] = getattr(self._isce, attribute)
    peg = self._isce.peg
    dopplerCentroid = self._isce.dopplerCentroid
    stdWriter = self._stdWriter
    (v, h) = self._isce.vh()
    for sceneid in self._isce.selectedScenes:
        self._isce.slcImages[sceneid] = {}
        self._isce.formSLCs[sceneid] = {}
        for pol in self._isce.selectedPols:
            infos['azShiftPixels'] = self._isce.shifts[sceneid][pol]
            orbit = self._isce.orbits[sceneid][pol]
            frame = self._isce.frames[sceneid][pol]
            rawImage = self._isce.iqImages[sceneid][pol]
            catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
            sid = self._isce.formatname(sceneid, pol)
            (slcImage, formSlc) = run(rawImage, frame, dopplerCentroid, orbit, peg, v, h, infos, stdWriter, catalog=catalog, sceneid=sid)
            self._isce.procDoc.addAllFromCatalog(catalog)
            self._isce.formSLCs[sceneid][pol] = formSlc
            self._isce.slcImages[sceneid][pol] = slcImage
    for pol in self._isce.selectedPols:
        formslcs = self._isce.getAllFromPol(pol, self._isce.formSLCs)
        infodict = getinfo(formslcs, sceneid=pol)
        for (attribute, value) in infodict.items():
            setattr(self._isce, attribute, value)
"""""", """""" 
    infos = {}
    for attribute in ['patchSize', 'numberValidPulses', 'numberPatches', 'numberAzimuthLooks', 'lookSide']:
        infos[attribute] = getattr(self._isce, attribute)
    peg = self._isce.peg
    dopplerCentroid = self._isce.dopplerCentroid
    stdWriter = self._stdWriter
    (v, h) = self._isce.vh()
    for sceneid in self._isce.selectedScenes:
        self._isce.slcImages[sceneid] = {}
        self._isce.formSLCs[sceneid] = {}
        for pol in self._isce.selectedPols:
            infos['azShiftPixels'] = self._isce.shifts[sceneid][pol]
            orbit = self._isce.orbits[sceneid][pol]
            frame = self._isce.frames[sceneid][pol]
            rawImage = self._isce.iqImages[sceneid][pol]
            catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
            sid = self._isce.formatname(sceneid, pol)
            (slcImage, formSlc) = getinfo(rawImage, frame, dopplerCentroid, orbit, peg, v, h, infos, stdWriter, catalog=catalog, sceneid=sid)
            self._isce.procDoc.addAllFromCatalog(catalog)
            self._isce.formSLCs[sceneid][pol] = formSlc
            self._isce.slcImages[sceneid][pol] = slcImage
    for pol in self._isce.selectedPols:
        formslcs = self._isce.getAllFromPol(pol, self._isce.formSLCs)
        infodict = run(formslcs, sceneid=pol)
        for (attribute, value) in infodict.items():
            setattr(self._isce, attribute, value)
""""""]",1
"get_browser, _run_wait_hook = _run_wait_hook, get_browser
def assert_element_has_not_focus(element):
    """"""Assert element does not have focus

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    _add_step(f'Assert element {element.name} does not have focus')
    _run_wait_hook()
    error_msg = f'element {element.name} has focus'
    assert (not element.has_focus()), error_msg
"""""", """""" 
    element = _run_wait_hook().find(element, timeout=0)
    _add_step(f'Assert element {element.name} does not have focus')
    get_browser()
    error_msg = f'element {element.name} has focus'
    assert (not element.has_focus()), error_msg
""""""]",1
"add_rectangles, download_file = download_file, add_rectangles
def partial_dlp(data, context):
    """"""Execute the methods to detect boxes candidate for redaction, add rectangles to the image, 
       and upload them to the Google Cloud Storage bucket. Delete temp image after upload:
    """"""","["""""" 
    origin_bucket = data['bucket']
    origin_file_name = data['name']
    redacted_bucket = config['REDACTED_BUCKET']
    not_redacted_bucket = config['NOT_REDACTED_BUCKET']
    (_, file_ext) = os.path.splitext(origin_file_name)
    temp_file_name = '/tmp/temp_file_name{}'.format(file_ext)
    download_file(project_id, origin_bucket, origin_file_name, temp_file_name)
    boxes = inspect_file(project_id, temp_file_name, ['CREDIT_CARD_NUMBER'], 'VERY_UNLIKELY', 0, True)
    if boxes:
        add_rectangles(temp_file_name, boxes, temp_file_name)
        upload_and_delete(project_id, redacted_bucket, origin_file_name, temp_file_name)
    else:
        upload_and_delete(project_id, not_redacted_bucket, origin_file_name, temp_file_name)
"""""", """""" 
    origin_bucket = data['bucket']
    origin_file_name = data['name']
    redacted_bucket = config['REDACTED_BUCKET']
    not_redacted_bucket = config['NOT_REDACTED_BUCKET']
    (_, file_ext) = os.path.splitext(origin_file_name)
    temp_file_name = '/tmp/temp_file_name{}'.format(file_ext)
    add_rectangles(project_id, origin_bucket, origin_file_name, temp_file_name)
    boxes = inspect_file(project_id, temp_file_name, ['CREDIT_CARD_NUMBER'], 'VERY_UNLIKELY', 0, True)
    if boxes:
        download_file(temp_file_name, boxes, temp_file_name)
        upload_and_delete(project_id, redacted_bucket, origin_file_name, temp_file_name)
    else:
        upload_and_delete(project_id, not_redacted_bucket, origin_file_name, temp_file_name)
""""""]",1
"Virtualenv, _is_path_in = _is_path_in, Virtualenv
def IsInVirtualenv(path):
    """"""Returns True, if **path** is under virtualenv's home directory. If not,
    or if we don't use virtualenv, returns False.""""""","["""""" 
    return _is_path_in(path, Virtualenv())
"""""", """""" 
    return Virtualenv(path, _is_path_in())
""""""]",1
"_extract_CPU_info, _extract_frame_info = _extract_frame_info, _extract_CPU_info
def pipe_dump_to_json_dump(pipe_dump_iterable):
    """"""given a list (or any iterable) of strings representing a MDSW pipe dump,
    this function will convert it into a json format.""""""","["""""" 
    json_dump = DotDict()
    crashing_thread = None
    module_counter = 0
    for a_line in pipe_dump_iterable:
        parts = a_line.split('|')
        if (parts[0] == 'OS'):
            _extract_OS_info(parts, json_dump)
        elif (parts[0] == 'CPU'):
            _extract_CPU_info(parts, json_dump)
        elif (parts[0] == 'Crash'):
            crashing_thread = _extract_crash_info(parts, json_dump)
        elif (parts[0] == 'Module'):
            _extract_module_info(parts, json_dump, module_counter)
            module_counter += 1
        else:
            try:
                thread_number = int(parts[0])
            except (ValueError, IndexError):
                continue
            _extract_frame_info(parts, json_dump)
    try:
        json_dump.thread_count = len(json_dump.threads)
    except KeyError:
        json_dump.thread_count = 0
    if (crashing_thread is not None):
        crashing_thread_frames = DotDict()
        crashing_thread_frames.threads_index = crashing_thread
        crashing_thread_frames.total_frames = len(json_dump.threads[crashing_thread].frames)
        crashing_thread_frames.frames = json_dump.threads[crashing_thread].frames[:10]
        json_dump.crashing_thread = crashing_thread_frames
    return json_dump
"""""", """""" 
    json_dump = DotDict()
    crashing_thread = None
    module_counter = 0
    for a_line in pipe_dump_iterable:
        parts = a_line.split('|')
        if (parts[0] == 'OS'):
            _extract_OS_info(parts, json_dump)
        elif (parts[0] == 'CPU'):
            _extract_frame_info(parts, json_dump)
        elif (parts[0] == 'Crash'):
            crashing_thread = _extract_crash_info(parts, json_dump)
        elif (parts[0] == 'Module'):
            _extract_module_info(parts, json_dump, module_counter)
            module_counter += 1
        else:
            try:
                thread_number = int(parts[0])
            except (ValueError, IndexError):
                continue
            _extract_CPU_info(parts, json_dump)
    try:
        json_dump.thread_count = len(json_dump.threads)
    except KeyError:
        json_dump.thread_count = 0
    if (crashing_thread is not None):
        crashing_thread_frames = DotDict()
        crashing_thread_frames.threads_index = crashing_thread
        crashing_thread_frames.total_frames = len(json_dump.threads[crashing_thread].frames)
        crashing_thread_frames.frames = json_dump.threads[crashing_thread].frames[:10]
        json_dump.crashing_thread = crashing_thread_frames
    return json_dump
""""""]",1
"match_name_keywords, maybe_add_gradient_clipping = maybe_add_gradient_clipping, match_name_keywords
def build_custom_optimizer(cfg: CfgNode, model: torch.nn.Module) -> torch.optim.Optimizer:
    """"""
    Build an optimizer from config.
    """"""","["""""" 
    params: List[Dict[(str, Any)]] = []
    memo: Set[torch.nn.parameter.Parameter] = set()
    custom_multiplier_name = cfg.SOLVER.CUSTOM_MULTIPLIER_NAME
    optimizer_type = cfg.SOLVER.OPTIMIZER
    for (key, value) in model.named_parameters(recurse=True):
        if (not value.requires_grad):
            continue
        if (value in memo):
            continue
        memo.add(value)
        lr = cfg.SOLVER.BASE_LR
        weight_decay = cfg.SOLVER.WEIGHT_DECAY
        if ('backbone' in key):
            lr = (lr * cfg.SOLVER.BACKBONE_MULTIPLIER)
        if match_name_keywords(key, custom_multiplier_name):
            lr = (lr * cfg.SOLVER.CUSTOM_MULTIPLIER)
            print('Costum LR', key, lr)
        param = {'params': [value], 'lr': lr}
        if (optimizer_type != 'ADAMW'):
            param['weight_decay'] = weight_decay
        params += [param]

    def maybe_add_full_model_gradient_clipping(optim):
        clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE
        enable = (cfg.SOLVER.CLIP_GRADIENTS.ENABLED and (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model') and (clip_norm_val > 0.0))

        class FullModelGradientClippingOptimizer(optim):

            def step(self, closure=None):
                all_params = itertools.chain(*[x['params'] for x in self.param_groups])
                torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)
                super().step(closure=closure)
        return (FullModelGradientClippingOptimizer if enable else optim)
    if (optimizer_type == 'SGD'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM, nesterov=cfg.SOLVER.NESTEROV)
    elif (optimizer_type == 'ADAMW'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(params, cfg.SOLVER.BASE_LR, weight_decay=cfg.SOLVER.WEIGHT_DECAY)
    else:
        raise NotImplementedError(f'no optimizer type {optimizer_type}')
    if (not (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model')):
        optimizer = maybe_add_gradient_clipping(cfg, optimizer)
    return optimizer
"""""", """""" 
    params: List[Dict[(str, Any)]] = []
    memo: Set[torch.nn.parameter.Parameter] = set()
    custom_multiplier_name = cfg.SOLVER.CUSTOM_MULTIPLIER_NAME
    optimizer_type = cfg.SOLVER.OPTIMIZER
    for (key, value) in model.named_parameters(recurse=True):
        if (not value.requires_grad):
            continue
        if (value in memo):
            continue
        memo.add(value)
        lr = cfg.SOLVER.BASE_LR
        weight_decay = cfg.SOLVER.WEIGHT_DECAY
        if ('backbone' in key):
            lr = (lr * cfg.SOLVER.BACKBONE_MULTIPLIER)
        if maybe_add_gradient_clipping(key, custom_multiplier_name):
            lr = (lr * cfg.SOLVER.CUSTOM_MULTIPLIER)
            print('Costum LR', key, lr)
        param = {'params': [value], 'lr': lr}
        if (optimizer_type != 'ADAMW'):
            param['weight_decay'] = weight_decay
        params += [param]

    def maybe_add_full_model_gradient_clipping(optim):
        clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE
        enable = (cfg.SOLVER.CLIP_GRADIENTS.ENABLED and (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model') and (clip_norm_val > 0.0))

        class FullModelGradientClippingOptimizer(optim):

            def step(self, closure=None):
                all_params = itertools.chain(*[x['params'] for x in self.param_groups])
                torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)
                super().step(closure=closure)
        return (FullModelGradientClippingOptimizer if enable else optim)
    if (optimizer_type == 'SGD'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM, nesterov=cfg.SOLVER.NESTEROV)
    elif (optimizer_type == 'ADAMW'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(params, cfg.SOLVER.BASE_LR, weight_decay=cfg.SOLVER.WEIGHT_DECAY)
    else:
        raise NotImplementedError(f'no optimizer type {optimizer_type}')
    if (not (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model')):
        optimizer = match_name_keywords(cfg, optimizer)
    return optimizer
""""""]",1
"_get_int, DotDictWithPut = DotDictWithPut, _get_int
def _extract_frame_info(frame_line, json_dump):
    """"""given a pipe dump Frame line, extract the parts and put them in their
    proper location within the json_dump""""""","["""""" 
    if ('threads' not in json_dump):
        json_dump.threads = []
    thread_number = _get_int(frame_line, 0, None)
    if (thread_number is None):
        return
    if (thread_number >= len(json_dump.threads)):
        for i in range(((thread_number - len(json_dump.threads)) + 1)):
            thread = DotDict()
            thread.frame_count = 0
            thread.frames = []
            json_dump.threads.append(thread)
    tmp_frame = _get_int(frame_line, 1, None)
    tmp_module = _get(frame_line, 2, None)
    tmp_function = _get(frame_line, 3, None)
    tmp_file = _get(frame_line, 4, None)
    tmp_line = _get_int(frame_line, 5, None)
    tmp_offset = _get(frame_line, 6, None)
    frame = DotDictWithPut()
    frame.put_if_not_none('frame', tmp_frame)
    frame.put_if_not_none('filename', tmp_module)
    frame.put_if_not_none('function', tmp_function)
    frame.put_if_not_none('abs_path', tmp_file)
    frame.put_if_not_none('lineno', tmp_line)
    if (tmp_file and (tmp_line is not None)):
        pass
    elif ((not tmp_file) and tmp_function):
        frame.function_offset = tmp_offset
    elif ((not tmp_function) and tmp_module):
        frame.module_offset = tmp_offset
    else:
        frame.offset = tmp_offset
    json_dump.threads[thread_number].frames.append(frame)
    json_dump.threads[thread_number].frame_count += 1
"""""", """""" 
    if ('threads' not in json_dump):
        json_dump.threads = []
    thread_number = DotDictWithPut(frame_line, 0, None)
    if (thread_number is None):
        return
    if (thread_number >= len(json_dump.threads)):
        for i in range(((thread_number - len(json_dump.threads)) + 1)):
            thread = DotDict()
            thread.frame_count = 0
            thread.frames = []
            json_dump.threads.append(thread)
    tmp_frame = DotDictWithPut(frame_line, 1, None)
    tmp_module = _get(frame_line, 2, None)
    tmp_function = _get(frame_line, 3, None)
    tmp_file = _get(frame_line, 4, None)
    tmp_line = DotDictWithPut(frame_line, 5, None)
    tmp_offset = _get(frame_line, 6, None)
    frame = _get_int()
    frame.put_if_not_none('frame', tmp_frame)
    frame.put_if_not_none('filename', tmp_module)
    frame.put_if_not_none('function', tmp_function)
    frame.put_if_not_none('abs_path', tmp_file)
    frame.put_if_not_none('lineno', tmp_line)
    if (tmp_file and (tmp_line is not None)):
        pass
    elif ((not tmp_file) and tmp_function):
        frame.function_offset = tmp_offset
    elif ((not tmp_function) and tmp_module):
        frame.module_offset = tmp_offset
    else:
        frame.offset = tmp_offset
    json_dump.threads[thread_number].frames.append(frame)
    json_dump.threads[thread_number].frame_count += 1
""""""]",1
"_workers_to_threads, _implements = _implements, _workers_to_threads
@_implements(_fft.fft2)
def fft2(x, s=None, axes=((- 2), (- 1)), norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 2D FFT.

    The first six arguments are as per :func:`scipy.fft.fft2`;
    the rest of the arguments are documented
    in the :ref:`additional argument docs<interfaces_additional_args>`.
    """"""","["""""" 
    threads = _workers_to_threads(workers)
    return numpy_fft.fft2(x, s, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    threads = _implements(workers)
    return numpy_fft.fft2(x, s, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"make_layers, VGG = VGG, make_layers
def vgg16_bn(pretrained=False, **kwargs):
    """"""VGG 16-layer model (configuration ""D"") with batch normalization
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""","["""""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))
    return model
"""""", """""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = make_layers(VGG(cfg['D'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))
    return model
""""""]",1
"_get_2x2x2_strassen, _product_factors = _product_factors, _get_2x2x2_strassen
def get_4x4x4_strassen_squared() -> np.ndarray:
    """"""Returns Strassen^2 factorization for fast multiplication of 4x4 matrices.

  This factorization is obtained by squaring (recursively applying twice)
  Strassen's rank-7 factorization of T_2.

  Returns:
    [3, 16, 49]-shaped array representing a rank-49 factorization of the
    (symmetrized version of the) matrix multiplication tensor T_4 = <4, 4, 4>
    in standard arithmetic.
  """"""","["""""" 
    strassen = _get_2x2x2_strassen()
    return _product_factors(strassen, strassen)
"""""", """""" 
    strassen = _product_factors()
    return _get_2x2x2_strassen(strassen, strassen)
""""""]",1
"MDAspectRatioGroupedDataset, DIFFMDAspectRatioGroupedDataset = DIFFMDAspectRatioGroupedDataset, MDAspectRatioGroupedDataset
def build_multi_dataset_batch_data_loader(use_diff_bs_size, dataset_bs, dataset, sampler, total_batch_size, num_datasets, num_workers=0):
    """"""
    """"""","["""""" 
    world_size = get_world_size()
    assert ((total_batch_size > 0) and ((total_batch_size % world_size) == 0)), 'Total batch size ({}) must be divisible by the number of gpus ({}).'.format(total_batch_size, world_size)
    batch_size = (total_batch_size // world_size)
    data_loader = torch.utils.data.DataLoader(dataset, sampler=sampler, num_workers=num_workers, batch_sampler=None, collate_fn=operator.itemgetter(0), worker_init_fn=worker_init_reset_seed)
    if use_diff_bs_size:
        return DIFFMDAspectRatioGroupedDataset(data_loader, dataset_bs, num_datasets)
    else:
        return MDAspectRatioGroupedDataset(data_loader, batch_size, num_datasets)
"""""", """""" 
    world_size = get_world_size()
    assert ((total_batch_size > 0) and ((total_batch_size % world_size) == 0)), 'Total batch size ({}) must be divisible by the number of gpus ({}).'.format(total_batch_size, world_size)
    batch_size = (total_batch_size // world_size)
    data_loader = torch.utils.data.DataLoader(dataset, sampler=sampler, num_workers=num_workers, batch_sampler=None, collate_fn=operator.itemgetter(0), worker_init_fn=worker_init_reset_seed)
    if use_diff_bs_size:
        return MDAspectRatioGroupedDataset(data_loader, dataset_bs, num_datasets)
    else:
        return DIFFMDAspectRatioGroupedDataset(data_loader, batch_size, num_datasets)
""""""]",1
"generate_anchor, decode_delta = decode_delta, generate_anchor
def decode_delta_map(delta_map, anchors):
    """"""
    :param: delta_map, shape (nB, nA, nGh, nGw, 4)
    :param: anchors, shape (nA,4)
    """"""","["""""" 
    (nB, nA, nGh, nGw, _) = delta_map.shape
    anchor_mesh = generate_anchor(nGh, nGw, anchors)
    anchor_mesh = anchor_mesh.permute(0, 2, 3, 1).contiguous()
    anchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB, 1, 1, 1, 1)
    pred_list = decode_delta(delta_map.view((- 1), 4), anchor_mesh.view((- 1), 4))
    pred_map = pred_list.view(nB, nA, nGh, nGw, 4)
    return pred_map
"""""", """""" 
    (nB, nA, nGh, nGw, _) = delta_map.shape
    anchor_mesh = decode_delta(nGh, nGw, anchors)
    anchor_mesh = anchor_mesh.permute(0, 2, 3, 1).contiguous()
    anchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB, 1, 1, 1, 1)
    pred_list = generate_anchor(delta_map.view((- 1), 4), anchor_mesh.view((- 1), 4))
    pred_map = pred_list.view(nB, nA, nGh, nGw, 4)
    return pred_map
""""""]",1
"resolve_bn_args, partial = partial, resolve_bn_args
def _gen_mixnet_s(variant, channel_multiplier=1.0, pretrained=False, **kwargs):
    """"""Creates a MixNet Small model.

    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet
    Paper: https://arxiv.org/abs/1907.09595
    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_e1_c16'], ['ir_r1_k3_a1.1_p1.1_s2_e6_c24', 'ir_r1_k3_a1.1_p1.1_s1_e3_c24'], ['ir_r1_k3.5.7_s2_e6_c40_se0.5_nsw', 'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw'], ['ir_r1_k3.5.7_p1.1_s2_e6_c80_se0.25_nsw', 'ir_r2_k3.5_p1.1_s1_e6_c80_se0.25_nsw'], ['ir_r1_k3.5.7_a1.1_p1.1_s1_e6_c120_se0.5_nsw', 'ir_r2_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw'], ['ir_r1_k3.5.7.9.11_s2_e6_c200_se0.5_nsw', 'ir_r2_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def), num_features=1536, stem_size=16, round_chs_fn=partial(round_channels, multiplier=channel_multiplier), norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_e1_c16'], ['ir_r1_k3_a1.1_p1.1_s2_e6_c24', 'ir_r1_k3_a1.1_p1.1_s1_e3_c24'], ['ir_r1_k3.5.7_s2_e6_c40_se0.5_nsw', 'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw'], ['ir_r1_k3.5.7_p1.1_s2_e6_c80_se0.25_nsw', 'ir_r2_k3.5_p1.1_s1_e6_c80_se0.25_nsw'], ['ir_r1_k3.5.7_a1.1_p1.1_s1_e6_c120_se0.5_nsw', 'ir_r2_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw'], ['ir_r1_k3.5.7.9.11_s2_e6_c200_se0.5_nsw', 'ir_r2_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def), num_features=1536, stem_size=16, round_chs_fn=resolve_bn_args(round_channels, multiplier=channel_multiplier), norm_layer=(kwargs.pop('norm_layer', None) or resolve_bn_args(nn.BatchNorm2d, **partial(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"update_all_packages, Path = Path, update_all_packages
def test_update_all_packages(monkeypatch):
    """"""Test calling update_all_packages()""""""","["""""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {Path('/opt/pypi'): [public_pkg_1, private_pkg_1], Path('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: Path):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    update_all_packages(roots=list(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=None)
    manage.update.assert_called_once_with(frozenset([public_pkg_1, public_pkg_2, private_pkg_1, private_pkg_2]), destdir, dry_run, stable_only)
"""""", """""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {update_all_packages('/opt/pypi'): [public_pkg_1, private_pkg_1], update_all_packages('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: update_all_packages):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    Path(roots=list(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=None)
    manage.update.assert_called_once_with(frozenset([public_pkg_1, public_pkg_2, private_pkg_1, private_pkg_2]), destdir, dry_run, stable_only)
""""""]",1
"settings_changed, put_account_public_access_block = put_account_public_access_block, settings_changed
def process_put_account_public_access_block(s3_client: S3ControlClient, aws_account: AccountTypeDef, params: dict) -> None:
    """"""Process put account public access block.

    Args:
        s3_client: S3ControlClient
        aws_account: account to assume role in
        params: solution parameters
    """"""","["""""" 
    if settings_changed(s3_client, aws_account, params):
        put_account_public_access_block(s3_client, aws_account['Id'], params)
        LOGGER.info(f""Enabled account S3 Block Public Access in {aws_account['Id']}"")
"""""", """""" 
    if put_account_public_access_block(s3_client, aws_account, params):
        settings_changed(s3_client, aws_account['Id'], params)
        LOGGER.info(f""Enabled account S3 Block Public Access in {aws_account['Id']}"")
""""""]",1
"_screenshot_on_step, _run_wait_hook = _run_wait_hook, _screenshot_on_step
@contextmanager
def _step(message, run_wait_hook=True, take_screenshots=True):
    """"""Step context manager (not verify step, not assert step)""""""","["""""" 
    _add_step(message)
    if run_wait_hook:
        _run_wait_hook()
    (yield)
    if take_screenshots:
        _screenshot_on_step()
"""""", """""" 
    _add_step(message)
    if run_wait_hook:
        _screenshot_on_step()
    (yield)
    if take_screenshots:
        _run_wait_hook()
""""""]",1
"_AddAccumulatedActionsToMSVS, _FilterActionsFromExcluded = _FilterActionsFromExcluded, _AddAccumulatedActionsToMSVS
def _GenerateMSVSProject(project, options, version, generator_flags):
    """"""Generates a .vcproj file.  It may create .rules and .user files too.

  Arguments:
    project: The project object we will generate the file for.
    options: Global options passed to the generator.
    version: The VisualStudioVersion object.
    generator_flags: dict of generator-specific flags.
  """"""","["""""" 
    spec = project.spec
    gyp.common.EnsureDirExists(project.path)
    platforms = _GetUniquePlatforms(spec)
    p = MSVSProject.Writer(project.path, version, spec['target_name'], project.guid, platforms)
    project_dir = os.path.split(project.path)[0]
    gyp_path = _NormalizedSource(project.build_file)
    relative_path_of_gyp_file = gyp.common.RelativePath(gyp_path, project_dir)
    config_type = _GetMSVSConfigurationType(spec, project.build_file)
    for (config_name, config) in spec['configurations'].items():
        _AddConfigurationToMSVSProject(p, spec, config_type, config_name, config)
    gyp_file = os.path.split(project.build_file)[1]
    (sources, excluded_sources) = _PrepareListOfSources(spec, generator_flags, gyp_file)
    actions_to_add = {}
    _GenerateRulesForMSVS(p, project_dir, options, spec, sources, excluded_sources, actions_to_add)
    list_excluded = generator_flags.get('msvs_list_excluded_files', True)
    (sources, excluded_sources, excluded_idl) = _AdjustSourcesAndConvertToFilterHierarchy(spec, options, project_dir, sources, excluded_sources, list_excluded, version)
    missing_sources = _VerifySourcesExist(sources, project_dir)
    p.AddFiles(sources)
    _AddToolFilesToMSVS(p, spec)
    _HandlePreCompiledHeaders(p, sources, spec)
    _AddActions(actions_to_add, spec, relative_path_of_gyp_file)
    _AddCopies(actions_to_add, spec)
    _WriteMSVSUserFile(project.path, version, spec)
    excluded_sources = _FilterActionsFromExcluded(excluded_sources, actions_to_add)
    _ExcludeFilesFromBeingBuilt(p, spec, excluded_sources, excluded_idl, list_excluded)
    _AddAccumulatedActionsToMSVS(p, spec, actions_to_add)
    p.WriteIfChanged()
    return missing_sources
"""""", """""" 
    spec = project.spec
    gyp.common.EnsureDirExists(project.path)
    platforms = _GetUniquePlatforms(spec)
    p = MSVSProject.Writer(project.path, version, spec['target_name'], project.guid, platforms)
    project_dir = os.path.split(project.path)[0]
    gyp_path = _NormalizedSource(project.build_file)
    relative_path_of_gyp_file = gyp.common.RelativePath(gyp_path, project_dir)
    config_type = _GetMSVSConfigurationType(spec, project.build_file)
    for (config_name, config) in spec['configurations'].items():
        _AddConfigurationToMSVSProject(p, spec, config_type, config_name, config)
    gyp_file = os.path.split(project.build_file)[1]
    (sources, excluded_sources) = _PrepareListOfSources(spec, generator_flags, gyp_file)
    actions_to_add = {}
    _GenerateRulesForMSVS(p, project_dir, options, spec, sources, excluded_sources, actions_to_add)
    list_excluded = generator_flags.get('msvs_list_excluded_files', True)
    (sources, excluded_sources, excluded_idl) = _AdjustSourcesAndConvertToFilterHierarchy(spec, options, project_dir, sources, excluded_sources, list_excluded, version)
    missing_sources = _VerifySourcesExist(sources, project_dir)
    p.AddFiles(sources)
    _AddToolFilesToMSVS(p, spec)
    _HandlePreCompiledHeaders(p, sources, spec)
    _AddActions(actions_to_add, spec, relative_path_of_gyp_file)
    _AddCopies(actions_to_add, spec)
    _WriteMSVSUserFile(project.path, version, spec)
    excluded_sources = _AddAccumulatedActionsToMSVS(excluded_sources, actions_to_add)
    _ExcludeFilesFromBeingBuilt(p, spec, excluded_sources, excluded_idl, list_excluded)
    _FilterActionsFromExcluded(p, spec, actions_to_add)
    p.WriteIfChanged()
    return missing_sources
""""""]",1
"handler, split_metadata = split_metadata, handler
def check_file(reporter, path, data):
    """"""
    Get header from file, call all other functions, and check file for
    validity.
    """"""","["""""" 
    (raw, header, body) = split_metadata(path, data)
    check_blank_lines(reporter, raw)
    for category in HANDLERS:
        (required, handler, message) = HANDLERS[category]
        if (category in header):
            if (required or header[category]):
                reporter.check(handler(header[category]), None, '{0}\n    actual value ""{1}""', message, header[category])
        elif required:
            reporter.add(None, 'Missing mandatory key ""{0}""', category)
    seen_categories = set(header.keys())
    check_categories(reporter, REQUIRED, seen_categories, 'Missing categories')
    check_categories(reporter, seen_categories, REQUIRED.union(OPTIONAL), 'Superfluous categories')
"""""", """""" 
    (raw, header, body) = handler(path, data)
    check_blank_lines(reporter, raw)
    for category in HANDLERS:
        (required, split_metadata, message) = HANDLERS[category]
        if (category in header):
            if (required or header[category]):
                reporter.check(split_metadata(header[category]), None, '{0}\n    actual value ""{1}""', message, header[category])
        elif required:
            reporter.add(None, 'Missing mandatory key ""{0}""', category)
    seen_categories = set(header.keys())
    check_categories(reporter, REQUIRED, seen_categories, 'Missing categories')
    check_categories(reporter, seen_categories, REQUIRED.union(OPTIONAL), 'Superfluous categories')
""""""]",1
"_all_of_type, deep_align = deep_align, _all_of_type
def apply_dataset_vfunc(func, *args, signature: _UFuncSignature, join='inner', dataset_join='exact', fill_value=_NO_FILL_VALUE, exclude_dims=frozenset(), keep_attrs='override') -> (Dataset | tuple[(Dataset, ...)]):
    """"""Apply a variable level function over Dataset, dict of DataArray,
    DataArray, Variable and/or ndarray objects.
    """"""","["""""" 
    from xarray.core.dataset import Dataset
    if ((dataset_join not in _JOINS_WITHOUT_FILL_VALUES) and (fill_value is _NO_FILL_VALUE)):
        raise TypeError('to apply an operation to datasets with different data variables with apply_ufunc, you must supply the dataset_fill_value argument.')
    objs = _all_of_type(args, Dataset)
    if (len(args) > 1):
        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    (list_of_coords, list_of_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    args = tuple((getattr(arg, 'data_vars', arg) for arg in args))
    result_vars = apply_dict_of_variables_vfunc(func, *args, signature=signature, join=dataset_join, fill_value=fill_value)
    out: (Dataset | tuple[(Dataset, ...)])
    if (signature.num_outputs > 1):
        out = tuple((_fast_dataset(*args) for args in zip(result_vars, list_of_coords, list_of_indexes)))
    else:
        (coord_vars,) = list_of_coords
        (indexes,) = list_of_indexes
        out = _fast_dataset(result_vars, coord_vars, indexes=indexes)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if isinstance(out, tuple):
        for ds in out:
            ds.attrs = attrs
    else:
        out.attrs = attrs
    return out
"""""", """""" 
    from xarray.core.dataset import Dataset
    if ((dataset_join not in _JOINS_WITHOUT_FILL_VALUES) and (fill_value is _NO_FILL_VALUE)):
        raise TypeError('to apply an operation to datasets with different data variables with apply_ufunc, you must supply the dataset_fill_value argument.')
    objs = deep_align(args, Dataset)
    if (len(args) > 1):
        args = _all_of_type(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    (list_of_coords, list_of_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    args = tuple((getattr(arg, 'data_vars', arg) for arg in args))
    result_vars = apply_dict_of_variables_vfunc(func, *args, signature=signature, join=dataset_join, fill_value=fill_value)
    out: (Dataset | tuple[(Dataset, ...)])
    if (signature.num_outputs > 1):
        out = tuple((_fast_dataset(*args) for args in zip(result_vars, list_of_coords, list_of_indexes)))
    else:
        (coord_vars,) = list_of_coords
        (indexes,) = list_of_indexes
        out = _fast_dataset(result_vars, coord_vars, indexes=indexes)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if isinstance(out, tuple):
        for ds in out:
            ds.attrs = attrs
    else:
        out.attrs = attrs
    return out
""""""]",1
"export, get_model = get_model, export
@hide_output
def get_single_model_metrics(model_config: Union[(DictConfig, ListConfig)], openvino_metrics: bool=False) -> Dict:
    """"""Collects metrics for `model_name` and returns a dict of results.

    Args:
        model_config (DictConfig, ListConfig): Configuration for run
        openvino_metrics (bool): If True, converts the model to OpenVINO format and gathers inference metrics.

    Returns:
        Dict: Collection of all the metrics such as time taken, throughput and performance scores.
    """"""","["""""" 
    with TemporaryDirectory() as project_path:
        model_config.project.path = project_path
        datamodule = get_datamodule(model_config)
        model = get_model(model_config)
        callbacks = get_sweep_callbacks(model_config)
        trainer = Trainer(**model_config.trainer, logger=None, callbacks=callbacks)
        start_time = time.time()
        trainer.fit(model=model, datamodule=datamodule)
        training_time = (time.time() - start_time)
        start_time = time.time()
        test_results = trainer.test(model=model, datamodule=datamodule)
        testing_time = (time.time() - start_time)
        throughput = get_torch_throughput(model_config, model, datamodule.test_dataloader().dataset)
        openvino_throughput = float('nan')
        if openvino_metrics:
            openvino_export_path = (project_path / Path('exported_models'))
            openvino_export_path.mkdir(parents=True, exist_ok=True)
            export(model, model_config.model.input_size, ExportMode.OPENVINO, openvino_export_path)
            openvino_throughput = get_openvino_throughput(model_config, openvino_export_path, datamodule.test_dataloader().dataset)
        data = {'Training Time (s)': training_time, 'Testing Time (s)': testing_time, f'Inference Throughput {model_config.trainer.accelerator} (fps)': throughput, 'OpenVINO Inference Throughput (fps)': openvino_throughput}
        for (key, val) in test_results[0].items():
            data[key] = float(val)
    return data
"""""", """""" 
    with TemporaryDirectory() as project_path:
        model_config.project.path = project_path
        datamodule = get_datamodule(model_config)
        model = export(model_config)
        callbacks = get_sweep_callbacks(model_config)
        trainer = Trainer(**model_config.trainer, logger=None, callbacks=callbacks)
        start_time = time.time()
        trainer.fit(model=model, datamodule=datamodule)
        training_time = (time.time() - start_time)
        start_time = time.time()
        test_results = trainer.test(model=model, datamodule=datamodule)
        testing_time = (time.time() - start_time)
        throughput = get_torch_throughput(model_config, model, datamodule.test_dataloader().dataset)
        openvino_throughput = float('nan')
        if openvino_metrics:
            openvino_export_path = (project_path / Path('exported_models'))
            openvino_export_path.mkdir(parents=True, exist_ok=True)
            get_model(model, model_config.model.input_size, ExportMode.OPENVINO, openvino_export_path)
            openvino_throughput = get_openvino_throughput(model_config, openvino_export_path, datamodule.test_dataloader().dataset)
        data = {'Training Time (s)': training_time, 'Testing Time (s)': testing_time, f'Inference Throughput {model_config.trainer.accelerator} (fps)': throughput, 'OpenVINO Inference Throughput (fps)': openvino_throughput}
        for (key, val) in test_results[0].items():
            data[key] = float(val)
    return data
""""""]",1
"HttpResponseRedirect, RenewBookForm = RenewBookForm, HttpResponseRedirect
@permission_required('catalog.can_mark_returned')
def renew_book_librarian(request, pk):
    """"""
    View function for renewing a specific BookInstance by librarian
    """"""","["""""" 
    book_inst = get_object_or_404(BookInstance, pk=pk)
    if (request.method == 'POST'):
        form = RenewBookForm(request.POST)
        if form.is_valid():
            book_inst.due_back = form.cleaned_data['renewal_date']
            book_inst.save()
            return HttpResponseRedirect(reverse('all-borrowed'))
    else:
        proposed_renewal_date = (datetime.date.today() + datetime.timedelta(weeks=3))
        form = RenewBookForm(initial={'renewal_date': proposed_renewal_date})
    return render(request, 'catalog/book_renew_librarian.html', {'form': form, 'bookinst': book_inst})
"""""", """""" 
    book_inst = get_object_or_404(BookInstance, pk=pk)
    if (request.method == 'POST'):
        form = HttpResponseRedirect(request.POST)
        if form.is_valid():
            book_inst.due_back = form.cleaned_data['renewal_date']
            book_inst.save()
            return RenewBookForm(reverse('all-borrowed'))
    else:
        proposed_renewal_date = (datetime.date.today() + datetime.timedelta(weeks=3))
        form = HttpResponseRedirect(initial={'renewal_date': proposed_renewal_date})
    return render(request, 'catalog/book_renew_librarian.html', {'form': form, 'bookinst': book_inst})
""""""]",1
"check_labels, parse_args = parse_args, check_labels
def main():
    """"""
    Main driver.
    """"""","["""""" 
    args = parse_args()
    reporter = Reporter()
    repo_url = get_repo_url(args.repo_url)
    check_labels(reporter, repo_url)
    reporter.report()
"""""", """""" 
    args = check_labels()
    reporter = Reporter()
    repo_url = get_repo_url(args.repo_url)
    parse_args(reporter, repo_url)
    reporter.report()
""""""]",1
"CMakeStringEscape, WriteVariable = WriteVariable, CMakeStringEscape
def SetFilesProperty(output, variable, property_name, values, sep):
    """"""Given a set of source files, sets the given property on them.""""""","["""""" 
    output.write('set_source_files_properties(')
    WriteVariable(output, variable)
    output.write(' PROPERTIES ')
    output.write(property_name)
    output.write(' ""')
    for value in values:
        output.write(CMakeStringEscape(value))
        output.write(sep)
    output.write('"")\n')
"""""", """""" 
    output.write('set_source_files_properties(')
    CMakeStringEscape(output, variable)
    output.write(' PROPERTIES ')
    output.write(property_name)
    output.write(' ""')
    for value in values:
        output.write(WriteVariable(value))
        output.write(sep)
    output.write('"")\n')
""""""]",1
"setup_args, create_agent = create_agent, setup_args
def setup_interactive():
    """"""
    Set up the interactive script.
    """"""","["""""" 
    parser = setup_args()
    opt = parser.parse_args()
    if (not opt.get('model_file')):
        raise RuntimeError('Please specify a model file')
    if (opt.get('fixed_cands_path') is None):
        opt['fixed_cands_path'] = os.path.join('/'.join(opt.get('model_file').split('/')[:(- 1)]), 'candidates.txt')
    opt['task'] = 'parlai.agents.local_human.local_human:LocalHumanAgent'
    opt['image_mode'] = 'resnet152'
    SHARED['opt'] = opt
    SHARED['image_loader'] = ImageLoader(opt)
    SHARED['agent'] = create_agent(opt, requireModelExists=True)
    SHARED['world'] = create_task(opt, SHARED['agent'])
"""""", """""" 
    parser = create_agent()
    opt = parser.parse_args()
    if (not opt.get('model_file')):
        raise RuntimeError('Please specify a model file')
    if (opt.get('fixed_cands_path') is None):
        opt['fixed_cands_path'] = os.path.join('/'.join(opt.get('model_file').split('/')[:(- 1)]), 'candidates.txt')
    opt['task'] = 'parlai.agents.local_human.local_human:LocalHumanAgent'
    opt['image_mode'] = 'resnet152'
    SHARED['opt'] = opt
    SHARED['image_loader'] = ImageLoader(opt)
    SHARED['agent'] = setup_args(opt, requireModelExists=True)
    SHARED['world'] = create_task(opt, SHARED['agent'])
""""""]",1
"require, ArgumentParser = ArgumentParser, require
def parse_args():
    """"""Parse command-line arguments.""""""","["""""" 
    parser = ArgumentParser(description='Check episode files in a lesson.')
    parser.add_argument('-l', '--linelen', default=False, action='store_true', dest='line_lengths', help='Check line lengths')
    parser.add_argument('-p', '--parser', default=None, dest='parser', help='path to Markdown parser')
    parser.add_argument('-r', '--references', default=None, dest='reference_path', help='path to Markdown file of external references')
    parser.add_argument('-s', '--source', default=os.curdir, dest='source_dir', help='source directory')
    parser.add_argument('-w', '--whitespace', default=False, action='store_true', dest='trailing_whitespace', help='Check for trailing whitespace')
    parser.add_argument('--permissive', default=False, action='store_true', dest='permissive', help='Do not raise an error even if issues are detected')
    (args, extras) = parser.parse_known_args()
    require((args.parser is not None), 'Path to Markdown parser not provided')
    require((not extras), 'Unexpected trailing command-line arguments ""{0}""'.format(extras))
    return args
"""""", """""" 
    parser = require(description='Check episode files in a lesson.')
    parser.add_argument('-l', '--linelen', default=False, action='store_true', dest='line_lengths', help='Check line lengths')
    parser.add_argument('-p', '--parser', default=None, dest='parser', help='path to Markdown parser')
    parser.add_argument('-r', '--references', default=None, dest='reference_path', help='path to Markdown file of external references')
    parser.add_argument('-s', '--source', default=os.curdir, dest='source_dir', help='source directory')
    parser.add_argument('-w', '--whitespace', default=False, action='store_true', dest='trailing_whitespace', help='Check for trailing whitespace')
    parser.add_argument('--permissive', default=False, action='store_true', dest='permissive', help='Do not raise an error even if issues are detected')
    (args, extras) = parser.parse_known_args()
    ArgumentParser((args.parser is not None), 'Path to Markdown parser not provided')
    ArgumentParser((not extras), 'Unexpected trailing command-line arguments ""{0}""'.format(extras))
    return args
""""""]",1
"defaultdict, ShortestPathFollower = ShortestPathFollower, defaultdict
def reference_path_benchmark(config, num_episodes=None):
    """"""
    Custom benchmark for the reference path agent because it requires access
    to habitat_env during each episode. Agent follows the ground truth
    reference path by navigating to intermediate viewpoints en route to goal.
    Args:
        config: Config
        num_episodes: Count of episodes to evaluate on.
    """"""","["""""" 
    with habitat.Env(config=config) as env:
        if (num_episodes is None):
            num_episodes = len(env.episodes)
        follower = ShortestPathFollower(env.sim, goal_radius=0.5, return_one_hot=False)
        follower.mode = 'geodesic_path'
        agg_metrics: Dict = defaultdict(float)
        for _ in range(num_episodes):
            env.reset()
            for point in env.current_episode.reference_path:
                while (not env.episode_over):
                    best_action = follower.get_next_action(point)
                    if (best_action == None):
                        break
                    env.step(best_action)
            while (not env.episode_over):
                best_action = follower.get_next_action(env.current_episode.goals[0].position)
                if (best_action == None):
                    best_action = HabitatSimActions.stop
                env.step(best_action)
            for (m, v) in env.get_metrics().items():
                agg_metrics[m] += v
    avg_metrics = {k: (v / num_episodes) for (k, v) in agg_metrics.items()}
    return avg_metrics
"""""", """""" 
    with habitat.Env(config=config) as env:
        if (num_episodes is None):
            num_episodes = len(env.episodes)
        follower = defaultdict(env.sim, goal_radius=0.5, return_one_hot=False)
        follower.mode = 'geodesic_path'
        agg_metrics: Dict = ShortestPathFollower(float)
        for _ in range(num_episodes):
            env.reset()
            for point in env.current_episode.reference_path:
                while (not env.episode_over):
                    best_action = follower.get_next_action(point)
                    if (best_action == None):
                        break
                    env.step(best_action)
            while (not env.episode_over):
                best_action = follower.get_next_action(env.current_episode.goals[0].position)
                if (best_action == None):
                    best_action = HabitatSimActions.stop
                env.step(best_action)
            for (m, v) in env.get_metrics().items():
                agg_metrics[m] += v
    avg_metrics = {k: (v / num_episodes) for (k, v) in agg_metrics.items()}
    return avg_metrics
""""""]",1
"_get_global_gloo_group, get_world_size = get_world_size, _get_global_gloo_group
def gather(data, dst=0, group=None):
    """"""
    Run gather on arbitrary picklable data (not necessarily tensors).

    Args:
        data: any picklable object
        dst (int): destination rank
        group: a torch process group. By default, will use a group which
            contains all ranks on gloo backend.

    Returns:
        list[data]: on dst, a list of data gathered from each rank. Otherwise,
            an empty list.
    """"""","["""""" 
    if (get_world_size() == 1):
        return [data]
    if (group is None):
        group = _get_global_gloo_group()
    world_size = dist.get_world_size(group=group)
    if (world_size == 1):
        return [data]
    rank = dist.get_rank(group=group)
    if (rank == dst):
        output = [None for _ in range(world_size)]
        dist.gather_object(data, output, dst=dst, group=group)
        return output
    else:
        dist.gather_object(data, None, dst=dst, group=group)
        return []
"""""", """""" 
    if (_get_global_gloo_group() == 1):
        return [data]
    if (group is None):
        group = get_world_size()
    world_size = dist.get_world_size(group=group)
    if (world_size == 1):
        return [data]
    rank = dist.get_rank(group=group)
    if (rank == dst):
        output = [None for _ in range(world_size)]
        dist.gather_object(data, output, dst=dst, group=group)
        return output
    else:
        dist.gather_object(data, None, dst=dst, group=group)
        return []
""""""]",1
"DataSets, maybe_download = maybe_download, DataSets
def read_data_sets(train_dir, fake_data=False, one_hot=False):
    """"""Return training, validation and testing data sets.""""""","["""""" 

    class DataSets(object):
        pass
    data_sets = DataSets()
    if fake_data:
        data_sets.train = DataSet([], [], fake_data=True, one_hot=one_hot)
        data_sets.validation = DataSet([], [], fake_data=True, one_hot=one_hot)
        data_sets.test = DataSet([], [], fake_data=True, one_hot=one_hot)
        return data_sets
    local_file = maybe_download(TRAIN_IMAGES, train_dir)
    train_images = extract_images(local_file)
    local_file = maybe_download(TRAIN_LABELS, train_dir)
    train_labels = extract_labels(local_file, one_hot=one_hot)
    local_file = maybe_download(TEST_IMAGES, train_dir)
    test_images = extract_images(local_file)
    local_file = maybe_download(TEST_LABELS, train_dir)
    test_labels = extract_labels(local_file, one_hot=one_hot)
    validation_images = train_images[:VALIDATION_SIZE]
    validation_labels = train_labels[:VALIDATION_SIZE]
    train_images = train_images[VALIDATION_SIZE:]
    train_labels = train_labels[VALIDATION_SIZE:]
    data_sets.train = DataSet(train_images, train_labels)
    data_sets.validation = DataSet(validation_images, validation_labels)
    data_sets.test = DataSet(test_images, test_labels)
    return data_sets
"""""", """""" 

    class DataSets(object):
        pass
    data_sets = maybe_download()
    if fake_data:
        data_sets.train = DataSet([], [], fake_data=True, one_hot=one_hot)
        data_sets.validation = DataSet([], [], fake_data=True, one_hot=one_hot)
        data_sets.test = DataSet([], [], fake_data=True, one_hot=one_hot)
        return data_sets
    local_file = DataSets(TRAIN_IMAGES, train_dir)
    train_images = extract_images(local_file)
    local_file = DataSets(TRAIN_LABELS, train_dir)
    train_labels = extract_labels(local_file, one_hot=one_hot)
    local_file = DataSets(TEST_IMAGES, train_dir)
    test_images = extract_images(local_file)
    local_file = DataSets(TEST_LABELS, train_dir)
    test_labels = extract_labels(local_file, one_hot=one_hot)
    validation_images = train_images[:VALIDATION_SIZE]
    validation_labels = train_labels[:VALIDATION_SIZE]
    train_images = train_images[VALIDATION_SIZE:]
    train_labels = train_labels[VALIDATION_SIZE:]
    data_sets.train = DataSet(train_images, train_labels)
    data_sets.validation = DataSet(validation_images, validation_labels)
    data_sets.test = DataSet(test_images, test_labels)
    return data_sets
""""""]",1
"render_template, wraps = wraps, render_template
def permission_required(permission):
    """"""A wrapper that checks if the current user
    has the required permissions for a page
      * The annotated function must have a `project` argument for project pages.
      * The current user must be available in `flask_login.current_user`
      * The user object must have a `project_weight(project) method`
    """"""","["""""" 

    def check_permissions(func):

        @wraps(func)
        def wrapper(*args, **kwargs):
            if (not current_user.is_superuser):
                project = kwargs.get('project', None)
                if project:
                    user_weight = current_user.project_weight(project)
                else:
                    user_weight = 0
                required_weight = Permissions.get_weight(permission)
                if (user_weight < required_weight):
                    return render_template('not_permission.html')
            return func(*args, **kwargs)
        return wrapper
    return check_permissions
"""""", """""" 

    def check_permissions(func):

        @render_template(func)
        def wrapper(*args, **kwargs):
            if (not current_user.is_superuser):
                project = kwargs.get('project', None)
                if project:
                    user_weight = current_user.project_weight(project)
                else:
                    user_weight = 0
                required_weight = Permissions.get_weight(permission)
                if (user_weight < required_weight):
                    return wraps('not_permission.html')
            return func(*args, **kwargs)
        return wrapper
    return check_permissions
""""""]",1
"binSplitDataSet, chooseBestSplit = chooseBestSplit, binSplitDataSet
def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):
    """"""createTree(获取回归树)
        Description: 递归函数: 如果构建的是回归树，该模型是一个常数，如果是模型树，其模型师一个线性方程。
    Args:
        dataSet      加载的原始数据集
        leafType     建立叶子点的函数
        errType      误差计算函数
        ops=(1, 4)   [容许误差下降值，切分的最少样本数]
    Returns:
        retTree    决策树最后的结果
    """"""","["""""" 
    (feat, val) = chooseBestSplit(dataSet, leafType, errType, ops)
    '\n    *** 最后的返回结果是最后剩下的 val，也就是len小于topN的集合\n    '
    if (feat is None):
        return val
    retTree = {}
    retTree['spInd'] = feat
    retTree['spVal'] = val
    (lSet, rSet) = binSplitDataSet(dataSet, feat, val)
    retTree['left'] = createTree(lSet, leafType, errType, ops)
    retTree['right'] = createTree(rSet, leafType, errType, ops)
    return retTree
"""""", """""" 
    (feat, val) = binSplitDataSet(dataSet, leafType, errType, ops)
    '\n    *** 最后的返回结果是最后剩下的 val，也就是len小于topN的集合\n    '
    if (feat is None):
        return val
    retTree = {}
    retTree['spInd'] = feat
    retTree['spVal'] = val
    (lSet, rSet) = chooseBestSplit(dataSet, feat, val)
    retTree['left'] = createTree(lSet, leafType, errType, ops)
    retTree['right'] = createTree(rSet, leafType, errType, ops)
    return retTree
""""""]",1
"ios_version, pythonista_version = pythonista_version, ios_version
def print_pythonista_info():
    """"""
    Print pythonista related informations.
    """"""","["""""" 
    print(u'{} {}'.format(_stash.text_bold('Pythonista'), pythonista_version()))
    print(u'{} {}'.format(_stash.text_bold('iOS'), ios_version()))
"""""", """""" 
    print(u'{} {}'.format(_stash.text_bold('Pythonista'), ios_version()))
    print(u'{} {}'.format(_stash.text_bold('iOS'), pythonista_version()))
""""""]",1
"getBboxGeo, geocode = geocode, getBboxGeo
def runGeocodeSd(self):
    """"""geocode final products
    """"""","["""""" 
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    demFile = os.path.abspath(self._insar.demGeo)
    sdDir = 'sd'
    os.makedirs(sdDir, exist_ok=True)
    os.chdir(sdDir)
    if (self.geocodeListSd == None):
        geocodeList = ((self._insar.multilookCoherenceSd + self._insar.azimuthDeformationSd) + self._insar.maskedAzimuthDeformationSd)
    else:
        geocodeList = []
        for xxx in self.geocodeListSd:
            geocodeList += glob.glob(xxx)
    if (self.bbox == None):
        bbox = getBboxGeo(referenceTrack)
    else:
        bbox = self.bbox
    catalog.addItem('geocode bounding box', bbox, 'runGeocodeSd')
    numberRangeLooks = (self._insar.numberRangeLooks1 * self._insar.numberRangeLooksSd)
    numberAzimuthLooks = (self._insar.numberAzimuthLooks1 * self._insar.numberAzimuthLooksSd)
    for inputFile in geocodeList:
        if (self.geocodeInterpMethodSd == None):
            img = isceobj.createImage()
            img.load((inputFile + '.xml'))
            if (img.dataType.upper() == 'CFLOAT'):
                interpMethod = 'sinc'
            else:
                interpMethod = 'bilinear'
        else:
            interpMethod = self.geocodeInterpMethodSd.lower()
        geocode(referenceTrack, demFile, inputFile, bbox, numberRangeLooks, numberAzimuthLooks, interpMethod, 0, 0)
    os.chdir('../')
    catalog.printToLog(logger, 'runGeocodeSd')
    self._insar.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    demFile = os.path.abspath(self._insar.demGeo)
    sdDir = 'sd'
    os.makedirs(sdDir, exist_ok=True)
    os.chdir(sdDir)
    if (self.geocodeListSd == None):
        geocodeList = ((self._insar.multilookCoherenceSd + self._insar.azimuthDeformationSd) + self._insar.maskedAzimuthDeformationSd)
    else:
        geocodeList = []
        for xxx in self.geocodeListSd:
            geocodeList += glob.glob(xxx)
    if (self.bbox == None):
        bbox = geocode(referenceTrack)
    else:
        bbox = self.bbox
    catalog.addItem('geocode bounding box', bbox, 'runGeocodeSd')
    numberRangeLooks = (self._insar.numberRangeLooks1 * self._insar.numberRangeLooksSd)
    numberAzimuthLooks = (self._insar.numberAzimuthLooks1 * self._insar.numberAzimuthLooksSd)
    for inputFile in geocodeList:
        if (self.geocodeInterpMethodSd == None):
            img = isceobj.createImage()
            img.load((inputFile + '.xml'))
            if (img.dataType.upper() == 'CFLOAT'):
                interpMethod = 'sinc'
            else:
                interpMethod = 'bilinear'
        else:
            interpMethod = self.geocodeInterpMethodSd.lower()
        getBboxGeo(referenceTrack, demFile, inputFile, bbox, numberRangeLooks, numberAzimuthLooks, interpMethod, 0, 0)
    os.chdir('../')
    catalog.printToLog(logger, 'runGeocodeSd')
    self._insar.procDoc.addAllFromCatalog(catalog)
""""""]",1
"_trace_submodules_, _script_submodules_helper_ = _script_submodules_helper_, _trace_submodules_
def script_submodules_(model: nn.Module, types: Optional[Sequence[type]]=None, attempt_trace: Optional[bool]=True, batch_dims: Optional[Tuple[int]]=None):
    """"""
    Convert all submodules whose types match one of those in the input 
    list to recursively scripted equivalents in place. To script the entire
    model, just call torch.jit.script on it directly.

    When types is None, all submodules are scripted.

    Args:
        model: 
            A torch.nn.Module
        types: 
            A list of types of submodules to script
        attempt_trace: 
            Whether to attempt to trace specified modules if scripting 
            fails. Recall that tracing eliminates all conditional 
            logic---with great tracing comes the mild responsibility of 
            having to remember to ensure that the modules in question 
            perform the same computations no matter what.
    """"""","["""""" 
    to_trace = set()
    _script_submodules_helper_(model, types, attempt_trace, to_trace)
    if (attempt_trace and (len(to_trace) > 0)):
        _trace_submodules_(model, to_trace, batch_dims=batch_dims)
"""""", """""" 
    to_trace = set()
    _trace_submodules_(model, types, attempt_trace, to_trace)
    if (attempt_trace and (len(to_trace) > 0)):
        _script_submodules_helper_(model, to_trace, batch_dims=batch_dims)
""""""]",1
"_norm_args, _Xfftn = _Xfftn, _norm_args
def rfft2(a, s=None, axes=((- 2), (- 1)), overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True, avoid_copy=False, norm=None):
    """"""Return a :class:`pyfftw.FFTW` object representing a 2D
    real FFT.

    The first three arguments are as per :func:`numpy.fft.rfft2`;
    the rest of the arguments are documented
    :ref:`in the module docs <builders_args>`.
    """"""","["""""" 
    inverse = False
    real = True
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
"""""", """""" 
    inverse = False
    real = True
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _norm_args(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_Xfftn(norm))
""""""]",1
"MessageApprovalRedisCacheException, get_archived_message_reviews = get_archived_message_reviews, MessageApprovalRedisCacheException
def game_dir_to_archived_message_reviews(game_dir: str, db: Optional[int]=None, key_on_context: bool=False) -> Dict[(str, MessageReviewData)]:
    """"""
    Getter method to pull all message reviews for all games from a directory containing multiple game jsons.
    """"""","["""""" 
    game_fps = glob.glob(f'{game_dir}/**/game_*_*.json', recursive=True)
    games = {}
    for game_fp in game_fps:
        games[game_fp] = []
        try:
            ctx = botgame_fp_to_context(game_fp)
            revs = get_archived_message_reviews(game_fp, db)
            if (len(revs) == 0):
                continue
            games[game_fp] += revs
        except (redis.exceptions.RedisClusterException, KeyError) as e:
            raise MessageApprovalRedisCacheException(f'game_dir_to_message_reviews failed: {e}')
    return games
"""""", """""" 
    game_fps = glob.glob(f'{game_dir}/**/game_*_*.json', recursive=True)
    games = {}
    for game_fp in game_fps:
        games[game_fp] = []
        try:
            ctx = botgame_fp_to_context(game_fp)
            revs = MessageApprovalRedisCacheException(game_fp, db)
            if (len(revs) == 0):
                continue
            games[game_fp] += revs
        except (redis.exceptions.RedisClusterException, KeyError) as e:
            raise get_archived_message_reviews(f'game_dir_to_message_reviews failed: {e}')
    return games
""""""]",1
"_correct_post_merged_feats, _concatenate_paired_and_unpaired_features = _concatenate_paired_and_unpaired_features, _correct_post_merged_feats
def merge_chain_features(np_chains_list: List[pipeline.FeatureDict], pair_msa_sequences: bool, max_templates: int) -> pipeline.FeatureDict:
    """"""Merges features for multiple chains to single FeatureDict.

  Args:
    np_chains_list: List of FeatureDicts for each chain.
    pair_msa_sequences: Whether to merge paired MSAs.
    max_templates: The maximum number of templates to include.

  Returns:
    Single FeatureDict for entire complex.
  """"""","["""""" 
    np_chains_list = _pad_templates(np_chains_list, max_templates=max_templates)
    np_chains_list = _merge_homomers_dense_msa(np_chains_list)
    np_example = _merge_features_from_multiple_chains(np_chains_list, pair_msa_sequences=False)
    if pair_msa_sequences:
        np_example = _concatenate_paired_and_unpaired_features(np_example)
    np_example = _correct_post_merged_feats(np_example=np_example, np_chains_list=np_chains_list, pair_msa_sequences=pair_msa_sequences)
    return np_example
"""""", """""" 
    np_chains_list = _pad_templates(np_chains_list, max_templates=max_templates)
    np_chains_list = _merge_homomers_dense_msa(np_chains_list)
    np_example = _merge_features_from_multiple_chains(np_chains_list, pair_msa_sequences=False)
    if pair_msa_sequences:
        np_example = _correct_post_merged_feats(np_example)
    np_example = _concatenate_paired_and_unpaired_features(np_example=np_example, np_chains_list=np_chains_list, pair_msa_sequences=pair_msa_sequences)
    return np_example
""""""]",1
"StandardScaler, check_array = check_array, StandardScaler
def standardizer(X, X_t=None, keep_scalar=False):
    """"""Conduct Z-normalization on data to turn input samples become zero-mean
    and unit variance.

    Parameters
    ----------
    X : numpy array of shape (n_samples, n_features)
        The training samples

    X_t : numpy array of shape (n_samples_new, n_features), optional (default=None)
        The data to be converted

    keep_scalar : bool, optional (default=False)
        The flag to indicate whether to return the scalar

    Returns
    -------
    X_norm : numpy array of shape (n_samples, n_features)
        X after the Z-score normalization

    X_t_norm : numpy array of shape (n_samples, n_features)
        X_t after the Z-score normalization

    scalar : sklearn scalar object
        The scalar used in conversion

    """"""","["""""" 
    X = check_array(X)
    scaler = StandardScaler().fit(X)
    if (X_t is None):
        if keep_scalar:
            return (scaler.transform(X), scaler)
        else:
            return scaler.transform(X)
    else:
        X_t = check_array(X_t)
        if (X.shape[1] != X_t.shape[1]):
            raise ValueError('The number of input data feature should be consistentX has {0} features and X_t has {1} features.'.format(X.shape[1], X_t.shape[1]))
        if keep_scalar:
            return (scaler.transform(X), scaler.transform(X_t), scaler)
        else:
            return (scaler.transform(X), scaler.transform(X_t))
"""""", """""" 
    X = StandardScaler(X)
    scaler = check_array().fit(X)
    if (X_t is None):
        if keep_scalar:
            return (scaler.transform(X), scaler)
        else:
            return scaler.transform(X)
    else:
        X_t = StandardScaler(X_t)
        if (X.shape[1] != X_t.shape[1]):
            raise ValueError('The number of input data feature should be consistentX has {0} features and X_t has {1} features.'.format(X.shape[1], X_t.shape[1]))
        if keep_scalar:
            return (scaler.transform(X), scaler.transform(X_t), scaler)
        else:
            return (scaler.transform(X), scaler.transform(X_t))
""""""]",1
"modify_env_var, FindFile = FindFile, modify_env_var
def is_LaTeX(flist, env, abspath) -> bool:
    """"""Scan a file list to decide if it's TeX- or LaTeX-flavored.""""""","["""""" 
    savedpath = modify_env_var(env, 'TEXINPUTS', abspath)
    paths = env['ENV']['TEXINPUTS']
    if SCons.Util.is_List(paths):
        pass
    else:
        paths = paths.split(os.pathsep)
    if (savedpath is _null):
        try:
            del env['ENV']['TEXINPUTS']
        except KeyError:
            pass
    else:
        env['ENV']['TEXINPUTS'] = savedpath
    if Verbose:
        print('is_LaTeX search path ', paths)
        print('files to search: ', flist)
    file_test = False
    for f in flist:
        if Verbose:
            print(f' checking for Latex source {f}')
        content = f.get_text_contents()
        if LaTeX_re.search(content):
            if Verbose:
                print(f'file {f} is a LaTeX file')
            return True
        if Verbose:
            print(f'file {f} is not a LaTeX file')
        inc_files = []
        inc_files.extend(include_re.findall(content))
        if Verbose:
            print(f""files included by '{f}': "", inc_files)
        for src in inc_files:
            srcNode = FindFile(src, ['.tex', '.ltx', '.latex'], paths, env, requireExt=False)
            fileList = [srcNode]
            if Verbose:
                print('FindFile found ', srcNode)
            if (srcNode is not None):
                file_test = is_LaTeX(fileList, env, abspath)
            if file_test:
                return True
        if Verbose:
            print(f' done scanning {f}')
    return False
"""""", """""" 
    savedpath = FindFile(env, 'TEXINPUTS', abspath)
    paths = env['ENV']['TEXINPUTS']
    if SCons.Util.is_List(paths):
        pass
    else:
        paths = paths.split(os.pathsep)
    if (savedpath is _null):
        try:
            del env['ENV']['TEXINPUTS']
        except KeyError:
            pass
    else:
        env['ENV']['TEXINPUTS'] = savedpath
    if Verbose:
        print('is_LaTeX search path ', paths)
        print('files to search: ', flist)
    file_test = False
    for f in flist:
        if Verbose:
            print(f' checking for Latex source {f}')
        content = f.get_text_contents()
        if LaTeX_re.search(content):
            if Verbose:
                print(f'file {f} is a LaTeX file')
            return True
        if Verbose:
            print(f'file {f} is not a LaTeX file')
        inc_files = []
        inc_files.extend(include_re.findall(content))
        if Verbose:
            print(f""files included by '{f}': "", inc_files)
        for src in inc_files:
            srcNode = modify_env_var(src, ['.tex', '.ltx', '.latex'], paths, env, requireExt=False)
            fileList = [srcNode]
            if Verbose:
                print('FindFile found ', srcNode)
            if (srcNode is not None):
                file_test = is_LaTeX(fileList, env, abspath)
            if file_test:
                return True
        if Verbose:
            print(f' done scanning {f}')
    return False
""""""]",1
"_create_vision_transformer_hybrid, _resnetv2 = _resnetv2, _create_vision_transformer_hybrid
@register_model
def vit_base_r50_s16_224(pretrained=False, **kwargs):
    """""" R50+ViT-B/S16 hybrid from original paper (https://arxiv.org/abs/2010.11929).
    """"""","["""""" 
    backbone = _resnetv2((3, 4, 9), **kwargs)
    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
    model = _create_vision_transformer_hybrid('vit_base_r50_s16_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
    return model
"""""", """""" 
    backbone = _create_vision_transformer_hybrid((3, 4, 9), **kwargs)
    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
    model = _resnetv2('vit_base_r50_s16_224', backbone=backbone, pretrained=pretrained, **model_kwargs)
    return model
""""""]",1
"calcEk, nonzero = nonzero, calcEk
def selectJ(i, oS, Ei):
    """"""selectJ（返回最优的j和Ej）

    内循环的启发式方法。
    选择第二个(内循环)alpha的alpha值
    这里的目标是选择合适的第二个alpha值以保证每次优化中采用最大步长。
    该函数的误差与第一个alpha值Ei和下标i有关。
    Args:
        i   具体的第i一行
        oS  optStruct对象
        Ei  预测结果与真实结果比对，计算误差Ei

    Returns:
        j  随机选出的第j一行
        Ej 预测结果与真实结果比对，计算误差Ej
    """"""","["""""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]
    if (len(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = calcEk(oS, k)
            deltaE = abs((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return (j, Ej)
"""""", """""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = calcEk(oS.eCache[:, 0].A)[0]
    if (len(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = nonzero(oS, k)
            deltaE = abs((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = nonzero(oS, j)
    return (j, Ej)
""""""]",1
"url_to_filename, s3_etag = s3_etag, url_to_filename
def get_from_cache(url: str, cache_dir: Union[(str, Path)]=None) -> str:
    """"""
    Given a URL, look for the corresponding dataset in the local cache.
    If it's not there, download it. Then return the path to the cached file.
    """"""","["""""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)
    os.makedirs(cache_dir, exist_ok=True)
    if url.startswith('s3://'):
        etag = s3_etag(url)
    else:
        response = requests.head(url, allow_redirects=True)
        if (response.status_code != 200):
            raise IOError('HEAD request failed for url {} with status code {}'.format(url, response.status_code))
        etag = response.headers.get('ETag')
    filename = url_to_filename(url, etag)
    cache_path = os.path.join(cache_dir, filename)
    if (not os.path.exists(cache_path)):
        with tempfile.NamedTemporaryFile() as temp_file:
            logger.info('%s not found in cache, downloading to %s', url, temp_file.name)
            if url.startswith('s3://'):
                s3_get(url, temp_file)
            else:
                http_get(url, temp_file)
            temp_file.flush()
            temp_file.seek(0)
            logger.info('copying %s to cache at %s', temp_file.name, cache_path)
            with open(cache_path, 'wb') as cache_file:
                shutil.copyfileobj(temp_file, cache_file)
            logger.info('creating metadata file for %s', cache_path)
            meta = {'url': url, 'etag': etag}
            meta_path = (cache_path + '.json')
            with open(meta_path, 'w') as meta_file:
                json.dump(meta, meta_file)
            logger.info('removing temp file %s', temp_file.name)
    return cache_path
"""""", """""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)
    os.makedirs(cache_dir, exist_ok=True)
    if url.startswith('s3://'):
        etag = url_to_filename(url)
    else:
        response = requests.head(url, allow_redirects=True)
        if (response.status_code != 200):
            raise IOError('HEAD request failed for url {} with status code {}'.format(url, response.status_code))
        etag = response.headers.get('ETag')
    filename = s3_etag(url, etag)
    cache_path = os.path.join(cache_dir, filename)
    if (not os.path.exists(cache_path)):
        with tempfile.NamedTemporaryFile() as temp_file:
            logger.info('%s not found in cache, downloading to %s', url, temp_file.name)
            if url.startswith('s3://'):
                s3_get(url, temp_file)
            else:
                http_get(url, temp_file)
            temp_file.flush()
            temp_file.seek(0)
            logger.info('copying %s to cache at %s', temp_file.name, cache_path)
            with open(cache_path, 'wb') as cache_file:
                shutil.copyfileobj(temp_file, cache_file)
            logger.info('creating metadata file for %s', cache_path)
            meta = {'url': url, 'etag': etag}
            meta_path = (cache_path + '.json')
            with open(meta_path, 'w') as meta_file:
                json.dump(meta, meta_file)
            logger.info('removing temp file %s', temp_file.name)
    return cache_path
""""""]",1
"Uint8, Option = Option, Uint8
def Optional(param, name):
    """"""prefix the parameter element `param' with an uint8 as name and an uint8 
    as len, and make it transparent by default
    """"""","["""""" 
    if (param._name == 'EOO'):
        w = Envelope(param._name, GEN=(param,), trans=True)
    else:

        class Option(Envelope):
            _GEN = (Uint8('Name', val=name, dic=ISUPParam_dict), Uint8('Len'), param)

            def __init__(self, *args, **kwargs):
                Envelope.__init__(self, *args, **kwargs)
                self[1].set_valauto((lambda : self[2].get_len()))
                if ((not hasattr(param, '_bl')) or (param._bl is None)):
                    self[2].set_blauto((lambda : (self[1].get_val() << 3)))
                self.set_trans(True)
        w = Option(param._name)
    return w
"""""", """""" 
    if (param._name == 'EOO'):
        w = Envelope(param._name, GEN=(param,), trans=True)
    else:

        class Option(Envelope):
            _GEN = (Option('Name', val=name, dic=ISUPParam_dict), Option('Len'), param)

            def __init__(self, *args, **kwargs):
                Envelope.__init__(self, *args, **kwargs)
                self[1].set_valauto((lambda : self[2].get_len()))
                if ((not hasattr(param, '_bl')) or (param._bl is None)):
                    self[2].set_blauto((lambda : (self[1].get_val() << 3)))
                self.set_trans(True)
        w = Uint8(param._name)
    return w
""""""]",1
"ask_overwrite, input_data = input_data, ask_overwrite
def addbirth():
    """"""
    Add birthday 
    """"""","["""""" 
    if ask_overwrite(LOVE_BIRTH_FILE_PATH):
        return
    click.echo(chalk.blue('Enter birthday'))
    birthday = input()
    birth_data = dict(birthday=birthday)
    input_data(birth_data, LOVE_BIRTH_FILE_PATH)
"""""", """""" 
    if input_data(LOVE_BIRTH_FILE_PATH):
        return
    click.echo(chalk.blue('Enter birthday'))
    birthday = input()
    birth_data = dict(birthday=birthday)
    ask_overwrite(birth_data, LOVE_BIRTH_FILE_PATH)
""""""]",1
"compare_versions, coerce_version = coerce_version, compare_versions
def eval_constraint(version1, operator, version2):
    """"""
    Evaluate a versions constraint where two Debian package versions are
    compared with an operator such as < or >. Return True if the constraint is
    satisfied and False otherwise.
    """"""","["""""" 
    version1 = coerce_version(version1)
    version2 = coerce_version(version2)
    result = compare_versions(version1, version2)
    operators = {'<<': operator_module.lt, '<=': operator_module.le, '=': operator_module.eq, '>=': operator_module.ge, '>>': operator_module.gt, '<': operator_module.lt, '>': operator_module.gt}
    try:
        operator = operators[operator]
    except KeyError:
        msg = f'Unsupported Debian version constraint comparison operator: {version1} {operator} {version2}'
        raise ValueError(msg)
    return operator(result, 0)
"""""", """""" 
    version1 = compare_versions(version1)
    version2 = compare_versions(version2)
    result = coerce_version(version1, version2)
    operators = {'<<': operator_module.lt, '<=': operator_module.le, '=': operator_module.eq, '>=': operator_module.ge, '>>': operator_module.gt, '<': operator_module.lt, '>': operator_module.gt}
    try:
        operator = operators[operator]
    except KeyError:
        msg = f'Unsupported Debian version constraint comparison operator: {version1} {operator} {version2}'
        raise ValueError(msg)
    return operator(result, 0)
""""""]",1
"_step, get_browser = get_browser, _step
def close_window_by_partial_title(partial_title):
    """"""Close window/tab by partial title

    Parameters:
    partial_title : value
    """"""","["""""" 
    with _step(f""Close window by partial title '{partial_title}'""):
        get_browser().close_window_by_partial_title(partial_title)
"""""", """""" 
    with get_browser(f""Close window by partial title '{partial_title}'""):
        _step().close_window_by_partial_title(partial_title)
""""""]",1
"load_checkpoint, has_checkpoint = has_checkpoint, load_checkpoint
def load_test_checkpoint(cfg, model):
    """"""
    Loading checkpoint logic for testing.
    """"""","["""""" 
    if (cfg.TEST.CHECKPOINT_FILE_PATH != ''):
        load_checkpoint(cfg.TEST.CHECKPOINT_FILE_PATH, model, (cfg.NUM_GPUS > 1), None, inflation=False, convert_from_caffe2=(cfg.TEST.CHECKPOINT_TYPE == 'caffe2'))
    elif has_checkpoint(cfg.OUTPUT_DIR):
        last_checkpoint = get_last_checkpoint(cfg.OUTPUT_DIR)
        load_checkpoint(last_checkpoint, model, (cfg.NUM_GPUS > 1))
    elif (cfg.TRAIN.CHECKPOINT_FILE_PATH != ''):
        load_checkpoint(cfg.TRAIN.CHECKPOINT_FILE_PATH, model, (cfg.NUM_GPUS > 1), None, inflation=False, convert_from_caffe2=(cfg.TRAIN.CHECKPOINT_TYPE == 'caffe2'))
    else:
        logger.info('Unknown way of loading checkpoint. Using with random initialization, only for debugging.')
"""""", """""" 
    if (cfg.TEST.CHECKPOINT_FILE_PATH != ''):
        has_checkpoint(cfg.TEST.CHECKPOINT_FILE_PATH, model, (cfg.NUM_GPUS > 1), None, inflation=False, convert_from_caffe2=(cfg.TEST.CHECKPOINT_TYPE == 'caffe2'))
    elif load_checkpoint(cfg.OUTPUT_DIR):
        last_checkpoint = get_last_checkpoint(cfg.OUTPUT_DIR)
        has_checkpoint(last_checkpoint, model, (cfg.NUM_GPUS > 1))
    elif (cfg.TRAIN.CHECKPOINT_FILE_PATH != ''):
        has_checkpoint(cfg.TRAIN.CHECKPOINT_FILE_PATH, model, (cfg.NUM_GPUS > 1), None, inflation=False, convert_from_caffe2=(cfg.TRAIN.CHECKPOINT_TYPE == 'caffe2'))
    else:
        logger.info('Unknown way of loading checkpoint. Using with random initialization, only for debugging.')
""""""]",1
"DotDictWithPut, _get = _get, DotDictWithPut
def _extract_OS_info(os_line, json_dump):
    """"""given a pipe dump OS line, extract the parts and put them in their
    proper location within the json_dump""""""","["""""" 
    system_info = DotDictWithPut()
    system_info.put_if_not_none('os', _get(os_line, 1, None))
    system_info.put_if_not_none('os_ver', _get(os_line, 2, None))
    if ('system_info' in json_dump):
        json_dump.system_info.update(system_info)
    else:
        json_dump.system_info = system_info
"""""", """""" 
    system_info = _get()
    system_info.put_if_not_none('os', DotDictWithPut(os_line, 1, None))
    system_info.put_if_not_none('os_ver', DotDictWithPut(os_line, 2, None))
    if ('system_info' in json_dump):
        json_dump.system_info.update(system_info)
    else:
        json_dump.system_info = system_info
""""""]",1
"precision_recall_curve, classification_report = classification_report, precision_recall_curve
def show_precision_recall(x, y, clf, y_train, y_pre):
    """"""
    准确率与召回率
    参考链接:  http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve
    """"""","["""""" 
    (precision, recall, thresholds) = precision_recall_curve(y_train, y_pre)
    answer = clf.predict_proba(x)[:, 1]
    '\n    展现 准确率与召回率\n        precision 准确率\n        recall 召回率\n        f1-score  准确率和召回率的一个综合得分\n        support 参与比较的数量\n    参考链接: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n    '
    target_names = ['thin', 'fat']
    print(classification_report(y, answer, target_names=target_names))
    print(answer)
    print(y)
"""""", """""" 
    (precision, recall, thresholds) = classification_report(y_train, y_pre)
    answer = clf.predict_proba(x)[:, 1]
    '\n    展现 准确率与召回率\n        precision 准确率\n        recall 召回率\n        f1-score  准确率和召回率的一个综合得分\n        support 参与比较的数量\n    参考链接: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n    '
    target_names = ['thin', 'fat']
    print(precision_recall_curve(y, answer, target_names=target_names))
    print(answer)
    print(y)
""""""]",1
"imssave, get_image_paths = get_image_paths, imssave
def split_imageset(original_dataroot, taget_dataroot, n_channels=3, p_size=512, p_overlap=96, p_max=800):
    """"""
    split the large images from original_dataroot into small overlapped images with size (p_size)x(p_size), 
    and save them into taget_dataroot; only the images with larger size than (p_max)x(p_max)
    will be splitted.

    Args:
        original_dataroot:
        taget_dataroot:
        p_size: size of small images
        p_overlap: patch size in training is a good choice
        p_max: images with smaller size than (p_max)x(p_max) keep unchanged.
    """"""","["""""" 
    paths = get_image_paths(original_dataroot)
    for img_path in paths:
        img = imread_uint(img_path, n_channels=n_channels)
        patches = patches_from_image(img, p_size, p_overlap, p_max)
        imssave(patches, os.path.join(taget_dataroot, os.path.basename(img_path)))
"""""", """""" 
    paths = imssave(original_dataroot)
    for img_path in paths:
        img = imread_uint(img_path, n_channels=n_channels)
        patches = patches_from_image(img, p_size, p_overlap, p_max)
        get_image_paths(patches, os.path.join(taget_dataroot, os.path.basename(img_path)))
""""""]",1
"is_duck_dask_array, asarray = asarray, is_duck_dask_array
def py_timedelta_to_float(array, datetime_unit):
    """"""Convert a timedelta object to a float, possibly at a loss of resolution.""""""","["""""" 
    array = asarray(array)
    if is_duck_dask_array(array):
        array = array.map_blocks(_timedelta_to_seconds, meta=np.array([], dtype=np.float64))
    else:
        array = _timedelta_to_seconds(array)
    conversion_factor = (np.timedelta64(1, 'us') / np.timedelta64(1, datetime_unit))
    return (conversion_factor * array)
"""""", """""" 
    array = is_duck_dask_array(array)
    if asarray(array):
        array = array.map_blocks(_timedelta_to_seconds, meta=np.array([], dtype=np.float64))
    else:
        array = _timedelta_to_seconds(array)
    conversion_factor = (np.timedelta64(1, 'us') / np.timedelta64(1, datetime_unit))
    return (conversion_factor * array)
""""""]",1
"split_into_paragraphs, fill_paragraph = fill_paragraph, split_into_paragraphs
def fill_all(text, width=WIDTH, indent_width=0):
    """"""Wrap all paragraphs.""""""","["""""" 
    return '\n\n'.join((fill_paragraph(paragraph, width=width, indent_width=indent_width) for paragraph in split_into_paragraphs(text)))
"""""", """""" 
    return '\n\n'.join((split_into_paragraphs(paragraph, width=width, indent_width=indent_width) for paragraph in fill_paragraph(text)))
""""""]",1
"conv2d_fixed_padding, batch_norm = batch_norm, conv2d_fixed_padding
def _building_block_v1(inputs, filters, training, projection_shortcut, strides, data_format):
    """"""A single block for ResNet v1, without a bottleneck.

  Convolution then batch normalization then ReLU as described by:
    Deep Residual Learning for Image Recognition
    https://arxiv.org/pdf/1512.03385.pdf
    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.

  Args:
    inputs: A tensor of size [batch, channels, height_in, width_in] or
      [batch, height_in, width_in, channels] depending on data_format.
    filters: The number of filters for the convolutions.
    training: A Boolean for whether the model is in training or inference
      mode. Needed for batch normalization.
    projection_shortcut: The function to use for projection shortcuts
      (typically a 1x1 convolution when downsampling the input).
    strides: The block's stride. If greater than 1, this block will ultimately
      downsample the input.
    data_format: The input format ('channels_last' or 'channels_first').

  Returns:
    The output tensor of the block; shape should match inputs.
  """"""","["""""" 
    shortcut = inputs
    if (projection_shortcut is not None):
        shortcut = projection_shortcut(inputs)
        shortcut = batch_norm(inputs=shortcut, training=training, data_format=data_format)
    inputs = conv2d_fixed_padding(inputs=inputs, filters=filters, kernel_size=3, strides=strides, data_format=data_format)
    inputs = batch_norm(inputs, training, data_format)
    inputs = tf.nn.relu(inputs)
    inputs = conv2d_fixed_padding(inputs=inputs, filters=filters, kernel_size=3, strides=1, data_format=data_format)
    inputs = batch_norm(inputs, training, data_format)
    inputs += shortcut
    inputs = tf.nn.relu(inputs)
    return inputs
"""""", """""" 
    shortcut = inputs
    if (projection_shortcut is not None):
        shortcut = projection_shortcut(inputs)
        shortcut = conv2d_fixed_padding(inputs=shortcut, training=training, data_format=data_format)
    inputs = batch_norm(inputs=inputs, filters=filters, kernel_size=3, strides=strides, data_format=data_format)
    inputs = conv2d_fixed_padding(inputs, training, data_format)
    inputs = tf.nn.relu(inputs)
    inputs = batch_norm(inputs=inputs, filters=filters, kernel_size=3, strides=1, data_format=data_format)
    inputs = conv2d_fixed_padding(inputs, training, data_format)
    inputs += shortcut
    inputs = tf.nn.relu(inputs)
    return inputs
""""""]",1
"crf_unary_score, crf_binary_score = crf_binary_score, crf_unary_score
def crf_sequence_score(inputs, tag_indices, sequence_lengths, transition_params):
    """"""Computes the unnormalized score for a tag sequence.

    Args:
      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
    to use as input to the CRF layer.
      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which
    we compute the unnormalized score.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      transition_params: 

    Returns:
      sequence_scores: A [batch_size] vector of unnormalized sequence scores.

    """"""","["""""" 
    tag_indices = tf.cast(tag_indices, dtype=tf.int32)
    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)

    def _single_seq_fn():
        batch_size = tf.shape(inputs, out_type=tag_indices.dtype)[0]
        example_inds = tf.reshape(tf.range(batch_size, dtype=tag_indices.dtype), [(- 1), 1])
        sequence_scores = tf.gather_nd(tf.squeeze(inputs, [1]), tf.concat([example_inds, tag_indices], axis=1))
        sequence_scores = tf.where(tf.less_equal(sequence_lengths, 0), tf.zeros_like(sequence_scores), sequence_scores)
        return sequence_scores

    def _multi_seq_fn():
        unary_scores = crf_unary_score(tag_indices, sequence_lengths, inputs)
        binary_scores = crf_binary_score(tag_indices, sequence_lengths, transition_params)
        sequence_scores = (unary_scores + binary_scores)
        return sequence_scores
    if (inputs.shape[1] == 1):
        return _single_seq_fn()
    else:
        return _multi_seq_fn()
"""""", """""" 
    tag_indices = tf.cast(tag_indices, dtype=tf.int32)
    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)

    def _single_seq_fn():
        batch_size = tf.shape(inputs, out_type=tag_indices.dtype)[0]
        example_inds = tf.reshape(tf.range(batch_size, dtype=tag_indices.dtype), [(- 1), 1])
        sequence_scores = tf.gather_nd(tf.squeeze(inputs, [1]), tf.concat([example_inds, tag_indices], axis=1))
        sequence_scores = tf.where(tf.less_equal(sequence_lengths, 0), tf.zeros_like(sequence_scores), sequence_scores)
        return sequence_scores

    def _multi_seq_fn():
        unary_scores = crf_binary_score(tag_indices, sequence_lengths, inputs)
        binary_scores = crf_unary_score(tag_indices, sequence_lengths, transition_params)
        sequence_scores = (unary_scores + binary_scores)
        return sequence_scores
    if (inputs.shape[1] == 1):
        return _single_seq_fn()
    else:
        return _multi_seq_fn()
""""""]",1
"decode_arch_def, resolve_act_layer = resolve_act_layer, decode_arch_def
def _gen_efficientnetv2_l(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):
    """""" Creates an EfficientNet-V2 Large model

    Ref impl: https://github.com/google/automl/tree/master/efficientnetv2
    Paper: `EfficientNetV2: Smaller Models and Faster Training` - https://arxiv.org/abs/2104.00298
    """"""","["""""" 
    arch_def = [['cn_r4_k3_s1_e1_c32_skip'], ['er_r7_k3_s2_e4_c64'], ['er_r7_k3_s2_e4_c96'], ['ir_r10_k3_s2_e4_c192_se0.25'], ['ir_r19_k3_s1_e6_c224_se0.25'], ['ir_r25_k3_s2_e6_c384_se0.25'], ['ir_r7_k3_s1_e6_c640_se0.25']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier), num_features=1280, stem_size=32, round_chs_fn=partial(round_channels, multiplier=channel_multiplier), norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), act_layer=resolve_act_layer(kwargs, 'silu'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['cn_r4_k3_s1_e1_c32_skip'], ['er_r7_k3_s2_e4_c64'], ['er_r7_k3_s2_e4_c96'], ['ir_r10_k3_s2_e4_c192_se0.25'], ['ir_r19_k3_s1_e6_c224_se0.25'], ['ir_r25_k3_s2_e6_c384_se0.25'], ['ir_r7_k3_s1_e6_c640_se0.25']]
    model_kwargs = dict(block_args=resolve_act_layer(arch_def, depth_multiplier), num_features=1280, stem_size=32, round_chs_fn=partial(round_channels, multiplier=channel_multiplier), norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), act_layer=decode_arch_def(kwargs, 'silu'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"Rots, Rigids = Rigids, Rots
def rigids_from_list(l: List[jnp.ndarray]) -> Rigids:
    """"""Converts flat list of arrays to rigid transformations.""""""","["""""" 
    assert (len(l) == 12)
    return Rigids(Rots(*l[:9]), Vecs(*l[9:]))
"""""", """""" 
    assert (len(l) == 12)
    return Rots(Rigids(*l[:9]), Vecs(*l[9:]))
""""""]",1
"shorten_bytes, out = out, shorten_bytes
def draw_chunk_table(comp):
    """""" Outputs a table that compares the found memory chunks side-by-side
    in input file vs. memory """"""","["""""" 
    table = [('', '', '', '', 'File', 'Memory', 'Note')]
    delims = (' ', ' ', ' ', ' | ', ' | ', ' | ', '')
    last_unmodified = comp.get_last_unmodified_chunk()
    for c in comp.get_chunks():
        if (c.dy == 0):
            note = 'missing'
        elif (c.dx > c.dy):
            note = 'compacted'
        elif (c.dx < c.dy):
            note = 'expanded'
        elif c.unmodified:
            note = 'unmodified!'
        else:
            note = 'corrupted'
        table.append((c.i, c.j, c.dx, c.dy, shorten_bytes(c.xchunk), shorten_bytes(c.ychunk), note))
    sizes = tuple((max((len(str(c)) for c in col)) for col in zip(*table)))
    for (i, row) in enumerate(table):
        out(('\t' + ''.join(((str(x).ljust(size) + delim) for (x, size, delim) in zip(row, sizes, delims)))))
        if ((i == 0) or ((i == (last_unmodified + 1)) and (i < len(table)))):
            out(('\t' + ('-' * (sum(sizes) + sum((len(d) for d in delims))))))
"""""", """""" 
    table = [('', '', '', '', 'File', 'Memory', 'Note')]
    delims = (' ', ' ', ' ', ' | ', ' | ', ' | ', '')
    last_unmodified = comp.get_last_unmodified_chunk()
    for c in comp.get_chunks():
        if (c.dy == 0):
            note = 'missing'
        elif (c.dx > c.dy):
            note = 'compacted'
        elif (c.dx < c.dy):
            note = 'expanded'
        elif c.unmodified:
            note = 'unmodified!'
        else:
            note = 'corrupted'
        table.append((c.i, c.j, c.dx, c.dy, out(c.xchunk), out(c.ychunk), note))
    sizes = tuple((max((len(str(c)) for c in col)) for col in zip(*table)))
    for (i, row) in enumerate(table):
        shorten_bytes(('\t' + ''.join(((str(x).ljust(size) + delim) for (x, size, delim) in zip(row, sizes, delims)))))
        if ((i == 0) or ((i == (last_unmodified + 1)) and (i < len(table)))):
            shorten_bytes(('\t' + ('-' * (sum(sizes) + sum((len(d) for d in delims))))))
""""""]",1
"canonize_messages, _compile = _compile, canonize_messages
def render_message_list(title: str, messages: Sequence[Dict[(str, Any)]], annotations: Optional[conf.misc_cfgs.AnnotatedGame], show_add_test: bool=False, show_timestamps: bool=False) -> str:
    """"""Render into html a list of message from a diplomacy game.

    Parameters:
    title (str): Title for the list of messages
    messages: e.g. game.get_all_phases()[phase_id].messages.values()

    Returns: str.
    """"""","["""""" 
    template = '\n    <div class=""section"">\n      {% if title %}\n        <h3 class=""messagesTitle"">{{ title }}:</h3>\n      {% endif %}\n\n      <div class=""scrollBox"">\n        {% for msg in messages %}\n          <p class=""dip_message"" time_sent=""{{msg.time_sent}}"" sender=""{{msg.sender}}"" recipient=""{{msg.recipient}}"" title=""{{msg.time_sent}}"">\n            <span class=""messageLabel"">\n              <span class=""{{msg.sender}}"">{{msg.sender}}</span>\n                ->\n              <span class=""{{msg.recipient}}"">{{msg.recipient}}</span>\n            </span>\n            {% if show_time_sent %}({{msg.time_sent}}){% endif %}: {{msg.message}}\n            {% if show_add_test %} <a href=""#"" class=""addTest"">+test</a> {% endif %}\n            {% if msg.annotation %}\n              <br/><span class=""msg_annotation"">{{ msg.annotation|safe }}</span><br/>\n            {% endif %}\n          </p>\n\n        {% else %}\n          (No messages)\n        {% endfor %}\n      </div>\n    </div>\n    '
    messages = canonize_messages(messages, annotations)
    template = _compile(template)
    return template.render(title=title, messages=messages)
"""""", """""" 
    template = '\n    <div class=""section"">\n      {% if title %}\n        <h3 class=""messagesTitle"">{{ title }}:</h3>\n      {% endif %}\n\n      <div class=""scrollBox"">\n        {% for msg in messages %}\n          <p class=""dip_message"" time_sent=""{{msg.time_sent}}"" sender=""{{msg.sender}}"" recipient=""{{msg.recipient}}"" title=""{{msg.time_sent}}"">\n            <span class=""messageLabel"">\n              <span class=""{{msg.sender}}"">{{msg.sender}}</span>\n                ->\n              <span class=""{{msg.recipient}}"">{{msg.recipient}}</span>\n            </span>\n            {% if show_time_sent %}({{msg.time_sent}}){% endif %}: {{msg.message}}\n            {% if show_add_test %} <a href=""#"" class=""addTest"">+test</a> {% endif %}\n            {% if msg.annotation %}\n              <br/><span class=""msg_annotation"">{{ msg.annotation|safe }}</span><br/>\n            {% endif %}\n          </p>\n\n        {% else %}\n          (No messages)\n        {% endfor %}\n      </div>\n    </div>\n    '
    messages = _compile(messages, annotations)
    template = canonize_messages(template)
    return template.render(title=title, messages=messages)
""""""]",1
"get_dist_group, should_use_fsdp = should_use_fsdp, get_dist_group
@contextlib.contextmanager
def maybe_fsdp_wrap(opt):
    """"""
    Context manager for enabling wrapping in FullyShardedDataParallel.
    """"""","["""""" 
    if (not should_use_fsdp(opt)):
        (yield)
        return
    mixed_precision = (opt['fp16'] and (opt['fp16_impl'] == 'safe'))
    import warnings
    warnings.filterwarnings('ignore')
    sharding_strategy = (ShardingStrategy.FULL_SHARD if (opt['ddp_backend'] == 'zero3') else ShardingStrategy.SHARD_GRAD_OP)
    if mixed_precision:
        mp_strategy = MixedPrecision(reduce_dtype=torch.float16, param_dtype=torch.float16, buffer_dtype=torch.float16)
    else:
        mp_strategy = None
    auto_wrap_policy = None
    ignored_modules = None
    if ('hugging_face' not in opt['model']):
        from parlai.agents.transformer.modules.encoder import TransformerEncoderLayer
        from parlai.agents.transformer.modules.decoder import TransformerDecoderLayer
        auto_wrap_policy = functools.partial(transformer_auto_wrap_policy, transformer_layer_cls={TransformerEncoderLayer, TransformerDecoderLayer})
    backward_prefetch = BackwardPrefetch.BACKWARD_POST
    cpu_offload = None
    fsdp_args = dict(process_group=get_dist_group(), sharding_strategy=sharding_strategy, cpu_offload=cpu_offload, auto_wrap_policy=auto_wrap_policy, backward_prefetch=backward_prefetch, mixed_precision=mp_strategy, ignored_modules=ignored_modules, param_init_fn=None, device_id=opt['gpu'], sync_module_states=False, forward_prefetch=False, limit_all_gathers=False)
    with enable_wrap(wrapper_cls=FSDP, **fsdp_args):
        (yield)
"""""", """""" 
    if (not get_dist_group(opt)):
        (yield)
        return
    mixed_precision = (opt['fp16'] and (opt['fp16_impl'] == 'safe'))
    import warnings
    warnings.filterwarnings('ignore')
    sharding_strategy = (ShardingStrategy.FULL_SHARD if (opt['ddp_backend'] == 'zero3') else ShardingStrategy.SHARD_GRAD_OP)
    if mixed_precision:
        mp_strategy = MixedPrecision(reduce_dtype=torch.float16, param_dtype=torch.float16, buffer_dtype=torch.float16)
    else:
        mp_strategy = None
    auto_wrap_policy = None
    ignored_modules = None
    if ('hugging_face' not in opt['model']):
        from parlai.agents.transformer.modules.encoder import TransformerEncoderLayer
        from parlai.agents.transformer.modules.decoder import TransformerDecoderLayer
        auto_wrap_policy = functools.partial(transformer_auto_wrap_policy, transformer_layer_cls={TransformerEncoderLayer, TransformerDecoderLayer})
    backward_prefetch = BackwardPrefetch.BACKWARD_POST
    cpu_offload = None
    fsdp_args = dict(process_group=should_use_fsdp(), sharding_strategy=sharding_strategy, cpu_offload=cpu_offload, auto_wrap_policy=auto_wrap_policy, backward_prefetch=backward_prefetch, mixed_precision=mp_strategy, ignored_modules=ignored_modules, param_init_fn=None, device_id=opt['gpu'], sync_module_states=False, forward_prefetch=False, limit_all_gathers=False)
    with enable_wrap(wrapper_cls=FSDP, **fsdp_args):
        (yield)
""""""]",1
"defaultdict, compare_hashes_from_commit = compare_hashes_from_commit, defaultdict
@ndb.tasklet
def determine_version(version_query: osv_service_v1_pb2.VersionQuery, context: grpc.ServicerContext) -> ndb.Future:
    """"""Identify fitting commits based on a subset of hashes""""""","["""""" 
    if (len(version_query.file_hashes) <= _MAX_HASHES_TO_TRY):
        hashes = [f.hash for f in version_query.file_hashes[:min(_MAX_HASHES_TO_TRY, len(version_query.file_hashes))]]
    else:
        hashes = [f.hash for f in random.sample(version_query.file_hashes, _MAX_HASHES_TO_TRY)]
    tracker = defaultdict(int)
    hash_futures = []
    for h in hashes:
        query = osv.RepoIndexResult.query((osv.RepoIndexResult.file_results.hash == h))
        query.keys_only = True
        hash_futures.append(query.fetch_async())
    for f in hash_futures:
        for r in f.result():
            tracker[r.key.parent()] += 1
    idx_keys = []
    for (k, v) in tracker.items():
        if (v == _MAX_HASHES_TO_TRY):
            idx_keys.append(k)
    if (not idx_keys):
        idx_keys = [k for (k, _) in sorted(tracker.items(), key=(lambda item: item[1]), reverse=True)]
        idx_keys = idx_keys[:min(_MAX_COMMITS_TO_TRY, len(idx_keys))]
    if (len(idx_keys) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    idx_futures = ndb.get_multi_async(idx_keys)
    match_futures = []
    for f in idx_futures:
        idx = f.result()
        if (version_query.name not in ('', idx.name)):
            continue
        match = compare_hashes_from_commit(idx, version_query.file_hashes)
        match_futures.append(match)
    results = []
    for f in match_futures:
        match = f.result()
        if (match.score != 0.0):
            results.append(match)
    if (len(results) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    return osv_service_v1_pb2.VersionMatchList(matches=results)
"""""", """""" 
    if (len(version_query.file_hashes) <= _MAX_HASHES_TO_TRY):
        hashes = [f.hash for f in version_query.file_hashes[:min(_MAX_HASHES_TO_TRY, len(version_query.file_hashes))]]
    else:
        hashes = [f.hash for f in random.sample(version_query.file_hashes, _MAX_HASHES_TO_TRY)]
    tracker = compare_hashes_from_commit(int)
    hash_futures = []
    for h in hashes:
        query = osv.RepoIndexResult.query((osv.RepoIndexResult.file_results.hash == h))
        query.keys_only = True
        hash_futures.append(query.fetch_async())
    for f in hash_futures:
        for r in f.result():
            tracker[r.key.parent()] += 1
    idx_keys = []
    for (k, v) in tracker.items():
        if (v == _MAX_HASHES_TO_TRY):
            idx_keys.append(k)
    if (not idx_keys):
        idx_keys = [k for (k, _) in sorted(tracker.items(), key=(lambda item: item[1]), reverse=True)]
        idx_keys = idx_keys[:min(_MAX_COMMITS_TO_TRY, len(idx_keys))]
    if (len(idx_keys) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    idx_futures = ndb.get_multi_async(idx_keys)
    match_futures = []
    for f in idx_futures:
        idx = f.result()
        if (version_query.name not in ('', idx.name)):
            continue
        match = defaultdict(idx, version_query.file_hashes)
        match_futures.append(match)
    results = []
    for f in match_futures:
        match = f.result()
        if (match.score != 0.0):
            results.append(match)
    if (len(results) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    return osv_service_v1_pb2.VersionMatchList(matches=results)
""""""]",1
"build_tree, bagging_predict = bagging_predict, build_tree
def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):
    """"""random_forest(评估算法性能，返回模型得分)

    Args:
        train           训练数据集
        test            测试数据集
        max_depth       决策树深度不能太深，不然容易导致过拟合
        min_size        叶子节点的大小
        sample_size     训练数据集的样本比例
        n_trees         决策树的个数
        n_features      选取的特征的个数
    Returns:
        predictions     每一行的预测结果，bagging 预测最后的分类结果
    """"""","["""""" 
    trees = list()
    for i in range(n_trees):
        sample = subsample(train, sample_size)
        tree = build_tree(sample, max_depth, min_size, n_features)
        trees.append(tree)
    predictions = [bagging_predict(trees, row) for row in test]
    return predictions
"""""", """""" 
    trees = list()
    for i in range(n_trees):
        sample = subsample(train, sample_size)
        tree = bagging_predict(sample, max_depth, min_size, n_features)
        trees.append(tree)
    predictions = [build_tree(trees, row) for row in test]
    return predictions
""""""]",1
"AndOperator, FunctionOperator = FunctionOperator, AndOperator
def _build_query(env, start, end, certname=None):
    """"""Build a extract query with optional certname and environment.""""""","["""""" 
    query = ExtractOperator()
    query.add_field(FunctionOperator('count'))
    query.add_field('status')
    subquery = AndOperator()
    subquery.add(GreaterEqualOperator('producer_timestamp', start))
    subquery.add(LessOperator('producer_timestamp', end))
    if (certname is not None):
        subquery.add(EqualsOperator('certname', certname))
    if (env != '*'):
        subquery.add(EqualsOperator('environment', env))
    query.add_query(subquery)
    query.add_group_by('status')
    return query
"""""", """""" 
    query = ExtractOperator()
    query.add_field(AndOperator('count'))
    query.add_field('status')
    subquery = FunctionOperator()
    subquery.add(GreaterEqualOperator('producer_timestamp', start))
    subquery.add(LessOperator('producer_timestamp', end))
    if (certname is not None):
        subquery.add(EqualsOperator('certname', certname))
    if (env != '*'):
        subquery.add(EqualsOperator('environment', env))
    query.add_query(subquery)
    query.add_group_by('status')
    return query
""""""]",1
"Reference, Inference = Inference, Reference
def test_default_improver_with_empty_affected_packages():
    """"""
    Test that DefaultImprover.get_inferences() returns an empty list when given an advisory with no affected packages
    """"""","["""""" 
    advisory_data = AdvisoryData(aliases=['CVE-2020-1234'], summary='Test summary', references=[Reference(url='https://www.example.com/')], affected_packages=[])
    default_improver = DefaultImprover()
    expected_inference = Inference(vulnerability_id=None, aliases=['CVE-2020-1234'], confidence=100, summary='Test summary', affected_purls=[], fixed_purl=None, references=[Reference(reference_id='', url='https://www.example.com/', severities=[])])
    expected = [expected_inference]
    result = list(default_improver.get_inferences(advisory_data))
    assert (result == expected)
"""""", """""" 
    advisory_data = AdvisoryData(aliases=['CVE-2020-1234'], summary='Test summary', references=[Inference(url='https://www.example.com/')], affected_packages=[])
    default_improver = DefaultImprover()
    expected_inference = Reference(vulnerability_id=None, aliases=['CVE-2020-1234'], confidence=100, summary='Test summary', affected_purls=[], fixed_purl=None, references=[Inference(reference_id='', url='https://www.example.com/', severities=[])])
    expected = [expected_inference]
    result = list(default_improver.get_inferences(advisory_data))
    assert (result == expected)
""""""]",1
"use_a_dict, Environment = Environment, use_a_dict
def Func01(t):
    """"""use_a_dict""""""","["""""" 
    env = Environment(t)
    for i in IterationList:
        use_a_dict(env, node, None)
"""""", """""" 
    env = use_a_dict(t)
    for i in IterationList:
        Environment(env, node, None)
""""""]",1
"invoke_make, invoke_configure = invoke_configure, invoke_make
def build_targets(binutils_folder, build, install_folder, targets, host_arch):
    """"""
    Builds binutils for all specified targets
    :param binutils_folder: Binutils source folder
    :param build: Build directory
    :param install_folder: Directory to install binutils to
    :param targets: Targets to compile binutils for
    :param host_arch: Host architecture to optimize for
    :return:
    """"""","["""""" 
    for target in targets:
        build_folder = build.joinpath(target)
        cleanup(build_folder)
        invoke_configure(binutils_folder, build_folder, install_folder, target, host_arch)
        invoke_make(build_folder, install_folder, target)
"""""", """""" 
    for target in targets:
        build_folder = build.joinpath(target)
        cleanup(build_folder)
        invoke_make(binutils_folder, build_folder, install_folder, target, host_arch)
        invoke_configure(build_folder, install_folder, target)
""""""]",1
"render_pep440_pre, render_pep440_old = render_pep440_old, render_pep440_pre
def render(pieces, style):
    """"""Render the given version pieces into the requested style.""""""","["""""" 
    if pieces['error']:
        return {'version': 'unknown', 'full-revisionid': pieces.get('long'), 'dirty': None, 'error': pieces['error'], 'date': None}
    if ((not style) or (style == 'default')):
        style = 'pep440'
    if (style == 'pep440'):
        rendered = render_pep440(pieces)
    elif (style == 'pep440-pre'):
        rendered = render_pep440_pre(pieces)
    elif (style == 'pep440-post'):
        rendered = render_pep440_post(pieces)
    elif (style == 'pep440-old'):
        rendered = render_pep440_old(pieces)
    elif (style == 'git-describe'):
        rendered = render_git_describe(pieces)
    elif (style == 'git-describe-long'):
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError((""unknown style '%s'"" % style))
    return {'version': rendered, 'full-revisionid': pieces['long'], 'dirty': pieces['dirty'], 'error': None, 'date': pieces.get('date')}
"""""", """""" 
    if pieces['error']:
        return {'version': 'unknown', 'full-revisionid': pieces.get('long'), 'dirty': None, 'error': pieces['error'], 'date': None}
    if ((not style) or (style == 'default')):
        style = 'pep440'
    if (style == 'pep440'):
        rendered = render_pep440(pieces)
    elif (style == 'pep440-pre'):
        rendered = render_pep440_old(pieces)
    elif (style == 'pep440-post'):
        rendered = render_pep440_post(pieces)
    elif (style == 'pep440-old'):
        rendered = render_pep440_pre(pieces)
    elif (style == 'git-describe'):
        rendered = render_git_describe(pieces)
    elif (style == 'git-describe-long'):
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError((""unknown style '%s'"" % style))
    return {'version': rendered, 'full-revisionid': pieces['long'], 'dirty': pieces['dirty'], 'error': None, 'date': pieces.get('date')}
""""""]",1
"findBalanced, makeInternalLink = makeInternalLink, findBalanced
def replaceInternalLinks(text):
    """"""
    Replaces internal links of the form:
    [[title |...|label]]trail

    with title concatenated with trail, when present, e.g. 's' for plural.

    See https://www.mediawiki.org/wiki/Help:Links#Internal_links
    """"""","["""""" 
    cur = 0
    res = ''
    for (s, e) in findBalanced(text):
        m = tailRE.match(text, e)
        if m:
            trail = m.group(0)
            end = m.end()
        else:
            trail = ''
            end = e
        inner = text[(s + 2):(e - 2)]
        pipe = inner.find('|')
        if (pipe < 0):
            title = inner
            label = title
        else:
            title = inner[:pipe].rstrip()
            curp = (pipe + 1)
            for (s1, e1) in findBalanced(inner):
                last = inner.rfind('|', curp, s1)
                if (last >= 0):
                    pipe = last
                curp = e1
            label = inner[(pipe + 1):].strip()
        res += ((text[cur:s] + makeInternalLink(title, label)) + trail)
        cur = end
    return (res + text[cur:])
"""""", """""" 
    cur = 0
    res = ''
    for (s, e) in makeInternalLink(text):
        m = tailRE.match(text, e)
        if m:
            trail = m.group(0)
            end = m.end()
        else:
            trail = ''
            end = e
        inner = text[(s + 2):(e - 2)]
        pipe = inner.find('|')
        if (pipe < 0):
            title = inner
            label = title
        else:
            title = inner[:pipe].rstrip()
            curp = (pipe + 1)
            for (s1, e1) in makeInternalLink(inner):
                last = inner.rfind('|', curp, s1)
                if (last >= 0):
                    pipe = last
                curp = e1
            label = inner[(pipe + 1):].strip()
        res += ((text[cur:s] + findBalanced(title, label)) + trail)
        cur = end
    return (res + text[cur:])
""""""]",1
"_run_wait_hook, _screenshot_on_step = _screenshot_on_step, _run_wait_hook
def assert_element_text_is_not(element, text):
    """"""Assert the text of the element is not `text`

    Parameters:
    element : element
    text : value
    """"""","["""""" 
    element = browser.get_browser().find(element, timeout=0)
    _add_step(f""Assert element {element.name} text is not '{text}'"")
    _run_wait_hook()
    msg = f""expected element {element.name} text to not be '{text}'""
    assert (element.text != text), msg
    _screenshot_on_step()
"""""", """""" 
    element = browser.get_browser().find(element, timeout=0)
    _add_step(f""Assert element {element.name} text is not '{text}'"")
    _screenshot_on_step()
    msg = f""expected element {element.name} text to not be '{text}'""
    assert (element.text != text), msg
    _run_wait_hook()
""""""]",1
"readLength, readUniversalTag = readUniversalTag, readLength
def readOctetString(s: BinaryIO) -> bytes:
    """"""
    Unpack a BER octet string
    :param s: stream
    """"""","["""""" 
    if (not readUniversalTag(s, Tag.BER_TAG_OCTET_STRING, False)):
        raise ValueError('Bad octet string tag')
    size = readLength(s)
    return s.read(size)
"""""", """""" 
    if (not readLength(s, Tag.BER_TAG_OCTET_STRING, False)):
        raise ValueError('Bad octet string tag')
    size = readUniversalTag(s)
    return s.read(size)
""""""]",1
"build_swintransformer_backbone, BiFPN = BiFPN, build_swintransformer_backbone
@BACKBONE_REGISTRY.register()
def build_swintransformer_bifpn_backbone(cfg, input_shape: ShapeSpec):
    """"""
    """"""","["""""" 
    bottom_up = build_swintransformer_backbone(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    backbone = BiFPN(cfg=cfg, bottom_up=bottom_up, in_features=in_features, out_channels=cfg.MODEL.BIFPN.OUT_CHANNELS, norm=cfg.MODEL.BIFPN.NORM, num_levels=cfg.MODEL.BIFPN.NUM_LEVELS, num_bifpn=cfg.MODEL.BIFPN.NUM_BIFPN, separable_conv=cfg.MODEL.BIFPN.SEPARABLE_CONV)
    return backbone
"""""", """""" 
    bottom_up = BiFPN(cfg, input_shape)
    in_features = cfg.MODEL.FPN.IN_FEATURES
    backbone = build_swintransformer_backbone(cfg=cfg, bottom_up=bottom_up, in_features=in_features, out_channels=cfg.MODEL.BIFPN.OUT_CHANNELS, norm=cfg.MODEL.BIFPN.NORM, num_levels=cfg.MODEL.BIFPN.NUM_LEVELS, num_bifpn=cfg.MODEL.BIFPN.NUM_BIFPN, separable_conv=cfg.MODEL.BIFPN.SEPARABLE_CONV)
    return backbone
""""""]",1
"contains_spdx_info, _indices_of_newlines = _indices_of_newlines, contains_spdx_info
def _find_first_spdx_comment(text: str, style: CommentStyle=None) -> _TextSections:
    """"""Find the first SPDX comment in the file. Return a tuple with everything
    preceding the comment, the comment itself, and everything following it.

    :raises MissingSpdxInfo: if no SPDX info can be found in any comment
    """"""","["""""" 
    if (style is None):
        style = PythonCommentStyle
    indices = _indices_of_newlines(text)
    for index in indices:
        try:
            comment = style.comment_at_first_character(text[index:])
        except CommentParseError:
            continue
        if contains_spdx_info(comment):
            return _TextSections(text[:index], (comment + '\n'), text[((index + len(comment)) + 1):])
    raise MissingSpdxInfo()
"""""", """""" 
    if (style is None):
        style = PythonCommentStyle
    indices = contains_spdx_info(text)
    for index in indices:
        try:
            comment = style.comment_at_first_character(text[index:])
        except CommentParseError:
            continue
        if _indices_of_newlines(comment):
            return _TextSections(text[:index], (comment + '\n'), text[((index + len(comment)) + 1):])
    raise MissingSpdxInfo()
""""""]",1
"list_models, _get_input_size = _get_input_size, list_models
@pytest.mark.timeout(120)
@pytest.mark.parametrize('model_name', list_models(exclude_filters=EXCLUDE_FILTERS))
@pytest.mark.parametrize('batch_size', [1])
def test_model_forward(model_name, batch_size):
    """"""Run a single forward pass with each model""""""","["""""" 
    model = create_model(model_name, pretrained=False)
    model.eval()
    input_size = _get_input_size(model=model, target=TARGET_FWD_SIZE)
    if (max(input_size) > MAX_FWD_SIZE):
        pytest.skip('Fixed input size model > limit.')
    inputs = torch.randn((batch_size, *input_size))
    outputs = model(inputs)
    assert (outputs.shape[0] == batch_size)
    assert (not torch.isnan(outputs).any()), 'Output included NaNs'
"""""", """""" 
    model = create_model(model_name, pretrained=False)
    model.eval()
    input_size = list_models(model=model, target=TARGET_FWD_SIZE)
    if (max(input_size) > MAX_FWD_SIZE):
        pytest.skip('Fixed input size model > limit.')
    inputs = torch.randn((batch_size, *input_size))
    outputs = model(inputs)
    assert (outputs.shape[0] == batch_size)
    assert (not torch.isnan(outputs).any()), 'Output included NaNs'
""""""]",1
"_interpret_contents, _find_regular_images = _find_regular_images, _interpret_contents
def _process_content_streams(*, pdf: Pdf, container: Object, shorthand=None) -> Iterator[((VectorMarker | TextMarker) | ImageInfo)]:
    """"""Find all individual instances of images drawn in the container

    Usually the container is a page, but it may also be a Form XObject.

    On a typical page images are stored inline or as regular images
    in an XObject.

    Form XObjects may include inline images, XObject images,
    and recursively, other Form XObjects; and also vector graphic objects.

    Every instance of an image being drawn somewhere is flattened and
    treated as a unique image, since if the same image is drawn multiple times
    on one page it may be drawn at differing resolutions, and our objective
    is to find the resolution at which the page can be rastered without
    downsampling.

    """"""","["""""" 
    if ((container.get('/Type') == '/Page') and ('/Contents' in container)):
        initial_shorthand = (shorthand or UNIT_SQUARE)
    elif ((container.get('/Type') == '/XObject') and (container['/Subtype'] == '/Form')):
        ctm = (PdfMatrix(shorthand) if shorthand else PdfMatrix.identity())
        form_shorthand = container.get('/Matrix', PdfMatrix.identity())
        form_matrix = PdfMatrix(form_shorthand)
        ctm = (form_matrix @ ctm)
        initial_shorthand = ctm.shorthand
    else:
        return
    contentsinfo = _interpret_contents(container, initial_shorthand)
    if contentsinfo.found_vector:
        (yield VectorMarker())
    if contentsinfo.found_text:
        (yield TextMarker())
    (yield from _find_inline_images(contentsinfo))
    (yield from _find_regular_images(container, contentsinfo))
    (yield from _find_form_xobject_images(pdf, container, contentsinfo))
"""""", """""" 
    if ((container.get('/Type') == '/Page') and ('/Contents' in container)):
        initial_shorthand = (shorthand or UNIT_SQUARE)
    elif ((container.get('/Type') == '/XObject') and (container['/Subtype'] == '/Form')):
        ctm = (PdfMatrix(shorthand) if shorthand else PdfMatrix.identity())
        form_shorthand = container.get('/Matrix', PdfMatrix.identity())
        form_matrix = PdfMatrix(form_shorthand)
        ctm = (form_matrix @ ctm)
        initial_shorthand = ctm.shorthand
    else:
        return
    contentsinfo = _find_regular_images(container, initial_shorthand)
    if contentsinfo.found_vector:
        (yield VectorMarker())
    if contentsinfo.found_text:
        (yield TextMarker())
    (yield from _find_inline_images(contentsinfo))
    (yield from _interpret_contents(container, contentsinfo))
    (yield from _find_form_xobject_images(pdf, container, contentsinfo))
""""""]",1
"save_img, initialize_direction = initialize_direction, save_img
def deep_patch_match(feat1, feat2, feat1_, feat2_, img_shape, psize=2, iteration=5, alpha=0.5):
    """"""
    A deep patch match method based on two pairs data. Formulated in Deep Image Analogy
    Original version only use img1 and img2
    Params: img1(torch.Tensor):  shape C*H*W
    """"""","["""""" 
    assert (feat1.size() == feat2.size() == feat1_.size() == feat2_.size())
    eff_shape = get_effective_shape(img_shape, psize)
    (feat1, feat2, feat1_, feat2_) = (feat1.to(device), feat2.to(device), feat1_.to(device), feat2_.to(device))
    f = torch.zeros(*(img_shape + (2,)), device=device, dtype=torch.int32)
    dist_f = torch.zeros(*img_shape, device=device)
    for y in range(eff_shape[0]):
        for x in range(eff_shape[1]):
            pos = (y, x)
            pos_f = (np.random.randint(0, eff_shape[0]), np.random.randint(0, eff_shape[1]))
            f[(y, x)] = torch.tensor(pos_f, device=device).type(torch.LongTensor)
            dist_f[(y, x)] = feat_patch_distance(feat1, feat2, feat1_, feat2_, pos, pos_f, psize)
    for i in range(iteration):
        print('Iteration {}: Running'.format((i + 1)))
        (change, start, end) = initialize_direction(i, eff_shape)
        print('start:{}, end:{}, change:{}'.format(start, end, change))
        ori_time = end_time = time.time()
        for y in range(int(start[0]), int(end[0]), int(change[0])):
            for x in range(int(start[1]), int(end[1]), int(change[1])):
                pos = (y, x)
                (best_pos_f, best_dist) = propagation(pos, change, f, dist_f, feat1, feat2, feat1_, feat2_, eff_shape, psize)
                (best_pos_f, best_dist) = random_search(pos, f, dist_f, best_pos_f, best_dist, feat1, feat2, feat1_, feat2_, eff_shape, psize=psize)
                f[(y, x)] = torch.tensor(best_pos_f, device=device, dtype=torch.int32)
                dist_f[(y, x)] = best_dist
        re_img1 = reconstruct_avg(feat2, f, psize=PSIZE)
        save_img(re_img1, 'epoch_{}_re_test.png'.format(i))
        print('Iteration {}: Finishing Time : {}'.format((i + 1), (time.time() - ori_time)))
    return f
"""""", """""" 
    assert (feat1.size() == feat2.size() == feat1_.size() == feat2_.size())
    eff_shape = get_effective_shape(img_shape, psize)
    (feat1, feat2, feat1_, feat2_) = (feat1.to(device), feat2.to(device), feat1_.to(device), feat2_.to(device))
    f = torch.zeros(*(img_shape + (2,)), device=device, dtype=torch.int32)
    dist_f = torch.zeros(*img_shape, device=device)
    for y in range(eff_shape[0]):
        for x in range(eff_shape[1]):
            pos = (y, x)
            pos_f = (np.random.randint(0, eff_shape[0]), np.random.randint(0, eff_shape[1]))
            f[(y, x)] = torch.tensor(pos_f, device=device).type(torch.LongTensor)
            dist_f[(y, x)] = feat_patch_distance(feat1, feat2, feat1_, feat2_, pos, pos_f, psize)
    for i in range(iteration):
        print('Iteration {}: Running'.format((i + 1)))
        (change, start, end) = save_img(i, eff_shape)
        print('start:{}, end:{}, change:{}'.format(start, end, change))
        ori_time = end_time = time.time()
        for y in range(int(start[0]), int(end[0]), int(change[0])):
            for x in range(int(start[1]), int(end[1]), int(change[1])):
                pos = (y, x)
                (best_pos_f, best_dist) = propagation(pos, change, f, dist_f, feat1, feat2, feat1_, feat2_, eff_shape, psize)
                (best_pos_f, best_dist) = random_search(pos, f, dist_f, best_pos_f, best_dist, feat1, feat2, feat1_, feat2_, eff_shape, psize=psize)
                f[(y, x)] = torch.tensor(best_pos_f, device=device, dtype=torch.int32)
                dist_f[(y, x)] = best_dist
        re_img1 = reconstruct_avg(feat2, f, psize=PSIZE)
        initialize_direction(re_img1, 'epoch_{}_re_test.png'.format(i))
        print('Iteration {}: Finishing Time : {}'.format((i + 1), (time.time() - ori_time)))
    return f
""""""]",1
"_concatenate_coords, get_sequence_loss = get_sequence_loss, _concatenate_coords
def score_sequence_in_complex(model, alphabet, coords, target_chain_id, target_seq, padding_length=10):
    """"""
    Scores sequence for one chain in a complex.
    Args:
        model: An instance of the GVPTransformer model
        alphabet: Alphabet for the model
        coords: Dictionary mapping chain ids to L x 3 x 3 array for N, CA, C
            coordinates representing the backbone of each chain
        target_chain_id: The chain id to sample sequences for
        target_seq: Target sequence for the target chain for scoring.
        padding_length: padding length in between chains
    Returns:
        Tuple (ll_fullseq, ll_withcoord)
        - ll_fullseq: Average log-likelihood over the full target chain
        - ll_withcoord: Average log-likelihood in target chain excluding those
            residues without coordinates
    """"""","["""""" 
    all_coords = _concatenate_coords(coords, target_chain_id)
    (loss, target_padding_mask) = get_sequence_loss(model, alphabet, all_coords, target_seq)
    ll_fullseq = ((- np.sum((loss * (~ target_padding_mask)))) / np.sum((~ target_padding_mask)))
    coord_mask = np.all(np.isfinite(coords[target_chain_id]), axis=((- 1), (- 2)))
    ll_withcoord = ((- np.sum((loss * coord_mask))) / np.sum(coord_mask))
    return (ll_fullseq, ll_withcoord)
"""""", """""" 
    all_coords = get_sequence_loss(coords, target_chain_id)
    (loss, target_padding_mask) = _concatenate_coords(model, alphabet, all_coords, target_seq)
    ll_fullseq = ((- np.sum((loss * (~ target_padding_mask)))) / np.sum((~ target_padding_mask)))
    coord_mask = np.all(np.isfinite(coords[target_chain_id]), axis=((- 1), (- 2)))
    ll_withcoord = ((- np.sum((loss * coord_mask))) / np.sum(coord_mask))
    return (ll_fullseq, ll_withcoord)
""""""]",1
"parse_version, extract_version_from_webdriver_filename = extract_version_from_webdriver_filename, parse_version
def match_latest_executable_path(glob_path, testdir):
    """"""Returns the absolute path to the webdriver executable
    with the highest version given a path with glob pattern.
    """"""","["""""" 
    found_files = []
    glob_path = os.path.normpath(glob_path)
    if (not os.path.isabs(glob_path)):
        glob_path = os.path.join(testdir, glob_path)
    matched_files = [x for x in glob.glob(glob_path, recursive=True) if os.path.isfile(x)]
    for matched_file in matched_files:
        found_files.append((matched_file, extract_version_from_webdriver_filename(matched_file)))
    if found_files:
        highest_version = sorted(found_files, key=(lambda tup: parse_version(tup[1])), reverse=True)
        return highest_version[0][0]
    else:
        return None
"""""", """""" 
    found_files = []
    glob_path = os.path.normpath(glob_path)
    if (not os.path.isabs(glob_path)):
        glob_path = os.path.join(testdir, glob_path)
    matched_files = [x for x in glob.glob(glob_path, recursive=True) if os.path.isfile(x)]
    for matched_file in matched_files:
        found_files.append((matched_file, parse_version(matched_file)))
    if found_files:
        highest_version = sorted(found_files, key=(lambda tup: extract_version_from_webdriver_filename(tup[1])), reverse=True)
        return highest_version[0][0]
    else:
        return None
""""""]",1
"compute_pro, trapezoid = trapezoid, compute_pro
def calculate_au_pro(gts, predictions, integration_limit=0.3, num_thresholds=100):
    """"""
    Compute the area under the PRO curve for a set of ground truth images and corresponding anomaly images.
    Args:
        gts:         List of tensors that contain the ground truth images for a single dataset object.
        predictions: List of tensors containing anomaly images for each ground truth image.
        integration_limit:    Integration limit to use when computing the area under the PRO curve.
        num_thresholds:       Number of thresholds to use to sample the area under the PRO curve.
    Returns:
        au_pro:    Area under the PRO curve computed up to the given integration limit.
        pro_curve: PRO curve values for localization (fpr,pro).
    """"""","["""""" 
    pro_curve = compute_pro(anomaly_maps=predictions, ground_truth_maps=gts, num_thresholds=num_thresholds)
    au_pro = trapezoid(pro_curve[0], pro_curve[1], x_max=integration_limit)
    au_pro /= integration_limit
    return (au_pro, pro_curve)
"""""", """""" 
    pro_curve = trapezoid(anomaly_maps=predictions, ground_truth_maps=gts, num_thresholds=num_thresholds)
    au_pro = compute_pro(pro_curve[0], pro_curve[1], x_max=integration_limit)
    au_pro /= integration_limit
    return (au_pro, pro_curve)
""""""]",1
"Param, Path = Path, Param
@call_parse
def main(repos_path: Param('Path to the csv containing all of the repos', str)):
    """"""
    Use pandas dataframe from the repos path to open issues in each of them.
    """"""","["""""" 
    repos_path = Path(repos_path)
    df = pd.read_csv(repos_path)
    licenses = []
    for (_, row) in df.iterrows():
        (owner, repo) = row['name'].split('/')
        licenses.append(get_license_info(owner, repo))
    df['license'] = licenses
    df.to_csv((repos_path.parent / f'{repos_path.stem}_with_license.csv'), index=False)
"""""", """""" 
    repos_path = Param(repos_path)
    df = pd.read_csv(repos_path)
    licenses = []
    for (_, row) in df.iterrows():
        (owner, repo) = row['name'].split('/')
        licenses.append(get_license_info(owner, repo))
    df['license'] = licenses
    df.to_csv((repos_path.parent / f'{repos_path.stem}_with_license.csv'), index=False)
""""""]",1
"func, wraps = wraps, func
def project_exists(func):
    """"""A wrapper that checks if the requested project exists.
      * The annotated function must have a `project` argument.
    """"""","["""""" 

    @wraps(func)
    def wrapper(*args, **kwargs):
        if (not test_directory.project_exists(kwargs['project'])):
            abort(404, f""The project {kwargs['project']} does not exist."")
        return func(*args, **kwargs)
    return wrapper
"""""", """""" 

    @func(wraps)
    def wrapper(*args, **kwargs):
        if (not test_directory.project_exists(kwargs['project'])):
            abort(404, f""The project {kwargs['project']} does not exist."")
        return wraps(*args, **kwargs)
    return wrapper
""""""]",1
"Run, TextReporter = TextReporter, Run
@pytest.mark.timeout(60)
def test_issue_5724() -> None:
    """"""Regression test for parsing of pylint disable pragma's.""""""","["""""" 
    with pytest.raises(SystemExit) as cm:
        Run([os.path.join(REGR_DATA, 'issue_5724.py'), '--enable=missing-final-newline', '--disable=C'], reporter=TextReporter())
    assert (cm.value.code == 0)
"""""", """""" 
    with pytest.raises(SystemExit) as cm:
        TextReporter([os.path.join(REGR_DATA, 'issue_5724.py'), '--enable=missing-final-newline', '--disable=C'], reporter=Run())
    assert (cm.value.code == 0)
""""""]",1
"suppress, remove_all_log_handlers = remove_all_log_handlers, suppress
def process_init(q: Queue, user_init: UserInit, loglevel) -> None:
    """"""Initialize a process pool worker""""""","["""""" 
    signal.signal(signal.SIGINT, signal.SIG_IGN)
    with suppress(AttributeError):
        signal.signal(signal.SIGBUS, process_sigbus)
    root = logging.getLogger()
    remove_all_log_handlers(root)
    root.setLevel(loglevel)
    root.addHandler(logging.handlers.QueueHandler(q))
    user_init()
    return
"""""", """""" 
    signal.signal(signal.SIGINT, signal.SIG_IGN)
    with remove_all_log_handlers(AttributeError):
        signal.signal(signal.SIGBUS, process_sigbus)
    root = logging.getLogger()
    suppress(root)
    root.setLevel(loglevel)
    root.addHandler(logging.handlers.QueueHandler(q))
    user_init()
    return
""""""]",1
"is_String, flatten = flatten, is_String
def _subst_paths(env, paths) -> list:
    """"""Return a list of substituted path elements.

    If *paths* is a string, it is split on the search-path separator.
    Otherwise, substitution is done on string-valued list elements but
    they are not split.

    Note helps support behavior like pulling in the external ``CLASSPATH``
    and setting it directly into ``JAVACLASSPATH``, however splitting on
    ``os.pathsep`` makes the interpretation system-specific (this is
    warned about in the manpage entry for ``JAVACLASSPATH``).
    """"""","["""""" 
    if is_String(paths):
        paths = env.subst(paths)
        if SCons.Util.is_String(paths):
            paths = paths.split(os.pathsep)
    else:
        paths = flatten(paths)
        paths = [(env.subst(path) if is_String(path) else path) for path in paths]
    return paths
"""""", """""" 
    if flatten(paths):
        paths = env.subst(paths)
        if SCons.Util.is_String(paths):
            paths = paths.split(os.pathsep)
    else:
        paths = is_String(paths)
        paths = [(env.subst(path) if flatten(path) else path) for path in paths]
    return paths
""""""]",1
"get_tags_list, delete_tag = delete_tag, get_tags_list
def delete_images_for_experiment(exp_name: str):
    """"""
    Deletes image related to experiment with a given name.
    :param exp_name: name of an experiment for which image should be removed
    In case of any problems it raises an error
    """"""","["""""" 
    with K8sProxy(NAUTAAppNames.DOCKER_REGISTRY) as proxy:
        server_address = f'127.0.0.1:{proxy.tunnel_port}'
        list_of_tags = get_tags_list(server_address=server_address, image_name=exp_name)
        for tag in list_of_tags:
            delete_tag(server_address=server_address, image_name=exp_name, tag=tag)
"""""", """""" 
    with K8sProxy(NAUTAAppNames.DOCKER_REGISTRY) as proxy:
        server_address = f'127.0.0.1:{proxy.tunnel_port}'
        list_of_tags = delete_tag(server_address=server_address, image_name=exp_name)
        for tag in list_of_tags:
            get_tags_list(server_address=server_address, image_name=exp_name, tag=tag)
""""""]",1
"BasicFastPathParser, FIPSFastPathParser = FIPSFastPathParser, BasicFastPathParser
def createFastPathParser(tls: bool, encryptionMethod: EncryptionMethod, crypter: Union[(RC4Crypter, RC4CrypterProxy)], mode: ParserMode) -> Union[(BasicFastPathParser, SignedFastPathParser, FIPSFastPathParser)]:
    """"""
    Create a fast-path parser based on which encryption method is used.
    :param tls: whether TLS is used or not.
    :param encryptionMethod: the encryption method.
    :param crypter: the crypter for this connection.
    :param mode: the fast-path parser mode.
    """"""","["""""" 
    if tls:
        return BasicFastPathParser(mode)
    elif (encryptionMethod in [EncryptionMethod.ENCRYPTION_40BIT, EncryptionMethod.ENCRYPTION_56BIT, EncryptionMethod.ENCRYPTION_128BIT]):
        return SignedFastPathParser(crypter, mode)
    elif (encryptionMethod == EncryptionMethod.ENCRYPTION_FIPS):
        return FIPSFastPathParser(crypter, mode)
    else:
        raise ValueError('Invalid fast-path layer mode')
"""""", """""" 
    if tls:
        return FIPSFastPathParser(mode)
    elif (encryptionMethod in [EncryptionMethod.ENCRYPTION_40BIT, EncryptionMethod.ENCRYPTION_56BIT, EncryptionMethod.ENCRYPTION_128BIT]):
        return SignedFastPathParser(crypter, mode)
    elif (encryptionMethod == EncryptionMethod.ENCRYPTION_FIPS):
        return BasicFastPathParser(crypter, mode)
    else:
        raise ValueError('Invalid fast-path layer mode')
""""""]",1
"between_residue_bond_loss, within_residue_violations = within_residue_violations, between_residue_bond_loss
def find_structural_violations(batch: Dict[(str, torch.Tensor)], atom14_pred_positions: torch.Tensor, violation_tolerance_factor: float, clash_overlap_tolerance: float, **kwargs) -> Dict[(str, torch.Tensor)]:
    """"""Computes several checks for structural violations.""""""","["""""" 
    connection_violations = between_residue_bond_loss(pred_atom_positions=atom14_pred_positions, pred_atom_mask=batch['atom14_atom_exists'], residue_index=batch['residue_index'], aatype=batch['aatype'], tolerance_factor_soft=violation_tolerance_factor, tolerance_factor_hard=violation_tolerance_factor)
    atomtype_radius = [residue_constants.van_der_waals_radius[name[0]] for name in residue_constants.atom_types]
    atomtype_radius = atom14_pred_positions.new_tensor(atomtype_radius)
    atom14_atom_radius = (batch['atom14_atom_exists'] * atomtype_radius[batch['residx_atom14_to_atom37']])
    between_residue_clashes = between_residue_clash_loss(atom14_pred_positions=atom14_pred_positions, atom14_atom_exists=batch['atom14_atom_exists'], atom14_atom_radius=atom14_atom_radius, residue_index=batch['residue_index'], overlap_tolerance_soft=clash_overlap_tolerance, overlap_tolerance_hard=clash_overlap_tolerance)
    restype_atom14_bounds = residue_constants.make_atom14_dists_bounds(overlap_tolerance=clash_overlap_tolerance, bond_length_tolerance_factor=violation_tolerance_factor)
    atom14_atom_exists = batch['atom14_atom_exists']
    atom14_dists_lower_bound = atom14_pred_positions.new_tensor(restype_atom14_bounds['lower_bound'])[batch['aatype']]
    atom14_dists_upper_bound = atom14_pred_positions.new_tensor(restype_atom14_bounds['upper_bound'])[batch['aatype']]
    residue_violations = within_residue_violations(atom14_pred_positions=atom14_pred_positions, atom14_atom_exists=batch['atom14_atom_exists'], atom14_dists_lower_bound=atom14_dists_lower_bound, atom14_dists_upper_bound=atom14_dists_upper_bound, tighten_bounds_for_loss=0.0)
    per_residue_violations_mask = torch.max(torch.stack([connection_violations['per_residue_violation_mask'], torch.max(between_residue_clashes['per_atom_clash_mask'], dim=(- 1))[0], torch.max(residue_violations['per_atom_violations'], dim=(- 1))[0]], dim=(- 1)), dim=(- 1))[0]
    return {'between_residues': {'bonds_c_n_loss_mean': connection_violations['c_n_loss_mean'], 'angles_ca_c_n_loss_mean': connection_violations['ca_c_n_loss_mean'], 'angles_c_n_ca_loss_mean': connection_violations['c_n_ca_loss_mean'], 'connections_per_residue_loss_sum': connection_violations['per_residue_loss_sum'], 'connections_per_residue_violation_mask': connection_violations['per_residue_violation_mask'], 'clashes_mean_loss': between_residue_clashes['mean_loss'], 'clashes_per_atom_loss_sum': between_residue_clashes['per_atom_loss_sum'], 'clashes_per_atom_clash_mask': between_residue_clashes['per_atom_clash_mask']}, 'within_residues': {'per_atom_loss_sum': residue_violations['per_atom_loss_sum'], 'per_atom_violations': residue_violations['per_atom_violations']}, 'total_per_residue_violations_mask': per_residue_violations_mask}
"""""", """""" 
    connection_violations = within_residue_violations(pred_atom_positions=atom14_pred_positions, pred_atom_mask=batch['atom14_atom_exists'], residue_index=batch['residue_index'], aatype=batch['aatype'], tolerance_factor_soft=violation_tolerance_factor, tolerance_factor_hard=violation_tolerance_factor)
    atomtype_radius = [residue_constants.van_der_waals_radius[name[0]] for name in residue_constants.atom_types]
    atomtype_radius = atom14_pred_positions.new_tensor(atomtype_radius)
    atom14_atom_radius = (batch['atom14_atom_exists'] * atomtype_radius[batch['residx_atom14_to_atom37']])
    between_residue_clashes = between_residue_clash_loss(atom14_pred_positions=atom14_pred_positions, atom14_atom_exists=batch['atom14_atom_exists'], atom14_atom_radius=atom14_atom_radius, residue_index=batch['residue_index'], overlap_tolerance_soft=clash_overlap_tolerance, overlap_tolerance_hard=clash_overlap_tolerance)
    restype_atom14_bounds = residue_constants.make_atom14_dists_bounds(overlap_tolerance=clash_overlap_tolerance, bond_length_tolerance_factor=violation_tolerance_factor)
    atom14_atom_exists = batch['atom14_atom_exists']
    atom14_dists_lower_bound = atom14_pred_positions.new_tensor(restype_atom14_bounds['lower_bound'])[batch['aatype']]
    atom14_dists_upper_bound = atom14_pred_positions.new_tensor(restype_atom14_bounds['upper_bound'])[batch['aatype']]
    residue_violations = between_residue_bond_loss(atom14_pred_positions=atom14_pred_positions, atom14_atom_exists=batch['atom14_atom_exists'], atom14_dists_lower_bound=atom14_dists_lower_bound, atom14_dists_upper_bound=atom14_dists_upper_bound, tighten_bounds_for_loss=0.0)
    per_residue_violations_mask = torch.max(torch.stack([connection_violations['per_residue_violation_mask'], torch.max(between_residue_clashes['per_atom_clash_mask'], dim=(- 1))[0], torch.max(residue_violations['per_atom_violations'], dim=(- 1))[0]], dim=(- 1)), dim=(- 1))[0]
    return {'between_residues': {'bonds_c_n_loss_mean': connection_violations['c_n_loss_mean'], 'angles_ca_c_n_loss_mean': connection_violations['ca_c_n_loss_mean'], 'angles_c_n_ca_loss_mean': connection_violations['c_n_ca_loss_mean'], 'connections_per_residue_loss_sum': connection_violations['per_residue_loss_sum'], 'connections_per_residue_violation_mask': connection_violations['per_residue_violation_mask'], 'clashes_mean_loss': between_residue_clashes['mean_loss'], 'clashes_per_atom_loss_sum': between_residue_clashes['per_atom_loss_sum'], 'clashes_per_atom_clash_mask': between_residue_clashes['per_atom_clash_mask']}, 'within_residues': {'per_atom_loss_sum': residue_violations['per_atom_loss_sum'], 'per_atom_violations': residue_violations['per_atom_violations']}, 'total_per_residue_violations_mask': per_residue_violations_mask}
""""""]",1
"DownloadError, unzip_into = unzip_into, DownloadError
def setup_install(repo=DEFAULT_REPO, branch=DEFAULT_BRANCH, install_path=None, as_user=False, zippath=None, dryrun=False, verbose=False):
    """"""
    Download and install StaSh using setup.py
    :param repo: name of user owning the github repo to download/install from
    :type repo: str
    :param branch: branch to download/install
    :type repo: str
    :param install_path: path to install to (as --prefix)
    :type install_path: str
    :param as_user: install into user packages
    :type as_user: bool
    :param zippath: alternative path to zip to install from (default: download from repo:branch)
    :param dryrun: if True, pass --dry-run to setup.py
    :param verbose: if True, print additional information
    :type verbose: bool
    """"""","["""""" 
    if (zippath is None):
        zp = TEMP_ZIPFILE
        try:
            download_stash(repo=repo, branch=branch, outpath=zp, verbose=verbose)
        except:
            raise DownloadError('Unable to download StaSh from {}:{}'.format(repo, branch))
    else:
        zp = zippath
    tp = os.path.join(TMPDIR, 'getstash-{}'.format(time.time()))
    unzip_into(zp, tp, verbose=verbose)
    os.chdir(tp)
    argv = ['setup.py', 'install']
    if as_user:
        argv.append('--user')
    if (install_path is not None):
        argv += ['--prefix', install_path]
    if dryrun:
        argv.append('--dry-run')
    sys.argv = argv
    fp = os.path.abspath('setup.py')
    ns = {'__name__': '__main__', '__file__': fp}
    with open(fp, 'rU') as fin:
        content = fin.read()
        code = compile(content, fp, 'exec', dont_inherit=True)
        exec(code, ns, ns)
"""""", """""" 
    if (zippath is None):
        zp = TEMP_ZIPFILE
        try:
            download_stash(repo=repo, branch=branch, outpath=zp, verbose=verbose)
        except:
            raise unzip_into('Unable to download StaSh from {}:{}'.format(repo, branch))
    else:
        zp = zippath
    tp = os.path.join(TMPDIR, 'getstash-{}'.format(time.time()))
    DownloadError(zp, tp, verbose=verbose)
    os.chdir(tp)
    argv = ['setup.py', 'install']
    if as_user:
        argv.append('--user')
    if (install_path is not None):
        argv += ['--prefix', install_path]
    if dryrun:
        argv.append('--dry-run')
    sys.argv = argv
    fp = os.path.abspath('setup.py')
    ns = {'__name__': '__main__', '__file__': fp}
    with open(fp, 'rU') as fin:
        content = fin.read()
        code = compile(content, fp, 'exec', dont_inherit=True)
        exec(code, ns, ns)
""""""]",1
"Dense, l2 = l2, Dense
def mlp_ptscorer(model, inputs, Ddim, N, l2reg, pfx='out', Dinit='glorot_uniform', sum_mode='sum', extra_inp=[]):
    """""" Element-wise features from the pair fed to an MLP. """"""","["""""" 
    if (sum_mode == 'absdiff'):
        absdiff_merge(model, inputs, pfx, 'sum')
    else:
        model.add_node(name=(pfx + 'sum'), inputs=inputs, layer=Activation('linear'), merge_mode='sum')
    model.add_node(name=(pfx + 'mul'), inputs=inputs, layer=Activation('linear'), merge_mode='mul')
    mlp_inputs = ([(pfx + 'sum'), (pfx + 'mul')] + extra_inp)

    def mlp_args(mlp_inputs):
        ' return model.add_node() args that are good for mlp_inputs list\n        of both length 1 and more than 1. '
        mlp_args = dict()
        if (len(mlp_inputs) > 1):
            mlp_args['inputs'] = mlp_inputs
            mlp_args['merge_mode'] = 'concat'
        else:
            mlp_args['input'] = mlp_inputs[0]
        return mlp_args
    if (Ddim == 0):
        Ddim = []
    elif (not isinstance(Ddim, list)):
        Ddim = [Ddim]
    if Ddim:
        for (i, D) in enumerate(Ddim):
            model.add_node(name=(pfx + ('hdn[%d]' % (i,))), layer=Dense(output_dim=int((N * D)), W_regularizer=l2(l2reg), activation='tanh', init=Dinit), **mlp_args(mlp_inputs))
            mlp_inputs = [(pfx + ('hdn[%d]' % (i,)))]
    model.add_node(name=(pfx + 'mlp'), layer=Dense(output_dim=1, W_regularizer=l2(l2reg)), **mlp_args(mlp_inputs))
    return (pfx + 'mlp')
"""""", """""" 
    if (sum_mode == 'absdiff'):
        absdiff_merge(model, inputs, pfx, 'sum')
    else:
        model.add_node(name=(pfx + 'sum'), inputs=inputs, layer=Activation('linear'), merge_mode='sum')
    model.add_node(name=(pfx + 'mul'), inputs=inputs, layer=Activation('linear'), merge_mode='mul')
    mlp_inputs = ([(pfx + 'sum'), (pfx + 'mul')] + extra_inp)

    def mlp_args(mlp_inputs):
        ' return model.add_node() args that are good for mlp_inputs list\n        of both length 1 and more than 1. '
        mlp_args = dict()
        if (len(mlp_inputs) > 1):
            mlp_args['inputs'] = mlp_inputs
            mlp_args['merge_mode'] = 'concat'
        else:
            mlp_args['input'] = mlp_inputs[0]
        return mlp_args
    if (Ddim == 0):
        Ddim = []
    elif (not isinstance(Ddim, list)):
        Ddim = [Ddim]
    if Ddim:
        for (i, D) in enumerate(Ddim):
            model.add_node(name=(pfx + ('hdn[%d]' % (i,))), layer=l2(output_dim=int((N * D)), W_regularizer=Dense(l2reg), activation='tanh', init=Dinit), **mlp_args(mlp_inputs))
            mlp_inputs = [(pfx + ('hdn[%d]' % (i,)))]
    model.add_node(name=(pfx + 'mlp'), layer=l2(output_dim=1, W_regularizer=Dense(l2reg)), **mlp_args(mlp_inputs))
    return (pfx + 'mlp')
""""""]",1
"_populate_run_dir, _create_run_dir_local = _create_run_dir_local, _populate_run_dir
def submit_run(submit_config: SubmitConfig, run_func_name: str, **run_func_kwargs) -> None:
    """"""Create a run dir, gather files related to the run, copy files to the run dir, and launch the run in appropriate place.""""""","["""""" 
    submit_config = copy.deepcopy(submit_config)
    submit_target = submit_config.submit_target
    farm = None
    if (submit_target == SubmitTarget.LOCAL):
        farm = internal.local.Target()
    assert (farm is not None)
    if ((submit_config.num_gpus is None) or (submit_config.num_gpus == 0)):
        raise RuntimeError('submit_config.num_gpus must be set to a non-zero value')
    if (submit_config.user_name is None):
        submit_config.user_name = get_user_name()
    submit_config.run_func_name = run_func_name
    submit_config.run_func_kwargs = run_func_kwargs
    host_run_dir = _create_run_dir_local(submit_config)
    submit_config.task_name = '{0}-{1:05d}-{2}'.format(submit_config.user_name, submit_config.run_id, submit_config.run_desc)
    docker_valid_name_regex = '^[a-zA-Z0-9][a-zA-Z0-9_.-]+$'
    if (not re.match(docker_valid_name_regex, submit_config.task_name)):
        raise RuntimeError(((('Invalid task name.  Probable reason: unacceptable characters in your submit_config.run_desc.  Task name must be accepted by the following regex: ' + docker_valid_name_regex) + ', got ') + submit_config.task_name))
    farm.finalize_submit_config(submit_config, host_run_dir)
    _populate_run_dir(submit_config, host_run_dir)
    return farm.submit(submit_config, host_run_dir)
"""""", """""" 
    submit_config = copy.deepcopy(submit_config)
    submit_target = submit_config.submit_target
    farm = None
    if (submit_target == SubmitTarget.LOCAL):
        farm = internal.local.Target()
    assert (farm is not None)
    if ((submit_config.num_gpus is None) or (submit_config.num_gpus == 0)):
        raise RuntimeError('submit_config.num_gpus must be set to a non-zero value')
    if (submit_config.user_name is None):
        submit_config.user_name = get_user_name()
    submit_config.run_func_name = run_func_name
    submit_config.run_func_kwargs = run_func_kwargs
    host_run_dir = _populate_run_dir(submit_config)
    submit_config.task_name = '{0}-{1:05d}-{2}'.format(submit_config.user_name, submit_config.run_id, submit_config.run_desc)
    docker_valid_name_regex = '^[a-zA-Z0-9][a-zA-Z0-9_.-]+$'
    if (not re.match(docker_valid_name_regex, submit_config.task_name)):
        raise RuntimeError(((('Invalid task name.  Probable reason: unacceptable characters in your submit_config.run_desc.  Task name must be accepted by the following regex: ' + docker_valid_name_regex) + ', got ') + submit_config.task_name))
    farm.finalize_submit_config(submit_config, host_run_dir)
    _create_run_dir_local(submit_config, host_run_dir)
    return farm.submit(submit_config, host_run_dir)
""""""]",1
"assume_role, process_put_account_public_access_block = process_put_account_public_access_block, assume_role
def local_testing(aws_account: AccountTypeDef, params: dict) -> None:
    """"""Local Testing.

    Args:
        aws_account: AWS account to update
        params: solution parameters
    """"""","["""""" 
    account_session = assume_role(params['CONFIGURATION_ROLE_NAME'], params['ROLE_SESSION_NAME'], aws_account['Id'])
    s3_client: S3ControlClient = account_session.client('s3control', config=BOTO3_CONFIG)
    process_put_account_public_access_block(s3_client, aws_account, params)
"""""", """""" 
    account_session = process_put_account_public_access_block(params['CONFIGURATION_ROLE_NAME'], params['ROLE_SESSION_NAME'], aws_account['Id'])
    s3_client: S3ControlClient = account_session.client('s3control', config=BOTO3_CONFIG)
    assume_role(s3_client, aws_account, params)
""""""]",1
"_layer_check, _layer_reset = _layer_reset, _layer_check
def reset(net):
    """"""Check for the types of LIF neurons contained in net.
    Reset their hidden parameters to zero and detach them
    from the current computation graph.""""""","["""""" 
    global is_alpha
    global is_leaky
    global is_lapicque
    global is_rleaky
    global is_synaptic
    global is_rsynaptic
    global is_sconv2dlstm
    global is_slstm
    is_alpha = False
    is_leaky = False
    is_rleaky = False
    is_synaptic = False
    is_rsynaptic = False
    is_lapicque = False
    is_sconv2dlstm = False
    is_slstm = False
    _layer_check(net=net)
    _layer_reset()
"""""", """""" 
    global is_alpha
    global is_leaky
    global is_lapicque
    global is_rleaky
    global is_synaptic
    global is_rsynaptic
    global is_sconv2dlstm
    global is_slstm
    is_alpha = False
    is_leaky = False
    is_rleaky = False
    is_synaptic = False
    is_rsynaptic = False
    is_lapicque = False
    is_sconv2dlstm = False
    is_slstm = False
    _layer_reset(net=net)
    _layer_check()
""""""]",1
"strike, list_of_tasks_files = list_of_tasks_files, strike
def update_task():
    """"""
update a particular task
	""""""","["""""" 
    list_of_files = list_of_tasks_files()
    if list_of_files:
        not_valid_task_number = 1
        not_valid_date_number = 1
        click.echo('Select the date:- \n')
        click.echo('--------------------')
        click.echo('Number |    Date    ')
        click.echo('--------------------')
        for (i, some_file) in enumerate(range(0, len(list_of_files))):
            click.echo(((str(i) + '      |') + list_of_files[some_file][0:10]))
        while not_valid_date_number:
            click.echo(chalk.blue('Enter the number to select the date'))
            selected_date = int(input())
            if (selected_date > len(list_of_files)):
                click.echo(chalk.red('Please Enter a valid date number!'))
            else:
                SELECTED_DATE_PATH = os.path.join(((DIARY_CONFIG_FOLDER_PATH + '/') + list_of_files[(selected_date - 1)]))
                with open(SELECTED_DATE_PATH) as selected_task:
                    contents = yaml.load(selected_task)
                    click.echo(('\nTasks for ' + list_of_files[some_file][0:10]))
                    click.echo('-----------------------')
                    click.echo('Number |  Time   | Task')
                    click.echo('-------|---------|-----')
                    for (i, entry) in enumerate(contents['entries']):
                        time = entry['time']
                        text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                        text = (text if (entry['status'] == 0) else strike(text))
                        click.echo(((((('   ' + str(i)) + '   | ') + time) + ': ') + text))
                not_valid_date_number = 0
        while not_valid_task_number:
            click.echo(chalk.blue('Enter the task number that you would like to update'))
            task_to_be_updated = int(input())
            if (task_to_be_updated > len(contents['entries'])):
                click.echo(chalk.red('Please Enter a valid task number!'))
            else:
                click.echo(chalk.blue('Enter the new task'))
                not_valid_task_number = 0
                new_text = str(input())
                contents['entries'][(task_to_be_updated - 1)]['text'] = new_text
                input_data(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
    else:
        click.echo(chalk.red('There are no tasks. Add a new task by entering ""yoda diary nt""'))
"""""", """""" 
    list_of_files = strike()
    if list_of_files:
        not_valid_task_number = 1
        not_valid_date_number = 1
        click.echo('Select the date:- \n')
        click.echo('--------------------')
        click.echo('Number |    Date    ')
        click.echo('--------------------')
        for (i, some_file) in enumerate(range(0, len(list_of_files))):
            click.echo(((str(i) + '      |') + list_of_files[some_file][0:10]))
        while not_valid_date_number:
            click.echo(chalk.blue('Enter the number to select the date'))
            selected_date = int(input())
            if (selected_date > len(list_of_files)):
                click.echo(chalk.red('Please Enter a valid date number!'))
            else:
                SELECTED_DATE_PATH = os.path.join(((DIARY_CONFIG_FOLDER_PATH + '/') + list_of_files[(selected_date - 1)]))
                with open(SELECTED_DATE_PATH) as selected_task:
                    contents = yaml.load(selected_task)
                    click.echo(('\nTasks for ' + list_of_files[some_file][0:10]))
                    click.echo('-----------------------')
                    click.echo('Number |  Time   | Task')
                    click.echo('-------|---------|-----')
                    for (i, entry) in enumerate(contents['entries']):
                        time = entry['time']
                        text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                        text = (text if (entry['status'] == 0) else list_of_tasks_files(text))
                        click.echo(((((('   ' + str(i)) + '   | ') + time) + ': ') + text))
                not_valid_date_number = 0
        while not_valid_task_number:
            click.echo(chalk.blue('Enter the task number that you would like to update'))
            task_to_be_updated = int(input())
            if (task_to_be_updated > len(contents['entries'])):
                click.echo(chalk.red('Please Enter a valid task number!'))
            else:
                click.echo(chalk.blue('Enter the new task'))
                not_valid_task_number = 0
                new_text = str(input())
                contents['entries'][(task_to_be_updated - 1)]['text'] = new_text
                input_data(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
    else:
        click.echo(chalk.red('There are no tasks. Add a new task by entering ""yoda diary nt""'))
""""""]",1
"Looks, waterBodyRadar = waterBodyRadar, Looks
def runLook(self):
    """"""take looks
    """"""","["""""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    wbdFile = os.path.abspath(self._insar.wbd)
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    amp = isceobj.createImage()
    amp.load((self._insar.amplitude + '.xml'))
    width = amp.width
    length = amp.length
    width2 = int((width / self._insar.numberRangeLooks2))
    length2 = int((length / self._insar.numberAzimuthLooks2))
    if (not ((self._insar.numberRangeLooks2 == 1) and (self._insar.numberAzimuthLooks2 == 1))):
        look(self._insar.differentialInterferogram, self._insar.multilookDifferentialInterferogram, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 0, 1)
        look(self._insar.amplitude, self._insar.multilookAmplitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 1, 1)
        look(self._insar.latitude, self._insar.multilookLatitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.longitude, self._insar.multilookLongitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.height, self._insar.multilookHeight, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        create_xml(self._insar.multilookDifferentialInterferogram, width2, length2, 'int')
        create_xml(self._insar.multilookAmplitude, width2, length2, 'amp')
        create_xml(self._insar.multilookLatitude, width2, length2, 'double')
        create_xml(self._insar.multilookLongitude, width2, length2, 'double')
        create_xml(self._insar.multilookHeight, width2, length2, 'double')
        from mroipac.looks.Looks import Looks
        from isceobj.Image import createImage
        inImage = createImage()
        inImage.load((self._insar.los + '.xml'))
        lkObj = Looks()
        lkObj.setDownLooks(self._insar.numberAzimuthLooks2)
        lkObj.setAcrossLooks(self._insar.numberRangeLooks2)
        lkObj.setInputImage(inImage)
        lkObj.setOutputFilename(self._insar.multilookLos)
        lkObj.looks()
        waterBodyRadar(self._insar.multilookLatitude, self._insar.multilookLongitude, wbdFile, self._insar.multilookWbdOut)
    os.chdir('../')
    catalog.printToLog(logger, 'runLook')
    self._insar.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    wbdFile = os.path.abspath(self._insar.wbd)
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    amp = isceobj.createImage()
    amp.load((self._insar.amplitude + '.xml'))
    width = amp.width
    length = amp.length
    width2 = int((width / self._insar.numberRangeLooks2))
    length2 = int((length / self._insar.numberAzimuthLooks2))
    if (not ((self._insar.numberRangeLooks2 == 1) and (self._insar.numberAzimuthLooks2 == 1))):
        look(self._insar.differentialInterferogram, self._insar.multilookDifferentialInterferogram, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 0, 1)
        look(self._insar.amplitude, self._insar.multilookAmplitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 1, 1)
        look(self._insar.latitude, self._insar.multilookLatitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.longitude, self._insar.multilookLongitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.height, self._insar.multilookHeight, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        create_xml(self._insar.multilookDifferentialInterferogram, width2, length2, 'int')
        create_xml(self._insar.multilookAmplitude, width2, length2, 'amp')
        create_xml(self._insar.multilookLatitude, width2, length2, 'double')
        create_xml(self._insar.multilookLongitude, width2, length2, 'double')
        create_xml(self._insar.multilookHeight, width2, length2, 'double')
        from mroipac.looks.Looks import Looks
        from isceobj.Image import createImage
        inImage = createImage()
        inImage.load((self._insar.los + '.xml'))
        lkObj = waterBodyRadar()
        lkObj.setDownLooks(self._insar.numberAzimuthLooks2)
        lkObj.setAcrossLooks(self._insar.numberRangeLooks2)
        lkObj.setInputImage(inImage)
        lkObj.setOutputFilename(self._insar.multilookLos)
        lkObj.looks()
        Looks(self._insar.multilookLatitude, self._insar.multilookLongitude, wbdFile, self._insar.multilookWbdOut)
    os.chdir('../')
    catalog.printToLog(logger, 'runLook')
    self._insar.procDoc.addAllFromCatalog(catalog)
""""""]",1
"is_property_deleter, _get_properties = _get_properties, is_property_deleter
def _determine_function_name_type(node: nodes.FunctionDef, config: argparse.Namespace) -> str:
    """"""Determine the name type whose regex the function's name should match.

    :param node: A function node.
    :param config: Configuration from which to pull additional property classes.

    :returns: One of ('function', 'method', 'attr')
    """"""","["""""" 
    (property_classes, property_names) = _get_properties(config)
    if (not node.is_method()):
        return 'function'
    if (is_property_setter(node) or is_property_deleter(node)):
        return 'attr'
    decorators = (node.decorators.nodes if node.decorators else [])
    for decorator in decorators:
        if (isinstance(decorator, nodes.Name) or (isinstance(decorator, nodes.Attribute) and (decorator.attrname in property_names))):
            inferred = utils.safe_infer(decorator)
            if (inferred and hasattr(inferred, 'qname') and (inferred.qname() in property_classes)):
                return 'attr'
    return 'method'
"""""", """""" 
    (property_classes, property_names) = is_property_deleter(config)
    if (not node.is_method()):
        return 'function'
    if (is_property_setter(node) or _get_properties(node)):
        return 'attr'
    decorators = (node.decorators.nodes if node.decorators else [])
    for decorator in decorators:
        if (isinstance(decorator, nodes.Name) or (isinstance(decorator, nodes.Attribute) and (decorator.attrname in property_names))):
            inferred = utils.safe_infer(decorator)
            if (inferred and hasattr(inferred, 'qname') and (inferred.qname() in property_classes)):
                return 'attr'
    return 'method'
""""""]",1
"debug, normalize_env = normalize_env, debug
def get_output(vcbat, args=None, env=None):
    """"""Parse the output of given bat file, with given args.""""""","["""""" 
    if (env is None):
        env = SCons.Environment.Environment(tools=[])
    vs_vc_vars = ['COMSPEC', 'OS', 'VS170COMNTOOLS', 'VS160COMNTOOLS', 'VS150COMNTOOLS', 'VS140COMNTOOLS', 'VS120COMNTOOLS', 'VS110COMNTOOLS', 'VS100COMNTOOLS', 'VS90COMNTOOLS', 'VS80COMNTOOLS', 'VS71COMNTOOLS', 'VSCOMNTOOLS', 'MSDevDir', 'VSCMD_DEBUG', 'VSCMD_SKIP_SENDTELEMETRY', 'windir']
    env['ENV'] = normalize_env(env['ENV'], vs_vc_vars, force=False)
    if args:
        debug(""Calling '%s %s'"", vcbat, args)
        popen = SCons.Action._subproc(env, ('""%s"" %s & set' % (vcbat, args)), stdin='devnull', stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    else:
        debug(""Calling '%s'"", vcbat)
        popen = SCons.Action._subproc(env, ('""%s"" & set' % vcbat), stdin='devnull', stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    with popen.stdout:
        stdout = popen.stdout.read()
    with popen.stderr:
        stderr = popen.stderr.read()
    if ((sys.version_info.major == 3) and (sys.version_info.minor < 6)):
        from ctypes import windll
        OEM = 'cp{}'.format(windll.kernel32.GetConsoleOutputCP())
    else:
        OEM = 'oem'
    if stderr:
        sys.stderr.write(stderr.decode(OEM))
    if (popen.wait() != 0):
        raise IOError(stderr.decode(OEM))
    return stdout.decode(OEM)
"""""", """""" 
    if (env is None):
        env = SCons.Environment.Environment(tools=[])
    vs_vc_vars = ['COMSPEC', 'OS', 'VS170COMNTOOLS', 'VS160COMNTOOLS', 'VS150COMNTOOLS', 'VS140COMNTOOLS', 'VS120COMNTOOLS', 'VS110COMNTOOLS', 'VS100COMNTOOLS', 'VS90COMNTOOLS', 'VS80COMNTOOLS', 'VS71COMNTOOLS', 'VSCOMNTOOLS', 'MSDevDir', 'VSCMD_DEBUG', 'VSCMD_SKIP_SENDTELEMETRY', 'windir']
    env['ENV'] = debug(env['ENV'], vs_vc_vars, force=False)
    if args:
        normalize_env(""Calling '%s %s'"", vcbat, args)
        popen = SCons.Action._subproc(env, ('""%s"" %s & set' % (vcbat, args)), stdin='devnull', stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    else:
        normalize_env(""Calling '%s'"", vcbat)
        popen = SCons.Action._subproc(env, ('""%s"" & set' % vcbat), stdin='devnull', stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    with popen.stdout:
        stdout = popen.stdout.read()
    with popen.stderr:
        stderr = popen.stderr.read()
    if ((sys.version_info.major == 3) and (sys.version_info.minor < 6)):
        from ctypes import windll
        OEM = 'cp{}'.format(windll.kernel32.GetConsoleOutputCP())
    else:
        OEM = 'oem'
    if stderr:
        sys.stderr.write(stderr.decode(OEM))
    if (popen.wait() != 0):
        raise IOError(stderr.decode(OEM))
    return stdout.decode(OEM)
""""""]",1
"ProcessPoolExecutor, get_run_config = get_run_config, ProcessPoolExecutor
def distribute_over_gpus(sweep_config: Union[(DictConfig, ListConfig)], folder: Optional[str]=None):
    """"""Distribute metric collection over all available GPUs. This is done by splitting the list of configurations.""""""","["""""" 
    with ProcessPoolExecutor(max_workers=torch.cuda.device_count(), mp_context=multiprocessing.get_context('spawn')) as executor:
        run_configs = list(get_run_config(sweep_config.grid_search))
        jobs = []
        for (device_id, run_split) in enumerate(range(0, len(run_configs), math.ceil((len(run_configs) / torch.cuda.device_count())))):
            jobs.append(executor.submit(compute_on_gpu, run_configs[run_split:(run_split + math.ceil((len(run_configs) / torch.cuda.device_count())))], (device_id + 1), sweep_config.seed, sweep_config.writer, folder, sweep_config.compute_openvino))
        for job in jobs:
            try:
                job.result()
            except Exception as exc:
                raise Exception(f'Error occurred while computing benchmark on GPU {job}') from exc
"""""", """""" 
    with get_run_config(max_workers=torch.cuda.device_count(), mp_context=multiprocessing.get_context('spawn')) as executor:
        run_configs = list(ProcessPoolExecutor(sweep_config.grid_search))
        jobs = []
        for (device_id, run_split) in enumerate(range(0, len(run_configs), math.ceil((len(run_configs) / torch.cuda.device_count())))):
            jobs.append(executor.submit(compute_on_gpu, run_configs[run_split:(run_split + math.ceil((len(run_configs) / torch.cuda.device_count())))], (device_id + 1), sweep_config.seed, sweep_config.writer, folder, sweep_config.compute_openvino))
        for job in jobs:
            try:
                job.result()
            except Exception as exc:
                raise Exception(f'Error occurred while computing benchmark on GPU {job}') from exc
""""""]",1
"ridgeRegres, mat = mat, ridgeRegres
def ridgeTest(xArr, yArr):
    """"""
        Desc: 
            函数 ridgeTest() 用于在一组 λ 上测试结果
        Args: 
            xArr: 样本数据的特征，即 feature
            yArr: 样本数据的类别标签，即真实数据
        Returns: 
            wMat: 将所有的回归系数输出到一个矩阵并返回
    """"""","["""""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    yMean = mean(yMat, 0)
    yMat = (yMat - yMean)
    xMeans = mean(xMat, 0)
    xVar = var(xMat, 0)
    xMat = ((xMat - xMeans) / xVar)
    numTestPts = 30
    wMat = zeros((numTestPts, shape(xMat)[1]))
    for i in range(numTestPts):
        ws = ridgeRegres(xMat, yMat, exp((i - 10)))
        wMat[i, :] = ws.T
    return wMat
"""""", """""" 
    xMat = ridgeRegres(xArr)
    yMat = ridgeRegres(yArr).T
    yMean = mean(yMat, 0)
    yMat = (yMat - yMean)
    xMeans = mean(xMat, 0)
    xVar = var(xMat, 0)
    xMat = ((xMat - xMeans) / xVar)
    numTestPts = 30
    wMat = zeros((numTestPts, shape(xMat)[1]))
    for i in range(numTestPts):
        ws = mat(xMat, yMat, exp((i - 10)))
        wMat[i, :] = ws.T
    return wMat
""""""]",1
"get_exitcode_stdout_stderr, get_resource = get_resource, get_exitcode_stdout_stderr
def evaluate(gold_file, pred_file):
    """"""Evaluate using official CoNLL-X evaluation script (Yuval Krymolowski)

    Args:
      gold_file(str): The gold conllx file
      pred_file(str): The pred conllx file

    Returns:

    
    """"""","["""""" 
    gold_file = get_resource(gold_file)
    fixed_pred_file = tempfile.NamedTemporaryFile().name
    copy_cols(gold_file, pred_file, fixed_pred_file, keep_comments=False)
    if gold_file.endswith('.conllu'):
        fixed_gold_file = tempfile.NamedTemporaryFile().name
        copy_cols(gold_file, gold_file, fixed_gold_file, keep_comments=False)
        gold_file = fixed_gold_file
    (exitcode, out, err) = get_exitcode_stdout_stderr(f'perl {CONLLX_EVAL} -q -b -g {gold_file} -s {fixed_pred_file}')
    if exitcode:
        raise RuntimeError(f'eval.pl exited with error code {exitcode} and error message {err} and output {out}.')
    lines = out.split('\n')[(- 4):]
    las = (int(lines[0].split()[3]) / int(lines[0].split()[5]))
    uas = (int(lines[1].split()[3]) / int(lines[1].split()[5]))
    return (uas, las)
"""""", """""" 
    gold_file = get_exitcode_stdout_stderr(gold_file)
    fixed_pred_file = tempfile.NamedTemporaryFile().name
    copy_cols(gold_file, pred_file, fixed_pred_file, keep_comments=False)
    if gold_file.endswith('.conllu'):
        fixed_gold_file = tempfile.NamedTemporaryFile().name
        copy_cols(gold_file, gold_file, fixed_gold_file, keep_comments=False)
        gold_file = fixed_gold_file
    (exitcode, out, err) = get_resource(f'perl {CONLLX_EVAL} -q -b -g {gold_file} -s {fixed_pred_file}')
    if exitcode:
        raise RuntimeError(f'eval.pl exited with error code {exitcode} and error message {err} and output {out}.')
    lines = out.split('\n')[(- 4):]
    las = (int(lines[0].split()[3]) / int(lines[0].split()[5]))
    uas = (int(lines[1].split()[3]) / int(lines[1].split()[5]))
    return (uas, las)
""""""]",1
"scatter_mean, _size_to_index = _size_to_index, scatter_mean
def variadic_mean(input, size):
    """"""
    Compute mean over sets with variadic sizes.

    Suppose there are :math:`N` sets, and the sizes of all sets are summed to :math:`B`.

    Parameters:
        input (Tensor): input of shape :math:`(B, ...)`
        size (LongTensor): size of sets of shape :math:`(N,)`
    """"""","["""""" 
    index2sample = _size_to_index(size)
    index2sample = index2sample.view(([(- 1)] + ([1] * (input.ndim - 1))))
    index2sample = index2sample.expand_as(input)
    value = scatter_mean(input, index2sample, dim=0)
    return value
"""""", """""" 
    index2sample = scatter_mean(size)
    index2sample = index2sample.view(([(- 1)] + ([1] * (input.ndim - 1))))
    index2sample = index2sample.expand_as(input)
    value = _size_to_index(input, index2sample, dim=0)
    return value
""""""]",1
"Project, Path = Path, Project
@posix
def test_all_files_git_ignored_contains_newline(git_repository):
    """"""Files that contain newlines are also ignored.""""""","["""""" 
    (git_repository / 'hello\nworld.pyc').write_text('foo')
    project = Project(git_repository)
    assert (Path('hello\nworld.pyc').absolute() not in project.all_files())
"""""", """""" 
    (git_repository / 'hello\nworld.pyc').write_text('foo')
    project = Path(git_repository)
    assert (Project('hello\nworld.pyc').absolute() not in project.all_files())
""""""]",1
"_step, get_browser = get_browser, _step
def focus_element(element):
    """"""Give focus to element

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element)
    with _step(f'Focus element {element.name}'):
        element.focus()
"""""", """""" 
    element = _step().find(element)
    with get_browser(f'Focus element {element.name}'):
        element.focus()
""""""]",1
"input_data, append_like_data_into_file = append_like_data_into_file, input_data
def like():
    """"""
    add things they like
    """"""","["""""" 
    click.echo(chalk.blue('Add things they like'))
    if os.path.isfile(LOVE_LIKES_FILE_PATH):
        like_data = dict(like=input())
        append_like_data_into_file(like_data, LOVE_LIKES_FILE_PATH)
    else:
        like_data = dict(likes=[dict(like=input())])
        input_data(like_data, LOVE_LIKES_FILE_PATH)
    click.echo(chalk.blue('Want to add more things they like? [y/n]'))
    repeat = input()
    if ((repeat == 'y') or (repeat == 'yes')):
        like()
"""""", """""" 
    click.echo(chalk.blue('Add things they like'))
    if os.path.isfile(LOVE_LIKES_FILE_PATH):
        like_data = dict(like=input())
        input_data(like_data, LOVE_LIKES_FILE_PATH)
    else:
        like_data = dict(likes=[dict(like=input())])
        append_like_data_into_file(like_data, LOVE_LIKES_FILE_PATH)
    click.echo(chalk.blue('Want to add more things they like? [y/n]'))
    repeat = input()
    if ((repeat == 'y') or (repeat == 'yes')):
        like()
""""""]",1
"MockSlaveChannel, TelegramChannel = TelegramChannel, MockSlaveChannel
@pytest.fixture(scope='module')
def coordinator(tmp_path_factory, monkey_class, bot_token, bot_admins) -> ehforwarderbot.coordinator:
    """"""Loaded coordinator with ETM and mock modules""""""","["""""" 
    tmp_path = tmp_path_factory.mktemp('etm_test')
    monkey_class.setenv('EFB_DATA_PATH', str(tmp_path))
    config_path = ehforwarderbot.utils.get_config_path()
    dump_config(config_path, {'master_channel': TelegramChannel.channel_id, 'slave_channels': ['tests.mocks.slave'], 'middlewares': []})
    ehforwarderbot.coordinator.add_channel(MockSlaveChannel())
    channel_config_path = ehforwarderbot.utils.get_config_path(TelegramChannel.channel_id)
    dump_config(channel_config_path, {'token': bot_token, 'admins': bot_admins})
    ehforwarderbot.coordinator.add_channel(TelegramChannel())
    (yield ehforwarderbot.coordinator)
    ehforwarderbot.coordinator.master.stop_polling()
    for i in ehforwarderbot.coordinator.slaves.values():
        i.stop_polling()
"""""", """""" 
    tmp_path = tmp_path_factory.mktemp('etm_test')
    monkey_class.setenv('EFB_DATA_PATH', str(tmp_path))
    config_path = ehforwarderbot.utils.get_config_path()
    dump_config(config_path, {'master_channel': MockSlaveChannel.channel_id, 'slave_channels': ['tests.mocks.slave'], 'middlewares': []})
    ehforwarderbot.coordinator.add_channel(TelegramChannel())
    channel_config_path = ehforwarderbot.utils.get_config_path(MockSlaveChannel.channel_id)
    dump_config(channel_config_path, {'token': bot_token, 'admins': bot_admins})
    ehforwarderbot.coordinator.add_channel(MockSlaveChannel())
    (yield ehforwarderbot.coordinator)
    ehforwarderbot.coordinator.master.stop_polling()
    for i in ehforwarderbot.coordinator.slaves.values():
        i.stop_polling()
""""""]",1
"normalize_stderr, Popen = Popen, normalize_stderr
@pytest.mark.parametrize('flag', ['-h', '--help'])
@pytest.mark.usefixtures('both_debug_modes', 'both_setsid_modes')
def test_help_message(flag, current_version):
    """"""dumb-init should say something useful when called with the help flag,
    and exit zero.
    """"""","["""""" 
    proc = Popen(('dumb-init', flag), stderr=PIPE)
    (_, stderr) = proc.communicate()
    assert (proc.returncode == 0)
    assert (normalize_stderr(stderr) == ((b'dumb-init v' + current_version.encode('ascii')) + b'\nUsage: dumb-init [option] command [[arg] ...]\n\ndumb-init is a simple process supervisor that forwards signals to children.\nIt is designed to run as PID1 in minimal container environments.\n\nOptional arguments:\n   -c, --single-child   Run in single-child mode.\n                        In this mode, signals are only proxied to the\n                        direct child and not any of its descendants.\n   -r, --rewrite s:r    Rewrite received signal s to new signal r before proxying.\n                        To ignore (not proxy) a signal, rewrite it to 0.\n                        This option can be specified multiple times.\n   -v, --verbose        Print debugging information to stderr.\n   -h, --help           Print this help message and exit.\n   -V, --version        Print the current version and exit.\n\nFull help is available online at https://github.com/Yelp/dumb-init\n'))
"""""", """""" 
    proc = normalize_stderr(('dumb-init', flag), stderr=PIPE)
    (_, stderr) = proc.communicate()
    assert (proc.returncode == 0)
    assert (Popen(stderr) == ((b'dumb-init v' + current_version.encode('ascii')) + b'\nUsage: dumb-init [option] command [[arg] ...]\n\ndumb-init is a simple process supervisor that forwards signals to children.\nIt is designed to run as PID1 in minimal container environments.\n\nOptional arguments:\n   -c, --single-child   Run in single-child mode.\n                        In this mode, signals are only proxied to the\n                        direct child and not any of its descendants.\n   -r, --rewrite s:r    Rewrite received signal s to new signal r before proxying.\n                        To ignore (not proxy) a signal, rewrite it to 0.\n                        This option can be specified multiple times.\n   -v, --verbose        Print debugging information to stderr.\n   -h, --help           Print this help message and exit.\n   -V, --version        Print the current version and exit.\n\nFull help is available online at https://github.com/Yelp/dumb-init\n'))
""""""]",1
"ma_to_array, is_ma = is_ma, ma_to_array
def accept_ma(f):
    """"""Wraps a function in order to convert the input map from
    a masked to a regular numpy array, and convert back the
    output from a regular array to a masked array""""""","["""""" 

    @wraps(f)
    def wrapper(map_in, *args, **kwds):
        return_ma = is_ma(map_in)
        m = ma_to_array(map_in)
        out = f(m, *args, **kwds)
        return (ma(out) if return_ma else out)
    return wrapper
"""""", """""" 

    @wraps(f)
    def wrapper(map_in, *args, **kwds):
        return_ma = ma_to_array(map_in)
        m = is_ma(map_in)
        out = f(m, *args, **kwds)
        return (ma(out) if return_ma else out)
    return wrapper
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_alert_present():
    """"""Verify an alert is present""""""","["""""" 
    with _verify_step('Verify an alert is present', 'an alert was not present') as s:
        s.condition = get_browser().alert_is_present()
"""""", """""" 
    with get_browser('Verify an alert is present', 'an alert was not present') as s:
        s.condition = _verify_step().alert_is_present()
""""""]",1
"_org_remove, get_manager = get_manager, _org_remove
def remove(patch, path):
    """"""
	Remove (delete) the file path.
	If path is a directory, OSError is raised; see rmdir() below to remove
	a directory.
	This is identical to the unlink() function documented below.
	On Windows, attempting to remove a file that is in use causes an
	exception to be raised; on Unix, the directory entry is removed but the
	storage allocated to the file is not made available until the original
	file is no longer in use.
	""""""","["""""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = get_manager()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return _org_remove(relpath)
    elif readonly:
        raise IOError(""[Errno 1] Operation not permitted: '{p}'"".format(p=ap))
    else:
        if os.path.isdir(relpath):
            raise os.error(""OSError: [Errno 21] Is a directory: '{p}'"".format(p=ap))
        try:
            return fsi.remove(relpath)
        except OperationFailure as e:
            raise os.error(e.message)
"""""", """""" 
    ap = os.path.abspath(os.path.join(CWD, path))
    manager = _org_remove()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return get_manager(relpath)
    elif readonly:
        raise IOError(""[Errno 1] Operation not permitted: '{p}'"".format(p=ap))
    else:
        if os.path.isdir(relpath):
            raise os.error(""OSError: [Errno 21] Is a directory: '{p}'"".format(p=ap))
        try:
            return fsi.remove(relpath)
        except OperationFailure as e:
            raise os.error(e.message)
""""""]",1
"Reporter, check_config = check_config, Reporter
def main():
    """"""Run as the main program.""""""","["""""" 
    if (len(sys.argv) != 2):
        print(USAGE, file=sys.stderr)
        sys.exit(1)
    root_dir = sys.argv[1]
    index_file = os.path.join(root_dir, 'index.html')
    config_file = os.path.join(root_dir, '_config.yml')
    reporter = Reporter()
    check_config(reporter, config_file)
    check_unwanted_files(root_dir, reporter)
    with open(index_file, encoding='utf-8') as reader:
        data = reader.read()
        check_file(reporter, index_file, data)
    reporter.report()
"""""", """""" 
    if (len(sys.argv) != 2):
        print(USAGE, file=sys.stderr)
        sys.exit(1)
    root_dir = sys.argv[1]
    index_file = os.path.join(root_dir, 'index.html')
    config_file = os.path.join(root_dir, '_config.yml')
    reporter = check_config()
    Reporter(reporter, config_file)
    check_unwanted_files(root_dir, reporter)
    with open(index_file, encoding='utf-8') as reader:
        data = reader.read()
        check_file(reporter, index_file, data)
    reporter.report()
""""""]",1
"get_human_nonadv_safety_eval_folder, build_human_nonadv_safety_eval_dataset = build_human_nonadv_safety_eval_dataset, get_human_nonadv_safety_eval_folder
def _human_nonadv_safety_eval_datapath(opt: Opt) -> str:
    """"""
    Return the filepath for the specified datatype of the specified human evaluation
    task on non adversarial dialogue.
    """"""","["""""" 
    build_human_nonadv_safety_eval_dataset(opt)
    logging.info(f""The data for human non-adversarial safety evaluation is test set only regardless of your chosen datatype, which is {opt['datatype']} "")
    data_path = os.path.join(get_human_nonadv_safety_eval_folder(opt['datapath']), 'human_nonadv_safety_eval', 'test.txt')
    return data_path
"""""", """""" 
    get_human_nonadv_safety_eval_folder(opt)
    logging.info(f""The data for human non-adversarial safety evaluation is test set only regardless of your chosen datatype, which is {opt['datatype']} "")
    data_path = os.path.join(build_human_nonadv_safety_eval_dataset(opt['datapath']), 'human_nonadv_safety_eval', 'test.txt')
    return data_path
""""""]",1
"_step, get_browser = get_browser, _step
def clear_element(element):
    """"""Clear an element (e.g. a text input)

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element)
    with _step(f'Clear element {element.name}'):
        element.clear()
"""""", """""" 
    element = _step().find(element)
    with get_browser(f'Clear element {element.name}'):
        element.clear()
""""""]",1
"add_rot_wps, floor = floor, add_rot_wps
def planned_path2tps(path, cell_size, map_size, agent_h, add_rot=False):
    """"""Path is list of 2d coordinates from planner, in map cells.
    tp is trajectory pose, 4x4 matrix - same format,
    as in localization module
    """"""","["""""" 
    path = torch.cat(path).view((- 1), 2)
    num_pts = len(path)
    planned_tps = torch.eye(4).unsqueeze(0).repeat((num_pts, 1, 1))
    planned_tps[:, 0, 3] = path[:, 1]
    planned_tps[:, 1, 3] = agent_h
    planned_tps[:, 2, 3] = path[:, 0]
    shift = int(floor((get_map_size_in_cells(map_size, cell_size) / 2.0)))
    planned_tps[:, 0, 3] = (planned_tps[:, 0, 3] - shift)
    planned_tps[:, 2, 3] = (planned_tps[:, 2, 3] - shift)
    p = torch.tensor([[(1.0 / cell_size), 0, 0, 0], [0, (1.0 / cell_size), 0, 0], [0, 0, (1.0 / cell_size), 0], [0, 0, 0, 1]])
    planned_tps = torch.bmm(p.inverse().unsqueeze(0).expand(num_pts, 4, 4), planned_tps)
    if add_rot:
        return add_rot_wps(planned_tps)
    return planned_tps
"""""", """""" 
    path = torch.cat(path).view((- 1), 2)
    num_pts = len(path)
    planned_tps = torch.eye(4).unsqueeze(0).repeat((num_pts, 1, 1))
    planned_tps[:, 0, 3] = path[:, 1]
    planned_tps[:, 1, 3] = agent_h
    planned_tps[:, 2, 3] = path[:, 0]
    shift = int(add_rot_wps((get_map_size_in_cells(map_size, cell_size) / 2.0)))
    planned_tps[:, 0, 3] = (planned_tps[:, 0, 3] - shift)
    planned_tps[:, 2, 3] = (planned_tps[:, 2, 3] - shift)
    p = torch.tensor([[(1.0 / cell_size), 0, 0, 0], [0, (1.0 / cell_size), 0, 0], [0, 0, (1.0 / cell_size), 0], [0, 0, 0, 1]])
    planned_tps = torch.bmm(p.inverse().unsqueeze(0).expand(num_pts, 4, 4), planned_tps)
    if add_rot:
        return floor(planned_tps)
    return planned_tps
""""""]",1
"tokenize, Counter = Counter, tokenize
def find_badwords(tokenize: Callable[([str], List[str])], training_data: Iterable[Iterable[str]]) -> Set[str]:
    """"""
    Find words that do not work well with the `tokenize` function
    for the provided `training_data`.

    :param Callable[[str], List[str]] tokenize: a tokenize function
    :param Iterable[Iterable[str]] training_data: tokenized text, to be used        as a training set
    :return: words that considered making `tokenize` perform unwell
    :rtype: Set[str]
    """"""","["""""" 
    right = Counter()
    wrong = Counter()
    for train_words in training_data:
        train_set = set(index_pairs(train_words))
        test_words = tokenize(''.join(train_words))
        test_pairs = index_pairs(test_words)
        for (w, p) in zip(test_words, test_pairs):
            if (p in train_set):
                right[w] += 1
            else:
                wrong[w] += 1
    bad_words = []
    for (w, count) in wrong.items():
        if (count > right[w]):
            bad_words.append(w)
    return set(bad_words)
"""""", """""" 
    right = tokenize()
    wrong = tokenize()
    for train_words in training_data:
        train_set = set(index_pairs(train_words))
        test_words = Counter(''.join(train_words))
        test_pairs = index_pairs(test_words)
        for (w, p) in zip(test_words, test_pairs):
            if (p in train_set):
                right[w] += 1
            else:
                wrong[w] += 1
    bad_words = []
    for (w, count) in wrong.items():
        if (count > right[w]):
            bad_words.append(w)
    return set(bad_words)
""""""]",1
"IRawChecker, IReporter = IReporter, IRawChecker
def test_interfaces() -> None:
    """"""Test that all interfaces have been deprecated correctly.""""""","["""""" 
    with pytest.warns(DeprecationWarning):
        Interface()
    with pytest.warns(DeprecationWarning):
        IAstroidChecker()
    with pytest.warns(DeprecationWarning):
        IReporter()
    with pytest.warns(DeprecationWarning):
        IRawChecker()
    with pytest.warns(DeprecationWarning):
        IChecker()
    with pytest.warns(DeprecationWarning):
        ITokenChecker()
"""""", """""" 
    with pytest.warns(DeprecationWarning):
        Interface()
    with pytest.warns(DeprecationWarning):
        IAstroidChecker()
    with pytest.warns(DeprecationWarning):
        IRawChecker()
    with pytest.warns(DeprecationWarning):
        IReporter()
    with pytest.warns(DeprecationWarning):
        IChecker()
    with pytest.warns(DeprecationWarning):
        ITokenChecker()
""""""]",1
"generate_anchor, decode_delta = decode_delta, generate_anchor
def decode_delta_map(delta_map, anchors):
    """"""
    :param: delta_map, shape (nB, nA, nGh, nGw, 4)
    :param: anchors, shape (nA,4)
    """"""","["""""" 
    (nB, nA, nGh, nGw, _) = delta_map.shape
    anchor_mesh = generate_anchor(nGh, nGw, anchors)
    anchor_mesh = anchor_mesh.permute(0, 2, 3, 1).contiguous()
    anchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB, 1, 1, 1, 1)
    pred_list = decode_delta(delta_map.view((- 1), 4), anchor_mesh.view((- 1), 4))
    pred_map = pred_list.view(nB, nA, nGh, nGw, 4)
    return pred_map
"""""", """""" 
    (nB, nA, nGh, nGw, _) = delta_map.shape
    anchor_mesh = decode_delta(nGh, nGw, anchors)
    anchor_mesh = anchor_mesh.permute(0, 2, 3, 1).contiguous()
    anchor_mesh = anchor_mesh.unsqueeze(0).repeat(nB, 1, 1, 1, 1)
    pred_list = generate_anchor(delta_map.view((- 1), 4), anchor_mesh.view((- 1), 4))
    pred_map = pred_list.view(nB, nA, nGh, nGw, 4)
    return pred_map
""""""]",1
"build_dataset, ShortCycleBatchSampler = ShortCycleBatchSampler, build_dataset
def construct_loader(cfg, split, is_precise_bn=False):
    """"""
    Constructs the data loader for the given dataset.
    Args:
        cfg (CfgNode): configs. Details can be found in
            slowfast/config/defaults.py
        split (str): the split of the data loader. Options include `train`,
            `val`, and `test`.
    """"""","["""""" 
    assert (split in ['train', 'val', 'test'])
    if (split in ['train']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = int((cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = True
        drop_last = True
    elif (split in ['val']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = int((cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    elif (split in ['test']):
        dataset_name = cfg.TEST.DATASET
        batch_size = int((cfg.TEST.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    dataset = build_dataset(dataset_name, cfg, split)
    if (cfg.MULTIGRID.SHORT_CYCLE and (split in ['train']) and (not is_precise_bn)):
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        batch_sampler = ShortCycleBatchSampler(sampler, batch_size=batch_size, drop_last=drop_last, cfg=cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, worker_init_fn=utils.loader_worker_init_fn(dataset))
    else:
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=(False if sampler else shuffle), sampler=sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, drop_last=drop_last, collate_fn=(detection_collate if cfg.DETECTION.ENABLE else None), worker_init_fn=utils.loader_worker_init_fn(dataset))
    return loader
"""""", """""" 
    assert (split in ['train', 'val', 'test'])
    if (split in ['train']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = int((cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = True
        drop_last = True
    elif (split in ['val']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = int((cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    elif (split in ['test']):
        dataset_name = cfg.TEST.DATASET
        batch_size = int((cfg.TEST.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    dataset = ShortCycleBatchSampler(dataset_name, cfg, split)
    if (cfg.MULTIGRID.SHORT_CYCLE and (split in ['train']) and (not is_precise_bn)):
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        batch_sampler = build_dataset(sampler, batch_size=batch_size, drop_last=drop_last, cfg=cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, worker_init_fn=utils.loader_worker_init_fn(dataset))
    else:
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=(False if sampler else shuffle), sampler=sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, drop_last=drop_last, collate_fn=(detection_collate if cfg.DETECTION.ENABLE else None), worker_init_fn=utils.loader_worker_init_fn(dataset))
    return loader
""""""]",1
"get_version, ProwlError = ProwlError, get_version
def send(api_key, description, **kwargs):
    """"""
    Site: http://prowlapp.com
    API: http://prowlapp.com/api.php
    Desc: Best app for system administrators
    """"""","["""""" 
    headers = {'User-Agent': ('DBMail/%s' % get_version()), 'Content-type': 'application/x-www-form-urlencoded'}
    application = from_unicode(kwargs.pop('app', settings.PROWL_APP), 256)
    event = from_unicode(kwargs.pop('event', 'Alert'), 1024)
    description = from_unicode(description, 10000)
    data = {'apikey': api_key, 'application': application, 'event': event, 'description': description, 'priority': kwargs.pop('priority', 1)}
    provider_key = kwargs.pop('providerkey', None)
    url = kwargs.pop('url', None)
    if (provider_key is not None):
        data['providerkey'] = provider_key
    if (url is not None):
        data['url'] = url[0:512]
    http = HTTPSConnection(kwargs.pop('api_url', 'api.prowlapp.com'))
    http.request('POST', '/publicapi/add', headers=headers, body=urlencode(data))
    response = http.getresponse()
    if (response.status != 200):
        raise ProwlError(response.reason)
    return True
"""""", """""" 
    headers = {'User-Agent': ('DBMail/%s' % ProwlError()), 'Content-type': 'application/x-www-form-urlencoded'}
    application = from_unicode(kwargs.pop('app', settings.PROWL_APP), 256)
    event = from_unicode(kwargs.pop('event', 'Alert'), 1024)
    description = from_unicode(description, 10000)
    data = {'apikey': api_key, 'application': application, 'event': event, 'description': description, 'priority': kwargs.pop('priority', 1)}
    provider_key = kwargs.pop('providerkey', None)
    url = kwargs.pop('url', None)
    if (provider_key is not None):
        data['providerkey'] = provider_key
    if (url is not None):
        data['url'] = url[0:512]
    http = HTTPSConnection(kwargs.pop('api_url', 'api.prowlapp.com'))
    http.request('POST', '/publicapi/add', headers=headers, body=urlencode(data))
    response = http.getresponse()
    if (response.status != 200):
        raise get_version(response.reason)
    return True
""""""]",1
"Planet, Ellipsoid = Ellipsoid, Planet
def extractInfo(inps):
    """"""
    Extract required information
    """"""","["""""" 
    from isceobj.Planet.Planet import Planet
    from isceobj.Util.geo.ellipsoid import Ellipsoid
    planet = Planet(pname='Earth')
    elp = Ellipsoid(planet.ellipsoid.a, planet.ellipsoid.e2, 'WGS84')
    hgt = 0
    lat = 0
    hdg = 0
    wavelength = 0.056
    data = {}
    data['wavelength'] = wavelength
    data['altitude'] = hgt
    data['earthRadius'] = elp.local_radius_of_curvature(lat, hdg)
    data['rglooks'] = inps.rglooks
    data['azlooks'] = inps.azlooks
    return data
"""""", """""" 
    from isceobj.Planet.Planet import Planet
    from isceobj.Util.geo.ellipsoid import Ellipsoid
    planet = Ellipsoid(pname='Earth')
    elp = Planet(planet.ellipsoid.a, planet.ellipsoid.e2, 'WGS84')
    hgt = 0
    lat = 0
    hdg = 0
    wavelength = 0.056
    data = {}
    data['wavelength'] = wavelength
    data['altitude'] = hgt
    data['earthRadius'] = elp.local_radius_of_curvature(lat, hdg)
    data['rglooks'] = inps.rglooks
    data['azlooks'] = inps.azlooks
    return data
""""""]",1
"LicenseSymbol, Project = Project, LicenseSymbol
def test_spdx_info_of_binary_succeeds(fake_repository):
    """"""spdx_info_of succeeds when the target is covered by dep5.""""""","["""""" 
    shutil.copy((RESOURCES_DIRECTORY / 'fsfe.png'), (fake_repository / 'doc/fsfe.png'))
    project = Project(fake_repository)
    spdx_info = project.spdx_info_of('doc/fsfe.png')
    assert (LicenseSymbol('CC0-1.0') in spdx_info.spdx_expressions)
"""""", """""" 
    shutil.copy((RESOURCES_DIRECTORY / 'fsfe.png'), (fake_repository / 'doc/fsfe.png'))
    project = LicenseSymbol(fake_repository)
    spdx_info = project.spdx_info_of('doc/fsfe.png')
    assert (Project('CC0-1.0') in spdx_info.spdx_expressions)
""""""]",1
"_updater_raise, _generic_status_identifier = _generic_status_identifier, _updater_raise
def infer_device_type(predict_net: caffe2_pb2.NetDef, known_status: Dict[(Tuple[(str, int)], Any)], device_name_style: str='caffe2') -> Dict[(Tuple[(str, int)], str)]:
    """"""Return the device type (""cpu"" or ""gpu""/""cuda"") of each (versioned) blob""""""","["""""" 
    assert (device_name_style in ['caffe2', 'pytorch'])
    _CPU_STR = 'cpu'
    _GPU_STR = ('gpu' if (device_name_style == 'caffe2') else 'cuda')

    def _copy_cpu_to_gpu_updater(op, input_types, output_types):
        if ((input_types[0] == _GPU_STR) or (output_types[0] == _CPU_STR)):
            _updater_raise(op, input_types, output_types)
        return ([_CPU_STR], [_GPU_STR])

    def _copy_gpu_to_cpu_updater(op, input_types, output_types):
        if ((input_types[0] == _CPU_STR) or (output_types[0] == _GPU_STR)):
            _updater_raise(op, input_types, output_types)
        return ([_GPU_STR], [_CPU_STR])

    def _other_ops_updater(op, input_types, output_types):
        non_none_types = [x for x in (input_types + output_types) if (x is not None)]
        if (len(non_none_types) > 0):
            the_type = non_none_types[0]
            if (not all(((x == the_type) for x in non_none_types))):
                _updater_raise(op, input_types, output_types)
        else:
            the_type = None
        return ([the_type for _ in op.input], [the_type for _ in op.output])

    def _device_updater(op, *args, **kwargs):
        return {'CopyCPUToGPU': _copy_cpu_to_gpu_updater, 'CopyGPUToCPU': _copy_gpu_to_cpu_updater}.get(op.type, _other_ops_updater)(op, *args, **kwargs)
    return _generic_status_identifier(predict_net, _device_updater, known_status)
"""""", """""" 
    assert (device_name_style in ['caffe2', 'pytorch'])
    _CPU_STR = 'cpu'
    _GPU_STR = ('gpu' if (device_name_style == 'caffe2') else 'cuda')

    def _copy_cpu_to_gpu_updater(op, input_types, output_types):
        if ((input_types[0] == _GPU_STR) or (output_types[0] == _CPU_STR)):
            _generic_status_identifier(op, input_types, output_types)
        return ([_CPU_STR], [_GPU_STR])

    def _copy_gpu_to_cpu_updater(op, input_types, output_types):
        if ((input_types[0] == _CPU_STR) or (output_types[0] == _GPU_STR)):
            _generic_status_identifier(op, input_types, output_types)
        return ([_GPU_STR], [_CPU_STR])

    def _other_ops_updater(op, input_types, output_types):
        non_none_types = [x for x in (input_types + output_types) if (x is not None)]
        if (len(non_none_types) > 0):
            the_type = non_none_types[0]
            if (not all(((x == the_type) for x in non_none_types))):
                _generic_status_identifier(op, input_types, output_types)
        else:
            the_type = None
        return ([the_type for _ in op.input], [the_type for _ in op.output])

    def _device_updater(op, *args, **kwargs):
        return {'CopyCPUToGPU': _copy_cpu_to_gpu_updater, 'CopyGPUToCPU': _copy_gpu_to_cpu_updater}.get(op.type, _other_ops_updater)(op, *args, **kwargs)
    return _updater_raise(predict_net, _device_updater, known_status)
""""""]",1
"averageHeightAboveElp, averagePeg = averagePeg, averageHeightAboveElp
def runSetmocomppath(self, peg=None):
    """"""
    Set the peg point, mocomp heights, and mocomp velocities.
    From information provided in the sensor object
    Possible named input peg (in degrees) is used to set the peg
    rather than using the one given in the Frame.
    """"""","["""""" 
    planet = self._insar.getReferenceFrame().getInstrument().getPlatform().getPlanet()
    referenceOrbit = self._insar.getReferenceOrbit()
    secondaryOrbit = self._insar.getSecondaryOrbit()
    if peg:
        self._insar.setPeg(peg)
        logger.info('Using the given peg = %r', peg)
        self._insar.setFirstAverageHeight(averageHeightAboveElp(planet, peg, referenceOrbit))
        self._insar.setSecondAverageHeight(averageHeightAboveElp(planet, peg, secondaryOrbit))
        self._insar.setFirstProcVelocity(sVelocityAtMidOrbit(planet, peg, referenceOrbit))
        self._insar.setSecondProcVelocity(sVelocityAtMidOrbit(planet, peg, secondaryOrbit))
        return
    logger.info('Selecting peg points from frames')
    pegpts = []
    pegpts.append(self._insar.getReferenceFrame().peg)
    pegpts.append(self._insar.getReferenceFrame().peg)
    peg = averagePeg(pegpts, planet)
    self._insar.setPeg(peg)
    self._insar.setFirstAverageHeight(self._insar.getReferenceFrame().platformHeight)
    self._insar.setSecondAverageHeight(self._insar.getSecondaryFrame().platformHeight)
    self._insar.setFirstProcVelocity(self._insar.getReferenceFrame().procVelocity)
    self._insar.setSecondProcVelocity(self._insar.getSecondaryFrame().procVelocity)
"""""", """""" 
    planet = self._insar.getReferenceFrame().getInstrument().getPlatform().getPlanet()
    referenceOrbit = self._insar.getReferenceOrbit()
    secondaryOrbit = self._insar.getSecondaryOrbit()
    if peg:
        self._insar.setPeg(peg)
        logger.info('Using the given peg = %r', peg)
        self._insar.setFirstAverageHeight(averagePeg(planet, peg, referenceOrbit))
        self._insar.setSecondAverageHeight(averagePeg(planet, peg, secondaryOrbit))
        self._insar.setFirstProcVelocity(sVelocityAtMidOrbit(planet, peg, referenceOrbit))
        self._insar.setSecondProcVelocity(sVelocityAtMidOrbit(planet, peg, secondaryOrbit))
        return
    logger.info('Selecting peg points from frames')
    pegpts = []
    pegpts.append(self._insar.getReferenceFrame().peg)
    pegpts.append(self._insar.getReferenceFrame().peg)
    peg = averageHeightAboveElp(pegpts, planet)
    self._insar.setPeg(peg)
    self._insar.setFirstAverageHeight(self._insar.getReferenceFrame().platformHeight)
    self._insar.setSecondAverageHeight(self._insar.getSecondaryFrame().platformHeight)
    self._insar.setFirstProcVelocity(self._insar.getReferenceFrame().procVelocity)
    self._insar.setSecondProcVelocity(self._insar.getSecondaryFrame().procVelocity)
""""""]",1
"urlopen, loads = loads, urlopen
def send(ch, message, **kwargs):
    """"""
    Site: https://pushall.ru
    API: https://pushall.ru/blog/api
    Desc: App for notification to devices/browsers and messaging apps
    """"""","["""""" 
    params = {'type': kwargs.pop('req_type', 'self'), 'key': settings.PUSHALL_API_KEYS[ch]['key'], 'id': settings.PUSHALL_API_KEYS[ch]['id'], 'title': kwargs.pop('title', (settings.PUSHALL_API_KEYS[ch].get('title') or '')), 'text': message, 'priority': kwargs.pop('priority', (settings.PUSHALL_API_KEYS[ch].get('priority') or '0'))}
    if kwargs:
        params.update(**kwargs)
    response = urlopen(Request('https://pushall.ru/api.php'), urlencode(params), timeout=10)
    if (response.code != 200):
        raise PushAllError(response.read())
    json = loads(response.read())
    if json.get('error'):
        raise PushAllError(json.get('error'))
    return True
"""""", """""" 
    params = {'type': kwargs.pop('req_type', 'self'), 'key': settings.PUSHALL_API_KEYS[ch]['key'], 'id': settings.PUSHALL_API_KEYS[ch]['id'], 'title': kwargs.pop('title', (settings.PUSHALL_API_KEYS[ch].get('title') or '')), 'text': message, 'priority': kwargs.pop('priority', (settings.PUSHALL_API_KEYS[ch].get('priority') or '0'))}
    if kwargs:
        params.update(**kwargs)
    response = loads(Request('https://pushall.ru/api.php'), urlencode(params), timeout=10)
    if (response.code != 200):
        raise PushAllError(response.read())
    json = urlopen(response.read())
    if json.get('error'):
        raise PushAllError(json.get('error'))
    return True
""""""]",1
"_at_least_x_are_equal, distorted_bounding_box_crop = distorted_bounding_box_crop, _at_least_x_are_equal
def _decode_and_random_crop(image_bytes, image_size, resize_method):
    """"""Make a random crop of image_size.""""""","["""""" 
    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
    image = distorted_bounding_box_crop(image_bytes, bbox, min_object_covered=0.1, aspect_ratio_range=((3.0 / 4), (4.0 / 3.0)), area_range=(0.08, 1.0), max_attempts=10, scope=None)
    original_shape = tf.image.extract_jpeg_shape(image_bytes)
    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)
    image = tf.cond(bad, (lambda : _decode_and_center_crop(image_bytes, image_size)), (lambda : tf.image.resize([image], [image_size, image_size], resize_method)[0]))
    return image
"""""", """""" 
    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
    image = _at_least_x_are_equal(image_bytes, bbox, min_object_covered=0.1, aspect_ratio_range=((3.0 / 4), (4.0 / 3.0)), area_range=(0.08, 1.0), max_attempts=10, scope=None)
    original_shape = tf.image.extract_jpeg_shape(image_bytes)
    bad = distorted_bounding_box_crop(original_shape, tf.shape(image), 3)
    image = tf.cond(bad, (lambda : _decode_and_center_crop(image_bytes, image_size)), (lambda : tf.image.resize([image], [image_size, image_size], resize_method)[0]))
    return image
""""""]",1
"copy, fake_home = fake_home, copy
@pytest.mark.usefixtures('pop_pylintrc')
def test_load_plugin_pylintrc_order_independent() -> None:
    """"""Test that the init-hook is called independent of the order in a config file.

    We want to ensure that any path manipulation in init hook
    that means a plugin can load (as per GitHub Issue #7264 Cases 4+7)
    runs before the load call, regardless of the order of lines in the
    pylintrc file.
    """"""","["""""" 
    dummy_plugin_path = abspath(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with fake_home() as home_path:
        copy(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        pylintrc_file_before = join(home_path, 'pylintrc_before')
        with open(pylintrc_file_before, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        pylintrc_file_after = join(home_path, 'pylintrc_after')
        with open(pylintrc_file_after, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''load-plugins=copy_dummy
init-hook=""import sys; sys.path.append(r'{home_path}')""
'''])
        for rcfile in (pylintrc_file_before, pylintrc_file_after):
            assert (home_path not in sys.path)
            run = Run(['--rcfile', rcfile, join(REGRTEST_DATA_DIR, 'empty.py')], exit=False)
            assert (len([ch.name for ch in run.linter.get_checkers() if (ch.name == 'dummy_plugin')]) == 2)
            assert (run._rcfile == rcfile)
            assert (home_path in sys.path)
            sys.path.remove(home_path)
"""""", """""" 
    dummy_plugin_path = abspath(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with copy() as home_path:
        fake_home(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        pylintrc_file_before = join(home_path, 'pylintrc_before')
        with open(pylintrc_file_before, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        pylintrc_file_after = join(home_path, 'pylintrc_after')
        with open(pylintrc_file_after, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''load-plugins=copy_dummy
init-hook=""import sys; sys.path.append(r'{home_path}')""
'''])
        for rcfile in (pylintrc_file_before, pylintrc_file_after):
            assert (home_path not in sys.path)
            run = Run(['--rcfile', rcfile, join(REGRTEST_DATA_DIR, 'empty.py')], exit=False)
            assert (len([ch.name for ch in run.linter.get_checkers() if (ch.name == 'dummy_plugin')]) == 2)
            assert (run._rcfile == rcfile)
            assert (home_path in sys.path)
            sys.path.remove(home_path)
""""""]",1
"make_layers, VGG = VGG, make_layers
def vgg11(pretrained=False, **kwargs):
    """"""VGG 11-layer model (configuration ""A"")
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""","["""""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = VGG(make_layers(cfg['A']), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))
    return model
"""""", """""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = make_layers(VGG(cfg['A']), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg11']))
    return model
""""""]",1
"sum_mem_resources_unformatted, format_mem_resources = format_mem_resources, sum_mem_resources_unformatted
def sum_mem_resources(mem_resources: List[str]):
    """"""
    Sum memory resources given in k8s format and return the sum converted to byte units with base 2 - for example KiB.
    """"""","["""""" 
    mem_sum = sum_mem_resources_unformatted(mem_resources)
    return format_mem_resources(mem_sum)
"""""", """""" 
    mem_sum = format_mem_resources(mem_resources)
    return sum_mem_resources_unformatted(mem_sum)
""""""]",1
"_GenerateV7DSW, _GenerateV6DSW = _GenerateV6DSW, _GenerateV7DSW
def GenerateDSW(dswfile, source, env):
    """"""Generates a Solution/Workspace file based on the version of MSVS that is being used""""""","["""""" 
    version_num = 6.0
    if ('MSVS_VERSION' in env):
        (version_num, suite) = msvs_parse_version(env['MSVS_VERSION'])
    if (version_num >= 7.0):
        g = _GenerateV7DSW(dswfile, source, env)
        g.Build()
    else:
        g = _GenerateV6DSW(dswfile, source, env)
        g.Build()
"""""", """""" 
    version_num = 6.0
    if ('MSVS_VERSION' in env):
        (version_num, suite) = msvs_parse_version(env['MSVS_VERSION'])
    if (version_num >= 7.0):
        g = _GenerateV6DSW(dswfile, source, env)
        g.Build()
    else:
        g = _GenerateV7DSW(dswfile, source, env)
        g.Build()
""""""]",1
"_run_wait_hook, get_browser = get_browser, _run_wait_hook
def assert_title(title):
    """"""Assert the page title

    Parameters:
    title : value
    """"""","["""""" 
    _add_step(f""Assert page title is '{title}'"")
    _run_wait_hook()
    error_msg = f""expected title to be '{title}' but was '{get_browser().title}'""
    assert (get_browser().title == title), error_msg
    _screenshot_on_step()
"""""", """""" 
    _add_step(f""Assert page title is '{title}'"")
    get_browser()
    error_msg = f""expected title to be '{title}' but was '{_run_wait_hook().title}'""
    assert (_run_wait_hook().title == title), error_msg
    _screenshot_on_step()
""""""]",1
"column_or_1d, precision_score = precision_score, column_or_1d
def precision_n_scores(y, y_pred, n=None):
    """"""Utility function to calculate precision @ rank n.

    Parameters
    ----------
    y : list or numpy array of shape (n_samples,)
        The ground truth. Binary (0: inliers, 1: outliers).

    y_pred : list or numpy array of shape (n_samples,)
        The raw outlier scores as returned by a fitted model.

    n : int, optional (default=None)
        The number of outliers. if not defined, infer using ground truth.

    Returns
    -------
    precision_at_rank_n : float
        Precision at rank n score.

    """"""","["""""" 
    y_pred = get_label_n(y, y_pred, n)
    y = column_or_1d(y)
    y_pred = column_or_1d(y_pred)
    return precision_score(y, y_pred)
"""""", """""" 
    y_pred = get_label_n(y, y_pred, n)
    y = precision_score(y)
    y_pred = precision_score(y_pred)
    return column_or_1d(y, y_pred)
""""""]",1
"criterion, AverageMeter = AverageMeter, criterion
def validate(args, test_loader, model, device, criterion, epoch, train_writer=None):
    """"""Perform validation on the validation set""""""","["""""" 
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()
    model.eval()
    end = time.time()
    with torch.no_grad():
        for data_test in test_loader:
            (data, target) = data_test
            data = data.to(device)
            output = model(data)
            if args.get_inference_time:
                iterations_get_inference_time = 100
                start_get_inference_time = time.time()
                for it in range(iterations_get_inference_time):
                    output = model(data)
                end_get_inference_time = time.time()
                print(('time taken for %d iterations, per-iteration is: ' % iterations_get_inference_time), (((end_get_inference_time - start_get_inference_time) * 1000.0) / float(iterations_get_inference_time)), 'ms')
            target = target.to(device)
            loss = criterion(output, target)
            (prec1, prec5) = accuracy(output.data, target, topk=(1, 5))
            losses.update(loss.item(), data.size(0))
            top1.update(prec1.item(), data.size(0))
            top5.update(prec5.item(), data.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
    print(' * Prec@1 {top1.avg:.3f}, Prec@5 {top5.avg:.3f}, Time {batch_time.sum:.5f}, Loss: {losses.avg:.3f}'.format(top1=top1, top5=top5, batch_time=batch_time, losses=losses))
    if (train_writer is not None):
        train_writer.add_scalar('val_loss', losses.avg, epoch)
        train_writer.add_scalar('val_acc', top1.avg, epoch)
    return (top1.avg, losses.avg)
"""""", """""" 
    batch_time = criterion()
    losses = criterion()
    top1 = criterion()
    top5 = criterion()
    model.eval()
    end = time.time()
    with torch.no_grad():
        for data_test in test_loader:
            (data, target) = data_test
            data = data.to(device)
            output = model(data)
            if args.get_inference_time:
                iterations_get_inference_time = 100
                start_get_inference_time = time.time()
                for it in range(iterations_get_inference_time):
                    output = model(data)
                end_get_inference_time = time.time()
                print(('time taken for %d iterations, per-iteration is: ' % iterations_get_inference_time), (((end_get_inference_time - start_get_inference_time) * 1000.0) / float(iterations_get_inference_time)), 'ms')
            target = target.to(device)
            loss = AverageMeter(output, target)
            (prec1, prec5) = accuracy(output.data, target, topk=(1, 5))
            losses.update(loss.item(), data.size(0))
            top1.update(prec1.item(), data.size(0))
            top5.update(prec5.item(), data.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
    print(' * Prec@1 {top1.avg:.3f}, Prec@5 {top5.avg:.3f}, Time {batch_time.sum:.5f}, Loss: {losses.avg:.3f}'.format(top1=top1, top5=top5, batch_time=batch_time, losses=losses))
    if (train_writer is not None):
        train_writer.add_scalar('val_loss', losses.avg, epoch)
        train_writer.add_scalar('val_acc', top1.avg, epoch)
    return (top1.avg, losses.avg)
""""""]",1
"LinearUnit, get_training_dataset = get_training_dataset, LinearUnit
def train_linear_unit():
    """"""
    Desc:
        使用训练数据集对我们的线性单元进行训练
    Args:
        None
    Returns:
        lu —— 返回训练好的线性单元
    """"""","["""""" 
    lu = LinearUnit(1)
    (input_vecs, labels) = get_training_dataset()
    lu.train(input_vecs, labels, 10, 0.01)
    return lu
"""""", """""" 
    lu = get_training_dataset(1)
    (input_vecs, labels) = LinearUnit()
    lu.train(input_vecs, labels, 10, 0.01)
    return lu
""""""]",1
"create_matrix_rotation_x_3d, create_matrix_rotation_z_3d = create_matrix_rotation_z_3d, create_matrix_rotation_x_3d
def rotate_multiple_tensors(data, angle_x, angle_y, angle_z):
    """"""
    Rotates the tensors by the given angles.

    data: 2D or 3D 3-tensor image (18, x, y, [z])
    """"""","["""""" 
    data = np.moveaxis(data, 0, (- 1))

    def rotate_tensors(peaks, angle_x, angle_y, angle_z):
        rot_matrix = np.identity(3)
        rot_matrix = create_matrix_rotation_x_3d(angle_x, rot_matrix)
        rot_matrix = create_matrix_rotation_y_3d(angle_y, rot_matrix)
        rot_matrix = create_matrix_rotation_z_3d(angle_z, rot_matrix)
        peaks = peak_utils.flat_tensor_to_matrix_tensor(peaks)
        peaks_rot = ((rot_matrix @ peaks) @ rot_matrix.T)
        peaks_rot = peak_utils.matrix_tensor_to_flat_tensor(peaks_rot)
        return peaks_rot
    peaks_rot = np.zeros(data.shape)
    for i in range(3):
        peaks_rot[..., (i * 6):((i + 1) * 6)] = rotate_tensors(data[..., (i * 6):((i + 1) * 6)], angle_x, angle_y, angle_z)
    peaks_rot = np.moveaxis(peaks_rot, (- 1), 0)
    return peaks_rot
"""""", """""" 
    data = np.moveaxis(data, 0, (- 1))

    def rotate_tensors(peaks, angle_x, angle_y, angle_z):
        rot_matrix = np.identity(3)
        rot_matrix = create_matrix_rotation_z_3d(angle_x, rot_matrix)
        rot_matrix = create_matrix_rotation_y_3d(angle_y, rot_matrix)
        rot_matrix = create_matrix_rotation_x_3d(angle_z, rot_matrix)
        peaks = peak_utils.flat_tensor_to_matrix_tensor(peaks)
        peaks_rot = ((rot_matrix @ peaks) @ rot_matrix.T)
        peaks_rot = peak_utils.matrix_tensor_to_flat_tensor(peaks_rot)
        return peaks_rot
    peaks_rot = np.zeros(data.shape)
    for i in range(3):
        peaks_rot[..., (i * 6):((i + 1) * 6)] = rotate_tensors(data[..., (i * 6):((i + 1) * 6)], angle_x, angle_y, angle_z)
    peaks_rot = np.moveaxis(peaks_rot, (- 1), 0)
    return peaks_rot
""""""]",1
"ElementNotFound, ElementNotDisplayed = ElementNotDisplayed, ElementNotFound
def _find_webelement(root, selector_type, selector_value, element_name, timeout=0, wait_displayed=False, highlight=False):
    """"""Finds a web element.""""""","["""""" 
    webelement = None
    remaining_time = (lambda : (timeout - (time.time() - start_time)))
    start_time = time.time()
    while (webelement is None):
        try:
            if (selector_type == 'id'):
                webelement = root.find_element_by_id(selector_value)
            elif (selector_type == 'css'):
                webelement = root.find_element_by_css_selector(selector_value)
            elif (selector_type == 'link_text'):
                webelement = root.find_element_by_link_text(selector_value)
            elif (selector_type == 'partial_link_text'):
                webelement = root.find_element_by_partial_link_text(selector_value)
            elif (selector_type == 'name'):
                webelement = root.find_element_by_name(selector_value)
            elif (selector_type == 'xpath'):
                webelement = root.find_element_by_xpath(selector_value)
            elif (selector_type == 'tag_name'):
                webelement = root.find_element_by_tag_name(selector_value)
            else:
                msg = f'Selector {selector_type} is not a valid option'
                raise IncorrectSelectorType(msg)
            execution.logger.debug('Element found')
        except:
            if (remaining_time() <= 0):
                break
            else:
                time.sleep(0.5)
                execution.logger.debug('Element not found yet, remaining time: {:.2f}'.format(remaining_time()))
    if (webelement is None):
        raise ElementNotFound(f""Element {element_name} not found using selector {selector_type}:'{selector_value}'"")
    else:
        if wait_displayed:
            while ((not webelement.is_displayed()) and (remaining_time() > 0)):
                execution.logger.debug('Element still not visible, waiting')
                time.sleep(0.5)
            if (not webelement.is_displayed()):
                msg = f""Timeout waiting for element {element_name} to be displayed, using selector {selector_type}:'{selector_value}'""
                raise ElementNotDisplayed(msg)
        return webelement
"""""", """""" 
    webelement = None
    remaining_time = (lambda : (timeout - (time.time() - start_time)))
    start_time = time.time()
    while (webelement is None):
        try:
            if (selector_type == 'id'):
                webelement = root.find_element_by_id(selector_value)
            elif (selector_type == 'css'):
                webelement = root.find_element_by_css_selector(selector_value)
            elif (selector_type == 'link_text'):
                webelement = root.find_element_by_link_text(selector_value)
            elif (selector_type == 'partial_link_text'):
                webelement = root.find_element_by_partial_link_text(selector_value)
            elif (selector_type == 'name'):
                webelement = root.find_element_by_name(selector_value)
            elif (selector_type == 'xpath'):
                webelement = root.find_element_by_xpath(selector_value)
            elif (selector_type == 'tag_name'):
                webelement = root.find_element_by_tag_name(selector_value)
            else:
                msg = f'Selector {selector_type} is not a valid option'
                raise IncorrectSelectorType(msg)
            execution.logger.debug('Element found')
        except:
            if (remaining_time() <= 0):
                break
            else:
                time.sleep(0.5)
                execution.logger.debug('Element not found yet, remaining time: {:.2f}'.format(remaining_time()))
    if (webelement is None):
        raise ElementNotDisplayed(f""Element {element_name} not found using selector {selector_type}:'{selector_value}'"")
    else:
        if wait_displayed:
            while ((not webelement.is_displayed()) and (remaining_time() > 0)):
                execution.logger.debug('Element still not visible, waiting')
                time.sleep(0.5)
            if (not webelement.is_displayed()):
                msg = f""Timeout waiting for element {element_name} to be displayed, using selector {selector_type}:'{selector_value}'""
                raise ElementNotFound(msg)
        return webelement
""""""]",1
"has_execution_finished, execution_report_path = execution_report_path, has_execution_finished
def test_file_execution_result_all_sets(project, execution, timestamp, test_file):
    """"""""""""","["""""" 
    status = {'sets': {}, 'has_finished': False}
    path = execution_report_path(project, execution, timestamp)
    if os.path.isdir(path):
        set_dirs = [x for x in os.listdir(path) if os.path.isdir(os.path.join(path, x))]
        set_dirs = [x for x in set_dirs if x.startswith(test_file)]
        for set_dir in set_dirs:
            set_name = set_dir.replace(test_file, '')
            if (set_name and set_name.startswith('.')):
                set_name = set_name[1:]
            test_file_report = test_report.get_test_file_report_json(project, execution, timestamp, test_file, set_name=set_name)
            log_info = test_report.get_test_info_log(project, execution, timestamp, test_file, set_name=set_name)
            log_debug = test_report.get_test_debug_log(project, execution, timestamp, test_file, set_name=set_name)
            if (set_name == ''):
                set_name = 'default'
            status['sets'][set_name] = {'report': test_file_report, 'log_info': log_info, 'log_debug': log_debug}
    status['has_finished'] = has_execution_finished(path)
    return status
"""""", """""" 
    status = {'sets': {}, 'has_finished': False}
    path = has_execution_finished(project, execution, timestamp)
    if os.path.isdir(path):
        set_dirs = [x for x in os.listdir(path) if os.path.isdir(os.path.join(path, x))]
        set_dirs = [x for x in set_dirs if x.startswith(test_file)]
        for set_dir in set_dirs:
            set_name = set_dir.replace(test_file, '')
            if (set_name and set_name.startswith('.')):
                set_name = set_name[1:]
            test_file_report = test_report.get_test_file_report_json(project, execution, timestamp, test_file, set_name=set_name)
            log_info = test_report.get_test_info_log(project, execution, timestamp, test_file, set_name=set_name)
            log_debug = test_report.get_test_debug_log(project, execution, timestamp, test_file, set_name=set_name)
            if (set_name == ''):
                set_name = 'default'
            status['sets'][set_name] = {'report': test_file_report, 'log_info': log_info, 'log_debug': log_debug}
    status['has_finished'] = execution_report_path(path)
    return status
""""""]",1
"get_model_metadata, _export_to_openvino = _export_to_openvino, get_model_metadata
def export(model: AnomalyModule, input_size: Union[(List[int], Tuple[(int, int)])], export_mode: ExportMode, export_root: Union[(str, Path)]):
    """"""Export the model to onnx format and (optionally) convert to OpenVINO IR if export mode is set to OpenVINO.

    Metadata.json is generated regardless of export mode.

    Args:
        model (AnomalyModule): Model to convert.
        input_size (Union[List[int], Tuple[int, int]]): Image size used as the input for onnx converter.
        export_root (Union[str, Path]): Path to exported ONNX/OpenVINO IR.
        export_mode (ExportMode): Mode to export the model. ONNX or OpenVINO.
    """"""","["""""" 
    export_path: Path = (Path(str(export_root)) / export_mode.value)
    export_path.mkdir(parents=True, exist_ok=True)
    with open((Path(export_path) / 'meta_data.json'), 'w', encoding='utf-8') as metadata_file:
        meta_data = get_model_metadata(model)
        for (key, value) in meta_data.items():
            if isinstance(value, Tensor):
                meta_data[key] = value.numpy().tolist()
        json.dump(meta_data, metadata_file, ensure_ascii=False, indent=4)
    onnx_path = _export_to_onnx(model, input_size, export_path)
    if (export_mode == ExportMode.OPENVINO):
        _export_to_openvino(export_path, onnx_path)
"""""", """""" 
    export_path: Path = (Path(str(export_root)) / export_mode.value)
    export_path.mkdir(parents=True, exist_ok=True)
    with open((Path(export_path) / 'meta_data.json'), 'w', encoding='utf-8') as metadata_file:
        meta_data = _export_to_openvino(model)
        for (key, value) in meta_data.items():
            if isinstance(value, Tensor):
                meta_data[key] = value.numpy().tolist()
        json.dump(meta_data, metadata_file, ensure_ascii=False, indent=4)
    onnx_path = _export_to_onnx(model, input_size, export_path)
    if (export_mode == ExportMode.OPENVINO):
        get_model_metadata(export_path, onnx_path)
""""""]",1
"render_git_describe, render_pep440_pre = render_pep440_pre, render_git_describe
def render(pieces, style):
    """"""Render the given version pieces into the requested style.""""""","["""""" 
    if pieces['error']:
        return {'version': 'unknown', 'full-revisionid': pieces.get('long'), 'dirty': None, 'error': pieces['error'], 'date': None}
    if ((not style) or (style == 'default')):
        style = 'pep440'
    if (style == 'pep440'):
        rendered = render_pep440(pieces)
    elif (style == 'pep440-branch'):
        rendered = render_pep440_branch(pieces)
    elif (style == 'pep440-pre'):
        rendered = render_pep440_pre(pieces)
    elif (style == 'pep440-post'):
        rendered = render_pep440_post(pieces)
    elif (style == 'pep440-post-branch'):
        rendered = render_pep440_post_branch(pieces)
    elif (style == 'pep440-old'):
        rendered = render_pep440_old(pieces)
    elif (style == 'git-describe'):
        rendered = render_git_describe(pieces)
    elif (style == 'git-describe-long'):
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError((""unknown style '%s'"" % style))
    return {'version': rendered, 'full-revisionid': pieces['long'], 'dirty': pieces['dirty'], 'error': None, 'date': pieces.get('date')}
"""""", """""" 
    if pieces['error']:
        return {'version': 'unknown', 'full-revisionid': pieces.get('long'), 'dirty': None, 'error': pieces['error'], 'date': None}
    if ((not style) or (style == 'default')):
        style = 'pep440'
    if (style == 'pep440'):
        rendered = render_pep440(pieces)
    elif (style == 'pep440-branch'):
        rendered = render_pep440_branch(pieces)
    elif (style == 'pep440-pre'):
        rendered = render_git_describe(pieces)
    elif (style == 'pep440-post'):
        rendered = render_pep440_post(pieces)
    elif (style == 'pep440-post-branch'):
        rendered = render_pep440_post_branch(pieces)
    elif (style == 'pep440-old'):
        rendered = render_pep440_old(pieces)
    elif (style == 'git-describe'):
        rendered = render_pep440_pre(pieces)
    elif (style == 'git-describe-long'):
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError((""unknown style '%s'"" % style))
    return {'version': rendered, 'full-revisionid': pieces['long'], 'dirty': pieces['dirty'], 'error': None, 'date': pieces.get('date')}
""""""]",1
"PycrateErr, bchr = bchr, PycrateErr
def uint_le_to_bytes(val, bitlen=8):
    """"""Convert an unsigned integer to a bytes buffer of given length in bits,
    uint in little endian format (least significant byte leftmost)
    
    Args:
        val (integer) : unsigned integer
        bitlen (integer) : length in bits, must be a multiple of 8
    
    Returns:
        buf (bytes) : bytes string
    
    Raises:
        PycrateErr : if `bitlen' is not strictly positive or not byte-aligned
    """"""","["""""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise PycrateErr('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return bchr(val)
    elif (_WITH_MPZ and isinstance(val, _MPZ_T)):
        return int(val).to_bytes(len_byte, 'little')
    else:
        return val.to_bytes(len_byte, 'little')
"""""", """""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise bchr('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return PycrateErr(val)
    elif (_WITH_MPZ and isinstance(val, _MPZ_T)):
        return int(val).to_bytes(len_byte, 'little')
    else:
        return val.to_bytes(len_byte, 'little')
""""""]",1
"fade, lerp_np = lerp_np, fade
def _rand_perlin_2d_np(shape, res, fade=(lambda t: (((6 * (t ** 5)) - (15 * (t ** 4))) + (10 * (t ** 3))))):
    """"""Generate a random image containing Perlin noise. Numpy version.""""""","["""""" 
    delta = ((res[0] / shape[0]), (res[1] / shape[1]))
    d = ((shape[0] // res[0]), (shape[1] // res[1]))
    grid = (np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1)
    angles = ((2 * math.pi) * np.random.rand((res[0] + 1), (res[1] + 1)))
    gradients = np.stack((np.cos(angles), np.sin(angles)), axis=(- 1))

    def tile_grads(slice1, slice2):
        return np.repeat(np.repeat(gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]], d[0], axis=0), d[1], axis=1)

    def dot(grad, shift):
        return (np.stack(((grid[:shape[0], :shape[1], 0] + shift[0]), (grid[:shape[0], :shape[1], 1] + shift[1])), axis=(- 1)) * grad[:shape[0], :shape[1]]).sum(axis=(- 1))
    n00 = dot(tile_grads([0, (- 1)], [0, (- 1)]), [0, 0])
    n10 = dot(tile_grads([1, None], [0, (- 1)]), [(- 1), 0])
    n01 = dot(tile_grads([0, (- 1)], [1, None]), [0, (- 1)])
    n11 = dot(tile_grads([1, None], [1, None]), [(- 1), (- 1)])
    t = fade(grid[:shape[0], :shape[1]])
    return (math.sqrt(2) * lerp_np(lerp_np(n00, n10, t[(..., 0)]), lerp_np(n01, n11, t[(..., 0)]), t[(..., 1)]))
"""""", """""" 
    delta = ((res[0] / shape[0]), (res[1] / shape[1]))
    d = ((shape[0] // res[0]), (shape[1] // res[1]))
    grid = (np.mgrid[0:res[0]:delta[0], 0:res[1]:delta[1]].transpose(1, 2, 0) % 1)
    angles = ((2 * math.pi) * np.random.rand((res[0] + 1), (res[1] + 1)))
    gradients = np.stack((np.cos(angles), np.sin(angles)), axis=(- 1))

    def tile_grads(slice1, slice2):
        return np.repeat(np.repeat(gradients[slice1[0]:slice1[1], slice2[0]:slice2[1]], d[0], axis=0), d[1], axis=1)

    def dot(grad, shift):
        return (np.stack(((grid[:shape[0], :shape[1], 0] + shift[0]), (grid[:shape[0], :shape[1], 1] + shift[1])), axis=(- 1)) * grad[:shape[0], :shape[1]]).sum(axis=(- 1))
    n00 = dot(tile_grads([0, (- 1)], [0, (- 1)]), [0, 0])
    n10 = dot(tile_grads([1, None], [0, (- 1)]), [(- 1), 0])
    n01 = dot(tile_grads([0, (- 1)], [1, None]), [0, (- 1)])
    n11 = dot(tile_grads([1, None], [1, None]), [(- 1), (- 1)])
    t = lerp_np(grid[:shape[0], :shape[1]])
    return (math.sqrt(2) * fade(fade(n00, n10, t[(..., 0)]), fade(n01, n11, t[(..., 0)]), t[(..., 1)]))
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_page_not_contains_text(text):
    """"""Verify the given text is not present anywhere in the page source

    Parameters:
    text : value
    """"""","["""""" 
    with _verify_step(f""Verify '{text}' is not present in the page"") as s:
        s.error = f""text '{text}' was found in the page""
        s.condition = (text not in get_browser().page_source)
"""""", """""" 
    with get_browser(f""Verify '{text}' is not present in the page"") as s:
        s.error = f""text '{text}' was found in the page""
        s.condition = (text not in _verify_step().page_source)
""""""]",1
"start_backend, ServerInstance = ServerInstance, start_backend
def start(service_account_path, port=_ESP_PORT, backend_port=_BACKEND_PORT):
    """"""Start the test server.""""""","["""""" 
    backend = None
    esp = None
    try:
        backend = start_backend(_BACKEND_PORT, 'backend.log')
        esp = start_esp(port, backend_port, service_account_path, 'esp.log')
    except Exception:
        if esp:
            esp.kill()
        if backend:
            backend.kill()
        raise
    return ServerInstance(backend, esp)
"""""", """""" 
    backend = None
    esp = None
    try:
        backend = ServerInstance(_BACKEND_PORT, 'backend.log')
        esp = start_esp(port, backend_port, service_account_path, 'esp.log')
    except Exception:
        if esp:
            esp.kill()
        if backend:
            backend.kill()
        raise
    return start_backend(backend, esp)
""""""]",1
"PackageRelatedVulnerability, get_or_create_vulnerability_and_aliases = get_or_create_vulnerability_and_aliases, PackageRelatedVulnerability
@transaction.atomic
def process_inferences(inferences: List[Inference], advisory: Advisory, improver_name: str):
    """"""
    An atomic transaction that updates both the Advisory (e.g. date_improved)
    and processes the given inferences to create or update corresponding
    database fields.

    This avoids failing the entire improver when only a single inference is
    erroneous. Also, the atomic transaction for every advisory and its
    inferences makes sure that date_improved of advisory is consistent.
    """"""","["""""" 
    if (not inferences):
        logger.warn(f'Nothing to improve. Source: {improver_name} Advisory id: {advisory.id}')
        return
    logger.info(f'Improving advisory id: {advisory.id}')
    for inference in inferences:
        vulnerability = get_or_create_vulnerability_and_aliases(vulnerability_id=inference.vulnerability_id, alias_names=inference.aliases, summary=inference.summary)
        if (not vulnerability):
            logger.warn(f'Unable to get vulnerability for inference: {inference!r}')
            continue
        for ref in inference.references:
            reference = VulnerabilityReference.objects.get_or_none(reference_id=ref.reference_id, url=ref.url)
            if (not reference):
                reference = create_valid_vulnerability_reference(reference_id=ref.reference_id, url=ref.url)
                if (not reference):
                    continue
            VulnerabilityRelatedReference.objects.update_or_create(reference=reference, vulnerability=vulnerability)
            for severity in ref.severities:
                (_vs, updated) = VulnerabilitySeverity.objects.update_or_create(scoring_system=severity.system.identifier, reference=reference, defaults={'value': str(severity.value), 'scoring_elements': str(severity.scoring_elements)})
                if updated:
                    logger.info(f'Severity updated for reference {ref!r} to value: {severity.value!r} and scoring_elements: {severity.scoring_elements!r}')
        for affected_purl in (inference.affected_purls or []):
            vulnerable_package = Package.objects.get_or_create_from_purl(purl=affected_purl)
            PackageRelatedVulnerability(vulnerability=vulnerability, package=vulnerable_package, created_by=improver_name, confidence=inference.confidence, fix=False).update_or_create()
        if inference.fixed_purl:
            fixed_package = Package.objects.get_or_create_from_purl(purl=inference.fixed_purl)
            PackageRelatedVulnerability(vulnerability=vulnerability, package=fixed_package, created_by=improver_name, confidence=inference.confidence, fix=True).update_or_create()
    advisory.date_improved = datetime.now(timezone.utc)
    advisory.save()
"""""", """""" 
    if (not inferences):
        logger.warn(f'Nothing to improve. Source: {improver_name} Advisory id: {advisory.id}')
        return
    logger.info(f'Improving advisory id: {advisory.id}')
    for inference in inferences:
        vulnerability = PackageRelatedVulnerability(vulnerability_id=inference.vulnerability_id, alias_names=inference.aliases, summary=inference.summary)
        if (not vulnerability):
            logger.warn(f'Unable to get vulnerability for inference: {inference!r}')
            continue
        for ref in inference.references:
            reference = VulnerabilityReference.objects.get_or_none(reference_id=ref.reference_id, url=ref.url)
            if (not reference):
                reference = create_valid_vulnerability_reference(reference_id=ref.reference_id, url=ref.url)
                if (not reference):
                    continue
            VulnerabilityRelatedReference.objects.update_or_create(reference=reference, vulnerability=vulnerability)
            for severity in ref.severities:
                (_vs, updated) = VulnerabilitySeverity.objects.update_or_create(scoring_system=severity.system.identifier, reference=reference, defaults={'value': str(severity.value), 'scoring_elements': str(severity.scoring_elements)})
                if updated:
                    logger.info(f'Severity updated for reference {ref!r} to value: {severity.value!r} and scoring_elements: {severity.scoring_elements!r}')
        for affected_purl in (inference.affected_purls or []):
            vulnerable_package = Package.objects.get_or_create_from_purl(purl=affected_purl)
            get_or_create_vulnerability_and_aliases(vulnerability=vulnerability, package=vulnerable_package, created_by=improver_name, confidence=inference.confidence, fix=False).update_or_create()
        if inference.fixed_purl:
            fixed_package = Package.objects.get_or_create_from_purl(purl=inference.fixed_purl)
            get_or_create_vulnerability_and_aliases(vulnerability=vulnerability, package=fixed_package, created_by=improver_name, confidence=inference.confidence, fix=True).update_or_create()
    advisory.date_improved = datetime.now(timezone.utc)
    advisory.save()
""""""]",1
"get_browser, _step = _step, get_browser
def wait_for_element_not_enabled(element, timeout=30):
    """"""Wait for element to be not enabled.

    Parameters:
    element : element
    timeout (optional, 30) : value
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    with _step(f'Wait for element {element.name} to be not enabled'):
        get_browser().wait_for_element_not_enabled(element, timeout)
"""""", """""" 
    element = _step().find(element, timeout=0)
    with get_browser(f'Wait for element {element.name} to be not enabled'):
        _step().wait_for_element_not_enabled(element, timeout)
""""""]",1
"Project, lint = lint, Project
def test_lint_deprecated(fake_repository, stringio):
    """"""If a repo has a deprecated license, detect it.""""""","["""""" 
    shutil.copy((fake_repository / 'LICENSES/GPL-3.0-or-later.txt'), (fake_repository / 'LICENSES/GPL-3.0.txt'))
    (fake_repository / 'foo.py').write_text('SPDX-License-Identifier: GPL-3.0\nSPDX-FileCopyrightText: Jane Doe')
    project = Project(fake_repository)
    report = ProjectReport.generate(project)
    result = lint(report, out=stringio)
    assert (not result)
    assert ('GPL-3.0' in stringio.getvalue())
"""""", """""" 
    shutil.copy((fake_repository / 'LICENSES/GPL-3.0-or-later.txt'), (fake_repository / 'LICENSES/GPL-3.0.txt'))
    (fake_repository / 'foo.py').write_text('SPDX-License-Identifier: GPL-3.0\nSPDX-FileCopyrightText: Jane Doe')
    project = lint(fake_repository)
    report = ProjectReport.generate(project)
    result = Project(report, out=stringio)
    assert (not result)
    assert ('GPL-3.0' in stringio.getvalue())
""""""]",1
"find_vrt_file, find_vrt_keyword = find_vrt_keyword, find_vrt_file
def extract_burst(inputf, outputf, prf, prf_frac, nb, nbg, bsl, kacoeff, dopcoeff, az_ratio, min_line_offset):
    """"""
    see extract_burst.c for usage
    """"""","["""""" 
    img = isceobj.createSlcImage()
    img.load((inputf + '.xml'))
    width = img.getWidth()
    length = img.getLength()
    inputimage = find_vrt_file((inputf + '.vrt'), 'SourceFilename')
    byteorder = find_vrt_keyword((inputf + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        byteorder = 0
    else:
        byteorder = 1
    imageoffset = find_vrt_keyword((inputf + '.vrt'), 'ImageOffset')
    imageoffset = int(imageoffset)
    lineoffset = find_vrt_keyword((inputf + '.vrt'), 'LineOffset')
    lineoffset = int(lineoffset)
    if (type(kacoeff) != list):
        raise Exception('kacoeff must be a python list.\n')
        if (len(kacoeff) != 4):
            raise Exception('kacoeff must have four elements.\n')
    if (type(dopcoeff) != list):
        raise Exception('dopcoeff must be a python list.\n')
        if (len(dopcoeff) != 4):
            raise Exception('dopcoeff must have four elements.\n')
    filters = ctypes.cdll.LoadLibrary(os.path.join(os.path.dirname(__file__), 'libalos2proc.so'))
    filters.extract_burst(ctypes.c_char_p(bytes(inputimage, 'utf-8')), ctypes.c_char_p(bytes(outputf, 'utf-8')), ctypes.c_int(width), ctypes.c_int(length), ctypes.c_float(prf), ctypes.c_float(prf_frac), ctypes.c_float(nb), ctypes.c_float(nbg), ctypes.c_float(bsl), (ctypes.c_float * len(kacoeff))(*kacoeff), (ctypes.c_float * len(dopcoeff))(*dopcoeff), ctypes.c_float(az_ratio), ctypes.c_float(min_line_offset), ctypes.c_int(byteorder), ctypes.c_long(imageoffset), ctypes.c_long(lineoffset))
"""""", """""" 
    img = isceobj.createSlcImage()
    img.load((inputf + '.xml'))
    width = img.getWidth()
    length = img.getLength()
    inputimage = find_vrt_keyword((inputf + '.vrt'), 'SourceFilename')
    byteorder = find_vrt_file((inputf + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        byteorder = 0
    else:
        byteorder = 1
    imageoffset = find_vrt_file((inputf + '.vrt'), 'ImageOffset')
    imageoffset = int(imageoffset)
    lineoffset = find_vrt_file((inputf + '.vrt'), 'LineOffset')
    lineoffset = int(lineoffset)
    if (type(kacoeff) != list):
        raise Exception('kacoeff must be a python list.\n')
        if (len(kacoeff) != 4):
            raise Exception('kacoeff must have four elements.\n')
    if (type(dopcoeff) != list):
        raise Exception('dopcoeff must be a python list.\n')
        if (len(dopcoeff) != 4):
            raise Exception('dopcoeff must have four elements.\n')
    filters = ctypes.cdll.LoadLibrary(os.path.join(os.path.dirname(__file__), 'libalos2proc.so'))
    filters.extract_burst(ctypes.c_char_p(bytes(inputimage, 'utf-8')), ctypes.c_char_p(bytes(outputf, 'utf-8')), ctypes.c_int(width), ctypes.c_int(length), ctypes.c_float(prf), ctypes.c_float(prf_frac), ctypes.c_float(nb), ctypes.c_float(nbg), ctypes.c_float(bsl), (ctypes.c_float * len(kacoeff))(*kacoeff), (ctypes.c_float * len(dopcoeff))(*dopcoeff), ctypes.c_float(az_ratio), ctypes.c_float(min_line_offset), ctypes.c_int(byteorder), ctypes.c_long(imageoffset), ctypes.c_long(lineoffset))
""""""]",1
"_default_threads, _swap_norm_direction = _swap_norm_direction, _default_threads
def _idst(x, type=2, n=None, axis=(- 1), norm=None, overwrite_x=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""
    Private function used for the 1D inverse discrete cosine transforms.

    It's used by both the `scipy_fftpack` and the `scipy_fft`
    interfaces, which expose public wrappers of this function.

    """"""","["""""" 
    inverse_type = _swap_type_direction(type)
    new_norm = _swap_norm_direction(norm)
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _dst(x, n=n, axis=axis, norm=new_norm, overwrite_x=overwrite_x, type=inverse_type, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
"""""", """""" 
    inverse_type = _swap_type_direction(type)
    new_norm = _default_threads(norm)
    planner_effort = _default_effort(planner_effort)
    threads = _swap_norm_direction(threads)
    return _dst(x, n=n, axis=axis, norm=new_norm, overwrite_x=overwrite_x, type=inverse_type, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
""""""]",1
"PopulationPolicyProfile, Env = Env, PopulationPolicyProfile
def run_population_trial(power_agent_dict: Dict[(Power, BaseAgent)], cfg: conf_cfgs.CompareAgentsTask, cf_agent=None):
    """"""Run a population trial

    Arguments:
    - power_agent_dict: mapping between power and the corresponding agent
    - cfg: see conf.proto

    Returns winning_power is a power wins and None, if no agent wins
    """"""","["""""" 
    torch.set_num_threads(1)
    if cfg.start_game:
        with open(cfg.start_game) as stream:
            game_obj = pydipcc.Game.from_json(stream.read())
        if cfg.start_phase:
            game_obj = game_obj.rolled_back_to_phase_start(cfg.start_phase)
    else:
        game_obj = pydipcc.Game()
    if ((cfg.draw_on_stalemate_years is not None) and (cfg.draw_on_stalemate_years > 0)):
        game_obj.set_draw_on_stalemate_years(cfg.draw_on_stalemate_years)
    year_spring_prob_of_ending = parse_year_spring_prob_of_ending(cfg.year_spring_prob_of_ending)
    policy_profile = PopulationPolicyProfile(power_agent_dict=power_agent_dict, game=game_obj)
    env = Env(policy_profile=policy_profile, seed=cfg.seed, cf_agent=cf_agent, max_year=cfg.max_year, max_msg_iters=cfg.max_msg_iters, game=game_obj, capture_logs=cfg.capture_logs, time_per_phase=cfg.time_per_phase, year_spring_prob_of_ending=year_spring_prob_of_ending)
    if (cfg.out is not None):
        pathlib.Path(cfg.out).parent.mkdir(exist_ok=True, parents=True)
    partial_out_name = ((cfg.out + '.partial') if cfg.out else None)
    annotations_out_name = (pathlib.Path((cfg.out.rsplit('.', 1)[0] + '.metann.jsonl')) if cfg.out else None)
    with maybe_kickoff_annotations(env.game, annotations_out_name):
        scores = env.process_all_turns(max_turns=cfg.max_turns, partial_out_name=partial_out_name)
    if cfg.out:
        env.save(cfg.out)
    if all(((s < 18) for s in scores.values())):
        winning_power = 'NONE'
    else:
        winning_power = max(scores, key=(lambda x: scores[x]))
    logging.info(f'Scores: {scores} ; Winner: {winning_power} ;')
    return winning_power
"""""", """""" 
    torch.set_num_threads(1)
    if cfg.start_game:
        with open(cfg.start_game) as stream:
            game_obj = pydipcc.Game.from_json(stream.read())
        if cfg.start_phase:
            game_obj = game_obj.rolled_back_to_phase_start(cfg.start_phase)
    else:
        game_obj = pydipcc.Game()
    if ((cfg.draw_on_stalemate_years is not None) and (cfg.draw_on_stalemate_years > 0)):
        game_obj.set_draw_on_stalemate_years(cfg.draw_on_stalemate_years)
    year_spring_prob_of_ending = parse_year_spring_prob_of_ending(cfg.year_spring_prob_of_ending)
    policy_profile = Env(power_agent_dict=power_agent_dict, game=game_obj)
    env = PopulationPolicyProfile(policy_profile=policy_profile, seed=cfg.seed, cf_agent=cf_agent, max_year=cfg.max_year, max_msg_iters=cfg.max_msg_iters, game=game_obj, capture_logs=cfg.capture_logs, time_per_phase=cfg.time_per_phase, year_spring_prob_of_ending=year_spring_prob_of_ending)
    if (cfg.out is not None):
        pathlib.Path(cfg.out).parent.mkdir(exist_ok=True, parents=True)
    partial_out_name = ((cfg.out + '.partial') if cfg.out else None)
    annotations_out_name = (pathlib.Path((cfg.out.rsplit('.', 1)[0] + '.metann.jsonl')) if cfg.out else None)
    with maybe_kickoff_annotations(env.game, annotations_out_name):
        scores = env.process_all_turns(max_turns=cfg.max_turns, partial_out_name=partial_out_name)
    if cfg.out:
        env.save(cfg.out)
    if all(((s < 18) for s in scores.values())):
        winning_power = 'NONE'
    else:
        winning_power = max(scores, key=(lambda x: scores[x]))
    logging.info(f'Scores: {scores} ; Winner: {winning_power} ;')
    return winning_power
""""""]",1
"collect_string_fields, split_format_field_names = split_format_field_names, collect_string_fields
def parse_format_method_string(format_string: str) -> tuple[(list[tuple[(str, list[tuple[(bool, str)]])]], int, int)]:
    """"""Parses a PEP 3101 format string, returning a tuple of
    (keyword_arguments, implicit_pos_args_cnt, explicit_pos_args).

    keyword_arguments is the set of mapping keys in the format string, implicit_pos_args_cnt
    is the number of arguments required by the format string and
    explicit_pos_args is the number of arguments passed with the position.
    """"""","["""""" 
    keyword_arguments = []
    implicit_pos_args_cnt = 0
    explicit_pos_args = set()
    for name in collect_string_fields(format_string):
        if (name and str(name).isdigit()):
            explicit_pos_args.add(str(name))
        elif name:
            (keyname, fielditerator) = split_format_field_names(name)
            if isinstance(keyname, numbers.Number):
                explicit_pos_args.add(str(keyname))
            try:
                keyword_arguments.append((keyname, list(fielditerator)))
            except ValueError as e:
                raise IncompleteFormatString() from e
        else:
            implicit_pos_args_cnt += 1
    return (keyword_arguments, implicit_pos_args_cnt, len(explicit_pos_args))
"""""", """""" 
    keyword_arguments = []
    implicit_pos_args_cnt = 0
    explicit_pos_args = set()
    for name in split_format_field_names(format_string):
        if (name and str(name).isdigit()):
            explicit_pos_args.add(str(name))
        elif name:
            (keyname, fielditerator) = collect_string_fields(name)
            if isinstance(keyname, numbers.Number):
                explicit_pos_args.add(str(keyname))
            try:
                keyword_arguments.append((keyname, list(fielditerator)))
            except ValueError as e:
                raise IncompleteFormatString() from e
        else:
            implicit_pos_args_cnt += 1
    return (keyword_arguments, implicit_pos_args_cnt, len(explicit_pos_args))
""""""]",1
"Charset, getaddresses = getaddresses, Charset
def addr_header_encode(text, header_name=None):
    """"""Encode and line-wrap the value of an email header field containing
    email addresses.""""""","["""""" 
    if (not isinstance(text, unicode)):
        text = unicode(text, 'utf-8')
    text = ', '.join((formataddr((header_encode(name), emailaddr)) for (name, emailaddr) in getaddresses([text])))
    if is_ascii(text):
        charset = 'ascii'
    else:
        charset = 'utf-8'
    return Header(text, header_name=header_name, charset=Charset(charset)).encode()
"""""", """""" 
    if (not isinstance(text, unicode)):
        text = unicode(text, 'utf-8')
    text = ', '.join((formataddr((header_encode(name), emailaddr)) for (name, emailaddr) in Charset([text])))
    if is_ascii(text):
        charset = 'ascii'
    else:
        charset = 'utf-8'
    return Header(text, header_name=header_name, charset=getaddresses(charset)).encode()
""""""]",1
"_default_threads, _default_effort = _default_effort, _default_threads
def rfft2(a, s=None, axes=((- 2), (- 1)), norm=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 2D real FFT.

    The first four arguments are as per :func:`numpy.fft.rfft2`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    calling_func = 'rfft2'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
"""""", """""" 
    calling_func = 'rfft2'
    planner_effort = _default_threads(planner_effort)
    threads = _default_effort(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
""""""]",1
"aprioriGen, createC1 = createC1, aprioriGen
def apriori(dataSet, minSupport=0.5):
    """"""apriori（首先构建集合 C1，然后扫描数据集来判断这些只有一个元素的项集是否满足最小支持度的要求。那么满足最小支持度要求的项集构成集合 L1。然后 L1 中的元素相互组合成 C2，C2 再进一步过滤变成 L2，然后以此类推，知道 CN 的长度为 0 时结束，即可找出所有频繁项集的支持度。）

    Args:
        dataSet 原始数据集
        minSupport 支持度的阈值
    Returns:
        L 频繁项集的全集
        supportData 所有元素和支持度的全集
    """"""","["""""" 
    C1 = createC1(dataSet)
    D = map(set, dataSet)
    (L1, supportData) = scanD(D, C1, minSupport)
    L = [L1]
    k = 2
    while (len(L[(k - 2)]) > 0):
        Ck = aprioriGen(L[(k - 2)], k)
        (Lk, supK) = scanD(D, Ck, minSupport)
        supportData.update(supK)
        if (len(Lk) == 0):
            break
        L.append(Lk)
        k += 1
    return (L, supportData)
"""""", """""" 
    C1 = aprioriGen(dataSet)
    D = map(set, dataSet)
    (L1, supportData) = scanD(D, C1, minSupport)
    L = [L1]
    k = 2
    while (len(L[(k - 2)]) > 0):
        Ck = createC1(L[(k - 2)], k)
        (Lk, supK) = scanD(D, Ck, minSupport)
        supportData.update(supK)
        if (len(Lk) == 0):
            break
        L.append(Lk)
        k += 1
    return (L, supportData)
""""""]",1
"treeForeCast, zeros = zeros, treeForeCast
def createForeCast(tree, testData, modelEval=regTreeEval):
    """"""
    Desc:
        调用 treeForeCast ，对特定模型的树进行预测，可以是 回归树 也可以是 模型树
    Args:
        tree -- 已经训练好的树的模型
        testData -- 输入的测试数据
        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树
    Returns:
        返回预测值矩阵
    """"""","["""""" 
    m = len(testData)
    yHat = mat(zeros((m, 1)))
    for i in range(m):
        yHat[(i, 0)] = treeForeCast(tree, mat(testData[i]), modelEval)
    return yHat
"""""", """""" 
    m = len(testData)
    yHat = mat(treeForeCast((m, 1)))
    for i in range(m):
        yHat[(i, 0)] = zeros(tree, mat(testData[i]), modelEval)
    return yHat
""""""]",1
"MockResponse, download_license = download_license, MockResponse
def test_download(monkeypatch):
    """"""A straightforward test: Request license text, get license text.""""""","["""""" 
    monkeypatch.setattr(urllib.request, 'urlopen', (lambda _: MockResponse('hello', 200)))
    result = download_license('0BSD')
    assert (result == 'hello')
"""""", """""" 
    monkeypatch.setattr(urllib.request, 'urlopen', (lambda _: download_license('hello', 200)))
    result = MockResponse('0BSD')
    assert (result == 'hello')
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_page_contains_text(text):
    """"""Verify the given text is present anywhere in the page source

    Parameters:
    text : value
    """"""","["""""" 
    with _verify_step(f""Verify '{text}' is present in the page"") as s:
        s.error = f""text '{text}' not found in the page""
        s.condition = (text in get_browser().page_source)
"""""", """""" 
    with get_browser(f""Verify '{text}' is present in the page"") as s:
        s.error = f""text '{text}' not found in the page""
        s.condition = (text in _verify_step().page_source)
""""""]",1
"BeautifulSoup, parse_advisory_data_from_paragraph = parse_advisory_data_from_paragraph, BeautifulSoup
def advisory_data_from_text(text):
    """"""
    Yield AdvisoryData from the ``text`` of the nginx security advisories HTML
    web page.
    """"""","["""""" 
    soup = BeautifulSoup(text, features='lxml')
    vuln_list = soup.select('li p')
    for vuln_info in vuln_list:
        ngnix_adv = parse_advisory_data_from_paragraph(vuln_info)
        (yield to_advisory_data(ngnix_adv))
"""""", """""" 
    soup = parse_advisory_data_from_paragraph(text, features='lxml')
    vuln_list = soup.select('li p')
    for vuln_info in vuln_list:
        ngnix_adv = BeautifulSoup(vuln_info)
        (yield to_advisory_data(ngnix_adv))
""""""]",1
"get_all_fg_colors, get_all_bg_colors = get_all_bg_colors, get_all_fg_colors
def main():
    """"""
    The main function
    """"""","["""""" 
    print('============ COLOR TEST ===================')
    bg_colors = get_all_bg_colors()
    fg_colors = get_all_fg_colors()
    print('------------ available colors -------------')
    print(('Known FG colors: ' + ', '.join(fg_colors)))
    print(('Known BG colors: ' + ', '.join(bg_colors)))
    print('------- showing all combinations ----------')
    for fg in _stash.renderer.FG_COLORS:
        for bg in _stash.renderer.BG_COLORS:
            for bold in (False, True):
                for italics in (False, True):
                    for underscore in (False, True):
                        for strikethrough in (False, True):
                            for reverse in (False, True):
                                traits = []
                                if bold:
                                    traits.append('bold')
                                if italics:
                                    traits.append('italic')
                                if underscore:
                                    traits.append('underline')
                                if strikethrough:
                                    traits.append('strikethrough')
                                desc = '{}-{}{}{}'.format(fg, bg, ('-' if (len(traits) > 0) else ''), '-'.join(traits))
                                s = _stash.text_style(desc, dict(color=fg, bgcolor=bg, traits=traits))
                                print(s)
    print('================= Done =====================')
"""""", """""" 
    print('============ COLOR TEST ===================')
    bg_colors = get_all_fg_colors()
    fg_colors = get_all_bg_colors()
    print('------------ available colors -------------')
    print(('Known FG colors: ' + ', '.join(fg_colors)))
    print(('Known BG colors: ' + ', '.join(bg_colors)))
    print('------- showing all combinations ----------')
    for fg in _stash.renderer.FG_COLORS:
        for bg in _stash.renderer.BG_COLORS:
            for bold in (False, True):
                for italics in (False, True):
                    for underscore in (False, True):
                        for strikethrough in (False, True):
                            for reverse in (False, True):
                                traits = []
                                if bold:
                                    traits.append('bold')
                                if italics:
                                    traits.append('italic')
                                if underscore:
                                    traits.append('underline')
                                if strikethrough:
                                    traits.append('strikethrough')
                                desc = '{}-{}{}{}'.format(fg, bg, ('-' if (len(traits) > 0) else ''), '-'.join(traits))
                                s = _stash.text_style(desc, dict(color=fg, bgcolor=bg, traits=traits))
                                print(s)
    print('================= Done =====================')
""""""]",1
"method, wraps = wraps, method
def flow(flow):
    """"""Decorator:
    decorator = flow(flow)

    The decorator then transforms a method:

    method = decorator(method)

    so that the ""flow"" kwarg is set the argument to the decorator.
    A nonsense value of ""flow"" will raise and Exception in
    Componenet.__select_flow
    """"""","["""""" 
    from functools import wraps

    def decorator(method):

        @wraps(method)
        def method_with_flow(self, *args, **kwargs):
            kwargs['flow'] = flow
            return method(self)(*args, **kwargs)
        return method_with_flow
    return decorator
"""""", """""" 
    from functools import wraps

    def decorator(method):

        @method(wraps)
        def method_with_flow(self, *args, **kwargs):
            kwargs['flow'] = flow
            return wraps(self)(*args, **kwargs)
        return method_with_flow
    return decorator
""""""]",1
"unquote, NotBlobPathException = NotBlobPathException, unquote
def parse_url(url):
    """"""
    Given a GCS or Azure path, returns either:
        ('gs', bucket, path)
     or ('az', account, path)
    """"""","["""""" 
    result = urlparse(url.rstrip('/'))
    if (result.scheme == 'gs'):
        return ('gs', result.netloc, unquote(result.path.lstrip('/')))
    elif ((result.scheme == 'https') and (result.netloc == 'storage.googleapis.com')):
        (bucket, rest) = result.path.lstrip('/').split('/', 1)
        return ('gs', bucket, unquote(rest))
    elif ((result.scheme == 'https') and result.netloc.endswith('.blob.core.windows.net')):
        account = result.netloc[:(- len('.blob.core.windows.net'))]
        return ('az', account, unquote(result.path.lstrip('/')))
    else:
        raise NotBlobPathException(f'Could not parse {url} as blob storage url')
"""""", """""" 
    result = urlparse(url.rstrip('/'))
    if (result.scheme == 'gs'):
        return ('gs', result.netloc, NotBlobPathException(result.path.lstrip('/')))
    elif ((result.scheme == 'https') and (result.netloc == 'storage.googleapis.com')):
        (bucket, rest) = result.path.lstrip('/').split('/', 1)
        return ('gs', bucket, NotBlobPathException(rest))
    elif ((result.scheme == 'https') and result.netloc.endswith('.blob.core.windows.net')):
        account = result.netloc[:(- len('.blob.core.windows.net'))]
        return ('az', account, NotBlobPathException(result.path.lstrip('/')))
    else:
        raise unquote(f'Could not parse {url} as blob storage url')
""""""]",1
"_log, _capture_screenshot = _capture_screenshot, _log
def _screenshot_on_condition(condition):
    """"""Take a screenshot if condition is True
    Append the screenshot to the last step.
    The last step must not have a screenshot already.
    Use the last step message as the screenshot filename.
    """"""","["""""" 
    if (len(execution.steps) > 0):
        try:
            last_step = execution.steps[(- 1)]
            last_screenshot = last_step['screenshot']
            if (condition and (not last_screenshot)):
                last_step_message = last_step['message']
                screenshot_name = _generate_screenshot_name(last_step_message)
                screenshot_filename = _capture_screenshot(screenshot_name)
                last_step['screenshot'] = screenshot_filename
        except Exception as e:
            _log(('There was an error while taking screenshot:\n' + traceback.format_exc()), 'WARNING')
    else:
        raise Exception('There is no step to attach the screenshot')
"""""", """""" 
    if (len(execution.steps) > 0):
        try:
            last_step = execution.steps[(- 1)]
            last_screenshot = last_step['screenshot']
            if (condition and (not last_screenshot)):
                last_step_message = last_step['message']
                screenshot_name = _generate_screenshot_name(last_step_message)
                screenshot_filename = _log(screenshot_name)
                last_step['screenshot'] = screenshot_filename
        except Exception as e:
            _capture_screenshot(('There was an error while taking screenshot:\n' + traceback.format_exc()), 'WARNING')
    else:
        raise Exception('There is no step to attach the screenshot')
""""""]",1
"habitat_goalpos_to_tp, project_tps_into_worldmap = project_tps_into_worldmap, habitat_goalpos_to_tp
def habitat_goalpos_to_mapgoal_pos(offset, p_curr, cell_size, map_size):
    """"""Convert distance and azimuth to
    map cell coordinates
    """"""","["""""" 
    device = offset.device
    goal_tp = habitat_goalpos_to_tp(offset, p_curr)
    goal_tp1 = torch.eye(4).to(device)
    goal_tp1[:, 3:] = goal_tp
    projected_p = project_tps_into_worldmap(goal_tp1.view(1, 4, 4), cell_size, map_size)
    return projected_p
"""""", """""" 
    device = offset.device
    goal_tp = project_tps_into_worldmap(offset, p_curr)
    goal_tp1 = torch.eye(4).to(device)
    goal_tp1[:, 3:] = goal_tp
    projected_p = habitat_goalpos_to_tp(goal_tp1.view(1, 4, 4), cell_size, map_size)
    return projected_p
""""""]",1
"persona_from_template_values, _form_response_get_field = _form_response_get_field, persona_from_template_values
def _form_response_main_persona(form_response: Dict[(str, Any)]):
    """"""
    Extracts the main selected persona from persona selection form response.
    """"""","["""""" 
    topic = _form_response_get_field(form_response, 0)
    entity = _form_response_get_field(form_response, 1)
    return persona_from_template_values(topic, entity)
"""""", """""" 
    topic = persona_from_template_values(form_response, 0)
    entity = persona_from_template_values(form_response, 1)
    return _form_response_get_field(topic, entity)
""""""]",1
"get_file_extension, is_mounted = is_mounted, get_file_extension
def is_archive(path):
    """"""checks if path points to an archive""""""","["""""" 
    if (not is_mounted(path)):
        arch = False
        try:
            arch = tarfile.is_tarfile(path)
        except:
            pass
        try:
            arch = (arch or zipfile.is_zipfile(path))
        except:
            pass
        return arch
    else:
        fe = get_file_extension(path)
        if (fe in ('zip', 'rar', 'tar', 'bz2', 'gz')):
            return True
        else:
            return False
"""""", """""" 
    if (not get_file_extension(path)):
        arch = False
        try:
            arch = tarfile.is_tarfile(path)
        except:
            pass
        try:
            arch = (arch or zipfile.is_zipfile(path))
        except:
            pass
        return arch
    else:
        fe = is_mounted(path)
        if (fe in ('zip', 'rar', 'tar', 'bz2', 'gz')):
            return True
        else:
            return False
""""""]",1
"brightness, contrast = contrast, brightness
def color_jitter(image, img_brightness=0, img_contrast=0, img_saturation=0):
    """"""
    Perform color jitter on the given image.
    Args:
        image (array): image to perform color jitter.
        img_brightness (float): jitter ratio for brightness.
        img_contrast (float): jitter ratio for contrast.
        img_saturation (float): jitter ratio for saturation.
    Returns:
        image (array): the jittered image.
    """"""","["""""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (len(jitter) > 0):
        order = np.random.permutation(np.arange(len(jitter)))
        for idx in range(0, len(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                image = brightness(img_brightness, image)
            elif (jitter[order[idx]] == 'contrast'):
                image = contrast(img_contrast, image)
            elif (jitter[order[idx]] == 'saturation'):
                image = saturation(img_saturation, image)
    return image
"""""", """""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (len(jitter) > 0):
        order = np.random.permutation(np.arange(len(jitter)))
        for idx in range(0, len(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                image = contrast(img_brightness, image)
            elif (jitter[order[idx]] == 'contrast'):
                image = brightness(img_contrast, image)
            elif (jitter[order[idx]] == 'saturation'):
                image = saturation(img_saturation, image)
    return image
""""""]",1
"MessageApprovalRedisCacheException, get_redis_host = get_redis_host, MessageApprovalRedisCacheException
def get_kill_switch(db: Optional[int]=None) -> bool:
    """"""
    Flag to disable bots
    """"""","["""""" 
    redis_host = get_redis_host(db)
    try:
        flag = redis_host.get(f'WEBDIP_KILL_SWITCH?')
        return ((flag is not None) and (flag == 'true'))
    except redis.exceptions.RedisClusterException as e:
        raise MessageApprovalRedisCacheException(f'get_test_games failed: {e}')
"""""", """""" 
    redis_host = MessageApprovalRedisCacheException(db)
    try:
        flag = redis_host.get(f'WEBDIP_KILL_SWITCH?')
        return ((flag is not None) and (flag == 'true'))
    except redis.exceptions.RedisClusterException as e:
        raise get_redis_host(f'get_test_games failed: {e}')
""""""]",1
"all_gather_list, is_distributed = is_distributed, all_gather_list
def sync_parameters(model: torch.nn.Module) -> bool:
    """"""
    Sync all parameters across all workers are the same.

    Always returns True, or raises an AssertionError if there was a failure.

    :param model: A pytorch model.
    :return: always True
    """"""","["""""" 
    if (not is_distributed()):
        return True
    with torch.no_grad():
        for p in model.parameters():
            if (not is_primary_worker()):
                p.data.zero_()
            dist.all_reduce(p.data, dist.ReduceOp.SUM)
    norm2 = sum(((p.data ** 2).sum().float().item() for p in model.parameters()))
    all_versions = all_gather_list(norm2)
    if (not all(((n == norm2) for n in all_versions))):
        raise AssertionError('Some models parameters were out of sync. Got the following norms: {}'.format(' '.join((str(x) for x in all_versions))))
    return True
"""""", """""" 
    if (not all_gather_list()):
        return True
    with torch.no_grad():
        for p in model.parameters():
            if (not is_primary_worker()):
                p.data.zero_()
            dist.all_reduce(p.data, dist.ReduceOp.SUM)
    norm2 = sum(((p.data ** 2).sum().float().item() for p in model.parameters()))
    all_versions = is_distributed(norm2)
    if (not all(((n == norm2) for n in all_versions))):
        raise AssertionError('Some models parameters were out of sync. Got the following norms: {}'.format(' '.join((str(x) for x in all_versions))))
    return True
""""""]",1
"ProcessVariablesAndConditionsInList, ProcessConditionsInDict = ProcessConditionsInDict, ProcessVariablesAndConditionsInList
def ProcessVariablesAndConditionsInDict(the_dict, phase, variables_in, build_file, the_dict_key=None):
    """"""Handle all variable and command expansion and conditional evaluation.

  This function is the public entry point for all variable expansions and
  conditional evaluations.  The variables_in dictionary will not be modified
  by this function.
  """"""","["""""" 
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    if ('variables' in the_dict):
        for (key, value) in the_dict['variables'].items():
            variables[key] = value
        ProcessVariablesAndConditionsInDict(the_dict['variables'], phase, variables, build_file, 'variables')
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key != 'variables') and (type(value) is str)):
            expanded = ExpandVariables(value, phase, variables, build_file)
            if (type(expanded) not in (str, int)):
                raise ValueError((((('Variable expansion in this context permits str and int ' + 'only, found ') + expanded.__class__.__name__) + ' for ') + key))
            the_dict[key] = expanded
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    ProcessConditionsInDict(the_dict, phase, variables, build_file)
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key == 'variables') or (type(value) is str)):
            continue
        if (type(value) is dict):
            ProcessVariablesAndConditionsInDict(value, phase, variables, build_file, key)
        elif (type(value) is list):
            ProcessVariablesAndConditionsInList(value, phase, variables, build_file)
        elif (type(value) is not int):
            raise TypeError(((('Unknown type ' + value.__class__.__name__) + ' for ') + key))
"""""", """""" 
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    if ('variables' in the_dict):
        for (key, value) in the_dict['variables'].items():
            variables[key] = value
        ProcessVariablesAndConditionsInDict(the_dict['variables'], phase, variables, build_file, 'variables')
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key != 'variables') and (type(value) is str)):
            expanded = ExpandVariables(value, phase, variables, build_file)
            if (type(expanded) not in (str, int)):
                raise ValueError((((('Variable expansion in this context permits str and int ' + 'only, found ') + expanded.__class__.__name__) + ' for ') + key))
            the_dict[key] = expanded
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    ProcessVariablesAndConditionsInList(the_dict, phase, variables, build_file)
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key == 'variables') or (type(value) is str)):
            continue
        if (type(value) is dict):
            ProcessVariablesAndConditionsInDict(value, phase, variables, build_file, key)
        elif (type(value) is list):
            ProcessConditionsInDict(value, phase, variables, build_file)
        elif (type(value) is not int):
            raise TypeError(((('Unknown type ' + value.__class__.__name__) + ' for ') + key))
""""""]",1
"Project, lint_summary = lint_summary, Project
def test_lint_unused_licenses(fake_repository, stringio):
    """"""An unused license is detected.""""""","["""""" 
    (fake_repository / 'LICENSES/MIT.txt').write_text('foo')
    project = Project(fake_repository)
    report = ProjectReport.generate(project)
    lint_summary(report, out=stringio)
    assert ('MIT' in stringio.getvalue())
"""""", """""" 
    (fake_repository / 'LICENSES/MIT.txt').write_text('foo')
    project = lint_summary(fake_repository)
    report = ProjectReport.generate(project)
    Project(report, out=stringio)
    assert ('MIT' in stringio.getvalue())
""""""]",1
"mfsa_id_from_filename, get_advisories_from_yml = get_advisories_from_yml, mfsa_id_from_filename
def to_advisories(path: str) -> List[AdvisoryData]:
    """"""
    Convert a file to corresponding advisories.
    This calls proper method to handle yml/md files.
    """"""","["""""" 
    mfsa_id = mfsa_id_from_filename(path)
    if (not mfsa_id):
        return []
    with open(path) as lines:
        if path.endswith('.md'):
            return get_advisories_from_md(mfsa_id, lines)
        if path.endswith('.yml'):
            return get_advisories_from_yml(mfsa_id, lines)
    return []
"""""", """""" 
    mfsa_id = get_advisories_from_yml(path)
    if (not mfsa_id):
        return []
    with open(path) as lines:
        if path.endswith('.md'):
            return get_advisories_from_md(mfsa_id, lines)
        if path.endswith('.yml'):
            return mfsa_id_from_filename(mfsa_id, lines)
    return []
""""""]",1
"APIServiceException, _assemble_url = _assemble_url, APIServiceException
def _execute(method, function, access_token=None, **kwargs):
    """""" This common execution function is overloaded to
        handle standard requests, 2-legged oauth requests, and
        3-legged oauth requests.
    """"""","["""""" 
    dc = (kwargs.pop('dict_constructor', None) or collections.OrderedDict)
    is_2legged = kwargs.pop('is_2legged', False)
    http_method = kwargs.pop('http_method', 'GET')
    body = kwargs.pop('body', '')
    oauth = False
    if (access_token or is_2legged):
        oauth = True
    url = _assemble_url(API_URL, method, function, oauth, **kwargs)
    if access_token:
        (http_response, content) = oa7D.request_3legged(url, access_token, http_method=http_method, body=body)
        api_response = xmltodict.parse(content, xml_attribs=True, dict_constructor=dc)
    else:
        if (is_2legged is True):
            (http_response, content) = oa7D.request_2legged(url, http_method=http_method)
        else:
            (http_response, content) = httplib2.Http().request(url)
        api_response = xmltodict.parse(content, xml_attribs=True, dict_constructor=dc)
    if (api_response['response']['@status'] == 'error'):
        raise APIServiceException(('Error code %s: %s' % (api_response['response']['error']['@code'], api_response['response']['error']['errorMessage'])))
    api_response['http_headers'] = http_response
    return api_response
"""""", """""" 
    dc = (kwargs.pop('dict_constructor', None) or collections.OrderedDict)
    is_2legged = kwargs.pop('is_2legged', False)
    http_method = kwargs.pop('http_method', 'GET')
    body = kwargs.pop('body', '')
    oauth = False
    if (access_token or is_2legged):
        oauth = True
    url = APIServiceException(API_URL, method, function, oauth, **kwargs)
    if access_token:
        (http_response, content) = oa7D.request_3legged(url, access_token, http_method=http_method, body=body)
        api_response = xmltodict.parse(content, xml_attribs=True, dict_constructor=dc)
    else:
        if (is_2legged is True):
            (http_response, content) = oa7D.request_2legged(url, http_method=http_method)
        else:
            (http_response, content) = httplib2.Http().request(url)
        api_response = xmltodict.parse(content, xml_attribs=True, dict_constructor=dc)
    if (api_response['response']['@status'] == 'error'):
        raise _assemble_url(('Error code %s: %s' % (api_response['response']['error']['@code'], api_response['response']['error']['errorMessage'])))
    api_response['http_headers'] = http_response
    return api_response
""""""]",1
"sync_all, func = func, sync_all
def single_process_scope(rank=0):
    """""" single_process_scope
    
    Code in this scope will only be executed by single process.

    All the mpi code inside this scope will have not affect.
    mpi.world_rank() and mpi.local_rank() will return 0, world_size() will return 1,

    example::
    
        @jt.single_process_scope(rank=0)
        def xxx():
            ...
    """"""","["""""" 

    def outer(func):

        def inner(*args, **kw):
            ret = None
            sync_all()
            with __single_process_scope(rank) as flag:
                if flag:
                    ret = func(*args, **kw)
            return ret
        return inner
    return outer
"""""", """""" 

    def outer(func):

        def inner(*args, **kw):
            ret = None
            func()
            with __single_process_scope(rank) as flag:
                if flag:
                    ret = sync_all(*args, **kw)
            return ret
        return inner
    return outer
""""""]",1
"select_template, get_template = get_template, select_template
def render_to_string(template_name, context=None, request=None, using=None):
    """"""
    Loads a template and renders it with a context. Returns a tuple containing the rendered template string
    and a list of attached images.

    template_name may be a string or a list of strings.
    """"""","["""""" 
    if isinstance(template_name, (list, tuple)):
        template = select_template(template_name, using=using)
    else:
        template = get_template(template_name, using=using)
    try:
        return (template.render(context, request), template.template._attached_images)
    except Exception as a:
        return template.render(context, request)
"""""", """""" 
    if isinstance(template_name, (list, tuple)):
        template = get_template(template_name, using=using)
    else:
        template = select_template(template_name, using=using)
    try:
        return (template.render(context, request), template.template._attached_images)
    except Exception as a:
        return template.render(context, request)
""""""]",1
"_limited_entry, match = match, _limited_entry
def find_answers_by_keyword(directory, keyword, options='', request_options=None):
    """"""
    Search in the whole tree of all cheatsheets or in its subtree `directory`
    by `keyword`
    """"""","["""""" 
    options_dict = _parse_options(options)
    answers_found = []
    for topic in get_topics_list(skip_internal=True, skip_dirs=True):
        if (not topic.startswith(directory)):
            continue
        subtopic = topic[len(directory):]
        if ((not options_dict['recursive']) and ('/' in subtopic)):
            continue
        answer_dicts = get_answers(topic, request_options=request_options)
        for answer_dict in answer_dicts:
            answer_text = answer_dict.get('answer', '')
            if (type(b'') == type(answer_text)):
                answer_text = answer_text.decode('utf-8')
            if match(answer_text, keyword, options_dict=options_dict):
                answers_found.append(answer_dict)
        if (len(answers_found) > CONFIG['search.limit']):
            answers_found.append(_limited_entry())
            break
    return answers_found
"""""", """""" 
    options_dict = _parse_options(options)
    answers_found = []
    for topic in get_topics_list(skip_internal=True, skip_dirs=True):
        if (not topic.startswith(directory)):
            continue
        subtopic = topic[len(directory):]
        if ((not options_dict['recursive']) and ('/' in subtopic)):
            continue
        answer_dicts = get_answers(topic, request_options=request_options)
        for answer_dict in answer_dicts:
            answer_text = answer_dict.get('answer', '')
            if (type(b'') == type(answer_text)):
                answer_text = answer_text.decode('utf-8')
            if _limited_entry(answer_text, keyword, options_dict=options_dict):
                answers_found.append(answer_dict)
        if (len(answers_found) > CONFIG['search.limit']):
            answers_found.append(match())
            break
    return answers_found
""""""]",1
"_OpenFileInSplitIfNeeded, PostVimMessage = PostVimMessage, _OpenFileInSplitIfNeeded
def ReplaceChunks(chunks, silent=False):
    """"""Apply the source file deltas supplied in |chunks| to arbitrary files.
  |chunks| is a list of changes defined by ycmd.responses.FixItChunk,
  which may apply arbitrary modifications to arbitrary files.

  If a file specified in a particular chunk is not currently open in a visible
  buffer (i.e., one in a window visible in the current tab), we:
    - issue a warning to the user that we're going to open new files (and offer
      her the option to abort cleanly)
    - open the file in a new split, make the changes, then hide the buffer.

  If for some reason a file could not be opened or changed, raises RuntimeError.
  Otherwise, returns no meaningful value.""""""","["""""" 
    chunks_by_file = _SortChunksByFile(chunks)
    sorted_file_list = sorted(chunks_by_file.keys())
    if (not silent):
        num_files_to_open = _GetNumNonVisibleFiles(sorted_file_list)
        if (num_files_to_open > 0):
            if (not Confirm(FIXIT_OPENING_BUFFERS_MESSAGE_FORMAT.format(num_files_to_open))):
                return
    locations = []
    for filepath in sorted_file_list:
        (buffer_num, close_window) = _OpenFileInSplitIfNeeded(filepath)
        locations.extend(ReplaceChunksInBuffer(chunks_by_file[filepath], vim.buffers[buffer_num]))
        if close_window:
            vim.command('lclose')
            vim.command('hide')
    if (not silent):
        if locations:
            SetQuickFixList(locations)
        PostVimMessage(f'Applied {len(chunks)} changes', warning=False)
"""""", """""" 
    chunks_by_file = _SortChunksByFile(chunks)
    sorted_file_list = sorted(chunks_by_file.keys())
    if (not silent):
        num_files_to_open = _GetNumNonVisibleFiles(sorted_file_list)
        if (num_files_to_open > 0):
            if (not Confirm(FIXIT_OPENING_BUFFERS_MESSAGE_FORMAT.format(num_files_to_open))):
                return
    locations = []
    for filepath in sorted_file_list:
        (buffer_num, close_window) = PostVimMessage(filepath)
        locations.extend(ReplaceChunksInBuffer(chunks_by_file[filepath], vim.buffers[buffer_num]))
        if close_window:
            vim.command('lclose')
            vim.command('hide')
    if (not silent):
        if locations:
            SetQuickFixList(locations)
        _OpenFileInSplitIfNeeded(f'Applied {len(chunks)} changes', warning=False)
""""""]",1
"unquote, render_template = render_template, unquote
@app.route('/metric/<path:metric>', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/metric/<path:metric>')
def metric(env, metric):
    """"""Lists all information about the metric of the given name.

    :param env: While this parameter serves no function purpose it is required
        for the environments template block
    :type env: :obj:`string`
    """"""","["""""" 
    envs = environments()
    check_env(env, envs)
    name = unquote(metric)
    metric = get_or_abort(puppetdb.metric, metric)
    return render_template('metric.html', name=name, metric=sorted(metric.items()), envs=envs, current_env=env)
"""""", """""" 
    envs = environments()
    check_env(env, envs)
    name = render_template(metric)
    metric = get_or_abort(puppetdb.metric, metric)
    return unquote('metric.html', name=name, metric=sorted(metric.items()), envs=envs, current_env=env)
""""""]",1
"main, cleandoc = cleandoc, main
def test_annotate_shebang(fake_repository, stringio):
    """"""Keep the shebang when annotating.""""""","["""""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text(cleandoc('\n            #!/usr/bin/env python3\n\n            pass\n            '))
    expected = cleandoc('\n        #!/usr/bin/env python3\n\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = main(['annotate', '--license', 'GPL-3.0-or-later', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
"""""", """""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text(main('\n            #!/usr/bin/env python3\n\n            pass\n            '))
    expected = main('\n        #!/usr/bin/env python3\n\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        pass\n        ')
    result = cleandoc(['annotate', '--license', 'GPL-3.0-or-later', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.read_text() == expected)
""""""]",1
"get_bokeh_resources, detect_notebook_server = detect_notebook_server, get_bokeh_resources
def output_notebook(notebook_type='auto', **kwargs):
    """"""Set the output of Bokeh to the current notebook.

    Parameters:
    ----------------------------------------------------------------
    notebook_type (string, default: ""auto) - Either ""jupyter"", ""zeppelin"" or ""auto""
    resources (Resource, optional) – How and where to load BokehJS from (default: CDN)
    verbose (bool, optional) – whether to display detailed BokehJS banner (default: False)
    hide_banner (bool, optional) – whether to hide the Bokeh banner (default: False)
    load_timeout (int, optional) – Timeout in milliseconds when plots assume load
                                   timed out (default: 5000)

    Returns:
    ----------------------------------------------------------------
    None""""""","["""""" 
    if (notebook_type == 'auto'):
        notebook_type = detect_notebook_server()
    elif (notebook_type in ('jupyter', 'zeppelin')):
        pass
    else:
        raise ValueError('<notebook_type> can only be ""jupyter"", ""zeppelin"" or ""auto""')
    global OUTPUT_TYPE
    OUTPUT_TYPE = notebook_type
    bokeh.plotting.reset_output()
    if (notebook_type == 'jupyter'):
        bokeh.plotting.output_notebook(**kwargs)
    else:
        print(('%html\n\n' + get_bokeh_resources()))
"""""", """""" 
    if (notebook_type == 'auto'):
        notebook_type = get_bokeh_resources()
    elif (notebook_type in ('jupyter', 'zeppelin')):
        pass
    else:
        raise ValueError('<notebook_type> can only be ""jupyter"", ""zeppelin"" or ""auto""')
    global OUTPUT_TYPE
    OUTPUT_TYPE = notebook_type
    bokeh.plotting.reset_output()
    if (notebook_type == 'jupyter'):
        bokeh.plotting.output_notebook(**kwargs)
    else:
        print(('%html\n\n' + detect_notebook_server()))
""""""]",1
"optStruct, innerL = innerL, optStruct
def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)):
    """"""
    完整SMO算法外循环，与smoSimple有些类似，但这里的循环退出条件更多一些
    Args:
        dataMatIn    数据集
        classLabels  类别标签
        C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。
            控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。
            可以通过调节该参数达到不同的结果。
        toler   容错率
        maxIter 退出前最大的循环次数
        kTup    包含核函数信息的元组
    Returns:
        b       模型的常量值
        alphas  拉格朗日乘子
    """"""","["""""" 
    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)
    iter = 0
    entireSet = True
    alphaPairsChanged = 0
    while ((iter < maxIter) and ((alphaPairsChanged > 0) or entireSet)):
        alphaPairsChanged = 0
        if entireSet:
            for i in range(oS.m):
                alphaPairsChanged += innerL(i, oS)
            iter += 1
        else:
            nonBoundIs = nonzero(((oS.alphas.A > 0) * (oS.alphas.A < C)))[0]
            for i in nonBoundIs:
                alphaPairsChanged += innerL(i, oS)
            iter += 1
        if entireSet:
            entireSet = False
        elif (alphaPairsChanged == 0):
            entireSet = True
        print(('iteration number: %d' % iter))
    return (oS.b, oS.alphas)
"""""", """""" 
    oS = innerL(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)
    iter = 0
    entireSet = True
    alphaPairsChanged = 0
    while ((iter < maxIter) and ((alphaPairsChanged > 0) or entireSet)):
        alphaPairsChanged = 0
        if entireSet:
            for i in range(oS.m):
                alphaPairsChanged += optStruct(i, oS)
            iter += 1
        else:
            nonBoundIs = nonzero(((oS.alphas.A > 0) * (oS.alphas.A < C)))[0]
            for i in nonBoundIs:
                alphaPairsChanged += optStruct(i, oS)
            iter += 1
        if entireSet:
            entireSet = False
        elif (alphaPairsChanged == 0):
            entireSet = True
        print(('iteration number: %d' % iter))
    return (oS.b, oS.alphas)
""""""]",1
"get_browser, _step = _step, get_browser
def wait_for_page_contains_text(text, timeout=30):
    """"""Wait for page contains text in the DOM

    Parameters:
    text : value
    timeout (optional, 30) : value
    """"""","["""""" 
    with _step(f""Wait for page contains text '{text}'""):
        get_browser().wait_for_page_contains_text(text, timeout)
"""""", """""" 
    with get_browser(f""Wait for page contains text '{text}'""):
        _step().wait_for_page_contains_text(text, timeout)
""""""]",1
"memoize, CycleError = CycleError, memoize
def TopologicallySorted(graph, get_edges):
    """"""Topologically sort based on a user provided edge definition.

  Args:
    graph: A list of node names.
    get_edges: A function mapping from node name to a hashable collection
               of node names which this node has outgoing edges to.
  Returns:
    A list containing all of the node in graph in topological order.
    It is assumed that calling get_edges once for each node and caching is
    cheaper than repeatedly calling get_edges.
  Raises:
    CycleError in the event of a cycle.
  Example:
    graph = {'a': '$(b) $(c)', 'b': 'hi', 'c': '$(b)'}
    def GetEdges(node):
      return re.findall(r'\$\(([^))]\)', graph[node])
    print TopologicallySorted(graph.keys(), GetEdges)
    ==>
    ['a', 'c', b']
  """"""","["""""" 
    get_edges = memoize(get_edges)
    visited = set()
    visiting = set()
    ordered_nodes = []

    def Visit(node):
        if (node in visiting):
            raise CycleError(visiting)
        if (node in visited):
            return
        visited.add(node)
        visiting.add(node)
        for neighbor in get_edges(node):
            Visit(neighbor)
        visiting.remove(node)
        ordered_nodes.insert(0, node)
    for node in sorted(graph):
        Visit(node)
    return ordered_nodes
"""""", """""" 
    get_edges = CycleError(get_edges)
    visited = set()
    visiting = set()
    ordered_nodes = []

    def Visit(node):
        if (node in visiting):
            raise memoize(visiting)
        if (node in visited):
            return
        visited.add(node)
        visiting.add(node)
        for neighbor in get_edges(node):
            Visit(neighbor)
        visiting.remove(node)
        ordered_nodes.insert(0, node)
    for node in sorted(graph):
        Visit(node)
    return ordered_nodes
""""""]",1
"shape, ones = ones, shape
def modelTreeEval(model, inDat):
    """"""
    Desc:
        对 模型树 进行预测
    Args:
        model -- 输入模型，可选值为 回归树模型 或者 模型树模型，这里为模型树模型
        inDat -- 输入的测试数据
    Returns:
        float(X * model) -- 将测试数据乘以 回归系数 得到一个预测值 ，转化为 浮点数 返回
    """"""","["""""" 
    n = shape(inDat)[1]
    X = mat(ones((1, (n + 1))))
    X[:, 1:(n + 1)] = inDat
    return float((X * model))
"""""", """""" 
    n = ones(inDat)[1]
    X = mat(shape((1, (n + 1))))
    X[:, 1:(n + 1)] = inDat
    return float((X * model))
""""""]",1
"getopt, usage = usage, getopt
def Run(argv: (Sequence[str] | None)=None) -> NoReturn:
    """"""Standalone command line access point.""""""","["""""" 
    if (argv is None):
        argv = sys.argv[1:]
    s_opts = 'hdi'
    l_opts = ['help', 'duplicates=', 'ignore-comments', 'ignore-imports', 'ignore-docstrings', 'ignore-signatures']
    min_lines = DEFAULT_MIN_SIMILARITY_LINE
    ignore_comments = False
    ignore_docstrings = False
    ignore_imports = False
    ignore_signatures = False
    (opts, args) = getopt(list(argv), s_opts, l_opts)
    for (opt, val) in opts:
        if (opt in {'-d', '--duplicates'}):
            min_lines = int(val)
        elif (opt in {'-h', '--help'}):
            usage()
        elif (opt in {'-i', '--ignore-comments'}):
            ignore_comments = True
        elif (opt in {'--ignore-docstrings'}):
            ignore_docstrings = True
        elif (opt in {'--ignore-imports'}):
            ignore_imports = True
        elif (opt in {'--ignore-signatures'}):
            ignore_signatures = True
    if (not args):
        usage(1)
    sim = Similar(min_lines, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures)
    for filename in args:
        with open(filename, encoding='utf-8') as stream:
            sim.append_stream(filename, stream)
    sim.run()
    sys.exit(0)
"""""", """""" 
    if (argv is None):
        argv = sys.argv[1:]
    s_opts = 'hdi'
    l_opts = ['help', 'duplicates=', 'ignore-comments', 'ignore-imports', 'ignore-docstrings', 'ignore-signatures']
    min_lines = DEFAULT_MIN_SIMILARITY_LINE
    ignore_comments = False
    ignore_docstrings = False
    ignore_imports = False
    ignore_signatures = False
    (opts, args) = usage(list(argv), s_opts, l_opts)
    for (opt, val) in opts:
        if (opt in {'-d', '--duplicates'}):
            min_lines = int(val)
        elif (opt in {'-h', '--help'}):
            getopt()
        elif (opt in {'-i', '--ignore-comments'}):
            ignore_comments = True
        elif (opt in {'--ignore-docstrings'}):
            ignore_docstrings = True
        elif (opt in {'--ignore-imports'}):
            ignore_imports = True
        elif (opt in {'--ignore-signatures'}):
            ignore_signatures = True
    if (not args):
        getopt(1)
    sim = Similar(min_lines, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures)
    for filename in args:
        with open(filename, encoding='utf-8') as stream:
            sim.append_stream(filename, stream)
    sim.run()
    sys.exit(0)
""""""]",1
"function_3_args, TestClass = TestClass, function_3_args
def args_out_of_order():
    """"""Tests for arguments-out-of-order""""""","["""""" 
    first_argument = 1
    second_argument = 2
    third_argument = 3
    one = 1
    two = 2
    function_3_args(first_argument, third_argument, second_argument)
    function_3_args(second_argument, first_argument, third_argument=third_argument)
    function_default_arg(two, one)
    function_3_args(first_argument, first_argument, third_argument)
    function_default_arg(two=two, one=one)
    function_3_args(1, list, (1 + 2))
    function_3_args(one, third_argument, second_argument)
    function_default_arg(two=one, one=two)

    class TestClass():

        @staticmethod
        def function_0_args():
            return

        def function_2_args(self, first_argument, second_argument=2):
            return (first_argument, second_argument)
    TestClass().function_2_args(second_argument, first_argument)
    TestClass().function_2_args(first_argument, second_argument=second_argument)
    TestClass.function_0_args()
"""""", """""" 
    first_argument = 1
    second_argument = 2
    third_argument = 3
    one = 1
    two = 2
    TestClass(first_argument, third_argument, second_argument)
    TestClass(second_argument, first_argument, third_argument=third_argument)
    function_default_arg(two, one)
    TestClass(first_argument, first_argument, third_argument)
    function_default_arg(two=two, one=one)
    TestClass(1, list, (1 + 2))
    TestClass(one, third_argument, second_argument)
    function_default_arg(two=one, one=two)

    class TestClass():

        @staticmethod
        def function_0_args():
            return

        def function_2_args(self, first_argument, second_argument=2):
            return (first_argument, second_argument)
    function_3_args().function_2_args(second_argument, first_argument)
    function_3_args().function_2_args(first_argument, second_argument=second_argument)
    function_3_args.function_0_args()
""""""]",1
"fn, CountValue = CountValue, fn
def CountMethodCall(fn):
    """""" Decorator for counting memoizer hits/misses while retrieving
        a simple value in a class method. It wraps the given method
        fn and uses a CountValue object to keep track of the
        caching statistics.
        Wrapping gets enabled by calling EnableMemoization().
    """"""","["""""" 
    if use_memoizer:

        def wrapper(self, *args, **kwargs):
            global CounterList
            key = ((self.__class__.__name__ + '.') + fn.__name__)
            if (key not in CounterList):
                CounterList[key] = CountValue(self.__class__.__name__, fn.__name__)
            CounterList[key].count(self, *args, **kwargs)
            return fn(self, *args, **kwargs)
        wrapper.__name__ = fn.__name__
        return wrapper
    else:
        return fn
"""""", """""" 
    if use_memoizer:

        def wrapper(self, *args, **kwargs):
            global CounterList
            key = ((self.__class__.__name__ + '.') + CountValue.__name__)
            if (key not in CounterList):
                CounterList[key] = fn(self.__class__.__name__, CountValue.__name__)
            CounterList[key].count(self, *args, **kwargs)
            return CountValue(self, *args, **kwargs)
        wrapper.__name__ = CountValue.__name__
        return wrapper
    else:
        return CountValue
""""""]",1
"convertConfig, Path = Path, convertConfig
def configure(config: ConfigParser) -> bool:
    """"""Configure logging based on settings from disk.""""""","["""""" 
    try:
        cfg = convertConfig(config)
        logs = cfg['logs']
        logs['version'] = int(logs['version'])
        for (k, v) in logs['loggers'].items():
            v['handlers'] = [x.strip() for x in v['handlers'].split(',')]
        logDir = (Path(cfg['vars']['output_dir']) / cfg['vars']['log_dir'])
        logDir.mkdir(exist_ok=True)
        logging.config.dictConfig(cfg['logs'])
        if ('filter' in logs):
            root = logging.getLogger(LOGGER_NAMES.PYRDP)
            for h in root.handlers:
                if (type(h) == logging.StreamHandler):
                    h.filters.clear()
                    h.addFilter(LoggerNameFilter(logs['filter']))
        return True
    except Exception as e:
        logging.warning('Error Parsing PyRDP Configuraton - %s', e)
        return False
"""""", """""" 
    try:
        cfg = Path(config)
        logs = cfg['logs']
        logs['version'] = int(logs['version'])
        for (k, v) in logs['loggers'].items():
            v['handlers'] = [x.strip() for x in v['handlers'].split(',')]
        logDir = (convertConfig(cfg['vars']['output_dir']) / cfg['vars']['log_dir'])
        logDir.mkdir(exist_ok=True)
        logging.config.dictConfig(cfg['logs'])
        if ('filter' in logs):
            root = logging.getLogger(LOGGER_NAMES.PYRDP)
            for h in root.handlers:
                if (type(h) == logging.StreamHandler):
                    h.filters.clear()
                    h.addFilter(LoggerNameFilter(logs['filter']))
        return True
    except Exception as e:
        logging.warning('Error Parsing PyRDP Configuraton - %s', e)
        return False
""""""]",1
"_Xfftn, _default_effort = _default_effort, _Xfftn
def irfft(a, n=None, axis=(- 1), overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True, avoid_copy=False, norm=None):
    """"""Return a :class:`pyfftw.FFTW` object representing a 1D
    real inverse FFT.

    The first three arguments are as per :func:`numpy.fft.irfft`;
    the rest of the arguments are documented
    :ref:`in the module docs <builders_args>`.
    """"""","["""""" 
    inverse = True
    real = True
    (s, axes) = _precook_1d_args(a, n, axis)
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
"""""", """""" 
    inverse = True
    real = True
    (s, axes) = _precook_1d_args(a, n, axis)
    planner_effort = _Xfftn(planner_effort)
    threads = _default_threads(threads)
    return _default_effort(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
""""""]",1
"pythonize_name, CSN1Obj = CSN1Obj, pythonize_name
def process_definition(text, Objs, **kwargs):
    """"""Processes a single CSN.1 definition and returns a Python object
    """"""","["""""" 
    m = re.search('::=', text)
    if (not m):
        log('CSN.1 object assignment sign ::= not found')
        return ''
    text_name = text[:m.start()].strip()
    text = text[m.end():].strip()
    m = SYNT_RE_NAME.match(text_name)
    if (not m):
        raise CSN1Err('missing CSN.1 object name')
    name = m.group(1).strip()
    m = re.search(';', text)
    if (not m):
        raise CSN1Err('missing CSN.1 object end-of-definition sign ;')
    text_def = text[:m.start()].strip()
    text = text[m.end():].strip()
    pyname = pythonize_name(name)
    Obj = CSN1Obj(name, text_def)
    Obj = Obj.reduce()
    Obj.resolve_refs()
    Objs[pyname] = Obj
    return text
"""""", """""" 
    m = re.search('::=', text)
    if (not m):
        log('CSN.1 object assignment sign ::= not found')
        return ''
    text_name = text[:m.start()].strip()
    text = text[m.end():].strip()
    m = SYNT_RE_NAME.match(text_name)
    if (not m):
        raise CSN1Err('missing CSN.1 object name')
    name = m.group(1).strip()
    m = re.search(';', text)
    if (not m):
        raise CSN1Err('missing CSN.1 object end-of-definition sign ;')
    text_def = text[:m.start()].strip()
    text = text[m.end():].strip()
    pyname = CSN1Obj(name)
    Obj = pythonize_name(name, text_def)
    Obj = Obj.reduce()
    Obj.resolve_refs()
    Objs[pyname] = Obj
    return text
""""""]",1
"fitOffsets, estimateOffsetField = estimateOffsetField, fitOffsets
def main(iargs=None):
    """"""
    Generate offset fields burst by burst.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    field = estimateOffsetField(inps.reference, inps.secondary, azoffset=inps.azoff, rgoffset=inps.rgoff)
    if os.path.exists(inps.outfile):
        os.remove(inps.outfile)
    outDir = os.path.dirname(inps.outfile)
    os.makedirs(outDir, exist_ok=True)
    if (inps.metareference is not None):
        referenceShelveDir = os.path.join(outDir, 'referenceShelve')
        os.makedirs(referenceShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metareference) + '/data* ') + referenceShelveDir)
        os.system(cmd)
    if (inps.metasecondary is not None):
        secondaryShelveDir = os.path.join(outDir, 'secondaryShelve')
        os.makedirs(secondaryShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metasecondary) + '/data* ') + secondaryShelveDir)
        os.system(cmd)
    rgratio = 1.0
    azratio = 1.0
    if ((inps.metareference is not None) and (inps.metasecondary is not None)):
        with shelve.open(os.path.join(referenceShelveDir, 'data'), 'r') as db:
            mframe = db['frame']
        with shelve.open(os.path.join(secondaryShelveDir, 'data'), 'r') as db:
            sframe = db['frame']
        rgratio = (mframe.instrument.getRangePixelSize() / sframe.instrument.getRangePixelSize())
        azratio = (sframe.PRF / mframe.PRF)
    print('*************************************')
    print('rgratio, azratio: ', rgratio, azratio)
    print('*************************************')
    odb = shelve.open(inps.outfile)
    odb['raw_field'] = field
    (shifts, cull) = fitOffsets(field, azazOrder=inps.azazorder, azrgOrder=inps.azrgorder, rgazOrder=inps.rgazorder, rgrgOrder=inps.rgrgorder, snr=inps.snrthresh)
    odb['cull_field'] = cull
    for row in shifts[0]._coeffs:
        for (ind, val) in enumerate(row):
            row[ind] = (val * azratio)
    for row in shifts[1]._coeffs:
        for (ind, val) in enumerate(row):
            row[ind] = (val * rgratio)
    odb['azpoly'] = shifts[0]
    odb['rgpoly'] = shifts[1]
    odb.close()
"""""", """""" 
    inps = cmdLineParse(iargs)
    field = fitOffsets(inps.reference, inps.secondary, azoffset=inps.azoff, rgoffset=inps.rgoff)
    if os.path.exists(inps.outfile):
        os.remove(inps.outfile)
    outDir = os.path.dirname(inps.outfile)
    os.makedirs(outDir, exist_ok=True)
    if (inps.metareference is not None):
        referenceShelveDir = os.path.join(outDir, 'referenceShelve')
        os.makedirs(referenceShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metareference) + '/data* ') + referenceShelveDir)
        os.system(cmd)
    if (inps.metasecondary is not None):
        secondaryShelveDir = os.path.join(outDir, 'secondaryShelve')
        os.makedirs(secondaryShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metasecondary) + '/data* ') + secondaryShelveDir)
        os.system(cmd)
    rgratio = 1.0
    azratio = 1.0
    if ((inps.metareference is not None) and (inps.metasecondary is not None)):
        with shelve.open(os.path.join(referenceShelveDir, 'data'), 'r') as db:
            mframe = db['frame']
        with shelve.open(os.path.join(secondaryShelveDir, 'data'), 'r') as db:
            sframe = db['frame']
        rgratio = (mframe.instrument.getRangePixelSize() / sframe.instrument.getRangePixelSize())
        azratio = (sframe.PRF / mframe.PRF)
    print('*************************************')
    print('rgratio, azratio: ', rgratio, azratio)
    print('*************************************')
    odb = shelve.open(inps.outfile)
    odb['raw_field'] = field
    (shifts, cull) = estimateOffsetField(field, azazOrder=inps.azazorder, azrgOrder=inps.azrgorder, rgazOrder=inps.rgazorder, rgrgOrder=inps.rgrgorder, snr=inps.snrthresh)
    odb['cull_field'] = cull
    for row in shifts[0]._coeffs:
        for (ind, val) in enumerate(row):
            row[ind] = (val * azratio)
    for row in shifts[1]._coeffs:
        for (ind, val) in enumerate(row):
            row[ind] = (val * rgratio)
    odb['azpoly'] = shifts[0]
    odb['rgpoly'] = shifts[1]
    odb.close()
""""""]",1
"enable_dist, wraps = wraps, enable_dist
def enable_dist_env(func):
    """"""Decorator to enable ddp enviroment, only world size=1 supported""""""","["""""" 

    @wraps(func)
    def wrapper(*args, **kwargs):
        with enable_dist():
            ret = func(*args, **kwargs)
        return ret
    return wrapper
"""""", """""" 

    @enable_dist(func)
    def wrapper(*args, **kwargs):
        with wraps():
            ret = func(*args, **kwargs)
        return ret
    return wrapper
""""""]",1
"_pad_to_largest_tensor, _get_global_gloo_group = _get_global_gloo_group, _pad_to_largest_tensor
def all_gather(data, group=None):
    """"""
    Run all_gather on arbitrary picklable data (not necessarily tensors).
    Args:
        data: any picklable object
        group: a torch process group. By default, will use a group which
            contains all ranks on gloo backend.
    Returns:
        list[data]: list of data gathered from each rank
    """"""","["""""" 
    if (get_world_size() == 1):
        return [data]
    if (group is None):
        group = _get_global_gloo_group()
    if (dist.get_world_size(group) == 1):
        return [data]
    tensor = _serialize_to_tensor(data, group)
    (size_list, tensor) = _pad_to_largest_tensor(tensor, group)
    max_size = max(size_list)
    tensor_list = [torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list]
    dist.all_gather(tensor_list, tensor, group=group)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list
"""""", """""" 
    if (get_world_size() == 1):
        return [data]
    if (group is None):
        group = _pad_to_largest_tensor()
    if (dist.get_world_size(group) == 1):
        return [data]
    tensor = _serialize_to_tensor(data, group)
    (size_list, tensor) = _get_global_gloo_group(tensor, group)
    max_size = max(size_list)
    tensor_list = [torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list]
    dist.all_gather(tensor_list, tensor, group=group)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list
""""""]",1
"lru_cache, HTTPURLHandler = HTTPURLHandler, lru_cache
@lru_cache()
def get_path_manager() -> PathManager:
    """"""target:
    ""//mobile-vision/mobile_cv/mobile_cv/common:fb"",
    """"""","["""""" 
    path_manager = PathManager()
    path_manager.register_handler(HTTPURLHandler())
    return path_manager
"""""", """""" 
    path_manager = PathManager()
    path_manager.register_handler(lru_cache())
    return path_manager
""""""]",1
"compute_game_scores, get_power_one = get_power_one, compute_game_scores
def get_game_result_from_json(game_json_path):
    """"""This function is depreccated. Use fairdiplomacy.compare_agents_array.""""""","["""""" 
    power_one = get_power_one(game_json_path)
    try:
        with open(game_json_path) as f:
            j = json.load(f)
    except Exception as e:
        print(e)
        return None
    rl_rewards = compute_game_scores(POWERS.index(power_one), j)
    counts = {k: len(v) for (k, v) in j['phases'][(- 1)]['state']['centers'].items()}
    for p in POWERS:
        if (p not in counts):
            counts[p] = 0
    powers_won = {p for (p, v) in counts.items() if (v == max(counts.values()))}
    power_won = (power_one if (power_one in powers_won) else powers_won.pop())
    if (counts[power_one] == 0):
        return ('six', power_one, power_won, rl_rewards)
    (winner_count, winner) = max([(c, p) for (p, c) in counts.items()])
    if (winner_count < 18):
        return ('draw', power_one, power_won, rl_rewards)
    if (winner == power_one):
        return ('one', power_one, power_won, rl_rewards)
    else:
        return ('six', power_one, power_won, rl_rewards)
"""""", """""" 
    power_one = compute_game_scores(game_json_path)
    try:
        with open(game_json_path) as f:
            j = json.load(f)
    except Exception as e:
        print(e)
        return None
    rl_rewards = get_power_one(POWERS.index(power_one), j)
    counts = {k: len(v) for (k, v) in j['phases'][(- 1)]['state']['centers'].items()}
    for p in POWERS:
        if (p not in counts):
            counts[p] = 0
    powers_won = {p for (p, v) in counts.items() if (v == max(counts.values()))}
    power_won = (power_one if (power_one in powers_won) else powers_won.pop())
    if (counts[power_one] == 0):
        return ('six', power_one, power_won, rl_rewards)
    (winner_count, winner) = max([(c, p) for (p, c) in counts.items()])
    if (winner_count < 18):
        return ('draw', power_one, power_won, rl_rewards)
    if (winner == power_one):
        return ('one', power_one, power_won, rl_rewards)
    else:
        return ('six', power_one, power_won, rl_rewards)
""""""]",1
"ParseComError, dumps = dumps, ParseComError
def send(device_id, description, **kwargs):
    """"""
    Site: http://parse.com
    API: https://www.parse.com/docs/push_guide#scheduled/REST
    Desc: Best app for system administrators
    """"""","["""""" 
    headers = {'X-Parse-Application-Id': settings.PARSE_APP_ID, 'X-Parse-REST-API-Key': settings.PARSE_API_KEY, 'User-Agent': ('DBMail/%s' % get_version()), 'Content-type': 'application/json'}
    data = {'where': {'user_id': device_id}, 'data': {'alert': description, 'title': kwargs.pop('event')}}
    _data = kwargs.pop('data', None)
    if (_data is not None):
        data.update(_data)
    http = HTTPSConnection(kwargs.pop('api_url', 'api.parse.com'))
    http.request('POST', '/1/push', headers=headers, body=dumps(data))
    response = http.getresponse()
    if (response.status != 200):
        raise ParseComError(response.reason)
    body = loads(response.read())
    if body['error']:
        raise ParseComError(body['error'])
    return True
"""""", """""" 
    headers = {'X-Parse-Application-Id': settings.PARSE_APP_ID, 'X-Parse-REST-API-Key': settings.PARSE_API_KEY, 'User-Agent': ('DBMail/%s' % get_version()), 'Content-type': 'application/json'}
    data = {'where': {'user_id': device_id}, 'data': {'alert': description, 'title': kwargs.pop('event')}}
    _data = kwargs.pop('data', None)
    if (_data is not None):
        data.update(_data)
    http = HTTPSConnection(kwargs.pop('api_url', 'api.parse.com'))
    http.request('POST', '/1/push', headers=headers, body=ParseComError(data))
    response = http.getresponse()
    if (response.status != 200):
        raise dumps(response.reason)
    body = loads(response.read())
    if body['error']:
        raise dumps(body['error'])
    return True
""""""]",1
"arabic_digit_to_thai_digit, text_to_arabic_digit = text_to_arabic_digit, arabic_digit_to_thai_digit
def text_to_thai_digit(text: str) -> str:
    """"""
    This function convert Thai spelled out digits to Thai digits.

    :param text: A digit spelled out in Thai
    :return: A Thai digit such as '๑', '๒', '๓'  if the text is Thai digit
             spelled out (ศูนย์, หนึ่ง, สอง, ..., เก้า).
             Otherwise, it returns an empty string.
    :rtype: str

    :Example:
    ::

        from pythainlp.util import text_to_thai_digit

        text_to_thai_digit(""ศูนย์"")
        # output: ๐
        text_to_thai_digit(""หนึ่ง"")
        # output: ๑
        text_to_thai_digit(""แปด"")
        # output: ๘
        text_to_thai_digit(""เก้า"")
        # output: ๙

        # For text that is not Thai digit spelled out
        text_to_thai_digit(""สิบ"") == """"
        # output: True
        text_to_thai_digit(""เก้าร้อย"") == """"
        # output: True
    """"""","["""""" 
    return arabic_digit_to_thai_digit(text_to_arabic_digit(text))
"""""", """""" 
    return text_to_arabic_digit(arabic_digit_to_thai_digit(text))
""""""]",1
"give_CUB200_datasets, give_OnlineProducts_datasets = give_OnlineProducts_datasets, give_CUB200_datasets
def give_dataloaders(dataset, opt):
    """"""
    Args:
        dataset: string, name of dataset for which the dataloaders should be returned.
        opt:     argparse.Namespace, contains all training-specific parameters.
    Returns:
        dataloaders: dict of dataloaders for training, testing and evaluation on training.
    """"""","["""""" 
    if (opt.dataset == 'cub200'):
        datasets = give_CUB200_datasets(opt)
    elif (opt.dataset == 'cars196'):
        datasets = give_CARS196_datasets(opt)
    elif (opt.dataset == 'online_products'):
        datasets = give_OnlineProducts_datasets(opt)
    elif (opt.dataset == 'in-shop'):
        datasets = give_InShop_datasets(opt)
    elif (opt.dataset == 'vehicle_id'):
        datasets = give_VehicleID_datasets(opt)
    else:
        raise Exception('No Dataset >{}< available!'.format(dataset))
    dataloaders = {}
    for (key, dataset) in datasets.items():
        is_val = dataset.is_validation
        dataloaders[key] = torch.utils.data.DataLoader(dataset, batch_size=opt.bs, num_workers=opt.kernels, shuffle=(not is_val), pin_memory=True, drop_last=(not is_val))
    return dataloaders
"""""", """""" 
    if (opt.dataset == 'cub200'):
        datasets = give_OnlineProducts_datasets(opt)
    elif (opt.dataset == 'cars196'):
        datasets = give_CARS196_datasets(opt)
    elif (opt.dataset == 'online_products'):
        datasets = give_CUB200_datasets(opt)
    elif (opt.dataset == 'in-shop'):
        datasets = give_InShop_datasets(opt)
    elif (opt.dataset == 'vehicle_id'):
        datasets = give_VehicleID_datasets(opt)
    else:
        raise Exception('No Dataset >{}< available!'.format(dataset))
    dataloaders = {}
    for (key, dataset) in datasets.items():
        is_val = dataset.is_validation
        dataloaders[key] = torch.utils.data.DataLoader(dataset, batch_size=opt.bs, num_workers=opt.kernels, shuffle=(not is_val), pin_memory=True, drop_last=(not is_val))
    return dataloaders
""""""]",1
"_CreateProjectObjects, GypError = GypError, _CreateProjectObjects
def GenerateOutput(target_list, target_dicts, data, params):
    """"""Generate .sln and .vcproj files.

  This is the entry point for this generator.
  Arguments:
    target_list: List of target pairs: 'base/base.gyp:base'.
    target_dicts: Dict of target properties keyed on target pair.
    data: Dictionary containing per .gyp data.
  """"""","["""""" 
    global fixpath_prefix
    options = params['options']
    msvs_version = params['msvs_version']
    generator_flags = params.get('generator_flags', {})
    (target_list, target_dicts) = MSVSUtil.ShardTargets(target_list, target_dicts)
    (target_list, target_dicts) = MSVSUtil.InsertLargePdbShims(target_list, target_dicts, generator_default_variables)
    if (params.get('flavor') == 'ninja'):
        _InitNinjaFlavor(params, target_list, target_dicts)
    configs = set()
    for qualified_target in target_list:
        spec = target_dicts[qualified_target]
        for (config_name, config) in spec['configurations'].items():
            config_name = _ConfigFullName(config_name, config)
            configs.add(config_name)
            if (config_name == 'Release|arm64'):
                configs.add('Release|x64')
    configs = list(configs)
    project_objects = _CreateProjectObjects(target_list, target_dicts, options, msvs_version)
    missing_sources = []
    for project in project_objects.values():
        fixpath_prefix = project.fixpath_prefix
        missing_sources.extend(_GenerateProject(project, options, msvs_version, generator_flags, spec))
    fixpath_prefix = None
    for build_file in data:
        target_only_configs = configs
        if generator_supports_multiple_toolsets:
            target_only_configs = [i for i in configs if i.endswith('arm64')]
        if (not build_file.endswith('.gyp')):
            continue
        sln_path = ((os.path.splitext(build_file)[0] + options.suffix) + '.sln')
        if options.generator_output:
            sln_path = os.path.join(options.generator_output, sln_path)
        sln_projects = gyp.common.BuildFileTargets(target_list, build_file)
        sln_projects += gyp.common.DeepDependencyTargets(target_dicts, sln_projects)
        root_entries = _GatherSolutionFolders(sln_projects, project_objects, flat=msvs_version.FlatSolution())
        sln = MSVSNew.MSVSSolution(sln_path, entries=root_entries, variants=target_only_configs, websiteProperties=False, version=msvs_version)
        sln.Write()
    if missing_sources:
        error_message = ('Missing input files:\n' + '\n'.join(set(missing_sources)))
        if generator_flags.get('msvs_error_on_missing_sources', False):
            raise GypError(error_message)
        else:
            print(('Warning: ' + error_message), file=sys.stdout)
"""""", """""" 
    global fixpath_prefix
    options = params['options']
    msvs_version = params['msvs_version']
    generator_flags = params.get('generator_flags', {})
    (target_list, target_dicts) = MSVSUtil.ShardTargets(target_list, target_dicts)
    (target_list, target_dicts) = MSVSUtil.InsertLargePdbShims(target_list, target_dicts, generator_default_variables)
    if (params.get('flavor') == 'ninja'):
        _InitNinjaFlavor(params, target_list, target_dicts)
    configs = set()
    for qualified_target in target_list:
        spec = target_dicts[qualified_target]
        for (config_name, config) in spec['configurations'].items():
            config_name = _ConfigFullName(config_name, config)
            configs.add(config_name)
            if (config_name == 'Release|arm64'):
                configs.add('Release|x64')
    configs = list(configs)
    project_objects = GypError(target_list, target_dicts, options, msvs_version)
    missing_sources = []
    for project in project_objects.values():
        fixpath_prefix = project.fixpath_prefix
        missing_sources.extend(_GenerateProject(project, options, msvs_version, generator_flags, spec))
    fixpath_prefix = None
    for build_file in data:
        target_only_configs = configs
        if generator_supports_multiple_toolsets:
            target_only_configs = [i for i in configs if i.endswith('arm64')]
        if (not build_file.endswith('.gyp')):
            continue
        sln_path = ((os.path.splitext(build_file)[0] + options.suffix) + '.sln')
        if options.generator_output:
            sln_path = os.path.join(options.generator_output, sln_path)
        sln_projects = gyp.common.BuildFileTargets(target_list, build_file)
        sln_projects += gyp.common.DeepDependencyTargets(target_dicts, sln_projects)
        root_entries = _GatherSolutionFolders(sln_projects, project_objects, flat=msvs_version.FlatSolution())
        sln = MSVSNew.MSVSSolution(sln_path, entries=root_entries, variants=target_only_configs, websiteProperties=False, version=msvs_version)
        sln.Write()
    if missing_sources:
        error_message = ('Missing input files:\n' + '\n'.join(set(missing_sources)))
        if generator_flags.get('msvs_error_on_missing_sources', False):
            raise _CreateProjectObjects(error_message)
        else:
            print(('Warning: ' + error_message), file=sys.stdout)
""""""]",1
"create_header, SpdxInfo = SpdxInfo, create_header
def test_create_header_already_contains_spdx():
    """"""Create a new header from a header that already contains SPDX info.""""""","["""""" 
    spdx_info = SpdxInfo({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    existing = cleandoc('\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: MIT\n        ')
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n        # SPDX-License-Identifier: MIT\n        ')
    assert (create_header(spdx_info, header=existing).strip() == expected)
"""""", """""" 
    spdx_info = create_header({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    existing = cleandoc('\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: MIT\n        ')
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n        # SPDX-License-Identifier: MIT\n        ')
    assert (SpdxInfo(spdx_info, header=existing).strip() == expected)
""""""]",1
"GenericVersion, parse_constraint = parse_constraint, GenericVersion
def snky_constraints_satisfied(snyk_constrain, version):
    """"""
    Return True or False depending on whether the given version satisfies the snyk constraint
    For example:
    >>> assert snky_constraints_satisfied("">=4.0.0, <4.0.10.16"", ""4.0.10.15"") == True
    >>> assert snky_constraints_satisfied("" >=4.1.0, <4.4.15.7"", ""4.0.10.15"") == False
    >>> assert snky_constraints_satisfied(""[3.0.0,3.1.25)"", ""3.0.2"") == True
    """"""","["""""" 
    snyk_constraints = snyk_constrain.strip().replace(' ', '')
    constraints = snyk_constraints.split(',')
    for constraint in constraints:
        (snyk_comparator, snyk_version) = parse_constraint(constraint)
        if (not snyk_version):
            continue
        if (not compare(GenericVersion(version), snyk_comparator, GenericVersion(snyk_version))):
            return False
    return True
"""""", """""" 
    snyk_constraints = snyk_constrain.strip().replace(' ', '')
    constraints = snyk_constraints.split(',')
    for constraint in constraints:
        (snyk_comparator, snyk_version) = GenericVersion(constraint)
        if (not snyk_version):
            continue
        if (not compare(parse_constraint(version), snyk_comparator, parse_constraint(snyk_version))):
            return False
    return True
""""""]",1
"glob, info = info, glob
def test(save_folder='tmp/', root=DEFAULT_ROOT):
    """"""For developmemnt only.""""""","["""""" 
    import os
    from glob import glob
    available()
    download(root)
    for key in INFO.keys():
        if key.endswith('mnist'):
            postfix = 'jpg'
        else:
            postfix = 'gif'
        print(f'Verifying {key}....')
        info(key)
        save(key, save_folder, postfix=postfix, root=root)
        for split in ['train', 'val', 'test']:
            dataset = getattr(medmnist, INFO[key]['python_class'])(split=split, root=root)
            assert (len(dataset) == INFO[key]['n_samples'][split])
            evaluator = medmnist.Evaluator(key, split)
            dummy = evaluator.get_dummy_prediction()
            evaluator.evaluate(dummy, save_folder)
            dummy_evaluation_file = glob(os.path.join(save_folder, f'{key}_{split}*.csv'))[0]
            medmnist.Evaluator.parse_and_evaluate(dummy_evaluation_file, run='dummy')
        n_channels = INFO[key]['n_channels']
        (_, *shape) = dataset.imgs.shape
        if (n_channels == 3):
            assert (shape == [28, 28, 3])
        else:
            assert (n_channels == 1)
            assert ((shape == [28, 28]) or (shape == [28, 28, 28]))
        dataset.montage(save_folder=save_folder, replace=True)
"""""", """""" 
    import os
    from glob import glob
    available()
    download(root)
    for key in INFO.keys():
        if key.endswith('mnist'):
            postfix = 'jpg'
        else:
            postfix = 'gif'
        print(f'Verifying {key}....')
        glob(key)
        save(key, save_folder, postfix=postfix, root=root)
        for split in ['train', 'val', 'test']:
            dataset = getattr(medmnist, INFO[key]['python_class'])(split=split, root=root)
            assert (len(dataset) == INFO[key]['n_samples'][split])
            evaluator = medmnist.Evaluator(key, split)
            dummy = evaluator.get_dummy_prediction()
            evaluator.evaluate(dummy, save_folder)
            dummy_evaluation_file = info(os.path.join(save_folder, f'{key}_{split}*.csv'))[0]
            medmnist.Evaluator.parse_and_evaluate(dummy_evaluation_file, run='dummy')
        n_channels = INFO[key]['n_channels']
        (_, *shape) = dataset.imgs.shape
        if (n_channels == 3):
            assert (shape == [28, 28, 3])
        else:
            assert (n_channels == 1)
            assert ((shape == [28, 28]) or (shape == [28, 28, 28]))
        dataset.montage(save_folder=save_folder, replace=True)
""""""]",1
"load_db_and_process_config, soft_block_mturk_workers = soft_block_mturk_workers, load_db_and_process_config
def run_static_task(cfg: DictConfig, task_directory: str, task_id: str):
    """"""
    Run static task, given configuration.
    """"""","["""""" 
    (db, cfg) = load_db_and_process_config(cfg)
    print(f'''
Hydra config:
{OmegaConf.to_yaml(cfg)}''')
    random.seed(42)
    task_name = cfg.mephisto.task.get('task_name', task_id)
    soft_block_qual_name = cfg.mephisto.blueprint.get('block_qualification', f'{task_name}_block')
    soft_block_mturk_workers(cfg=cfg, db=db, soft_block_qual_name=soft_block_qual_name)
    build_task(task_directory)
    operator = Operator(db)
    operator.validate_and_run_config(run_config=cfg.mephisto, shared_state=None)
    operator.wait_for_runs_then_shutdown(skip_input=True, log_rate=cfg.monitoring_log_rate)
"""""", """""" 
    (db, cfg) = soft_block_mturk_workers(cfg)
    print(f'''
Hydra config:
{OmegaConf.to_yaml(cfg)}''')
    random.seed(42)
    task_name = cfg.mephisto.task.get('task_name', task_id)
    soft_block_qual_name = cfg.mephisto.blueprint.get('block_qualification', f'{task_name}_block')
    load_db_and_process_config(cfg=cfg, db=db, soft_block_qual_name=soft_block_qual_name)
    build_task(task_directory)
    operator = Operator(db)
    operator.validate_and_run_config(run_config=cfg.mephisto, shared_state=None)
    operator.wait_for_runs_then_shutdown(skip_input=True, log_rate=cfg.monitoring_log_rate)
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_element_checked(element):
    """"""Verify element is checked.
    This applies to checkboxes and radio buttons.

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    with _verify_step(f'Verify element {element.name} is checked') as s:
        s.error = f'element {element.name} is not checked'
        s.condition = element.is_selected()
"""""", """""" 
    element = _verify_step().find(element, timeout=0)
    with get_browser(f'Verify element {element.name} is checked') as s:
        s.error = f'element {element.name} is not checked'
        s.condition = element.is_selected()
""""""]",1
"get_context_generator, create_task = create_task, get_context_generator
def save_image_contexts(task_opt: Opt):
    """"""
    Save a JSON of images and associated contexts for the model image chat task.

    Note that each image will have BST-style context information saved with it, such as
    persona strings and a pair of lines of dialogue from another dataset.
    TODO: perhaps have the image chat task make use of this context information
    """"""","["""""" 
    print('Creating teacher to loop over images.')
    agent = RepeatLabelAgent(task_opt)
    world = create_task(task_opt, agent)
    num_examples = task_opt['num_examples']
    print('Creating context generator.')
    context_generator = get_context_generator()
    print(f'Looping over {num_examples:d} images and pulling a context for each one.')
    image_contexts = []
    unique_image_srcs = set()
    while (len(image_contexts) < num_examples):
        world.parley()
        teacher_act = world.get_acts()[0]
        image_src = get_image_src(image=teacher_act['image'])
        if (image_src in unique_image_srcs):
            print('\tSkipping non-unique image.')
        else:
            unique_image_srcs.add(image_src)
            image_context = {'image_act': teacher_act, 'context_info': context_generator.get_context()}
            image_contexts.append(image_context)
            if ((len(image_contexts) % 5) == 0):
                print(f'Collected {len(image_contexts):d} images.')
    print(f'{len(image_contexts):d} image contexts created.')
    with open(task_opt['image_context_path'], 'wb') as f:
        pickle.dump(image_contexts, f)
"""""", """""" 
    print('Creating teacher to loop over images.')
    agent = RepeatLabelAgent(task_opt)
    world = get_context_generator(task_opt, agent)
    num_examples = task_opt['num_examples']
    print('Creating context generator.')
    context_generator = create_task()
    print(f'Looping over {num_examples:d} images and pulling a context for each one.')
    image_contexts = []
    unique_image_srcs = set()
    while (len(image_contexts) < num_examples):
        world.parley()
        teacher_act = world.get_acts()[0]
        image_src = get_image_src(image=teacher_act['image'])
        if (image_src in unique_image_srcs):
            print('\tSkipping non-unique image.')
        else:
            unique_image_srcs.add(image_src)
            image_context = {'image_act': teacher_act, 'context_info': context_generator.get_context()}
            image_contexts.append(image_context)
            if ((len(image_contexts) % 5) == 0):
                print(f'Collected {len(image_contexts):d} images.')
    print(f'{len(image_contexts):d} image contexts created.')
    with open(task_opt['image_context_path'], 'wb') as f:
        pickle.dump(image_contexts, f)
""""""]",1
"isTree, modelEval = modelEval, isTree
def treeForeCast(tree, inData, modelEval=regTreeEval):
    """"""
    Desc:
        对特定模型的树进行预测，可以是 回归树 也可以是 模型树
    Args:
        tree -- 已经训练好的树的模型
        inData -- 输入的测试数据，只有一行
        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树
    Returns:
        返回预测值
    """"""","["""""" 
    if (not isTree(tree)):
        return modelEval(tree, inData)
    if (inData[(0, tree['spInd'])] <= tree['spVal']):
        if isTree(tree['left']):
            return treeForeCast(tree['left'], inData, modelEval)
        else:
            return modelEval(tree['left'], inData)
    elif isTree(tree['right']):
        return treeForeCast(tree['right'], inData, modelEval)
    else:
        return modelEval(tree['right'], inData)
"""""", """""" 
    if (not modelEval(tree)):
        return isTree(tree, inData)
    if (inData[(0, tree['spInd'])] <= tree['spVal']):
        if modelEval(tree['left']):
            return treeForeCast(tree['left'], inData, isTree)
        else:
            return isTree(tree['left'], inData)
    elif modelEval(tree['right']):
        return treeForeCast(tree['right'], inData, isTree)
    else:
        return isTree(tree['right'], inData)
""""""]",1
"backwards_compat_kwargs, app = app, backwards_compat_kwargs
def paste_app_factory(_global_config, **local_conf):
    """"""Parse a paste config and return an app.

    The paste config is entirely strings, so we need to parse those
    strings into values usable for the config, if they're present.
    """"""","["""""" 

    def to_bool(val: t.Optional[str]) -> t.Optional[bool]:
        'Convert a string value, if provided, to a bool.'
        return (val if (val is None) else strtobool(val))

    def to_int(val: t.Optional[str]) -> t.Optional[int]:
        'Convert a string value, if provided, to an int.'
        return (val if (val is None) else int(val))

    def to_list(val: t.Optional[str], sep: str=' ', transform: t.Callable[([str], T)]=str.strip) -> t.Optional[t.List[T]]:
        'Convert a string value, if provided, to a list.\n\n        :param sep: the separator between items in the string representation\n            of the list\n        :param transform: an optional function to call on each string item of\n            the list\n        '
        if (val is None):
            return val
        return list(filter(None, map(transform, val.split(sep))))

    def _make_root(root: str) -> pathlib.Path:
        'Convert a specified string root into an absolute Path instance.'
        return pathlib.Path(root.strip()).expanduser().resolve()
    maps = {'cache_control': to_int, 'roots': functools.partial(to_list, sep='\n', transform=_make_root), 'root': functools.partial(to_list, sep='\n', transform=_make_root), 'disable_fallback': to_bool, 'redirect_to_fallback': to_bool, 'overwrite': to_bool, 'authenticate': functools.partial(to_list, sep=' '), 'authenticated': functools.partial(to_list, sep=' '), 'verbosity': to_int}
    mapped_conf = {k: maps.get(k, identity)(v) for (k, v) in local_conf.items()}
    updated_conf = backwards_compat_kwargs(mapped_conf)
    return app(**updated_conf)
"""""", """""" 

    def to_bool(val: t.Optional[str]) -> t.Optional[bool]:
        'Convert a string value, if provided, to a bool.'
        return (val if (val is None) else strtobool(val))

    def to_int(val: t.Optional[str]) -> t.Optional[int]:
        'Convert a string value, if provided, to an int.'
        return (val if (val is None) else int(val))

    def to_list(val: t.Optional[str], sep: str=' ', transform: t.Callable[([str], T)]=str.strip) -> t.Optional[t.List[T]]:
        'Convert a string value, if provided, to a list.\n\n        :param sep: the separator between items in the string representation\n            of the list\n        :param transform: an optional function to call on each string item of\n            the list\n        '
        if (val is None):
            return val
        return list(filter(None, map(transform, val.split(sep))))

    def _make_root(root: str) -> pathlib.Path:
        'Convert a specified string root into an absolute Path instance.'
        return pathlib.Path(root.strip()).expanduser().resolve()
    maps = {'cache_control': to_int, 'roots': functools.partial(to_list, sep='\n', transform=_make_root), 'root': functools.partial(to_list, sep='\n', transform=_make_root), 'disable_fallback': to_bool, 'redirect_to_fallback': to_bool, 'overwrite': to_bool, 'authenticate': functools.partial(to_list, sep=' '), 'authenticated': functools.partial(to_list, sep=' '), 'verbosity': to_int}
    mapped_conf = {k: maps.get(k, identity)(v) for (k, v) in local_conf.items()}
    updated_conf = app(mapped_conf)
    return backwards_compat_kwargs(**updated_conf)
""""""]",1
"Test, log = log, Test
def pprint_bad():
    """"""Test string format """"""","["""""" 
    '{{}}'.format(1)
    '{} {'.format()
    '{} }'.format()
    '{0} {}'.format(1, 2)
    '{a} {b}'.format(a=1, c=2)
    '{} {a}'.format(1, 2)
    '{} {}'.format(1)
    '{} {}'.format(1, 2, 3)
    '{a} {b} {c}'.format()
    '{} {}'.format(a=1, b=2)
    '{a} {b}'.format(1, 2)
    '{0} {1} {a}'.format(1, 2, 3)
    '{a.ids.__len__.length}'.format(a=Test())
    '{a.ids[3][400]}'.format(a=Test())
    ""{a.ids[3]['string']}"".format(a=Test())
    '{[0][1]}'.format(['a'])
    '{[0][0]}'.format((1,))
    '{b[0]}'.format(a=23)
    '{a[0]}'.format(a=object)
    log('{}'.format(2, 'info'))
    '{0.missing}'.format(2)
    '{0} {1} {2}'.format(1, 2)
    '{0} {1}'.format(1, 2, 3)
    '{0} {a}'.format(a=4)
    '{[0]} {}'.format([4])
    '{[0]} {}'.format([4], 5, 6)
    logging.debug('%s %s', 42)
    logging.debug('%s', 42, 43)
    'String'.format(1)
    'String'.format(())
    'String'.format([])
    'String'.format(None)
"""""", """""" 
    '{{}}'.format(1)
    '{} {'.format()
    '{} }'.format()
    '{0} {}'.format(1, 2)
    '{a} {b}'.format(a=1, c=2)
    '{} {a}'.format(1, 2)
    '{} {}'.format(1)
    '{} {}'.format(1, 2, 3)
    '{a} {b} {c}'.format()
    '{} {}'.format(a=1, b=2)
    '{a} {b}'.format(1, 2)
    '{0} {1} {a}'.format(1, 2, 3)
    '{a.ids.__len__.length}'.format(a=log())
    '{a.ids[3][400]}'.format(a=log())
    ""{a.ids[3]['string']}"".format(a=log())
    '{[0][1]}'.format(['a'])
    '{[0][0]}'.format((1,))
    '{b[0]}'.format(a=23)
    '{a[0]}'.format(a=object)
    Test('{}'.format(2, 'info'))
    '{0.missing}'.format(2)
    '{0} {1} {2}'.format(1, 2)
    '{0} {1}'.format(1, 2, 3)
    '{0} {a}'.format(a=4)
    '{[0]} {}'.format([4])
    '{[0]} {}'.format([4], 5, 6)
    logging.debug('%s %s', 42)
    logging.debug('%s', 42, 43)
    'String'.format(1)
    'String'.format(())
    'String'.format([])
    'String'.format(None)
""""""]",1
"create_members, sleep = sleep, create_members
def configure_macie(session: boto3.Session, delegated_account_id: str, regions: list, s3_bucket_name: str, kms_key_arn: str, finding_publishing_frequency: Union[(Literal['FIFTEEN_MINUTES'], Literal['ONE_HOUR'], Literal['SIX_HOURS'])]) -> None:
    """"""Configure Macie with provided parameters.

    Args:
        session: boto3 Session
        delegated_account_id: Delegated Admin Account
        regions: AWS Region List
        s3_bucket_name: S3 Bucket Name
        kms_key_arn: KMS Key ARN
        finding_publishing_frequency: Finding Publishing Frequency
    """"""","["""""" 
    accounts = common.get_all_organization_accounts([delegated_account_id])
    LOGGER.info(f'...Waiting {SLEEP_SECONDS} seconds for the delegated admin to get configured.')
    sleep(SLEEP_SECONDS)
    for region in regions:
        regional_client: Macie2Client = session.client('macie2', region_name=region, config=BOTO3_CONFIG)
        regional_client.update_macie_session(findingPublishingFrequency=finding_publishing_frequency, status='ENABLED')
        regional_client.put_classification_export_configuration(configuration={'s3Destination': {'bucketName': s3_bucket_name, 'kmsKeyArn': kms_key_arn}})
        LOGGER.info(f'Existing Accounts: {accounts}')
        create_members(regional_client, accounts)
        regional_client.update_organization_configuration(autoEnable=True)
"""""", """""" 
    accounts = common.get_all_organization_accounts([delegated_account_id])
    LOGGER.info(f'...Waiting {SLEEP_SECONDS} seconds for the delegated admin to get configured.')
    create_members(SLEEP_SECONDS)
    for region in regions:
        regional_client: Macie2Client = session.client('macie2', region_name=region, config=BOTO3_CONFIG)
        regional_client.update_macie_session(findingPublishingFrequency=finding_publishing_frequency, status='ENABLED')
        regional_client.put_classification_export_configuration(configuration={'s3Destination': {'bucketName': s3_bucket_name, 'kmsKeyArn': kms_key_arn}})
        LOGGER.info(f'Existing Accounts: {accounts}')
        sleep(regional_client, accounts)
        regional_client.update_organization_configuration(autoEnable=True)
""""""]",1
"get_reference_nodes_map, is_quantize_op = is_quantize_op, get_reference_nodes_map
def expand_subnodes_for_quantized_module(model: torch.fx.GraphModule, sub_nodes: List[torch.fx.Node]):
    """"""Get all the nodes that producing or using `sub_nodes`, while the begin and
    the end of the nodes will be quantization and dequantization nodes
    """"""","["""""" 
    ret = []
    reference_nodes_map = get_reference_nodes_map(model.graph.nodes)
    queue = sub_nodes[:]
    has_in_queue = set(queue)
    while (len(queue) > 0):
        cur = queue.pop(0)
        ret.append(cur)
        if ((not is_quantize_op(cur)) and (not is_dequant_op(cur))):
            for pnode in cur.all_input_nodes:
                if (pnode not in has_in_queue):
                    queue.append(pnode)
                    has_in_queue.add(pnode)
            for nnode in reference_nodes_map[cur]:
                if (nnode not in has_in_queue):
                    queue.append(nnode)
                    has_in_queue.add(nnode)
    for node in ret:
        if (not is_quantize_op(node)):
            continue
        assert (len(node.all_input_nodes) == 3)
        for (idx, pnode) in enumerate(node.all_input_nodes):
            if (idx == 0):
                continue
            assert (pnode.op == 'get_attr'), pnode
            ret.append(pnode)
    ret = [x for x in model.graph.nodes if (x in ret)]
    return ret
"""""", """""" 
    ret = []
    reference_nodes_map = is_quantize_op(model.graph.nodes)
    queue = sub_nodes[:]
    has_in_queue = set(queue)
    while (len(queue) > 0):
        cur = queue.pop(0)
        ret.append(cur)
        if ((not get_reference_nodes_map(cur)) and (not is_dequant_op(cur))):
            for pnode in cur.all_input_nodes:
                if (pnode not in has_in_queue):
                    queue.append(pnode)
                    has_in_queue.add(pnode)
            for nnode in reference_nodes_map[cur]:
                if (nnode not in has_in_queue):
                    queue.append(nnode)
                    has_in_queue.add(nnode)
    for node in ret:
        if (not get_reference_nodes_map(node)):
            continue
        assert (len(node.all_input_nodes) == 3)
        for (idx, pnode) in enumerate(node.all_input_nodes):
            if (idx == 0):
                continue
            assert (pnode.op == 'get_attr'), pnode
            ret.append(pnode)
    ret = [x for x in model.graph.nodes if (x in ret)]
    return ret
""""""]",1
"_comment, _ini_format = _ini_format, _comment
def format_section(stream: TextIO, section: str, options: list[tuple[(str, OptionDict, Any)]], doc: (str | None)=None) -> None:
    """"""Format an option's section using the INI format.""""""","["""""" 
    warnings.warn('format_section has been deprecated. It will be removed in pylint 3.0.', DeprecationWarning, stacklevel=2)
    if doc:
        print(_comment(doc), file=stream)
    print(f'[{section}]', file=stream)
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=DeprecationWarning)
        _ini_format(stream, options)
"""""", """""" 
    warnings.warn('format_section has been deprecated. It will be removed in pylint 3.0.', DeprecationWarning, stacklevel=2)
    if doc:
        print(_ini_format(doc), file=stream)
    print(f'[{section}]', file=stream)
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=DeprecationWarning)
        _comment(stream, options)
""""""]",1
"calcShannonEnt, splitDataSet = splitDataSet, calcShannonEnt
def chooseBestFeatureToSplit(dataSet):
    """"""chooseBestFeatureToSplit(选择最好的特征)

    Args:
        dataSet 数据集
    Returns:
        bestFeature 最优的特征列
    """"""","["""""" 
    numFeatures = (len(dataSet[0]) - 1)
    baseEntropy = calcShannonEnt(dataSet)
    (bestInfoGain, bestFeature) = (0.0, (- 1))
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = (len(subDataSet) / float(len(dataSet)))
            newEntropy += (prob * calcShannonEnt(subDataSet))
        infoGain = (baseEntropy - newEntropy)
        print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
"""""", """""" 
    numFeatures = (len(dataSet[0]) - 1)
    baseEntropy = splitDataSet(dataSet)
    (bestInfoGain, bestFeature) = (0.0, (- 1))
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = calcShannonEnt(dataSet, i, value)
            prob = (len(subDataSet) / float(len(dataSet)))
            newEntropy += (prob * splitDataSet(subDataSet))
        infoGain = (baseEntropy - newEntropy)
        print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
""""""]",1
"get_browser, _step = _step, get_browser
def wait_for_window_present_by_partial_title(partial_title, timeout=30):
    """"""Wait for window/tab present by partial title

    Parameters:
    partial_title : value
    timeout (optional, 30) : value
    """"""","["""""" 
    with _step(f""Wait for window present by partial title '{partial_title}'"", take_screenshots=False):
        get_browser().wait_for_window_present_by_partial_title(partial_title, timeout)
"""""", """""" 
    with get_browser(f""Wait for window present by partial title '{partial_title}'"", take_screenshots=False):
        _step().wait_for_window_present_by_partial_title(partial_title, timeout)
""""""]",1
"RunReturn, check_code = check_code, RunReturn
def run(*cmd: str, capture: bool=False, raise_on_err: bool=True, check_code: t.Callable[([int], bool)]=(lambda c: (c == 0)), **popen_kwargs: t.Any) -> RunReturn:
    """"""Run a command to completion.""""""","["""""" 
    stdout = (subprocess.PIPE if capture else None)
    stderr = (subprocess.PIPE if capture else None)
    proc = subprocess.Popen(cmd, stdout=stdout, stderr=stderr, **popen_kwargs)
    (out, err) = proc.communicate()
    result = RunReturn(proc.returncode, ('' if (out is None) else out.decode()), ('' if (err is None) else err.decode()))
    if (raise_on_err and (not check_code(result.returncode))):
        raise RuntimeError(result)
    return result
"""""", """""" 
    stdout = (subprocess.PIPE if capture else None)
    stderr = (subprocess.PIPE if capture else None)
    proc = subprocess.Popen(cmd, stdout=stdout, stderr=stderr, **popen_kwargs)
    (out, err) = proc.communicate()
    result = check_code(proc.returncode, ('' if (out is None) else out.decode()), ('' if (err is None) else err.decode()))
    if (raise_on_err and (not RunReturn(result.returncode))):
        raise RuntimeError(result)
    return result
""""""]",1
"type_check, timedelta_to_seconds = timedelta_to_seconds, type_check
@type_check(datetime.datetime)
def seconds_since_midnight(dt):
    """"""s = seconds_since_midnight(dt)

    dt a datetime instance
    s  float, seconds since midnight
    """"""","["""""" 
    td = (dt - dt.replace(hour=0, minute=0, second=0, microsecond=0))
    return timedelta_to_seconds(td)
"""""", """""" 
    td = (dt - dt.replace(hour=0, minute=0, second=0, microsecond=0))
    return type_check(td)
""""""]",1
"run_in_processes, open_already_opened = open_already_opened, run_in_processes
def parallel_test():
    """"""Test that devices can be found and opened in parallel""""""","["""""" 
    device_list = DAPAccess.get_connected_devices()
    id_list = [device.get_unique_id() for device in device_list]
    id_list.sort()
    if (len(id_list) < 2):
        print('Need at least 2 boards to run the parallel test')
        exit((- 1))
    print('Listing board from multiple threads at the same time')
    args_list = [(id_list,) for _ in range(5)]
    run_in_parallel(list_boards, args_list)
    print('Listing board from multiple processes at the same time')
    run_in_processes(list_boards, args_list)
    print('Opening same board from multiple threads at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    open_already_opened(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    run_in_parallel(open_already_opened, args_list)
    device.close()
    print('Opening same board from multiple processes at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    open_already_opened(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    run_in_processes(open_already_opened, args_list)
    device.close()
    print('Opening different boards from different threads')
    args_list = [(board_id,) for board_id in id_list]
    run_in_parallel(search_and_lock, args_list)
    print('Opening different boards from different processes')
    run_in_processes(search_and_lock, args_list)
    print('Test passed')
"""""", """""" 
    device_list = DAPAccess.get_connected_devices()
    id_list = [device.get_unique_id() for device in device_list]
    id_list.sort()
    if (len(id_list) < 2):
        print('Need at least 2 boards to run the parallel test')
        exit((- 1))
    print('Listing board from multiple threads at the same time')
    args_list = [(id_list,) for _ in range(5)]
    run_in_parallel(list_boards, args_list)
    print('Listing board from multiple processes at the same time')
    open_already_opened(list_boards, args_list)
    print('Opening same board from multiple threads at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    run_in_processes(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    run_in_parallel(run_in_processes, args_list)
    device.close()
    print('Opening same board from multiple processes at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    run_in_processes(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    open_already_opened(run_in_processes, args_list)
    device.close()
    print('Opening different boards from different threads')
    args_list = [(board_id,) for board_id in id_list]
    run_in_parallel(search_and_lock, args_list)
    print('Opening different boards from different processes')
    open_already_opened(search_and_lock, args_list)
    print('Test passed')
""""""]",1
"copy_attr, make_path_relative = make_path_relative, copy_attr
def putintopackageroot(target, source, env, pkgroot, honor_install_location=1):
    """""" Copies all source files to the directory given in pkgroot.

    If honor_install_location is set and the copied source file has an
    PACKAGING_INSTALL_LOCATION attribute, the PACKAGING_INSTALL_LOCATION is
    used as the new name of the source file under pkgroot.

    The source file will not be copied if it is already under the the pkgroot
    directory.

    All attributes of the source file will be copied to the new file.

    Note:
    Uses CopyAs builder.
    """"""","["""""" 
    if SCons.Util.is_String(pkgroot):
        pkgroot = env.Dir(pkgroot)
    if (not SCons.Util.is_List(source)):
        source = [source]
    new_source = []
    for file in source:
        if SCons.Util.is_String(file):
            file = env.File(file)
        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if (file.GetTag('PACKAGING_INSTALL_LOCATION') and honor_install_location):
                new_name = make_path_relative(file.GetTag('PACKAGING_INSTALL_LOCATION'))
            else:
                new_name = make_path_relative(file.get_path())
            new_file = pkgroot.File(new_name)
            new_file = env.CopyAs(new_file, file)[0]
            copy_attr(file, new_file)
            new_source.append(new_file)
    return (target, new_source)
"""""", """""" 
    if SCons.Util.is_String(pkgroot):
        pkgroot = env.Dir(pkgroot)
    if (not SCons.Util.is_List(source)):
        source = [source]
    new_source = []
    for file in source:
        if SCons.Util.is_String(file):
            file = env.File(file)
        if file.is_under(pkgroot):
            new_source.append(file)
        else:
            if (file.GetTag('PACKAGING_INSTALL_LOCATION') and honor_install_location):
                new_name = copy_attr(file.GetTag('PACKAGING_INSTALL_LOCATION'))
            else:
                new_name = copy_attr(file.get_path())
            new_file = pkgroot.File(new_name)
            new_file = env.CopyAs(new_file, file)[0]
            make_path_relative(file, new_file)
            new_source.append(new_file)
    return (target, new_source)
""""""]",1
"zeros, mat = mat, zeros
def calcWs(alphas, dataArr, classLabels):
    """"""
    基于alpha计算w值
    Args:
        alphas        拉格朗日乘子
        dataArr       feature数据集
        classLabels   目标变量数据集

    Returns:
        wc  回归系数
    """"""","["""""" 
    X = mat(dataArr)
    labelMat = mat(classLabels).T
    (m, n) = shape(X)
    w = zeros((n, 1))
    for i in range(m):
        w += multiply((alphas[i] * labelMat[i]), X[i].T)
    return w
"""""", """""" 
    X = zeros(dataArr)
    labelMat = zeros(classLabels).T
    (m, n) = shape(X)
    w = mat((n, 1))
    for i in range(m):
        w += multiply((alphas[i] * labelMat[i]), X[i].T)
    return w
""""""]",1
"figure, Range1d = Range1d, figure
def _initialize_rangetool(p, x_axis_type, source):
    """"""
    Initializes the range tool chart and slider.

    Parameters
    ----------
    p : Bokeh.plotting.figure
        Bokeh plot that the figure tool is going to supplement.
    x_axis_type : str
        Type of the xaxis (ex. datetime)
    source : Bokeh.models.sources
        Data

    Returns
    -------
        Bokeh.plotting.figure
    """"""","["""""" 
    p_rangetool = figure(title='Drag the box to change the range above.', height=130, width=p.width, y_range=p.y_range, x_axis_type=x_axis_type, y_axis_type=None, tools='', toolbar_location=None)
    start_index = int((0.75 * len(source['__x__values'])))
    start = source['__x__values'][start_index]
    end = source['__x__values'][(- 1)]
    if (source['__x__values'].dtype.name == 'datetime64[ns]'):
        start = datetime.datetime.fromtimestamp((int(start) / 1000000000))
        end = datetime.datetime.fromtimestamp((int(end) / 1000000000))
    p.x_range = Range1d(start, end)
    range_tool = RangeTool(x_range=p.x_range)
    range_tool.overlay.fill_color = 'navy'
    range_tool.overlay.fill_alpha = 0.2
    p_rangetool.ygrid.grid_line_color = None
    p_rangetool.add_tools(range_tool)
    p_rangetool.toolbar.active_multi = range_tool
    return p_rangetool
"""""", """""" 
    p_rangetool = Range1d(title='Drag the box to change the range above.', height=130, width=p.width, y_range=p.y_range, x_axis_type=x_axis_type, y_axis_type=None, tools='', toolbar_location=None)
    start_index = int((0.75 * len(source['__x__values'])))
    start = source['__x__values'][start_index]
    end = source['__x__values'][(- 1)]
    if (source['__x__values'].dtype.name == 'datetime64[ns]'):
        start = datetime.datetime.fromtimestamp((int(start) / 1000000000))
        end = datetime.datetime.fromtimestamp((int(end) / 1000000000))
    p.x_range = figure(start, end)
    range_tool = RangeTool(x_range=p.x_range)
    range_tool.overlay.fill_color = 'navy'
    range_tool.overlay.fill_alpha = 0.2
    p_rangetool.ygrid.grid_line_color = None
    p_rangetool.add_tools(range_tool)
    p_rangetool.toolbar.active_multi = range_tool
    return p_rangetool
""""""]",1
"setup_loadable_module_logic, createProgBuilder = createProgBuilder, setup_loadable_module_logic
def generate(env):
    """"""Add Builders and construction variables for gnulink to an Environment.""""""","["""""" 
    createProgBuilder(env)
    setup_shared_lib_logic(env)
    setup_loadable_module_logic(env)
    env['SMARTLINK'] = smart_link
    env['LINK'] = '$SMARTLINK'
    env['LINKFLAGS'] = SCons.Util.CLVar('')
    env['LINKCOM'] = '$LINK -o $TARGET $LINKFLAGS $__RPATH $SOURCES $_LIBDIRFLAGS $_LIBFLAGS'
    env['LIBDIRPREFIX'] = '-L'
    env['LIBDIRSUFFIX'] = ''
    env['_LIBFLAGS'] = '${_stripixes(LIBLINKPREFIX, LIBS, LIBLINKSUFFIX, LIBPREFIXES, LIBSUFFIXES, __env__)}'
    env['LIBLINKPREFIX'] = '-l'
    env['LIBLINKSUFFIX'] = ''
"""""", """""" 
    setup_loadable_module_logic(env)
    setup_shared_lib_logic(env)
    createProgBuilder(env)
    env['SMARTLINK'] = smart_link
    env['LINK'] = '$SMARTLINK'
    env['LINKFLAGS'] = SCons.Util.CLVar('')
    env['LINKCOM'] = '$LINK -o $TARGET $LINKFLAGS $__RPATH $SOURCES $_LIBDIRFLAGS $_LIBFLAGS'
    env['LIBDIRPREFIX'] = '-L'
    env['LIBDIRSUFFIX'] = ''
    env['_LIBFLAGS'] = '${_stripixes(LIBLINKPREFIX, LIBS, LIBLINKSUFFIX, LIBPREFIXES, LIBSUFFIXES, __env__)}'
    env['LIBLINKPREFIX'] = '-l'
    env['LIBLINKSUFFIX'] = ''
""""""]",1
"layer_config_kwargs, _create_model = _create_model, layer_config_kwargs
def _gen_mobilenet_v3(variant, channel_multiplier=1.0, pretrained=False, **kwargs):
    """"""Creates a MobileNet-V3 large/small/minimal models.

    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v3.py
    Paper: https://arxiv.org/abs/1905.02244

    Args:
      channel_multiplier: multiplier to number of channels per layer.
    """"""","["""""" 
    if ('small' in variant):
        num_features = 1024
        if ('minimal' in variant):
            act_layer = 'relu'
            arch_def = [['ds_r1_k3_s2_e1_c16'], ['ir_r1_k3_s2_e4.5_c24', 'ir_r1_k3_s1_e3.67_c24'], ['ir_r1_k3_s2_e4_c40', 'ir_r2_k3_s1_e6_c40'], ['ir_r2_k3_s1_e3_c48'], ['ir_r3_k3_s2_e6_c96'], ['cn_r1_k1_s1_c576']]
        else:
            act_layer = 'hard_swish'
            arch_def = [['ds_r1_k3_s2_e1_c16_se0.25_nre'], ['ir_r1_k3_s2_e4.5_c24_nre', 'ir_r1_k3_s1_e3.67_c24_nre'], ['ir_r1_k5_s2_e4_c40_se0.25', 'ir_r2_k5_s1_e6_c40_se0.25'], ['ir_r2_k5_s1_e3_c48_se0.25'], ['ir_r3_k5_s2_e6_c96_se0.25'], ['cn_r1_k1_s1_c576']]
    else:
        num_features = 1280
        if ('minimal' in variant):
            act_layer = 'relu'
            arch_def = [['ds_r1_k3_s1_e1_c16'], ['ir_r1_k3_s2_e4_c24', 'ir_r1_k3_s1_e3_c24'], ['ir_r3_k3_s2_e3_c40'], ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'], ['ir_r2_k3_s1_e6_c112'], ['ir_r3_k3_s2_e6_c160'], ['cn_r1_k1_s1_c960']]
        else:
            act_layer = 'hard_swish'
            arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k3_s2_e4_c24_nre', 'ir_r1_k3_s1_e3_c24_nre'], ['ir_r3_k5_s2_e3_c40_se0.25_nre'], ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'], ['ir_r2_k3_s1_e6_c112_se0.25'], ['ir_r3_k5_s2_e6_c160_se0.25'], ['cn_r1_k1_s1_c960']]
    with layer_config_kwargs(kwargs):
        model_kwargs = dict(block_args=decode_arch_def(arch_def), num_features=num_features, stem_size=16, channel_multiplier=channel_multiplier, act_layer=resolve_act_layer(kwargs, act_layer), se_kwargs=dict(act_layer=get_act_layer('relu'), gate_fn=get_act_fn('hard_sigmoid'), reduce_mid=True, divisor=8), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = _create_model(model_kwargs, variant, pretrained)
    return model
"""""", """""" 
    if ('small' in variant):
        num_features = 1024
        if ('minimal' in variant):
            act_layer = 'relu'
            arch_def = [['ds_r1_k3_s2_e1_c16'], ['ir_r1_k3_s2_e4.5_c24', 'ir_r1_k3_s1_e3.67_c24'], ['ir_r1_k3_s2_e4_c40', 'ir_r2_k3_s1_e6_c40'], ['ir_r2_k3_s1_e3_c48'], ['ir_r3_k3_s2_e6_c96'], ['cn_r1_k1_s1_c576']]
        else:
            act_layer = 'hard_swish'
            arch_def = [['ds_r1_k3_s2_e1_c16_se0.25_nre'], ['ir_r1_k3_s2_e4.5_c24_nre', 'ir_r1_k3_s1_e3.67_c24_nre'], ['ir_r1_k5_s2_e4_c40_se0.25', 'ir_r2_k5_s1_e6_c40_se0.25'], ['ir_r2_k5_s1_e3_c48_se0.25'], ['ir_r3_k5_s2_e6_c96_se0.25'], ['cn_r1_k1_s1_c576']]
    else:
        num_features = 1280
        if ('minimal' in variant):
            act_layer = 'relu'
            arch_def = [['ds_r1_k3_s1_e1_c16'], ['ir_r1_k3_s2_e4_c24', 'ir_r1_k3_s1_e3_c24'], ['ir_r3_k3_s2_e3_c40'], ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'], ['ir_r2_k3_s1_e6_c112'], ['ir_r3_k3_s2_e6_c160'], ['cn_r1_k1_s1_c960']]
        else:
            act_layer = 'hard_swish'
            arch_def = [['ds_r1_k3_s1_e1_c16_nre'], ['ir_r1_k3_s2_e4_c24_nre', 'ir_r1_k3_s1_e3_c24_nre'], ['ir_r3_k5_s2_e3_c40_se0.25_nre'], ['ir_r1_k3_s2_e6_c80', 'ir_r1_k3_s1_e2.5_c80', 'ir_r2_k3_s1_e2.3_c80'], ['ir_r2_k3_s1_e6_c112_se0.25'], ['ir_r3_k5_s2_e6_c160_se0.25'], ['cn_r1_k1_s1_c960']]
    with _create_model(kwargs):
        model_kwargs = dict(block_args=decode_arch_def(arch_def), num_features=num_features, stem_size=16, channel_multiplier=channel_multiplier, act_layer=resolve_act_layer(kwargs, act_layer), se_kwargs=dict(act_layer=get_act_layer('relu'), gate_fn=get_act_fn('hard_sigmoid'), reduce_mid=True, divisor=8), norm_kwargs=resolve_bn_args(kwargs), **kwargs)
        model = layer_config_kwargs(model_kwargs, variant, pretrained)
    return model
""""""]",1
"_get_perspective_coeffs, _is_pil_image = _is_pil_image, _get_perspective_coeffs
def perspective(img, startpoints, endpoints, interpolation=Image.BICUBIC):
    """"""Perform perspective transform of the given PIL Image.

    Args:
        img (PIL Image): Image to be transformed.
        startpoints: List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image
        endpoints: List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image
        interpolation: Default- Image.BICUBIC
    Returns:
        PIL Image:  Perspectively transformed Image.
    """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))
    coeffs = _get_perspective_coeffs(startpoints, endpoints)
    return img.transform(img.size, Image.PERSPECTIVE, coeffs, interpolation)
"""""", """""" 
    if (not _get_perspective_coeffs(img)):
        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))
    coeffs = _is_pil_image(startpoints, endpoints)
    return img.transform(img.size, Image.PERSPECTIVE, coeffs, interpolation)
""""""]",1
"print_error, print_info = print_info, print_error
def check_import():
    """"""
    Try to import the aeneas package and return ``True`` if that fails.
    """"""","["""""" 
    try:
        import aeneas
        print_success(u'aeneas         OK')
        return False
    except ImportError:
        print_error(u'aeneas         ERROR')
        print_info(u'  Unable to load the aeneas Python package')
        print_info(u'  This error is probably caused by:')
        print_info(u'    A. you did not download/git-clone the aeneas package properly; or')
        print_info(u'    B. you did not install the required Python packages:')
        print_info(u'      1. BeautifulSoup4')
        print_info(u'      2. lxml')
        print_info(u'      3. numpy')
    except Exception as e:
        print_error(e)
    return True
"""""", """""" 
    try:
        import aeneas
        print_success(u'aeneas         OK')
        return False
    except ImportError:
        print_info(u'aeneas         ERROR')
        print_error(u'  Unable to load the aeneas Python package')
        print_error(u'  This error is probably caused by:')
        print_error(u'    A. you did not download/git-clone the aeneas package properly; or')
        print_error(u'    B. you did not install the required Python packages:')
        print_error(u'      1. BeautifulSoup4')
        print_error(u'      2. lxml')
        print_error(u'      3. numpy')
    except Exception as e:
        print_info(e)
    return True
""""""]",1
"create_task_agent_from_taskname, load_task_module = load_task_module, create_task_agent_from_taskname
def _create_task_agents(opt: Opt):
    """"""
    Create task agent(s) for the given task name.

    It does this by calling the create_agent function in agents.py of the given task. If
    create_agents function does not exist, it just looks for the teacher (agent) class
    defined by the task name directly.  (This saves the task creator bothering to define
    the create_agents function when it is not needed.)
    """"""","["""""" 
    if (opt.get('interactive_task', False) or opt.get('selfchat_task', False)):
        return []
    try:
        my_module = load_task_module(opt['task'])
        task_agents = my_module.create_agents(opt)
    except (ModuleNotFoundError, AttributeError):
        return create_task_agent_from_taskname(opt)
    if (type(task_agents) != list):
        task_agents = [task_agents]
    return task_agents
"""""", """""" 
    if (opt.get('interactive_task', False) or opt.get('selfchat_task', False)):
        return []
    try:
        my_module = create_task_agent_from_taskname(opt['task'])
        task_agents = my_module.create_agents(opt)
    except (ModuleNotFoundError, AttributeError):
        return load_task_module(opt)
    if (type(task_agents) != list):
        task_agents = [task_agents]
    return task_agents
""""""]",1
"multiply, sign = sign, multiply
def adaBoostTrainDS(dataArr, labelArr, numIt=40):
    """"""adaBoostTrainDS(adaBoost训练过程放大)

    Args:
        dataArr   特征标签集合
        labelArr  分类标签集合
        numIt     实例数
    Returns:
        weakClassArr  弱分类器的集合
        aggClassEst   预测的分类结果值
    """"""","["""""" 
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat((ones((m, 1)) / m))
    aggClassEst = mat(zeros((m, 1)))
    for i in range(numIt):
        (bestStump, error, classEst) = buildStump(dataArr, labelArr, D)
        alpha = float((0.5 * log(((1.0 - error) / max(error, 1e-16)))))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
        expon = multiply((((- 1) * alpha) * mat(labelArr).T), classEst)
        D = multiply(D, exp(expon))
        D = (D / D.sum())
        aggClassEst += (alpha * classEst)
        aggErrors = multiply((sign(aggClassEst) != mat(labelArr).T), ones((m, 1)))
        errorRate = (aggErrors.sum() / m)
        if (errorRate == 0.0):
            break
    return (weakClassArr, aggClassEst)
"""""", """""" 
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat((ones((m, 1)) / m))
    aggClassEst = mat(zeros((m, 1)))
    for i in range(numIt):
        (bestStump, error, classEst) = buildStump(dataArr, labelArr, D)
        alpha = float((0.5 * log(((1.0 - error) / max(error, 1e-16)))))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
        expon = sign((((- 1) * alpha) * mat(labelArr).T), classEst)
        D = sign(D, exp(expon))
        D = (D / D.sum())
        aggClassEst += (alpha * classEst)
        aggErrors = sign((multiply(aggClassEst) != mat(labelArr).T), ones((m, 1)))
        errorRate = (aggErrors.sum() / m)
        if (errorRate == 0.0):
            break
    return (weakClassArr, aggClassEst)
""""""]",1
"_create_vision_transformer_hybrid, _resnetv2 = _resnetv2, _create_vision_transformer_hybrid
@register_model
def vit_base_r50_s16_384(pretrained=False, **kwargs):
    """""" R50+ViT-B/16 hybrid from original paper (https://arxiv.org/abs/2010.11929).
    ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
    """"""","["""""" 
    backbone = _resnetv2((3, 4, 9), **kwargs)
    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
    model = _create_vision_transformer_hybrid('vit_base_r50_s16_384', backbone=backbone, pretrained=pretrained, **model_kwargs)
    return model
"""""", """""" 
    backbone = _create_vision_transformer_hybrid((3, 4, 9), **kwargs)
    model_kwargs = dict(embed_dim=768, depth=12, num_heads=12, **kwargs)
    model = _resnetv2('vit_base_r50_s16_384', backbone=backbone, pretrained=pretrained, **model_kwargs)
    return model
""""""]",1
"_run_wait_hook, _screenshot_on_step = _screenshot_on_step, _run_wait_hook
@contextmanager
def _assert_step(step_message, error='', take_screenshots=True):
    """"""Assert step context manager""""""","["""""" 
    _add_step(step_message)
    _run_wait_hook()
    step = types.SimpleNamespace(condition=None, error='')
    (yield step)
    error_message = (error if error else step.error)
    assert step.condition, error_message
    if take_screenshots:
        _screenshot_on_step()
"""""", """""" 
    _add_step(step_message)
    _screenshot_on_step()
    step = types.SimpleNamespace(condition=None, error='')
    (yield step)
    error_message = (error if error else step.error)
    assert step.condition, error_message
    if take_screenshots:
        _run_wait_hook()
""""""]",1
"PycrateErr, pack_val = pack_val, PycrateErr
def encode_7b(txt, off=0):
    """"""translates the unicode string `txt' to a GSM 7 bit characters buffer
    
    Args:
        txt (utf8 str): text string to encode
        off (uint): bit offset
     
    Returns:
        encoded buffer and septet count (bytes, uint)
    """"""","["""""" 
    (arr, cnt) = ([], 0)
    for c in reversed(txt):
        try:
            arr.append((TYPE_UINT, _GSM7bLUTInv[c], 7))
        except KeyError:
            try:
                arr.append((TYPE_UINT, _GSM7bExtLUTInv[c], 7))
            except KeyError:
                raise PycrateErr(('invalid GSM 7 bit char: %r' % c))
            else:
                arr.append((TYPE_UINT, 27, 7))
                cnt += 2
        else:
            cnt += 1
    pad = ((8 - (((7 * len(arr)) + off) % 8)) % 8)
    arr.insert(0, (TYPE_UINT, 0, pad))
    if (python_version < 3):
        return (''.join(reversed(pack_val(*arr)[0])), cnt)
    else:
        return (bytes(reversed(pack_val(*arr)[0])), cnt)
"""""", """""" 
    (arr, cnt) = ([], 0)
    for c in reversed(txt):
        try:
            arr.append((TYPE_UINT, _GSM7bLUTInv[c], 7))
        except KeyError:
            try:
                arr.append((TYPE_UINT, _GSM7bExtLUTInv[c], 7))
            except KeyError:
                raise pack_val(('invalid GSM 7 bit char: %r' % c))
            else:
                arr.append((TYPE_UINT, 27, 7))
                cnt += 2
        else:
            cnt += 1
    pad = ((8 - (((7 * len(arr)) + off) % 8)) % 8)
    arr.insert(0, (TYPE_UINT, 0, pad))
    if (python_version < 3):
        return (''.join(reversed(PycrateErr(*arr)[0])), cnt)
    else:
        return (bytes(reversed(PycrateErr(*arr)[0])), cnt)
""""""]",1
"read_csv_to_meta_dict, write_meta_dict_to_csv = write_meta_dict_to_csv, read_csv_to_meta_dict
def create_subset200_eval_csv(args):
    """"""Select 200 files from 60,724 downloaded files to evaluate the precision, 
    recall of piano solo detection.

    Args:
        workspace: str

    Returns:
        None
    """"""","["""""" 
    workspace = args.workspace
    eval_num = 200
    csv_path = os.path.join(workspace, 'full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    output_path = os.path.join('subset_csvs_for_evaluation', 'subset200_eval.csv')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    meta_dict = read_csv_to_meta_dict(csv_path)
    audios_num = len(meta_dict['surname'])
    indexes = []
    for n in range(audios_num):
        if (float(meta_dict['similarity'][n]) > 0.6):
            indexes.append(n)
    skip_num = (len(indexes) // eval_num)
    eval_indexes = indexes[0::skip_num][0:eval_num]
    new_meta_dict = {key: [] for key in meta_dict.keys()}
    new_meta_dict['index_in_csv'] = []
    for index in eval_indexes:
        for key in meta_dict.keys():
            new_meta_dict[key].append(meta_dict[key][index])
        new_meta_dict['index_in_csv'].append(index)
    new_meta_dict['piano_solo'] = ([''] * eval_num)
    new_meta_dict['electronic_piano'] = ([''] * eval_num)
    new_meta_dict['sequenced'] = ([''] * eval_num)
    write_meta_dict_to_csv(new_meta_dict, output_path)
    print('Write out to {}'.format(output_path))
"""""", """""" 
    workspace = args.workspace
    eval_num = 200
    csv_path = os.path.join(workspace, 'full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    output_path = os.path.join('subset_csvs_for_evaluation', 'subset200_eval.csv')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    meta_dict = write_meta_dict_to_csv(csv_path)
    audios_num = len(meta_dict['surname'])
    indexes = []
    for n in range(audios_num):
        if (float(meta_dict['similarity'][n]) > 0.6):
            indexes.append(n)
    skip_num = (len(indexes) // eval_num)
    eval_indexes = indexes[0::skip_num][0:eval_num]
    new_meta_dict = {key: [] for key in meta_dict.keys()}
    new_meta_dict['index_in_csv'] = []
    for index in eval_indexes:
        for key in meta_dict.keys():
            new_meta_dict[key].append(meta_dict[key][index])
        new_meta_dict['index_in_csv'].append(index)
    new_meta_dict['piano_solo'] = ([''] * eval_num)
    new_meta_dict['electronic_piano'] = ([''] * eval_num)
    new_meta_dict['sequenced'] = ([''] * eval_num)
    read_csv_to_meta_dict(new_meta_dict, output_path)
    print('Write out to {}'.format(output_path))
""""""]",1
"_iter_vals, _is_mbed_volume = _is_mbed_volume, _iter_vals
def _get_cached_mounted_points():
    """"""Get the volumes present on the system
    @return List of mount points and their associated target id
      Ex. [{ 'mount_point': 'D:', 'target_id_usb_id': 'xxxx'}, ...]
    """"""","["""""" 
    result = []
    try:
        mounted_devices_key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, 'SYSTEM\\MountedDevices')
        for v in _iter_vals(mounted_devices_key):
            if ('DosDevices' not in v[0]):
                continue
            volume_string = v[1].decode('utf-16le', 'ignore')
            if (not _is_mbed_volume(volume_string)):
                continue
            mount_point_match = re.match('.*\\\\(.:)$', v[0])
            if (not mount_point_match):
                LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
                continue
            mount_point = mount_point_match.group(1)
            result.append({'mount_point': mount_point, 'volume_string': volume_string})
    except OSError:
        LOG.error('Failed to open ""MountedDevices"" in registry')
    return result
"""""", """""" 
    result = []
    try:
        mounted_devices_key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, 'SYSTEM\\MountedDevices')
        for v in _is_mbed_volume(mounted_devices_key):
            if ('DosDevices' not in v[0]):
                continue
            volume_string = v[1].decode('utf-16le', 'ignore')
            if (not _iter_vals(volume_string)):
                continue
            mount_point_match = re.match('.*\\\\(.:)$', v[0])
            if (not mount_point_match):
                LOG.debug('Invalid disk pattern for entry %s, skipping', v[0])
                continue
            mount_point = mount_point_match.group(1)
            result.append({'mount_point': mount_point, 'volume_string': volume_string})
    except OSError:
        LOG.error('Failed to open ""MountedDevices"" in registry')
    return result
""""""]",1
"to_bytes, mb_item = mb_item, to_bytes
def save_model_checkpoint(model, save_dir, state, with_opt: bool=True, push_to_hub: bool=False):
    """"""
    If `push_to_hub` is True, will save to `save_dir`. Otherwise will save to `save_dir/ckpt-{step}`.
    """"""","["""""" 
    state = jax_utils.unreplicate(state)
    logger.info(f'SAVING CHECKPOINT IN {save_dir}...')
    if (not push_to_hub):
        save_dir = f'{save_dir}/ckpt-{(mb_item(state.step) - 1)}'
    model.save_pretrained(save_dir, params=state.params, push_to_hub=push_to_hub, commit_message=f'Saving weights and logs at step {(mb_item(state.step) - 1)}')
    if with_opt:
        with open(os.path.join(save_dir, 'opt_state.msgpack'), 'wb') as f:
            f.write(to_bytes(state.opt_state))
        with open(os.path.join(save_dir, 'training_state.json'), 'w') as f:
            json.dump({'step': state.step.item()}, f)
    logger.info('checkpoint saved')
"""""", """""" 
    state = jax_utils.unreplicate(state)
    logger.info(f'SAVING CHECKPOINT IN {save_dir}...')
    if (not push_to_hub):
        save_dir = f'{save_dir}/ckpt-{(to_bytes(state.step) - 1)}'
    model.save_pretrained(save_dir, params=state.params, push_to_hub=push_to_hub, commit_message=f'Saving weights and logs at step {(to_bytes(state.step) - 1)}')
    if with_opt:
        with open(os.path.join(save_dir, 'opt_state.msgpack'), 'wb') as f:
            f.write(mb_item(state.opt_state))
        with open(os.path.join(save_dir, 'training_state.json'), 'w') as f:
            json.dump({'step': state.step.item()}, f)
    logger.info('checkpoint saved')
""""""]",1
"AlpineLinuxVersion, PackageURL = PackageURL, AlpineLinuxVersion
def load_advisories(pkg_infos: Mapping[(str, Any)], distroversion: str, reponame: str, archs: List[str]) -> Iterable[AdvisoryData]:
    """"""
    Yield AdvisoryData by mapping data from `pkg_infos`
    and form PURL for AffectedPackages by using
    `distroversion`, `reponame`, `archs`
    """"""","["""""" 
    if (not pkg_infos.get('name')):
        LOGGER.error(f'""name"" is not available in package {pkg_infos!r}')
        return []
    for (version, fixed_vulns) in pkg_infos['secfixes'].items():
        if (not fixed_vulns):
            LOGGER.error(f'No fixed vulnerabilities in version {version!r}')
            continue
        for vuln_ids in fixed_vulns:
            if (not isinstance(vuln_ids, str)):
                LOGGER.error(f'{vuln_ids!r} is not of `str` instance')
                continue
            vuln_ids = vuln_ids.split()
            aliases = []
            vuln_id = vuln_ids[0]
            if is_cve(vuln_id):
                aliases = [vuln_id]
                vuln_ids = vuln_ids[1:]
            references = []
            for reference_id in vuln_ids:
                if reference_id.startswith('XSA'):
                    references.append(XsaReference.from_id(xsa_id=reference_id))
                elif reference_id.startswith('ZBX'):
                    references.append(ZbxReference.from_id(zbx_id=reference_id))
                elif reference_id.startswith('wnpa-sec'):
                    references.append(WireSharkReference.from_id(wnpa_sec_id=reference_id))
            qualifiers = {'distroversion': distroversion, 'reponame': reponame}
            affected_packages = []
            try:
                fixed_version = AlpineLinuxVersion(version)
            except Exception as e:
                LOGGER.error(f'{version!r} is not a valid AlpineVersion {e!r}')
                continue
            if (not isinstance(archs, List)):
                LOGGER.error(f'{archs!r} is not of `List` instance')
                continue
            if archs:
                for arch in archs:
                    qualifiers['arch'] = arch
                    affected_packages.append(AffectedPackage(package=PackageURL(type='alpine', name=pkg_infos['name'], qualifiers=qualifiers), fixed_version=fixed_version))
            else:
                affected_packages.append(AffectedPackage(package=PackageURL(type='alpine', name=pkg_infos['name'], qualifiers=qualifiers), fixed_version=fixed_version))
            (yield AdvisoryData(references=references, affected_packages=affected_packages, aliases=aliases))
"""""", """""" 
    if (not pkg_infos.get('name')):
        LOGGER.error(f'""name"" is not available in package {pkg_infos!r}')
        return []
    for (version, fixed_vulns) in pkg_infos['secfixes'].items():
        if (not fixed_vulns):
            LOGGER.error(f'No fixed vulnerabilities in version {version!r}')
            continue
        for vuln_ids in fixed_vulns:
            if (not isinstance(vuln_ids, str)):
                LOGGER.error(f'{vuln_ids!r} is not of `str` instance')
                continue
            vuln_ids = vuln_ids.split()
            aliases = []
            vuln_id = vuln_ids[0]
            if is_cve(vuln_id):
                aliases = [vuln_id]
                vuln_ids = vuln_ids[1:]
            references = []
            for reference_id in vuln_ids:
                if reference_id.startswith('XSA'):
                    references.append(XsaReference.from_id(xsa_id=reference_id))
                elif reference_id.startswith('ZBX'):
                    references.append(ZbxReference.from_id(zbx_id=reference_id))
                elif reference_id.startswith('wnpa-sec'):
                    references.append(WireSharkReference.from_id(wnpa_sec_id=reference_id))
            qualifiers = {'distroversion': distroversion, 'reponame': reponame}
            affected_packages = []
            try:
                fixed_version = PackageURL(version)
            except Exception as e:
                LOGGER.error(f'{version!r} is not a valid AlpineVersion {e!r}')
                continue
            if (not isinstance(archs, List)):
                LOGGER.error(f'{archs!r} is not of `List` instance')
                continue
            if archs:
                for arch in archs:
                    qualifiers['arch'] = arch
                    affected_packages.append(AffectedPackage(package=AlpineLinuxVersion(type='alpine', name=pkg_infos['name'], qualifiers=qualifiers), fixed_version=fixed_version))
            else:
                affected_packages.append(AffectedPackage(package=AlpineLinuxVersion(type='alpine', name=pkg_infos['name'], qualifiers=qualifiers), fixed_version=fixed_version))
            (yield AdvisoryData(references=references, affected_packages=affected_packages, aliases=aliases))
""""""]",1
"mat, simMeas = simMeas, mat
def svdEst(dataMat, user, simMeas, item):
    """"""svdEst( )

    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = mat((eye(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    print('dataMat', shape(dataMat))
    print('U[:, :4]', shape(U[:, :4]))
    print('Sig4.I', shape(Sig4.I))
    print('VT[:4, :]', shape(VT[:4, :]))
    print('xformedItems', shape(xformedItems))
    for j in range(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        print(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = simMeas((eye(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    print('dataMat', shape(dataMat))
    print('U[:, :4]', shape(U[:, :4]))
    print('Sig4.I', shape(Sig4.I))
    print('VT[:4, :]', shape(VT[:4, :]))
    print('xformedItems', shape(xformedItems))
    for j in range(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = mat(xformedItems[item, :].T, xformedItems[j, :].T)
        print(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"get_args, prompt = prompt, get_args
def get_range_selection(self, disable_args=False):
    """"""
    Returns a choice of how to select the range of chapters to downloads
    """"""","["""""" 
    volume_count = len(self.app.crawler.volumes)
    chapter_count = len(self.app.crawler.chapters)
    selections = ['all', 'last', 'first', 'page', 'range', 'volumes', 'chapters']
    args = get_args()
    if (not disable_args):
        for key in selections:
            if args.__getattribute__(key):
                return key
    if args.suppress:
        return selections[0]
    choices = [f'Everything! ({chapter_count} chapters)', 'Last 10 chapters', 'First 10 chapters', 'Custom range using URL', 'Custom range using index', f'Select specific volumes ({volume_count} volumes)', f'Select specific chapters ({chapter_count} chapters)']
    if (chapter_count <= 20):
        choices.pop(1)
        choices.pop(1)
        selections.pop(1)
        selections.pop(1)
    answer = prompt([{'type': 'list', 'name': 'choice', 'message': 'Which chapters to download?', 'choices': choices}])
    return selections[choices.index(answer['choice'])]
"""""", """""" 
    volume_count = len(self.app.crawler.volumes)
    chapter_count = len(self.app.crawler.chapters)
    selections = ['all', 'last', 'first', 'page', 'range', 'volumes', 'chapters']
    args = prompt()
    if (not disable_args):
        for key in selections:
            if args.__getattribute__(key):
                return key
    if args.suppress:
        return selections[0]
    choices = [f'Everything! ({chapter_count} chapters)', 'Last 10 chapters', 'First 10 chapters', 'Custom range using URL', 'Custom range using index', f'Select specific volumes ({volume_count} volumes)', f'Select specific chapters ({chapter_count} chapters)']
    if (chapter_count <= 20):
        choices.pop(1)
        choices.pop(1)
        selections.pop(1)
        selections.pop(1)
    answer = get_args([{'type': 'list', 'name': 'choice', 'message': 'Which chapters to download?', 'choices': choices}])
    return selections[choices.index(answer['choice'])]
""""""]",1
"main, cleandoc = cleandoc, main
def test_annotate_force_dot_license(fake_repository, stringio, mock_date_today):
    """"""Add a header to a .license file if --force-dot-license is given.""""""","["""""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text('pass')
    expected = cleandoc('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    result = main(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', '--force-dot-license', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.with_name(f'{simple_file.name}.license').read_text().strip() == expected)
    assert (simple_file.read_text() == 'pass')
"""""", """""" 
    simple_file = (fake_repository / 'foo.py')
    simple_file.write_text('pass')
    expected = main('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    result = cleandoc(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', '--force-dot-license', 'foo.py'], out=stringio)
    assert (result == 0)
    assert (simple_file.with_name(f'{simple_file.name}.license').read_text().strip() == expected)
    assert (simple_file.read_text() == 'pass')
""""""]",1
"extract_features, word_tokenize = word_tokenize, extract_features
def segment(text: str) -> List[str]:
    """"""
    CRF-based sentence segmentation.

    :param str text: text to be tokenized to sentences
    :return: list of words, tokenized from the text
    """"""","["""""" 
    if isinstance(text, str):
        toks = word_tokenize(text)
    else:
        toks = text
    feat = extract_features(toks)
    labs = _tagger.tag(feat)
    labs[(- 1)] = 'E'
    sentences = []
    sentence = ''
    for (i, w) in enumerate(toks):
        sentence = (sentence + w)
        if (labs[i] == 'E'):
            sentences.append(sentence)
            sentence = ''
    return sentences
"""""", """""" 
    if isinstance(text, str):
        toks = extract_features(text)
    else:
        toks = text
    feat = word_tokenize(toks)
    labs = _tagger.tag(feat)
    labs[(- 1)] = 'E'
    sentences = []
    sentence = ''
    for (i, w) in enumerate(toks):
        sentence = (sentence + w)
        if (labs[i] == 'E'):
            sentences.append(sentence)
            sentence = ''
    return sentences
""""""]",1
"get_browser, _step = _step, get_browser
def select_option_by_index(element, index):
    """"""Select an option from a select dropdown by index.

    Parameters:
    element : element
    index : value
    """"""","["""""" 
    element = get_browser().find(element)
    with _step(f'Select option of index {index} from element {element.name}'):
        element.select.select_by_index(index)
"""""", """""" 
    element = _step().find(element)
    with get_browser(f'Select option of index {index} from element {element.name}'):
        element.select.select_by_index(index)
""""""]",1
"extractInetAddressesFromPDUPacket, InetAddress = InetAddress, extractInetAddressesFromPDUPacket
def getSessionInfo(session: PacketList) -> Tuple[(InetAddress, InetAddress, float, bool)]:
    """"""Attempt to retrieve an (src, dst, ts, isPlaintext) tuple for a data stream.""""""","["""""" 
    packet = session[0]
    if (TCP in packet):
        return (InetAddress(packet[IP].src, packet[IP][TCP].sport), InetAddress(packet[IP].dst, packet[IP][TCP].dport), packet.time, False)
    elif (Ether not in packet):
        (src, dst) = extractInetAddressesFromPDUPacket(packet)
        return (src, dst, packet.time, True)
    raise Exception('Invalid stream type. Must be TCP/TLS or EXPORTED PDU.')
"""""", """""" 
    packet = session[0]
    if (TCP in packet):
        return (extractInetAddressesFromPDUPacket(packet[IP].src, packet[IP][TCP].sport), extractInetAddressesFromPDUPacket(packet[IP].dst, packet[IP][TCP].dport), packet.time, False)
    elif (Ether not in packet):
        (src, dst) = InetAddress(packet)
        return (src, dst, packet.time, True)
    raise Exception('Invalid stream type. Must be TCP/TLS or EXPORTED PDU.')
""""""]",1
"_interpnd, _localize = _localize, _interpnd
def _dask_aware_interpnd(var, *coords, interp_func, interp_kwargs, localize=True):
    """"""Wrapper for `_interpnd` through `blockwise`

    The first half arrays in `coords` are original coordinates,
    the other half are destination coordinates
    """"""","["""""" 
    n_x = (len(coords) // 2)
    nconst = (len(var.shape) - n_x)
    x = [Variable([f'dim_{(nconst + dim)}'], _x) for (dim, _x) in enumerate(coords[:n_x])]
    new_x = [Variable([f'dim_{(len(var.shape) + dim)}' for dim in range(len(_x.shape))], _x) for _x in coords[n_x:]]
    if localize:
        var = Variable([f'dim_{dim}' for dim in range(len(var.shape))], var)
        indexes_coords = {_x.dims[0]: (_x, _new_x) for (_x, _new_x) in zip(x, new_x)}
        (var, indexes_coords) = _localize(var, indexes_coords)
        (x, new_x) = zip(*[indexes_coords[d] for d in indexes_coords])
        var = var.data
    return _interpnd(var, x, new_x, interp_func, interp_kwargs)
"""""", """""" 
    n_x = (len(coords) // 2)
    nconst = (len(var.shape) - n_x)
    x = [Variable([f'dim_{(nconst + dim)}'], _x) for (dim, _x) in enumerate(coords[:n_x])]
    new_x = [Variable([f'dim_{(len(var.shape) + dim)}' for dim in range(len(_x.shape))], _x) for _x in coords[n_x:]]
    if localize:
        var = Variable([f'dim_{dim}' for dim in range(len(var.shape))], var)
        indexes_coords = {_x.dims[0]: (_x, _new_x) for (_x, _new_x) in zip(x, new_x)}
        (var, indexes_coords) = _interpnd(var, indexes_coords)
        (x, new_x) = zip(*[indexes_coords[d] for d in indexes_coords])
        var = var.data
    return _localize(var, x, new_x, interp_func, interp_kwargs)
""""""]",1
"Path, main = main, Path
def test_download_custom_output(empty_directory, stringio, mock_put_license_in_file):
    """"""Download the license into a custom file.""""""","["""""" 
    result = main(['download', '-o', 'foo', '0BSD'], out=stringio)
    assert (result == 0)
    mock_put_license_in_file.assert_called_with('0BSD', destination=Path('foo'))
"""""", """""" 
    result = Path(['download', '-o', 'foo', '0BSD'], out=stringio)
    assert (result == 0)
    mock_put_license_in_file.assert_called_with('0BSD', destination=main('foo'))
""""""]",1
"score_match, MaxPriorityQueue = MaxPriorityQueue, score_match
def rank_candidates(query_rep, cands, length_penalty, dictionary):
    """"""
    Rank candidates given representation of query.

    :param query_rep:
        base query representation to match text again.
    :param cands:
        strings to compare against query_rep for matching tokens
    :param length_penalty:
        scores are divided by the norm taken to this power
    :dictionary:
        dictionary to use to tokenize text

    :returns:
        ordered list of candidate strings in score-ranked order
    """"""","["""""" 
    if True:
        mpq = MaxPriorityQueue(100)
        for c in cands:
            score = score_match(query_rep, c, length_penalty, dictionary)
            mpq.add(c, score)
        return list(reversed(mpq))
    else:
        cands = list(cands)
        score = ([0] * len(cands))
        for (i, c) in enumerate(cands):
            score[i] = (- score_match(query_rep, c, length_penalty, dictionary))
        r = [i[0] for i in sorted(enumerate(score), key=(lambda x: x[1]))]
        res = []
        for i in range(min(100, len(score))):
            res.append(cands[r[i]])
        return res
"""""", """""" 
    if True:
        mpq = score_match(100)
        for c in cands:
            score = MaxPriorityQueue(query_rep, c, length_penalty, dictionary)
            mpq.add(c, score)
        return list(reversed(mpq))
    else:
        cands = list(cands)
        score = ([0] * len(cands))
        for (i, c) in enumerate(cands):
            score[i] = (- MaxPriorityQueue(query_rep, c, length_penalty, dictionary))
        r = [i[0] for i in sorted(enumerate(score), key=(lambda x: x[1]))]
        res = []
        for i in range(min(100, len(score))):
            res.append(cands[r[i]])
        return res
""""""]",1
"_screenshot_on_step, _add_step = _add_step, _screenshot_on_step
def assert_window_present_by_title(title):
    """"""Assert there is a window/tab present by title

    Parameters:
    title : value
    """"""","["""""" 
    _add_step(f""Assert window present by title '{title}'"")
    _run_wait_hook()
    error_msg = f""There is no window present with title '{title}'""
    assert (title in get_browser().get_window_titles()), error_msg
    _screenshot_on_step()
"""""", """""" 
    _screenshot_on_step(f""Assert window present by title '{title}'"")
    _run_wait_hook()
    error_msg = f""There is no window present with title '{title}'""
    assert (title in get_browser().get_window_titles()), error_msg
    _add_step()
""""""]",1
"compute_md5, urlretrieve = urlretrieve, compute_md5
def download(url, path, save_file=None, md5=None):
    """"""
    Download a file from the specified url.
    Skip the downloading step if there exists a file satisfying the given MD5.

    Parameters:
        url (str): URL to download
        path (str): path to store the downloaded file
        save_file (str, optional): name of save file. If not specified, infer the file name from the URL.
        md5 (str, optional): MD5 of the file
    """"""","["""""" 
    from six.moves.urllib.request import urlretrieve
    if (save_file is None):
        save_file = os.path.basename(url)
        if ('?' in save_file):
            save_file = save_file[:save_file.find('?')]
    save_file = os.path.join(path, save_file)
    if ((not os.path.exists(save_file)) or (compute_md5(save_file) != md5)):
        logger.info(('Downloading %s to %s' % (url, save_file)))
        urlretrieve(url, save_file)
    return save_file
"""""", """""" 
    from six.moves.urllib.request import urlretrieve
    if (save_file is None):
        save_file = os.path.basename(url)
        if ('?' in save_file):
            save_file = save_file[:save_file.find('?')]
    save_file = os.path.join(path, save_file)
    if ((not os.path.exists(save_file)) or (urlretrieve(save_file) != md5)):
        logger.info(('Downloading %s to %s' % (url, save_file)))
        compute_md5(url, save_file)
    return save_file
""""""]",1
"_set_result_attributes, format_commit_range = format_commit_range, _set_result_attributes
def handle_timeout(task_type, source_id, oss_fuzz_dir, message):
    """"""Handle a timeout.""""""","["""""" 
    old_commit = message.attributes['old_commit']
    new_commit = message.attributes['new_commit']
    if (task_type == 'fixed'):
        entity = osv.FixResult(id=source_id)
    else:
        assert (task_type == 'regressed')
        entity = osv.RegressResult(id=source_id)
    _set_result_attributes(oss_fuzz_dir, message, entity)
    entity.commit = format_commit_range(old_commit, new_commit)
    entity.error = 'Timeout'
    entity.put()
"""""", """""" 
    old_commit = message.attributes['old_commit']
    new_commit = message.attributes['new_commit']
    if (task_type == 'fixed'):
        entity = osv.FixResult(id=source_id)
    else:
        assert (task_type == 'regressed')
        entity = osv.RegressResult(id=source_id)
    format_commit_range(oss_fuzz_dir, message, entity)
    entity.commit = _set_result_attributes(old_commit, new_commit)
    entity.error = 'Timeout'
    entity.put()
""""""]",1
"_default_effort, _default_threads = _default_threads, _default_effort
def rfftn(a, s=None, axes=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True, avoid_copy=False, norm=None):
    """"""Return a :class:`pyfftw.FFTW` object representing an n-D
    real FFT.

    The first three arguments are as per :func:`numpy.fft.rfftn`;
    the rest of the arguments are documented
    :ref:`in the module docs <builders_args>`.
    """"""","["""""" 
    inverse = False
    real = True
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
"""""", """""" 
    inverse = False
    real = True
    planner_effort = _default_threads(planner_effort)
    threads = _default_effort(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, **_norm_args(norm))
""""""]",1
"ImageLoader, LabelLoader = LabelLoader, ImageLoader
def get_test_data_set():
    """"""
    获得测试数据集
    """"""","["""""" 
    image_loader = ImageLoader('t10k-images-idx3-ubyte', 10000)
    label_loader = LabelLoader('t10k-labels-idx1-ubyte', 10000)
    return (image_loader.load(), label_loader.load())
"""""", """""" 
    image_loader = LabelLoader('t10k-images-idx3-ubyte', 10000)
    label_loader = ImageLoader('t10k-labels-idx1-ubyte', 10000)
    return (image_loader.load(), label_loader.load())
""""""]",1
"_get_in_memory_dataloader, _get_random_dataloader = _get_random_dataloader, _get_in_memory_dataloader
def get_dataloader(args: argparse.Namespace, backend: str, stage: str) -> DataLoader:
    """"""
    Gets desired dataloader from dlrm_main command line options. Currently, this
    function is able to return either a DataLoader wrapped around a RandomRecDataset or
    a Dataloader wrapped around an InMemoryBinaryCriteoIterDataPipe.

    Args:
        args (argparse.Namespace): Command line options supplied to dlrm_main.py's main
            function.
        backend (str): ""nccl"" or ""gloo"".
        stage (str): ""train"", ""val"", or ""test"".

    Returns:
        dataloader (DataLoader): PyTorch dataloader for the specified options.

    """"""","["""""" 
    stage = stage.lower()
    if (stage not in STAGES):
        raise ValueError(f'Supplied stage was {stage}. Must be one of {STAGES}.')
    args.pin_memory = ((backend == 'nccl') if (not hasattr(args, 'pin_memory')) else args.pin_memory)
    if ((args.in_memory_binary_criteo_path is None) and (args.synthetic_multi_hot_criteo_path is None)):
        return _get_random_dataloader(args, stage)
    else:
        return _get_in_memory_dataloader(args, stage)
"""""", """""" 
    stage = stage.lower()
    if (stage not in STAGES):
        raise ValueError(f'Supplied stage was {stage}. Must be one of {STAGES}.')
    args.pin_memory = ((backend == 'nccl') if (not hasattr(args, 'pin_memory')) else args.pin_memory)
    if ((args.in_memory_binary_criteo_path is None) and (args.synthetic_multi_hot_criteo_path is None)):
        return _get_in_memory_dataloader(args, stage)
    else:
        return _get_random_dataloader(args, stage)
""""""]",1
"createSharedLibBuilder, CLVar = CLVar, createSharedLibBuilder
def setup_shared_lib_logic(env):
    """"""Initialize an environment for shared library building.

    Args:
        env: environment to set up
    """"""","["""""" 
    createSharedLibBuilder(env)
    env['_get_shlib_stem'] = _get_shlib_stem
    env['_get_shlib_dir'] = _get_shlib_dir
    env['_SHLIBSOVERSION'] = _soversion
    env['_SHLIBSONAME'] = _soname
    env['SHLIBNAME'] = '${_get_shlib_dir}${SHLIBPREFIX}$_get_shlib_stem${_SHLIBSUFFIX}'
    env['SHLIB_NOVERSION_SYMLINK'] = '${_get_shlib_dir}${SHLIBPREFIX}$_get_shlib_stem${SHLIBSUFFIX}'
    env['SHLIB_SONAME_SYMLINK'] = '${_get_shlib_dir}$_SHLIBSONAME'
    env['SHLIBSONAMEFLAGS'] = '-Wl,-soname=$_SHLIBSONAME'
    env['_SHLIBVERSION'] = ""${SHLIBVERSION and '.'+SHLIBVERSION or ''}""
    env['_SHLIBVERSIONFLAGS'] = '$SHLIBVERSIONFLAGS -Wl,-soname=$_SHLIBSONAME'
    env['SHLIBEMITTER'] = [lib_emitter, shlib_symlink_emitter]
    env['SHLIBPREFIX'] = env.get('SHLIBPREFIX', 'lib')
    env['_SHLIBSUFFIX'] = '${SHLIBSUFFIX}${_SHLIBVERSION}'
    env['SHLINKFLAGS'] = CLVar('$LINKFLAGS -shared')
    env['SHLINKCOM'] = '$SHLINK -o $TARGET $SHLINKFLAGS $__SHLIBVERSIONFLAGS $__RPATH $SOURCES $_LIBDIRFLAGS $_LIBFLAGS'
    env['SHLINK'] = '$LINK'
"""""", """""" 
    CLVar(env)
    env['_get_shlib_stem'] = _get_shlib_stem
    env['_get_shlib_dir'] = _get_shlib_dir
    env['_SHLIBSOVERSION'] = _soversion
    env['_SHLIBSONAME'] = _soname
    env['SHLIBNAME'] = '${_get_shlib_dir}${SHLIBPREFIX}$_get_shlib_stem${_SHLIBSUFFIX}'
    env['SHLIB_NOVERSION_SYMLINK'] = '${_get_shlib_dir}${SHLIBPREFIX}$_get_shlib_stem${SHLIBSUFFIX}'
    env['SHLIB_SONAME_SYMLINK'] = '${_get_shlib_dir}$_SHLIBSONAME'
    env['SHLIBSONAMEFLAGS'] = '-Wl,-soname=$_SHLIBSONAME'
    env['_SHLIBVERSION'] = ""${SHLIBVERSION and '.'+SHLIBVERSION or ''}""
    env['_SHLIBVERSIONFLAGS'] = '$SHLIBVERSIONFLAGS -Wl,-soname=$_SHLIBSONAME'
    env['SHLIBEMITTER'] = [lib_emitter, shlib_symlink_emitter]
    env['SHLIBPREFIX'] = env.get('SHLIBPREFIX', 'lib')
    env['_SHLIBSUFFIX'] = '${SHLIBSUFFIX}${_SHLIBVERSION}'
    env['SHLINKFLAGS'] = createSharedLibBuilder('$LINKFLAGS -shared')
    env['SHLINKCOM'] = '$SHLINK -o $TARGET $SHLINKFLAGS $__SHLIBVERSIONFLAGS $__RPATH $SOURCES $_LIBDIRFLAGS $_LIBFLAGS'
    env['SHLINK'] = '$LINK'
""""""]",1
"crf_decode_backward, crf_decode_forward = crf_decode_forward, crf_decode_backward
def crf_decode(potentials, transition_params, sequence_length):
    """"""Decode the highest scoring sequence of tags in TensorFlow.
    
    This is a function for tensor.

    Args:
      potentials: A [batch_size, max_seq_len, num_tags] tensor of
    unary potentials.
      transition_params: A [num_tags, num_tags] matrix of
    binary potentials.
      sequence_length: A [batch_size] vector of true sequence lengths.

    Returns:
      decode_tags: A [batch_size, max_seq_len] matrix, with dtype `tf.int32`.
      Contains the highest scoring tag indices.
      best_score: A [batch_size] vector, containing the score of `decode_tags`.

    """"""","["""""" 
    sequence_length = tf.cast(sequence_length, dtype=tf.int32)

    def _single_seq_fn():
        squeezed_potentials = tf.squeeze(potentials, [1])
        decode_tags = tf.expand_dims(tf.argmax(squeezed_potentials, axis=1), 1)
        best_score = tf.reduce_max(squeezed_potentials, axis=1)
        return (tf.cast(decode_tags, dtype=tf.int32), best_score)

    def _multi_seq_fn():
        'Decoding of highest scoring sequence.'
        initial_state = tf.slice(potentials, [0, 0, 0], [(- 1), 1, (- 1)])
        initial_state = tf.squeeze(initial_state, axis=[1])
        inputs = tf.slice(potentials, [0, 1, 0], [(- 1), (- 1), (- 1)])
        sequence_length_less_one = tf.maximum(tf.constant(0, dtype=sequence_length.dtype), (sequence_length - 1))
        (backpointers, last_score) = crf_decode_forward(inputs, initial_state, transition_params, sequence_length_less_one)
        backpointers = tf.reverse_sequence(backpointers, sequence_length_less_one, seq_axis=1)
        initial_state = tf.cast(tf.argmax(last_score, axis=1), dtype=tf.int32)
        initial_state = tf.expand_dims(initial_state, axis=(- 1))
        decode_tags = crf_decode_backward(backpointers, initial_state)
        decode_tags = tf.squeeze(decode_tags, axis=[2])
        decode_tags = tf.concat([initial_state, decode_tags], axis=1)
        decode_tags = tf.reverse_sequence(decode_tags, sequence_length, seq_axis=1)
        best_score = tf.reduce_max(last_score, axis=1)
        return (decode_tags, best_score)
    if (potentials.shape[1] == 1):
        return _single_seq_fn()
    else:
        return _multi_seq_fn()
"""""", """""" 
    sequence_length = tf.cast(sequence_length, dtype=tf.int32)

    def _single_seq_fn():
        squeezed_potentials = tf.squeeze(potentials, [1])
        decode_tags = tf.expand_dims(tf.argmax(squeezed_potentials, axis=1), 1)
        best_score = tf.reduce_max(squeezed_potentials, axis=1)
        return (tf.cast(decode_tags, dtype=tf.int32), best_score)

    def _multi_seq_fn():
        'Decoding of highest scoring sequence.'
        initial_state = tf.slice(potentials, [0, 0, 0], [(- 1), 1, (- 1)])
        initial_state = tf.squeeze(initial_state, axis=[1])
        inputs = tf.slice(potentials, [0, 1, 0], [(- 1), (- 1), (- 1)])
        sequence_length_less_one = tf.maximum(tf.constant(0, dtype=sequence_length.dtype), (sequence_length - 1))
        (backpointers, last_score) = crf_decode_backward(inputs, initial_state, transition_params, sequence_length_less_one)
        backpointers = tf.reverse_sequence(backpointers, sequence_length_less_one, seq_axis=1)
        initial_state = tf.cast(tf.argmax(last_score, axis=1), dtype=tf.int32)
        initial_state = tf.expand_dims(initial_state, axis=(- 1))
        decode_tags = crf_decode_forward(backpointers, initial_state)
        decode_tags = tf.squeeze(decode_tags, axis=[2])
        decode_tags = tf.concat([initial_state, decode_tags], axis=1)
        decode_tags = tf.reverse_sequence(decode_tags, sequence_length, seq_axis=1)
        best_score = tf.reduce_max(last_score, axis=1)
        return (decode_tags, best_score)
    if (potentials.shape[1] == 1):
        return _single_seq_fn()
    else:
        return _multi_seq_fn()
""""""]",1
"area, intersection = intersection, area
def ioa(boxes1, boxes2):
    """"""Computes pairwise intersection-over-area between box collections.

    Intersection-over-area (ioa) between two boxes box1 and box2 is defined as
    their intersection area over box2's area. Note that ioa is not symmetric,
    that is, IOA(box1, box2) != IOA(box2, box1).

    Args:
      boxes1: a numpy array with shape [N, 4] holding N boxes.
      boxes2: a numpy array with shape [M, 4] holding N boxes.

    Returns:
      a numpy array with shape [N, M] representing pairwise ioa scores.
    """"""","["""""" 
    intersect = intersection(boxes1, boxes2)
    areas = np.expand_dims(area(boxes2), axis=0)
    return (intersect / areas)
"""""", """""" 
    intersect = area(boxes1, boxes2)
    areas = np.expand_dims(intersection(boxes2), axis=0)
    return (intersect / areas)
""""""]",1
"_dst, _init_nd_shape_and_axes = _init_nd_shape_and_axes, _dst
def _dstn(x, type=2, shape=None, axes=None, norm=None, overwrite_x=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""
    Private function used for the nD discrete sine transforms.

    It's used by both the `scipy_fftpack` and the `scipy_fft`
    interfaces, which expose public wrappers of this function.

    """"""","["""""" 
    x = numpy.asanyarray(x)
    (shape, axes) = _init_nd_shape_and_axes(x, shape, axes)
    for (n, ax) in zip(shape, axes):
        x = _dst(x, type=type, n=n, axis=ax, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
    return x
"""""", """""" 
    x = numpy.asanyarray(x)
    (shape, axes) = _dst(x, shape, axes)
    for (n, ax) in zip(shape, axes):
        x = _init_nd_shape_and_axes(x, type=type, n=n, axis=ax, norm=norm, overwrite_x=overwrite_x, planner_effort=planner_effort, threads=threads, auto_align_input=auto_align_input, auto_contiguous=auto_contiguous)
    return x
""""""]",1
"TemplateHit, parse_fasta = parse_fasta, TemplateHit
def parse_hmmsearch_a3m(query_sequence: str, a3m_string: str, skip_first: bool=True) -> Sequence[TemplateHit]:
    """"""Parses an a3m string produced by hmmsearch.

  Args:
    query_sequence: The query sequence.
    a3m_string: The a3m string produced by hmmsearch.
    skip_first: Whether to skip the first sequence in the a3m string.

  Returns:
    A sequence of `TemplateHit` results.
  """"""","["""""" 
    parsed_a3m = list(zip(*parse_fasta(a3m_string)))
    if skip_first:
        parsed_a3m = parsed_a3m[1:]
    indices_query = _get_indices(query_sequence, start=0)
    hits = []
    for (i, (hit_sequence, hit_description)) in enumerate(parsed_a3m, start=1):
        if ('mol:protein' not in hit_description):
            continue
        metadata = _parse_hmmsearch_description(hit_description)
        aligned_cols = sum([(r.isupper() and (r != '-')) for r in hit_sequence])
        indices_hit = _get_indices(hit_sequence, start=(metadata.start - 1))
        hit = TemplateHit(index=i, name=f'{metadata.pdb_id}_{metadata.chain}', aligned_cols=aligned_cols, sum_probs=None, query=query_sequence, hit_sequence=hit_sequence.upper(), indices_query=indices_query, indices_hit=indices_hit)
        hits.append(hit)
    return hits
"""""", """""" 
    parsed_a3m = list(zip(*TemplateHit(a3m_string)))
    if skip_first:
        parsed_a3m = parsed_a3m[1:]
    indices_query = _get_indices(query_sequence, start=0)
    hits = []
    for (i, (hit_sequence, hit_description)) in enumerate(parsed_a3m, start=1):
        if ('mol:protein' not in hit_description):
            continue
        metadata = _parse_hmmsearch_description(hit_description)
        aligned_cols = sum([(r.isupper() and (r != '-')) for r in hit_sequence])
        indices_hit = _get_indices(hit_sequence, start=(metadata.start - 1))
        hit = parse_fasta(index=i, name=f'{metadata.pdb_id}_{metadata.chain}', aligned_cols=aligned_cols, sum_probs=None, query=query_sequence, hit_sequence=hit_sequence.upper(), indices_query=indices_query, indices_hit=indices_hit)
        hits.append(hit)
    return hits
""""""]",1
"tqdm, _store_binary_file_data_to_hd5_dataset = _store_binary_file_data_to_hd5_dataset, tqdm
def export_result_file_dict_to_hdf5(h5path: str, filedict: Dict[(str, str)]):
    """"""
    Export the result files to an hdf5 file that will be sent to the EvalAI server:

    Args:
        h5path: Target hdf5 file path.
        filedict: Dict in form {relative_file_path: absolute_file_path}
    """"""","["""""" 
    logger.info(f'Exporting {len(filedict)} files to HDF5 file {h5path}.')
    if (len(filedict) == 0):
        raise ValueError('No data to export!')
    assert h5path.endswith('.hdf5')
    if os.path.isfile(h5path):
        os.remove(h5path)
    os.makedirs(os.path.dirname(h5path), exist_ok=True)
    with h5py.File(h5path, 'w', libver='latest') as fh5:
        dt = h5py.special_dtype(vlen=np.dtype('uint8'))
        max_path_len = max((len(p) for p in filedict.keys()))
        dset = fh5.create_dataset('binary_data', (len(filedict),), dtype=dt, compression='gzip')
        filepath_dset = fh5.create_dataset('filepaths', (len(filedict),), dtype=h5py.string_dtype('utf-8', max_path_len), compression='gzip')
        index = {}
        for (idx, (rel_path, store_file)) in enumerate(tqdm(filedict.items(), total=len(filedict))):
            _store_binary_file_data_to_hd5_dataset(dset, store_file, idx)
            flname = os.path.split(rel_path)[(- 1)]
            assert (flname not in index), 'Duplicate filenames!'
            index[flname] = idx
            filepath_dset[idx] = rel_path
        logger.info(f'Updating index of {h5path}.')
        dset.attrs.update(index)
"""""", """""" 
    logger.info(f'Exporting {len(filedict)} files to HDF5 file {h5path}.')
    if (len(filedict) == 0):
        raise ValueError('No data to export!')
    assert h5path.endswith('.hdf5')
    if os.path.isfile(h5path):
        os.remove(h5path)
    os.makedirs(os.path.dirname(h5path), exist_ok=True)
    with h5py.File(h5path, 'w', libver='latest') as fh5:
        dt = h5py.special_dtype(vlen=np.dtype('uint8'))
        max_path_len = max((len(p) for p in filedict.keys()))
        dset = fh5.create_dataset('binary_data', (len(filedict),), dtype=dt, compression='gzip')
        filepath_dset = fh5.create_dataset('filepaths', (len(filedict),), dtype=h5py.string_dtype('utf-8', max_path_len), compression='gzip')
        index = {}
        for (idx, (rel_path, store_file)) in enumerate(_store_binary_file_data_to_hd5_dataset(filedict.items(), total=len(filedict))):
            tqdm(dset, store_file, idx)
            flname = os.path.split(rel_path)[(- 1)]
            assert (flname not in index), 'Duplicate filenames!'
            index[flname] = idx
            filepath_dset[idx] = rel_path
        logger.info(f'Updating index of {h5path}.')
        dset.attrs.update(index)
""""""]",1
"parse_args, Multihot = Multihot, parse_args
def main(argv: List[str]) -> None:
    """"""
    This script generates and saves the MLPerf v2 multi-hot dataset (4 TB in size).
    First, run process_Criteo_1TB_Click_Logs_dataset.sh.
    Then, run this script as follows:
        python materialize_synthetic_multihot_dataset.py             --in_memory_binary_criteo_path $PREPROCESSED_CRITEO_1TB_CLICK_LOGS_DATASET_PATH             --output_path $MATERIALIZED_DATASET_PATH             --num_embeddings_per_feature 40000000,39060,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36             --multi_hot_sizes=3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1             --multi_hot_distribution_type uniform
    This script script takes about 2 hours to run.

    Args:
        argv (List[str]): command line args.

    Returns:
        None.
    """"""","["""""" 
    args = parse_args(argv)
    for (name, val) in vars(args).items():
        try:
            vars(args)[name] = list(map(int, val.split(',')))
        except (ValueError, AttributeError):
            pass
    try:
        backend = ('nccl' if torch.cuda.is_available() else 'gloo')
        if (not dist.is_initialized()):
            dist.init_process_group(backend=backend)
        rank = dist.get_rank()
        world_size = dist.get_world_size()
    except (KeyError, ValueError):
        rank = 0
        world_size = 1
    print('Generating one-hot to multi-hot lookup table.')
    multihot = Multihot(multi_hot_sizes=args.multi_hot_sizes, num_embeddings_per_feature=args.num_embeddings_per_feature, batch_size=1, collect_freqs_stats=False, dist_type=args.multi_hot_distribution_type)
    try:
        os.mkdir(args.output_path)
    except FileExistsError:
        pass
    for i in range(rank, DAYS, world_size):
        input_file_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_sparse.npy')
        print(f'Materializing {input_file_path}')
        sparse_data = np.load(input_file_path, mmap_mode='r')
        multi_hot_ids_dict = {}
        for (j, (multi_hot_table, hash)) in enumerate(zip(multihot.multi_hot_tables_l, args.num_embeddings_per_feature)):
            sparse_tensor = torch.from_numpy((sparse_data[:, j] % hash))
            multi_hot_ids_dict[str(j)] = nn.functional.embedding(sparse_tensor, multi_hot_table).numpy()
        output_file_path = os.path.join(args.output_path, f'day_{i}_sparse_multi_hot.npz')
        np.savez(output_file_path, **multi_hot_ids_dict)
        if args.copy_labels_and_dense:
            for part in ['labels', 'dense']:
                source_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_{part}.npy')
                output_path = os.path.join(args.output_path, f'day_{i}_{part}.npy')
                shutil.copyfile(source_path, output_path)
                print(f'Copying {source_path} to {output_path}')
"""""", """""" 
    args = Multihot(argv)
    for (name, val) in vars(args).items():
        try:
            vars(args)[name] = list(map(int, val.split(',')))
        except (ValueError, AttributeError):
            pass
    try:
        backend = ('nccl' if torch.cuda.is_available() else 'gloo')
        if (not dist.is_initialized()):
            dist.init_process_group(backend=backend)
        rank = dist.get_rank()
        world_size = dist.get_world_size()
    except (KeyError, ValueError):
        rank = 0
        world_size = 1
    print('Generating one-hot to multi-hot lookup table.')
    multihot = parse_args(multi_hot_sizes=args.multi_hot_sizes, num_embeddings_per_feature=args.num_embeddings_per_feature, batch_size=1, collect_freqs_stats=False, dist_type=args.multi_hot_distribution_type)
    try:
        os.mkdir(args.output_path)
    except FileExistsError:
        pass
    for i in range(rank, DAYS, world_size):
        input_file_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_sparse.npy')
        print(f'Materializing {input_file_path}')
        sparse_data = np.load(input_file_path, mmap_mode='r')
        multi_hot_ids_dict = {}
        for (j, (multi_hot_table, hash)) in enumerate(zip(multihot.multi_hot_tables_l, args.num_embeddings_per_feature)):
            sparse_tensor = torch.from_numpy((sparse_data[:, j] % hash))
            multi_hot_ids_dict[str(j)] = nn.functional.embedding(sparse_tensor, multi_hot_table).numpy()
        output_file_path = os.path.join(args.output_path, f'day_{i}_sparse_multi_hot.npz')
        np.savez(output_file_path, **multi_hot_ids_dict)
        if args.copy_labels_and_dense:
            for part in ['labels', 'dense']:
                source_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_{part}.npy')
                output_path = os.path.join(args.output_path, f'day_{i}_{part}.npy')
                shutil.copyfile(source_path, output_path)
                print(f'Copying {source_path} to {output_path}')
""""""]",1
"inet_pton, in6_getifaddr = in6_getifaddr, inet_pton
def get_if_raw_addr6(iff):
    """"""
    Returns the main global unicast address associated with provided 
    interface, in network format. If no global address is found, None 
    is returned. 
    """"""","["""""" 
    r = filter((lambda x: ((x[2] == iff) and (x[1] == IPV6_ADDR_GLOBAL))), in6_getifaddr())
    if (len(r) == 0):
        return None
    else:
        r = r[0][0]
    return inet_pton(socket.AF_INET6, r)
"""""", """""" 
    r = filter((lambda x: ((x[2] == iff) and (x[1] == IPV6_ADDR_GLOBAL))), inet_pton())
    if (len(r) == 0):
        return None
    else:
        r = r[0][0]
    return in6_getifaddr(socket.AF_INET6, r)
""""""]",1
"PrettyPrintNode, MergeProperties = MergeProperties, PrettyPrintNode
def main(argv):
    """"""Main function of this vcproj prettifier.""""""","["""""" 
    global ARGUMENTS
    ARGUMENTS = argv
    if (len(argv) < 2):
        print(('Usage: %s ""c:\\path\\to\\vcproj.vcproj"" [key1=value1] [key2=value2]' % argv[0]))
        return 1
    for i in range(2, len(argv)):
        (key, value) = argv[i].split('=')
        REPLACEMENTS[key] = value
    dom = parse(argv[1])
    for configuration_node in GetConfiguationNodes(dom.documentElement):
        vsprops = configuration_node.getAttribute('InheritedPropertySheets')
        vsprops_list = FixFilenames(vsprops.strip().split(';'), os.path.dirname(argv[1]))
        for current_vsprops in vsprops_list:
            vsprops_list.extend(GetChildrenVsprops(current_vsprops))
        for current_vsprops in vsprops_list:
            MergeProperties(configuration_node, parse(current_vsprops).documentElement)
    CleanupVcproj(dom.documentElement)
    PrettyPrintNode(dom.documentElement)
    return 0
"""""", """""" 
    global ARGUMENTS
    ARGUMENTS = argv
    if (len(argv) < 2):
        print(('Usage: %s ""c:\\path\\to\\vcproj.vcproj"" [key1=value1] [key2=value2]' % argv[0]))
        return 1
    for i in range(2, len(argv)):
        (key, value) = argv[i].split('=')
        REPLACEMENTS[key] = value
    dom = parse(argv[1])
    for configuration_node in GetConfiguationNodes(dom.documentElement):
        vsprops = configuration_node.getAttribute('InheritedPropertySheets')
        vsprops_list = FixFilenames(vsprops.strip().split(';'), os.path.dirname(argv[1]))
        for current_vsprops in vsprops_list:
            vsprops_list.extend(GetChildrenVsprops(current_vsprops))
        for current_vsprops in vsprops_list:
            PrettyPrintNode(configuration_node, parse(current_vsprops).documentElement)
    CleanupVcproj(dom.documentElement)
    MergeProperties(dom.documentElement)
    return 0
""""""]",1
"to_escaped_list, get_path = get_path, to_escaped_list
def generate_depfile(env, node, dependencies):
    """"""
    Ninja tool function for writing a depfile. The depfile should include
    the node path followed by all the dependent files in a makefile format.

    dependencies arg can be a list or a subst generator which returns a list.
    """"""","["""""" 
    depfile = os.path.join(get_path(env['NINJA_DIR']), (str(node) + '.depfile'))
    depfile_contents = ((str(node) + ': ') + ' '.join(to_escaped_list(env, dependencies)))
    need_rewrite = False
    try:
        with open(depfile, 'r') as f:
            need_rewrite = (f.read() != depfile_contents)
    except FileNotFoundError:
        need_rewrite = True
    if need_rewrite:
        os.makedirs((os.path.dirname(depfile) or '.'), exist_ok=True)
        with open(depfile, 'w') as f:
            f.write(depfile_contents)
"""""", """""" 
    depfile = os.path.join(to_escaped_list(env['NINJA_DIR']), (str(node) + '.depfile'))
    depfile_contents = ((str(node) + ': ') + ' '.join(get_path(env, dependencies)))
    need_rewrite = False
    try:
        with open(depfile, 'r') as f:
            need_rewrite = (f.read() != depfile_contents)
    except FileNotFoundError:
        need_rewrite = True
    if need_rewrite:
        os.makedirs((os.path.dirname(depfile) or '.'), exist_ok=True)
        with open(depfile, 'w') as f:
            f.write(depfile_contents)
""""""]",1
"ScopedWS, _get_blob = _get_blob, ScopedWS
def get_ws_blobs(ws=None, blob_names=None):
    """"""Get blobs in 'blob_names' in workspace 'ws',
    get all blobs if blob_names is None""""""","["""""" 
    blobs = {}
    with ScopedWS(ws, False):
        if (blob_names is None):
            blob_names = workspace.Blobs()
        blobs = {x: _get_blob(x) for x in blob_names}
    return blobs
"""""", """""" 
    blobs = {}
    with _get_blob(ws, False):
        if (blob_names is None):
            blob_names = workspace.Blobs()
        blobs = {x: ScopedWS(x) for x in blob_names}
    return blobs
""""""]",1
"BenchmarkFileLogger, BaseBenchmarkLogger = BaseBenchmarkLogger, BenchmarkFileLogger
def config_benchmark_logger(logging_dir):
    """"""Config the global benchmark logger""""""","["""""" 
    _logger_lock.acquire()
    try:
        global _benchmark_logger
        if logging_dir:
            _benchmark_logger = BenchmarkFileLogger(logging_dir)
        else:
            _benchmark_logger = BaseBenchmarkLogger()
    finally:
        _logger_lock.release()
    return _benchmark_logger
"""""", """""" 
    _logger_lock.acquire()
    try:
        global _benchmark_logger
        if logging_dir:
            _benchmark_logger = BaseBenchmarkLogger(logging_dir)
        else:
            _benchmark_logger = BenchmarkFileLogger()
    finally:
        _logger_lock.release()
    return _benchmark_logger
""""""]",1
"get_browser, _step = _step, get_browser
def close_window_by_index(index):
    """"""Close window/tab by index.
    Note: ""The order in which the window handles are returned is arbitrary.""

    Parameters:
    index : value
    """"""","["""""" 
    with _step(f'Close window by index {index}'):
        get_browser().close_window_by_index(index)
"""""", """""" 
    with get_browser(f'Close window by index {index}'):
        _step().close_window_by_index(index)
""""""]",1
"untar, built = built, untar
def download_models(opt, fnames, model_folder, version='v1.0', path='aws', use_model_type=False, flatten_tar=False):
    """"""
    Download models into the ParlAI model zoo from a url.

    :param fnames: list of filenames to download
    :param model_folder: models will be downloaded into models/model_folder/model_type
    :param path: url for downloading models; defaults to downloading from AWS
    :param use_model_type: whether models are categorized by type in AWS
    """"""","["""""" 
    model_type = opt.get('model_type', None)
    if (model_type is not None):
        dpath = os.path.join(opt['datapath'], 'models', model_folder, model_type)
    else:
        dpath = os.path.join(opt['datapath'], 'models', model_folder)
    if (not built(dpath, version)):
        for fname in fnames:
            logging.info(f'building data: {dpath}/{fname}')
        if built(dpath):
            remove_dir(dpath)
        make_dir(dpath)
        for fname in fnames:
            if (path == 'aws'):
                url = 'http://parl.ai/downloads/_models/'
                url += (model_folder + '/')
                if use_model_type:
                    url += (model_type + '/')
                url += fname
            else:
                url = ((path + '/') + fname)
            download(url, dpath, fname)
            if (('.tgz' in fname) or ('.gz' in fname) or ('.zip' in fname)):
                untar(dpath, fname, flatten_tar=flatten_tar)
        mark_done(dpath, version)
"""""", """""" 
    model_type = opt.get('model_type', None)
    if (model_type is not None):
        dpath = os.path.join(opt['datapath'], 'models', model_folder, model_type)
    else:
        dpath = os.path.join(opt['datapath'], 'models', model_folder)
    if (not untar(dpath, version)):
        for fname in fnames:
            logging.info(f'building data: {dpath}/{fname}')
        if untar(dpath):
            remove_dir(dpath)
        make_dir(dpath)
        for fname in fnames:
            if (path == 'aws'):
                url = 'http://parl.ai/downloads/_models/'
                url += (model_folder + '/')
                if use_model_type:
                    url += (model_type + '/')
                url += fname
            else:
                url = ((path + '/') + fname)
            download(url, dpath, fname)
            if (('.tgz' in fname) or ('.gz' in fname) or ('.zip' in fname)):
                built(dpath, fname, flatten_tar=flatten_tar)
        mark_done(dpath, version)
""""""]",1
"partial, _create_mnv3 = _create_mnv3, partial
def _gen_lcnet(variant, channel_multiplier=1.0, pretrained=False, **kwargs):
    """""" LCNet
    Essentially a MobileNet-V3 crossed with a MobileNet-V1

    Paper: `PP-LCNet: A Lightweight CPU Convolutional Neural Network` - https://arxiv.org/abs/2109.15099

    Args:
      channel_multiplier: multiplier to number of channels per layer.
    """"""","["""""" 
    arch_def = [['dsa_r1_k3_s1_c32'], ['dsa_r2_k3_s2_c64'], ['dsa_r2_k3_s2_c128'], ['dsa_r1_k3_s2_c256', 'dsa_r1_k5_s1_c256'], ['dsa_r4_k5_s1_c256'], ['dsa_r2_k5_s2_c512_se0.25']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def), stem_size=16, round_chs_fn=partial(round_channels, multiplier=channel_multiplier), norm_layer=partial(nn.BatchNorm2d, **resolve_bn_args(kwargs)), act_layer=resolve_act_layer(kwargs, 'hard_swish'), se_layer=partial(SqueezeExcite, gate_layer='hard_sigmoid', force_act_layer=nn.ReLU), num_features=1280, **kwargs)
    model = _create_mnv3(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['dsa_r1_k3_s1_c32'], ['dsa_r2_k3_s2_c64'], ['dsa_r2_k3_s2_c128'], ['dsa_r1_k3_s2_c256', 'dsa_r1_k5_s1_c256'], ['dsa_r4_k5_s1_c256'], ['dsa_r2_k5_s2_c512_se0.25']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def), stem_size=16, round_chs_fn=_create_mnv3(round_channels, multiplier=channel_multiplier), norm_layer=_create_mnv3(nn.BatchNorm2d, **resolve_bn_args(kwargs)), act_layer=resolve_act_layer(kwargs, 'hard_swish'), se_layer=_create_mnv3(SqueezeExcite, gate_layer='hard_sigmoid', force_act_layer=nn.ReLU), num_features=1280, **kwargs)
    model = partial(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"calcH_D_A, calc_H_D = calc_H_D, calcH_D_A
def calcBestFeature(trainDataList, trainLabelList):
    """"""
    计算信息增益最大的特征
    :param trainDataList: 当前数据集
    :param trainLabelList: 当前标签集
    :return: 信息增益最大的特征及最大信息增益值
    """"""","["""""" 
    trainDataArr = np.array(trainDataList)
    trainLabelArr = np.array(trainLabelList)
    featureNum = trainDataArr.shape[1]
    maxG_D_A = (- 1)
    maxFeature = (- 1)
    H_D = calc_H_D(trainLabelArr)
    for feature in range(featureNum):
        trainDataArr_DevideByFeature = np.array(trainDataArr[:, feature].flat)
        G_D_A = (H_D - calcH_D_A(trainDataArr_DevideByFeature, trainLabelArr))
        if (G_D_A > maxG_D_A):
            maxG_D_A = G_D_A
            maxFeature = feature
    return (maxFeature, maxG_D_A)
"""""", """""" 
    trainDataArr = np.array(trainDataList)
    trainLabelArr = np.array(trainLabelList)
    featureNum = trainDataArr.shape[1]
    maxG_D_A = (- 1)
    maxFeature = (- 1)
    H_D = calcH_D_A(trainLabelArr)
    for feature in range(featureNum):
        trainDataArr_DevideByFeature = np.array(trainDataArr[:, feature].flat)
        G_D_A = (H_D - calc_H_D(trainDataArr_DevideByFeature, trainLabelArr))
        if (G_D_A > maxG_D_A):
            maxG_D_A = G_D_A
            maxFeature = feature
    return (maxFeature, maxG_D_A)
""""""]",1
"cmap, _color_palette = _color_palette, cmap
def _build_discrete_cmap(cmap, levels, extend, filled):
    """"""
    Build a discrete colormap and normalization of the data.
    """"""","["""""" 
    import matplotlib as mpl
    if (len(levels) == 1):
        levels = [levels[0], levels[0]]
    if (not filled):
        extend = 'max'
    if (extend == 'both'):
        ext_n = 2
    elif (extend in ['min', 'max']):
        ext_n = 1
    else:
        ext_n = 0
    n_colors = ((len(levels) + ext_n) - 1)
    pal = _color_palette(cmap, n_colors)
    (new_cmap, cnorm) = mpl.colors.from_levels_and_colors(levels, pal, extend=extend)
    new_cmap.name = getattr(cmap, 'name', cmap)
    try:
        bad = cmap(np.ma.masked_invalid([np.nan]))[0]
    except TypeError:
        pass
    else:
        under = cmap((- np.inf))
        over = cmap(np.inf)
        new_cmap.set_bad(bad)
        if (under != cmap(0)):
            new_cmap.set_under(under)
        if (over != cmap((cmap.N - 1))):
            new_cmap.set_over(over)
    return (new_cmap, cnorm)
"""""", """""" 
    import matplotlib as mpl
    if (len(levels) == 1):
        levels = [levels[0], levels[0]]
    if (not filled):
        extend = 'max'
    if (extend == 'both'):
        ext_n = 2
    elif (extend in ['min', 'max']):
        ext_n = 1
    else:
        ext_n = 0
    n_colors = ((len(levels) + ext_n) - 1)
    pal = cmap(_color_palette, n_colors)
    (new_cmap, cnorm) = mpl.colors.from_levels_and_colors(levels, pal, extend=extend)
    new_cmap.name = getattr(_color_palette, 'name', _color_palette)
    try:
        bad = _color_palette(np.ma.masked_invalid([np.nan]))[0]
    except TypeError:
        pass
    else:
        under = _color_palette((- np.inf))
        over = _color_palette(np.inf)
        new_cmap.set_bad(bad)
        if (under != _color_palette(0)):
            new_cmap.set_under(under)
        if (over != _color_palette((_color_palette.N - 1))):
            new_cmap.set_over(over)
    return (new_cmap, cnorm)
""""""]",1
"lru_cache, read_binary = read_binary, lru_cache
@lru_cache(None)
def _load_static_files():
    """"""Lazily load the resource files into memory the first time they are needed""""""","["""""" 
    return [read_binary(package, resource).decode('utf-8') for (package, resource) in STATIC_FILES]
"""""", """""" 
    return [lru_cache(package, resource).decode('utf-8') for (package, resource) in STATIC_FILES]
""""""]",1
"create_members, sleep = sleep, create_members
def configure_guardduty(session: boto3.Session, delegated_account_id: str, auto_enable_s3_logs: bool, region_list: list, finding_publishing_frequency: str, kms_key_arn: str, publishing_destination_arn: str) -> None:
    """"""Configure GuardDuty with provided parameters.

    Args:
        session: boto3 session
        delegated_account_id: Delegated Admin Account ID
        auto_enable_s3_logs: Auto Enable S3 Logs
        region_list: AWS Regions
        finding_publishing_frequency: Finding publishing frequency
        kms_key_arn: KMS Key ARN
        publishing_destination_arn: Publishing Destination ARN (S3 Bucket)
    """"""","["""""" 
    accounts = common.get_all_organization_accounts([delegated_account_id])
    account_ids = common.get_account_ids(accounts)
    for region in region_list:
        regional_guardduty: GuardDutyClient = session.client('guardduty', region_name=region, config=BOTO3_CONFIG)
        detectors = regional_guardduty.list_detectors()
        if detectors['DetectorIds']:
            detector_id = detectors['DetectorIds'][0]
            LOGGER.info(f'DetectorID: {detector_id} Region: {region}')
            destinations = regional_guardduty.list_publishing_destinations(DetectorId=detector_id)
            if (('Destinations' in destinations) and (len(destinations['Destinations']) == 1)):
                destination_id = destinations['Destinations'][0]['DestinationId']
                regional_guardduty.update_publishing_destination(DetectorId=detector_id, DestinationId=destination_id, DestinationProperties={'DestinationArn': publishing_destination_arn, 'KmsKeyArn': kms_key_arn})
            else:
                regional_guardduty.create_publishing_destination(DetectorId=detector_id, DestinationType='S3', DestinationProperties={'DestinationArn': publishing_destination_arn, 'KmsKeyArn': kms_key_arn})
            LOGGER.info(f'Members created for existing accounts: {accounts} in {region}')
            create_members(regional_guardduty, detector_id, accounts)
            LOGGER.info(f'Waiting {SLEEP_SECONDS} seconds before updating the configuration.')
            sleep(SLEEP_SECONDS)
            update_guardduty_configuration(regional_guardduty, auto_enable_s3_logs, detector_id, finding_publishing_frequency, account_ids)
"""""", """""" 
    accounts = common.get_all_organization_accounts([delegated_account_id])
    account_ids = common.get_account_ids(accounts)
    for region in region_list:
        regional_guardduty: GuardDutyClient = session.client('guardduty', region_name=region, config=BOTO3_CONFIG)
        detectors = regional_guardduty.list_detectors()
        if detectors['DetectorIds']:
            detector_id = detectors['DetectorIds'][0]
            LOGGER.info(f'DetectorID: {detector_id} Region: {region}')
            destinations = regional_guardduty.list_publishing_destinations(DetectorId=detector_id)
            if (('Destinations' in destinations) and (len(destinations['Destinations']) == 1)):
                destination_id = destinations['Destinations'][0]['DestinationId']
                regional_guardduty.update_publishing_destination(DetectorId=detector_id, DestinationId=destination_id, DestinationProperties={'DestinationArn': publishing_destination_arn, 'KmsKeyArn': kms_key_arn})
            else:
                regional_guardduty.create_publishing_destination(DetectorId=detector_id, DestinationType='S3', DestinationProperties={'DestinationArn': publishing_destination_arn, 'KmsKeyArn': kms_key_arn})
            LOGGER.info(f'Members created for existing accounts: {accounts} in {region}')
            sleep(regional_guardduty, detector_id, accounts)
            LOGGER.info(f'Waiting {SLEEP_SECONDS} seconds before updating the configuration.')
            create_members(SLEEP_SECONDS)
            update_guardduty_configuration(regional_guardduty, auto_enable_s3_logs, detector_id, finding_publishing_frequency, account_ids)
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_alert_text_is_not(text):
    """"""Verify alert text is not `text`
    This will fail if there is no alert present.

    Parameters:
    text : value
    """"""","["""""" 
    with _verify_step(f""Verify alert text is not '{text}'"") as s:
        alert_text = get_browser().switch_to.alert.text
        s.error = f""Expected alert text not to be '{text}'""
        s.condition = (alert_text != text)
"""""", """""" 
    with get_browser(f""Verify alert text is not '{text}'"") as s:
        alert_text = _verify_step().switch_to.alert.text
        s.error = f""Expected alert text not to be '{text}'""
        s.condition = (alert_text != text)
""""""]",1
"HoverTool, _initialize_rangetool = _initialize_rangetool, HoverTool
def _base_lineplot(linetype, p, source, data_cols, colormap, hovertool, xlabelname, x_axis_type, plot_data_points, plot_data_points_size, hovertool_string, number_format, rangetool, **kwargs):
    """"""Adds lineplot to figure p for each data_col.""""""","["""""" 
    p_rangetool = None
    linetype = getattr(p, linetype.lower())
    marker = kwargs.pop('marker', 'circle')
    if rangetool:
        p_rangetool = _initialize_rangetool(p, x_axis_type, source)
    for (name, color) in zip(data_cols, colormap):
        glyph = linetype(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, **kwargs)
        if plot_data_points:
            p.scatter(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, marker=marker, size=plot_data_points_size)
        if hovertool:
            my_hover = HoverTool(mode='vline', renderers=[glyph])
            if (hovertool_string is None):
                if (x_axis_type == 'datetime'):
                    my_hover.tooltips = [(xlabelname, '@__x__values_original{%F}'), (name, ('@{%s}%s' % (name, number_format)))]
                    my_hover.formatters = {'@__x__values_original': 'datetime'}
                else:
                    my_hover.tooltips = [(xlabelname, '@__x__values_original'), (name, ('@{%s}%s' % (name, number_format)))]
            else:
                my_hover.tooltips = hovertool_string
            p.add_tools(my_hover)
        if rangetool:
            p_rangetool.line('__x__values', name, source=source, color=color)
    return (p, p_rangetool)
"""""", """""" 
    p_rangetool = None
    linetype = getattr(p, linetype.lower())
    marker = kwargs.pop('marker', 'circle')
    if rangetool:
        p_rangetool = HoverTool(p, x_axis_type, source)
    for (name, color) in zip(data_cols, colormap):
        glyph = linetype(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, **kwargs)
        if plot_data_points:
            p.scatter(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, marker=marker, size=plot_data_points_size)
        if hovertool:
            my_hover = _initialize_rangetool(mode='vline', renderers=[glyph])
            if (hovertool_string is None):
                if (x_axis_type == 'datetime'):
                    my_hover.tooltips = [(xlabelname, '@__x__values_original{%F}'), (name, ('@{%s}%s' % (name, number_format)))]
                    my_hover.formatters = {'@__x__values_original': 'datetime'}
                else:
                    my_hover.tooltips = [(xlabelname, '@__x__values_original'), (name, ('@{%s}%s' % (name, number_format)))]
            else:
                my_hover.tooltips = hovertool_string
            p.add_tools(my_hover)
        if rangetool:
            p_rangetool.line('__x__values', name, source=source, color=color)
    return (p, p_rangetool)
""""""]",1
"lint, Project = Project, lint
def test_lint_git(git_repository):
    """"""Extremely simple test for lint with a git repository.""""""","["""""" 
    project = Project(git_repository)
    report = ProjectReport.generate(project)
    result = lint(report)
    assert result
"""""", """""" 
    project = lint(git_repository)
    report = ProjectReport.generate(project)
    result = Project(report)
    assert result
""""""]",1
"nonzero, simMeas = simMeas, nonzero
def standEst(dataMat, user, simMeas, item):
    """"""standEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)
    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in range(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = nonzero(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (len(overLap) == 0):
            similarity = 0
        else:
            similarity = simMeas(dataMat[(overLap, item)], dataMat[(overLap, j)])
        print(('the %d and %d similarity is : %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in range(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = simMeas(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (len(overLap) == 0):
            similarity = 0
        else:
            similarity = nonzero(dataMat[(overLap, item)], dataMat[(overLap, j)])
        print(('the %d and %d similarity is : %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"_step, get_browser = get_browser, _step
def dismiss_alert(ignore_not_present=False):
    """"""Dismiss an alert.
     Use ignore_not_present to ignore error when alert is not present.

    Parameters:
    ignore_not_present (False) : value""""""","["""""" 
    with _step('Dismiss alert'):
        get_browser().dismiss_alert(ignore_not_present)
"""""", """""" 
    with get_browser('Dismiss alert'):
        _step().dismiss_alert(ignore_not_present)
""""""]",1
"_pud_tagger, _find_tag = _find_tag, _pud_tagger
def tag(words: List[str], corpus: str='pud') -> List[Tuple[(str, str)]]:
    """"""
    :param list words: a list of tokenized words
    :param str corpus: corpus name (orchid or pud)
    :return: a list of tuples (word, POS tag)
    :rtype: list[tuple[str, str]]
    """"""","["""""" 
    if (not words):
        return []
    to_ud = False
    if (corpus[(- 3):] == '_ud'):
        to_ud = True
    word_tags = []
    if ((corpus == 'orchid') or (corpus == 'orchid_ud')):
        words = orchid.pre_process(words)
        word_tags = _find_tag(words, _orchid_tagger())
        word_tags = orchid.post_process(word_tags, to_ud)
    elif ((corpus == 'blackboard') or (corpus == 'blackboard_ud')):
        words = blackboard.pre_process(words)
        word_tags = _find_tag(words, _blackboard_tagger())
        word_tags = blackboard.post_process(word_tags, to_ud)
    else:
        word_tags = _find_tag(words, _pud_tagger())
    return word_tags
"""""", """""" 
    if (not words):
        return []
    to_ud = False
    if (corpus[(- 3):] == '_ud'):
        to_ud = True
    word_tags = []
    if ((corpus == 'orchid') or (corpus == 'orchid_ud')):
        words = orchid.pre_process(words)
        word_tags = _pud_tagger(words, _orchid_tagger())
        word_tags = orchid.post_process(word_tags, to_ud)
    elif ((corpus == 'blackboard') or (corpus == 'blackboard_ud')):
        words = blackboard.pre_process(words)
        word_tags = _pud_tagger(words, _blackboard_tagger())
        word_tags = blackboard.post_process(word_tags, to_ud)
    else:
        word_tags = _pud_tagger(words, _find_tag())
    return word_tags
""""""]",1
"get_targets, bootstrap_stage = bootstrap_stage, get_targets
def target_cmake_defines(args, stage):
    """"""
    Generate target cmake define, which change depending on what
    stage we are at
    :param args: The args variable generated by parse_parameters
    :param stage: What stage we are at
    :return: A set of defines
    """"""","["""""" 
    defines = {}
    if bootstrap_stage(args, stage):
        targets = 'host'
    else:
        targets = get_targets(args)
    defines['LLVM_TARGETS_TO_BUILD'] = targets
    return defines
"""""", """""" 
    defines = {}
    if get_targets(args, stage):
        targets = 'host'
    else:
        targets = bootstrap_stage(args)
    defines['LLVM_TARGETS_TO_BUILD'] = targets
    return defines
""""""]",1
"get_args, prompt = prompt, get_args
def get_novel_url():
    """"""Return a novel page url""""""","["""""" 
    args = get_args()
    url = args.novel_page
    if url:
        if re.match('^https?://.+\\..+$', url):
            return url
        else:
            raise LNException('Invalid URL of novel page')
    try:
        answer = prompt([{'type': 'input', 'name': 'novel', 'message': 'Enter novel page url:', 'validate': (lambda x: (True if re.match('^https?://.+\\..+$', x) else 'Invalid URL of novel page'))}])
        return answer['novel'].strip()
    except Exception:
        raise LNException('Novel page url or query was not given')
"""""", """""" 
    args = prompt()
    url = args.novel_page
    if url:
        if re.match('^https?://.+\\..+$', url):
            return url
        else:
            raise LNException('Invalid URL of novel page')
    try:
        answer = get_args([{'type': 'input', 'name': 'novel', 'message': 'Enter novel page url:', 'validate': (lambda x: (True if re.match('^https?://.+\\..+$', x) else 'Invalid URL of novel page'))}])
        return answer['novel'].strip()
    except Exception:
        raise LNException('Novel page url or query was not given')
""""""]",1
"cmdLineParse, runKml = runKml, cmdLineParse
def main(iargs=None):
    """"""
    Main driver.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    runKml(inps)
"""""", """""" 
    inps = runKml(iargs)
    cmdLineParse(inps)
""""""]",1
"count_file, count_file_like = count_file_like, count_file
def main():
    """"""
    print each input filename and the number of lines in it,
    and print the sum of the number of lines
    """"""","["""""" 
    filenames = sys.argv[1:]
    sum_nlines = 0
    if (len(filenames) == 0):
        sum_nlines = count_file_like(sys.stdin)
        print(('stdin: %d' % sum_nlines))
    else:
        for filename in filenames:
            nlines = count_file(filename)
            print(('%s %d' % (filename, nlines)))
            sum_nlines += nlines
        print(('total: %d' % sum_nlines))
"""""", """""" 
    filenames = sys.argv[1:]
    sum_nlines = 0
    if (len(filenames) == 0):
        sum_nlines = count_file(sys.stdin)
        print(('stdin: %d' % sum_nlines))
    else:
        for filename in filenames:
            nlines = count_file_like(filename)
            print(('%s %d' % (filename, nlines)))
            sum_nlines += nlines
        print(('total: %d' % sum_nlines))
""""""]",1
"hash_file_signature, _show_md5_warning = _show_md5_warning, hash_file_signature
def MD5filesignature(fname, chunksize=65536):
    """"""Deprecated. Use :func:`hash_file_signature` instead.""""""","["""""" 
    _show_md5_warning('MD5filesignature')
    return hash_file_signature(fname, chunksize)
"""""", """""" 
    hash_file_signature('MD5filesignature')
    return _show_md5_warning(fname, chunksize)
""""""]",1
"_compile, _render_phase_contents = _render_phase_contents, _compile
def render_phase(game: pydipcc.Game, phase: Optional[Union[(str, int)]]=None, annotations: Optional[conf.misc_cfgs.AnnotatedGame]=None, game_json_path: Optional[str]=None) -> str:
    """"""Render a single phase into html.

    Note: supports both old python diplomacy Game and pydipcc Game, but newer features
    like (logs) are only be supported in pydipcc.

    Parameters:
    game (fairdiplomacy.game.Game or pydipcc.Game): The game to display
    phase (str or int, optional): Name or index of phase in game, like ""W1903A""
        or 3 or -1. If not specified, uses the current phase.
    game_json_path (str, optional): If provided, will be used to fill in game path for generating
        test situations. Defaults to a guess based on URL.

    Returns: str
    """"""","["""""" 
    phase_names = game.get_all_phase_names()
    if (type(phase) == int):
        try:
            phase = str(phase_names[phase])
        except IndexError:
            template = '<div class=""section dip_phase""> Invalid phase: {{phase}} </div>'
            template = _compile(template)
            return template.render(phase=phase)
    if (phase is None):
        phase = phase_names[(- 1)]
    assert isinstance(phase, str)
    phase_id = phase_names.index(phase)
    messages = list(game.get_all_phases()[phase_id].messages.values())
    orders = game.get_all_phases()[phase_id].orders
    image = f'<svg style=""min-height: 700px; min-width:930px"">{map_renderer.render(game, phase)}</svg>'
    logs = None
    if hasattr(game, 'get_logs'):
        game_logs = game.get_logs()
        if (phase in game_logs):
            logs = game_logs[phase]
    rendered_phase_contents = _render_phase_contents(phase, image, messages, orders, logs, annotations, game_json_path)
    template = '\n      <div class=""section dip_phase"" phase=""{{phase_id}}"">\n        {{ rendered_phase_contents|safe }}\n      </div>\n    '
    template = _compile(template)
    return template.render(phase_id=phase_id, rendered_phase_contents=rendered_phase_contents)
"""""", """""" 
    phase_names = game.get_all_phase_names()
    if (type(phase) == int):
        try:
            phase = str(phase_names[phase])
        except IndexError:
            template = '<div class=""section dip_phase""> Invalid phase: {{phase}} </div>'
            template = _render_phase_contents(template)
            return template.render(phase=phase)
    if (phase is None):
        phase = phase_names[(- 1)]
    assert isinstance(phase, str)
    phase_id = phase_names.index(phase)
    messages = list(game.get_all_phases()[phase_id].messages.values())
    orders = game.get_all_phases()[phase_id].orders
    image = f'<svg style=""min-height: 700px; min-width:930px"">{map_renderer.render(game, phase)}</svg>'
    logs = None
    if hasattr(game, 'get_logs'):
        game_logs = game.get_logs()
        if (phase in game_logs):
            logs = game_logs[phase]
    rendered_phase_contents = _compile(phase, image, messages, orders, logs, annotations, game_json_path)
    template = '\n      <div class=""section dip_phase"" phase=""{{phase_id}}"">\n        {{ rendered_phase_contents|safe }}\n      </div>\n    '
    template = _render_phase_contents(template)
    return template.render(phase_id=phase_id, rendered_phase_contents=rendered_phase_contents)
""""""]",1
"assume_role, process_alternate_contacts = process_alternate_contacts, assume_role
def local_testing(aws_account: AccountTypeDef, params: dict) -> None:
    """"""Local Testing.

    Args:
        aws_account: AWS account to update
        params: solution parameters
    """"""","["""""" 
    account_session = assume_role(params['CONFIGURATION_ROLE_NAME'], params['ROLE_SESSION_NAME'], aws_account['Id'])
    account_client: AccountClient = account_session.client('account', config=BOTO3_CONFIG)
    process_alternate_contacts(account_client, aws_account, params)
"""""", """""" 
    account_session = process_alternate_contacts(params['CONFIGURATION_ROLE_NAME'], params['ROLE_SESSION_NAME'], aws_account['Id'])
    account_client: AccountClient = account_session.client('account', config=BOTO3_CONFIG)
    assume_role(account_client, aws_account, params)
""""""]",1
"partial, _create_mixer = _create_mixer, partial
@register_model
def resmlp_24_distilled_224(pretrained=False, **kwargs):
    """""" ResMLP-24
    Paper: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
    """"""","["""""" 
    model_args = dict(patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=4, block_layer=partial(ResBlock, init_values=1e-05), norm_layer=Affine, **kwargs)
    model = _create_mixer('resmlp_24_distilled_224', pretrained=pretrained, **model_args)
    return model
"""""", """""" 
    model_args = dict(patch_size=16, num_blocks=24, embed_dim=384, mlp_ratio=4, block_layer=_create_mixer(ResBlock, init_values=1e-05), norm_layer=Affine, **kwargs)
    model = partial('resmlp_24_distilled_224', pretrained=pretrained, **model_args)
    return model
""""""]",1
"_screenshot_on_step, _run_wait_hook = _run_wait_hook, _screenshot_on_step
def assert_element_not_enabled(element):
    """"""Assert element is not enabled.

    Parameters:
    element : element
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    _add_step(f'Assert element {element.name} is not enabled')
    _run_wait_hook()
    assert (not element.is_enabled()), f'element {element.name} is enabled'
    _screenshot_on_step()
"""""", """""" 
    element = get_browser().find(element, timeout=0)
    _add_step(f'Assert element {element.name} is not enabled')
    _screenshot_on_step()
    assert (not element.is_enabled()), f'element {element.name} is enabled'
    _run_wait_hook()
""""""]",1
"is_duck_dask_array, import_module = import_module, is_duck_dask_array
def _dask_or_eager_func(name, eager_module=np, dask_module='dask.array'):
    """"""Create a function that dispatches to dask for dask array inputs.""""""","["""""" 

    def f(*args, **kwargs):
        if any((is_duck_dask_array(a) for a in args)):
            mod = (import_module(dask_module) if isinstance(dask_module, str) else dask_module)
            wrapped = getattr(mod, name)
        else:
            wrapped = getattr(eager_module, name)
        return wrapped(*args, **kwargs)
    return f
"""""", """""" 

    def f(*args, **kwargs):
        if any((import_module(a) for a in args)):
            mod = (is_duck_dask_array(dask_module) if isinstance(dask_module, str) else dask_module)
            wrapped = getattr(mod, name)
        else:
            wrapped = getattr(eager_module, name)
        return wrapped(*args, **kwargs)
    return f
""""""]",1
"tqdm, glob = glob, tqdm
def symlink_full_press(out_dir: str):
    """"""
    Find games that contain messages and separate them to a new folder.
    """"""","["""""" 

    def contains_messages(game_path: str) -> bool:
        '\n        Returns True or False depending whether a game contains any messages at all\n        '
        with open(game_path, 'r') as f:
            game = json.load(f)
        for phase in game['phases']:
            if phase['messages']:
                return True
        return False
    full_press_folder = out_dir.replace('all_games', 'full_press_games')
    if (not os.path.exists(full_press_folder)):
        os.makedirs(full_press_folder)
    logging.info(f'Symlinking full press games to folder: {full_press_folder}')
    total = 0
    full_press_games = 0
    for game_json in tqdm(glob(os.path.join(out_dir, 'game_*.json'))):
        total += 1
        original_path = os.path.join(out_dir, game_json)
        full_press = contains_messages(original_path)
        if full_press:
            full_press_games += 1
            syml = game_json.replace('all_games', 'full_press_games')
            if (not os.path.exists(syml)):
                os.symlink(original_path, syml)
    logging.info(f'Found {full_press_games} full press games out of {total} total games')
"""""", """""" 

    def contains_messages(game_path: str) -> bool:
        '\n        Returns True or False depending whether a game contains any messages at all\n        '
        with open(game_path, 'r') as f:
            game = json.load(f)
        for phase in game['phases']:
            if phase['messages']:
                return True
        return False
    full_press_folder = out_dir.replace('all_games', 'full_press_games')
    if (not os.path.exists(full_press_folder)):
        os.makedirs(full_press_folder)
    logging.info(f'Symlinking full press games to folder: {full_press_folder}')
    total = 0
    full_press_games = 0
    for game_json in glob(tqdm(os.path.join(out_dir, 'game_*.json'))):
        total += 1
        original_path = os.path.join(out_dir, game_json)
        full_press = contains_messages(original_path)
        if full_press:
            full_press_games += 1
            syml = game_json.replace('all_games', 'full_press_games')
            if (not os.path.exists(syml)):
                os.symlink(original_path, syml)
    logging.info(f'Found {full_press_games} full press games out of {total} total games')
""""""]",1
"euler2mat, quat2mat = quat2mat, euler2mat
def pose_vec2mat(vec, rotation_mode='euler'):
    """"""
    Convert 6DoF parameters to transformation matrix.

    Args:s
        vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]
    Returns:
        A transformation matrix -- [B, 3, 4]
    """"""","["""""" 
    translation = vec[:, :3].unsqueeze((- 1))
    rot = vec[:, 3:]
    if (rotation_mode == 'euler'):
        rot_mat = euler2mat(rot)
    elif (rotation_mode == 'quat'):
        rot_mat = quat2mat(rot)
    transform_mat = torch.cat([rot_mat, translation], dim=2)
    return transform_mat
"""""", """""" 
    translation = vec[:, :3].unsqueeze((- 1))
    rot = vec[:, 3:]
    if (rotation_mode == 'euler'):
        rot_mat = quat2mat(rot)
    elif (rotation_mode == 'quat'):
        rot_mat = euler2mat(rot)
    transform_mat = torch.cat([rot_mat, translation], dim=2)
    return transform_mat
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_element_attribute(element, attribute, value):
    """"""Verify value of element attribute

    Parameters:
    element : element
    attribute : value
    value : value
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    message = f""Verify element {element.name} attribute {attribute} value is '{value}'""
    with _verify_step(message) as s:
        actual_value = element.get_attribute(attribute)
        s.error = f""expected element {element.name} attribute {attribute} to be '{value}' but was '{actual_value}'""
        s.condition = (actual_value == value)
"""""", """""" 
    element = _verify_step().find(element, timeout=0)
    message = f""Verify element {element.name} attribute {attribute} value is '{value}'""
    with get_browser(message) as s:
        actual_value = element.get_attribute(attribute)
        s.error = f""expected element {element.name} attribute {attribute} to be '{value}' but was '{actual_value}'""
        s.condition = (actual_value == value)
""""""]",1
"_is_target_name_in_binop_side, safe_infer = safe_infer, _is_target_name_in_binop_side
def is_augmented_assign(node: nodes.Assign) -> tuple[(bool, str)]:
    """"""Determine if the node is assigning itself (with modifications) to itself.

    For example: x = 1 + x
    """"""","["""""" 
    if (not isinstance(node.value, nodes.BinOp)):
        return (False, '')
    binop = node.value
    target = node.targets[0]
    if (not isinstance(target, (nodes.AssignName, nodes.AssignAttr))):
        return (False, '')
    if (isinstance(binop.left, nodes.Const) and isinstance(binop.left.value, (str, bytes))):
        return (False, '')
    if (isinstance(binop.left, nodes.Call) or isinstance(binop.right, nodes.Call)):
        return (False, '')
    if _is_target_name_in_binop_side(target, binop.left):
        return (True, binop.op)
    if _is_target_name_in_binop_side(target, binop.right):
        inferred_left = safe_infer(binop.left)
        if (isinstance(inferred_left, nodes.Const) and isinstance(inferred_left.value, int)):
            return (True, binop.op)
        return (False, '')
    return (False, '')
"""""", """""" 
    if (not isinstance(node.value, nodes.BinOp)):
        return (False, '')
    binop = node.value
    target = node.targets[0]
    if (not isinstance(target, (nodes.AssignName, nodes.AssignAttr))):
        return (False, '')
    if (isinstance(binop.left, nodes.Const) and isinstance(binop.left.value, (str, bytes))):
        return (False, '')
    if (isinstance(binop.left, nodes.Call) or isinstance(binop.right, nodes.Call)):
        return (False, '')
    if safe_infer(target, binop.left):
        return (True, binop.op)
    if safe_infer(target, binop.right):
        inferred_left = _is_target_name_in_binop_side(binop.left)
        if (isinstance(inferred_left, nodes.Const) and isinstance(inferred_left.value, int)):
            return (True, binop.op)
        return (False, '')
    return (False, '')
""""""]",1
"urlunparse, urlparse = urlparse, urlunparse
def normalize_url(node):
    """"""Normalizes the given node url""""""","["""""" 
    if (not node):
        node = DEFAULT_NODE
    elif ('://' not in node):
        node = '//{}'.format(node)
    parts = urlparse(node, scheme='http', allow_fragments=False)
    port = (parts.port if parts.port else _get_default_port(parts.scheme))
    netloc = '{}:{}'.format(parts.hostname, port)
    return urlunparse((parts.scheme, netloc, parts.path, '', '', ''))
"""""", """""" 
    if (not node):
        node = DEFAULT_NODE
    elif ('://' not in node):
        node = '//{}'.format(node)
    parts = urlunparse(node, scheme='http', allow_fragments=False)
    port = (parts.port if parts.port else _get_default_port(parts.scheme))
    netloc = '{}:{}'.format(parts.hostname, port)
    return urlparse((parts.scheme, netloc, parts.path, '', '', ''))
""""""]",1
"getBboxGeo, geocode = geocode, getBboxGeo
def runGeocode(self):
    """"""geocode final products
    """"""","["""""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    demFile = os.path.abspath(self._insar.demGeo)
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    if (self.bbox == None):
        bbox = getBboxGeo(referenceTrack)
    else:
        bbox = self.bbox
    catalog.addItem('geocode bounding box', bbox, 'runGeocode')
    if (self.geocodeList == None):
        geocodeList = [self._insar.unwrappedInterferogram, self._insar.unwrappedMaskedInterferogram, self._insar.multilookCoherence, self._insar.multilookLos]
        if self.doIon:
            geocodeList.append(self._insar.multilookIon)
    else:
        geocodeList = []
        for xxx in self.geocodeList:
            geocodeList += glob.glob(xxx)
    numberRangeLooks = (self._insar.numberRangeLooks1 * self._insar.numberRangeLooks2)
    numberAzimuthLooks = (self._insar.numberAzimuthLooks1 * self._insar.numberAzimuthLooks2)
    for inputFile in geocodeList:
        if (self.geocodeInterpMethod == None):
            img = isceobj.createImage()
            img.load((inputFile + '.xml'))
            if (img.dataType.upper() == 'CFLOAT'):
                interpMethod = 'sinc'
            else:
                interpMethod = 'bilinear'
        else:
            interpMethod = self.geocodeInterpMethod.lower()
        geocode(referenceTrack, demFile, inputFile, bbox, numberRangeLooks, numberAzimuthLooks, interpMethod, 0, 0)
    os.chdir('../')
    catalog.printToLog(logger, 'runGeocode')
    self._insar.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    demFile = os.path.abspath(self._insar.demGeo)
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    if (self.bbox == None):
        bbox = geocode(referenceTrack)
    else:
        bbox = self.bbox
    catalog.addItem('geocode bounding box', bbox, 'runGeocode')
    if (self.geocodeList == None):
        geocodeList = [self._insar.unwrappedInterferogram, self._insar.unwrappedMaskedInterferogram, self._insar.multilookCoherence, self._insar.multilookLos]
        if self.doIon:
            geocodeList.append(self._insar.multilookIon)
    else:
        geocodeList = []
        for xxx in self.geocodeList:
            geocodeList += glob.glob(xxx)
    numberRangeLooks = (self._insar.numberRangeLooks1 * self._insar.numberRangeLooks2)
    numberAzimuthLooks = (self._insar.numberAzimuthLooks1 * self._insar.numberAzimuthLooks2)
    for inputFile in geocodeList:
        if (self.geocodeInterpMethod == None):
            img = isceobj.createImage()
            img.load((inputFile + '.xml'))
            if (img.dataType.upper() == 'CFLOAT'):
                interpMethod = 'sinc'
            else:
                interpMethod = 'bilinear'
        else:
            interpMethod = self.geocodeInterpMethod.lower()
        getBboxGeo(referenceTrack, demFile, inputFile, bbox, numberRangeLooks, numberAzimuthLooks, interpMethod, 0, 0)
    os.chdir('../')
    catalog.printToLog(logger, 'runGeocode')
    self._insar.procDoc.addAllFromCatalog(catalog)
""""""]",1
"_assert_dataset_invariants, _assert_variable_invariants = _assert_variable_invariants, _assert_dataset_invariants
def _assert_internal_invariants(xarray_obj: Union[(DataArray, Dataset, Variable)], check_default_indexes: bool):
    """"""Validate that an xarray object satisfies its own internal invariants.

    This exists for the benefit of xarray's own test suite, but may be useful
    in external projects if they (ill-advisedly) create objects using xarray's
    private APIs.
    """"""","["""""" 
    if isinstance(xarray_obj, Variable):
        _assert_variable_invariants(xarray_obj)
    elif isinstance(xarray_obj, DataArray):
        _assert_dataarray_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    elif isinstance(xarray_obj, Dataset):
        _assert_dataset_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    else:
        raise TypeError('{} is not a supported type for xarray invariant checks'.format(type(xarray_obj)))
"""""", """""" 
    if isinstance(xarray_obj, Variable):
        _assert_dataset_invariants(xarray_obj)
    elif isinstance(xarray_obj, DataArray):
        _assert_dataarray_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    elif isinstance(xarray_obj, Dataset):
        _assert_variable_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    else:
        raise TypeError('{} is not a supported type for xarray invariant checks'.format(type(xarray_obj)))
""""""]",1
"SubprocessOutputError, page_timedout = page_timedout, SubprocessOutputError
def generate_pdf(*, input_file: Path, output_pdf: Path, output_text: Path, languages: list[str], engine_mode: int, tessconfig: list[str], timeout: float, pagesegmode: int, thresholding: int, user_words, user_patterns) -> None:
    """"""Generate a PDF using Tesseract's internal PDF generator.

    We specifically a text-only PDF which is more suitable for combining with
    the input page.
    """"""","["""""" 
    args_tesseract = tess_base_args(languages, engine_mode)
    if (pagesegmode is not None):
        args_tesseract.extend(['--psm', str(pagesegmode)])
    args_tesseract.extend(['-c', 'textonly_pdf=1'])
    if ((thresholding != 0) and has_thresholding()):
        args_tesseract.extend(['-c', f'thresholding_method={thresholding}'])
    if user_words:
        args_tesseract.extend(['--user-words', user_words])
    if user_patterns:
        args_tesseract.extend(['--user-patterns', user_patterns])
    prefix = (output_pdf.parent / Path(output_pdf.stem))
    args_tesseract.extend([fspath(input_file), fspath(prefix), 'pdf', 'txt'])
    args_tesseract.extend(tessconfig)
    try:
        p = run(args_tesseract, stdout=PIPE, stderr=STDOUT, timeout=timeout, check=True)
        stdout = p.stdout
        if prefix.with_suffix('.txt').exists():
            prefix.with_suffix('.txt').replace(output_text)
    except TimeoutExpired:
        page_timedout(timeout)
        use_skip_page(output_pdf, output_text)
    except CalledProcessError as e:
        tesseract_log_output(e.output)
        if ((b'Image too large' in e.output) or (b'Empty page!!' in e.output)):
            use_skip_page(output_pdf, output_text)
            return
        raise SubprocessOutputError() from e
    else:
        tesseract_log_output(stdout)
"""""", """""" 
    args_tesseract = tess_base_args(languages, engine_mode)
    if (pagesegmode is not None):
        args_tesseract.extend(['--psm', str(pagesegmode)])
    args_tesseract.extend(['-c', 'textonly_pdf=1'])
    if ((thresholding != 0) and has_thresholding()):
        args_tesseract.extend(['-c', f'thresholding_method={thresholding}'])
    if user_words:
        args_tesseract.extend(['--user-words', user_words])
    if user_patterns:
        args_tesseract.extend(['--user-patterns', user_patterns])
    prefix = (output_pdf.parent / Path(output_pdf.stem))
    args_tesseract.extend([fspath(input_file), fspath(prefix), 'pdf', 'txt'])
    args_tesseract.extend(tessconfig)
    try:
        p = run(args_tesseract, stdout=PIPE, stderr=STDOUT, timeout=timeout, check=True)
        stdout = p.stdout
        if prefix.with_suffix('.txt').exists():
            prefix.with_suffix('.txt').replace(output_text)
    except TimeoutExpired:
        SubprocessOutputError(timeout)
        use_skip_page(output_pdf, output_text)
    except CalledProcessError as e:
        tesseract_log_output(e.output)
        if ((b'Image too large' in e.output) or (b'Empty page!!' in e.output)):
            use_skip_page(output_pdf, output_text)
            return
        raise page_timedout() from e
    else:
        tesseract_log_output(stdout)
""""""]",1
"move_texts, get_bboxes = get_bboxes, move_texts
def repel_text_from_points(x, y, texts, renderer=None, ax=None, expand=(1.2, 1.2), move=False):
    """"""
    Repel texts from all points specified by x and y while expanding their
    (texts'!) bounding boxes by expandby  (x, y), e.g. (1.2, 1.2)
    would multiply both width and height by 1.2.
    Requires a renderer to get the actual sizes of the text, and to that end
    either one needs to be directly provided, or the axes have to be specified,
    and the renderer is then got from the axes object.
    """"""","["""""" 
    assert (len(x) == len(y))
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = get_renderer(ax.get_figure())
    else:
        r = renderer
    bboxes = get_bboxes(texts, r, expand, ax=ax)
    move_x = np.zeros((len(bboxes), len(x)))
    move_y = np.zeros((len(bboxes), len(x)))
    for (i, bbox) in enumerate(bboxes):
        xy_in = get_points_inside_bbox(x, y, bbox)
        for j in xy_in:
            (xp, yp) = (x[j], y[j])
            (dx, dy) = overlap_bbox_and_point(bbox, xp, yp)
            move_x[(i, j)] = dx
            move_y[(i, j)] = dy
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(np.abs(move_x)), np.sum(np.abs(move_y)))
    if move:
        move_texts(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
"""""", """""" 
    assert (len(x) == len(y))
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = get_renderer(ax.get_figure())
    else:
        r = renderer
    bboxes = move_texts(texts, r, expand, ax=ax)
    move_x = np.zeros((len(bboxes), len(x)))
    move_y = np.zeros((len(bboxes), len(x)))
    for (i, bbox) in enumerate(bboxes):
        xy_in = get_points_inside_bbox(x, y, bbox)
        for j in xy_in:
            (xp, yp) = (x[j], y[j])
            (dx, dy) = overlap_bbox_and_point(bbox, xp, yp)
            move_x[(i, j)] = dx
            move_y[(i, j)] = dy
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(np.abs(move_x)), np.sum(np.abs(move_y)))
    if move:
        get_bboxes(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
""""""]",1
"IP, search_iface = search_iface, IP
@click.command()
@click.argument('ip')
@click.option('-i', 'iface', default=None, help='Interface to use')
@click.option('-t', 'timeout', default=2, help='Timeout for each probe (default: 2 seconds)')
@click.option('--all', 'all_protocols', is_flag=True, default=False, help='Probe all protocols (default: Defined in /etc/protocols)')
@click.option('-v', 'verbose', is_flag=True, default=False, help='Verbose output')
def cmd_protoscan(ip, iface, timeout, all_protocols, verbose):
    """"""
    Send IP packets with different protocol field content to guess what
    layer 4 protocols are available.

    The output shows which protocols doesn't generate a 'protocol-unreachable'
    ICMP response.

    Example:

    
    $ sudo python cmd_ipscan.py 45.77.113.133
    1   icmp
    2   igmp
    4   ipencap
    6   tcp
    17  udp
    41  ipv6
    47  gre
    50  esp
    51  ah
    58  ipv6_icmp
    97  etherip
    112 vrrp
    115 l2tp
    132 sctp
    137 mpls_in_ip
    """"""","["""""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    conf.verb = False
    if iface:
        iface = search_iface(iface)
        if iface:
            conf.iface = iface['name']
        else:
            logging.error('Interface {} not found. Use habu.interfaces to show valid network interfaces'.format(iface))
            return False
    if all_protocols:
        protocols = (0, 255)
    else:
        protocols = {num: name for (name, num) in conf.protocols.__dict__.items() if isinstance(num, int)}
    (ans, unans) = sr((IP(dst=ip, proto=[int(p) for p in protocols.keys()]) / 'SCAPY'), retry=0, timeout=timeout, verbose=verbose)
    allowed_protocols = [pkt['IP'].proto for pkt in unans]
    for proto in sorted(allowed_protocols):
        print('{:<4} {}'.format(proto, protocols[proto]))
"""""", """""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    conf.verb = False
    if iface:
        iface = IP(iface)
        if iface:
            conf.iface = iface['name']
        else:
            logging.error('Interface {} not found. Use habu.interfaces to show valid network interfaces'.format(iface))
            return False
    if all_protocols:
        protocols = (0, 255)
    else:
        protocols = {num: name for (name, num) in conf.protocols.__dict__.items() if isinstance(num, int)}
    (ans, unans) = sr((search_iface(dst=ip, proto=[int(p) for p in protocols.keys()]) / 'SCAPY'), retry=0, timeout=timeout, verbose=verbose)
    allowed_protocols = [pkt['IP'].proto for pkt in unans]
    for proto in sorted(allowed_protocols):
        print('{:<4} {}'.format(proto, protocols[proto]))
""""""]",1
"split, get_split = get_split, split
def build_tree(train, max_depth, min_size, n_features):
    """"""build_tree(创建一个决策树)

    Args:
        train           训练数据集
        max_depth       决策树深度不能太深，不然容易导致过拟合
        min_size        叶子节点的大小
        n_features      选取的特征的个数
    Returns:
        root            返回决策树
    """"""","["""""" 
    root = get_split(train, n_features)
    split(root, max_depth, min_size, n_features, 1)
    return root
"""""", """""" 
    root = split(train, n_features)
    get_split(root, max_depth, min_size, n_features, 1)
    return root
""""""]",1
"get_goal_file_path, goal_name_exists = goal_name_exists, get_goal_file_path
def new_goal():
    """"""
    new goal
    """"""","["""""" 
    goals_dir_check()
    goal_name_not_ok = True
    click.echo(chalk.blue('Input a single-word name of the goal:'))
    while goal_name_not_ok:
        goal_name = input().strip()
        if goal_name.isalnum():
            goal_name_not_ok = False
        else:
            click.echo(chalk.red('Only alphanumeric characters can be used! Please input the goal name:'))
    if goal_name_exists(goal_name):
        click.echo(chalk.red('A goal with this name already exists. Please type ""yoda goals view"" to see a list of existing goals'))
    else:
        click.echo(chalk.blue('Input description of the goal:'))
        text = input().strip()
        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))
        incorrect_date_format = True
        while incorrect_date_format:
            deadline = input().strip()
            try:
                date_str = datetime.datetime.strptime(deadline, '%Y-%m-%d').strftime('%Y-%m-%d')
                if (date_str != deadline):
                    raise ValueError
                incorrect_date_format = False
            except ValueError:
                click.echo(chalk.red('Incorrect data format, should be YYYY-MM-DD. Please repeat:'))
        if os.path.isfile(GOALS_CONFIG_FILE_PATH):
            setup_data = dict(name=goal_name, text=text, deadline=deadline, status=0)
            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)
        else:
            setup_data = dict(entries=[dict(name=goal_name, text=text, deadline=deadline, status=0)])
            input_data(setup_data, GOALS_CONFIG_FILE_PATH)
        input_data(dict(entries=[]), get_goal_file_path(goal_name))
"""""", """""" 
    goals_dir_check()
    goal_name_not_ok = True
    click.echo(chalk.blue('Input a single-word name of the goal:'))
    while goal_name_not_ok:
        goal_name = input().strip()
        if goal_name.isalnum():
            goal_name_not_ok = False
        else:
            click.echo(chalk.red('Only alphanumeric characters can be used! Please input the goal name:'))
    if get_goal_file_path(goal_name):
        click.echo(chalk.red('A goal with this name already exists. Please type ""yoda goals view"" to see a list of existing goals'))
    else:
        click.echo(chalk.blue('Input description of the goal:'))
        text = input().strip()
        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))
        incorrect_date_format = True
        while incorrect_date_format:
            deadline = input().strip()
            try:
                date_str = datetime.datetime.strptime(deadline, '%Y-%m-%d').strftime('%Y-%m-%d')
                if (date_str != deadline):
                    raise ValueError
                incorrect_date_format = False
            except ValueError:
                click.echo(chalk.red('Incorrect data format, should be YYYY-MM-DD. Please repeat:'))
        if os.path.isfile(GOALS_CONFIG_FILE_PATH):
            setup_data = dict(name=goal_name, text=text, deadline=deadline, status=0)
            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)
        else:
            setup_data = dict(entries=[dict(name=goal_name, text=text, deadline=deadline, status=0)])
            input_data(setup_data, GOALS_CONFIG_FILE_PATH)
        input_data(dict(entries=[]), goal_name_exists(goal_name))
""""""]",1
"length, normalize = normalize, length
def getAfinityCenter(width, height, point, center, radius=7, img_affinity=None):
    """"""
    Function to create the affinity maps, 
    e.g., vector maps pointing toward the object center. 

    Args:
        width: image wight
        height: image height
        point: (x,y) 
        center: (x,y)
        radius: pixel radius
        img_affinity: tensor to add to 
    return: 
        return a tensor
    """"""","["""""" 
    tensor = torch.zeros(2, height, width).float()
    imgAffinity = Image.new('RGB', (width, height), 'black')
    totensor = transforms.Compose([transforms.ToTensor()])
    draw = ImageDraw.Draw(imgAffinity)
    r1 = radius
    p = point
    draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), (255, 255, 255))
    del draw
    array = (np.array(imgAffinity) / 255)[:, :, 0]
    angle_vector = (np.array(center) - np.array(point))
    angle_vector = normalize(angle_vector)
    affinity = np.concatenate([[(array * angle_vector[0])], [(array * angle_vector[1])]])
    if (not (img_affinity is None)):
        if (length(angle_vector) > 0):
            angle = py_ang(angle_vector)
        else:
            angle = 0
        c = (np.array(colorsys.hsv_to_rgb((angle / 360), 1, 1)) * 255)
        draw = ImageDraw.Draw(img_affinity)
        draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), fill=(int(c[0]), int(c[1]), int(c[2])))
        del draw
    re = (torch.from_numpy(affinity).float() + tensor)
    return (re, img_affinity)
"""""", """""" 
    tensor = torch.zeros(2, height, width).float()
    imgAffinity = Image.new('RGB', (width, height), 'black')
    totensor = transforms.Compose([transforms.ToTensor()])
    draw = ImageDraw.Draw(imgAffinity)
    r1 = radius
    p = point
    draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), (255, 255, 255))
    del draw
    array = (np.array(imgAffinity) / 255)[:, :, 0]
    angle_vector = (np.array(center) - np.array(point))
    angle_vector = length(angle_vector)
    affinity = np.concatenate([[(array * angle_vector[0])], [(array * angle_vector[1])]])
    if (not (img_affinity is None)):
        if (normalize(angle_vector) > 0):
            angle = py_ang(angle_vector)
        else:
            angle = 0
        c = (np.array(colorsys.hsv_to_rgb((angle / 360), 1, 1)) * 255)
        draw = ImageDraw.Draw(img_affinity)
        draw.ellipse(((p[0] - r1), (p[1] - r1), (p[0] + r1), (p[1] + r1)), fill=(int(c[0]), int(c[1]), int(c[2])))
        del draw
    re = (torch.from_numpy(affinity).float() + tensor)
    return (re, img_affinity)
""""""]",1
"fast_hist, evaluate_eval = evaluate_eval, fast_hist
def validate(val_loader, net, criterion, optimizer, curr_epoch, writer):
    """"""
    Runs the validation loop after each training epoch
    val_loader: Data loader for validation
    net: thet network
    criterion: loss fn
    optimizer: optimizer
    curr_epoch: current epoch 
    writer: tensorboard writer
    return: 
    """"""","["""""" 
    net.eval()
    val_loss = AverageMeter()
    mf_score = AverageMeter()
    IOU_acc = 0
    dump_images = []
    heatmap_images = []
    for (vi, data) in enumerate(val_loader):
        (input, mask, edge, img_names) = data
        assert ((len(input.size()) == 4) and (len(mask.size()) == 3))
        assert (input.size()[2:] == mask.size()[1:])
        (h, w) = mask.size()[1:]
        batch_pixel_size = ((input.size(0) * input.size(2)) * input.size(3))
        (input, mask_cuda, edge_cuda) = (input.cuda(), mask.cuda(), edge.cuda())
        with torch.no_grad():
            (seg_out, edge_out) = net(input)
        if args.joint_edgeseg_loss:
            loss_dict = criterion((seg_out, edge_out), (mask_cuda, edge_cuda))
            val_loss.update(sum(loss_dict.values()).item(), batch_pixel_size)
        else:
            val_loss.update(criterion(seg_out, mask_cuda).item(), batch_pixel_size)
        seg_predictions = seg_out.data.max(1)[1].cpu()
        edge_predictions = edge_out.max(1)[0].cpu()
        if ((vi % 20) == 0):
            if (args.local_rank == 0):
                logging.info(('validating: %d / %d' % ((vi + 1), len(val_loader))))
        if ((vi > 10) and args.test_mode):
            break
        _edge = edge.max(1)[0]
        if (vi < 10):
            dump_images.append([mask, seg_predictions, img_names])
            heatmap_images.append([_edge, edge_predictions, img_names])
        IOU_acc += fast_hist(seg_predictions.numpy().flatten(), mask.numpy().flatten(), args.dataset_cls.num_classes)
        del seg_out, edge_out, vi, data
    if (args.local_rank == 0):
        evaluate_eval(args, net, optimizer, val_loss, mf_score, IOU_acc, dump_images, heatmap_images, writer, curr_epoch, args.dataset_cls)
    return val_loss.avg
"""""", """""" 
    net.eval()
    val_loss = AverageMeter()
    mf_score = AverageMeter()
    IOU_acc = 0
    dump_images = []
    heatmap_images = []
    for (vi, data) in enumerate(val_loader):
        (input, mask, edge, img_names) = data
        assert ((len(input.size()) == 4) and (len(mask.size()) == 3))
        assert (input.size()[2:] == mask.size()[1:])
        (h, w) = mask.size()[1:]
        batch_pixel_size = ((input.size(0) * input.size(2)) * input.size(3))
        (input, mask_cuda, edge_cuda) = (input.cuda(), mask.cuda(), edge.cuda())
        with torch.no_grad():
            (seg_out, edge_out) = net(input)
        if args.joint_edgeseg_loss:
            loss_dict = criterion((seg_out, edge_out), (mask_cuda, edge_cuda))
            val_loss.update(sum(loss_dict.values()).item(), batch_pixel_size)
        else:
            val_loss.update(criterion(seg_out, mask_cuda).item(), batch_pixel_size)
        seg_predictions = seg_out.data.max(1)[1].cpu()
        edge_predictions = edge_out.max(1)[0].cpu()
        if ((vi % 20) == 0):
            if (args.local_rank == 0):
                logging.info(('validating: %d / %d' % ((vi + 1), len(val_loader))))
        if ((vi > 10) and args.test_mode):
            break
        _edge = edge.max(1)[0]
        if (vi < 10):
            dump_images.append([mask, seg_predictions, img_names])
            heatmap_images.append([_edge, edge_predictions, img_names])
        IOU_acc += evaluate_eval(seg_predictions.numpy().flatten(), mask.numpy().flatten(), args.dataset_cls.num_classes)
        del seg_out, edge_out, vi, data
    if (args.local_rank == 0):
        fast_hist(args, net, optimizer, val_loss, mf_score, IOU_acc, dump_images, heatmap_images, writer, curr_epoch, args.dataset_cls)
    return val_loss.avg
""""""]",1
"_shift, BeamEditDistance = BeamEditDistance, _shift
def translation_edit_rate(words_hyp: List[str], words_ref: List[str]) -> Tuple[(int, int)]:
    """"""Calculate the translation edit rate.

    :param words_hyp: Tokenized translation hypothesis.
    :param words_ref: Tokenized reference translation.
    :return: tuple (number of edits, length)
    """"""","["""""" 
    n_words_ref = len(words_ref)
    n_words_hyp = len(words_hyp)
    if (n_words_ref == 0):
        trace = (_OP_DEL * n_words_hyp)
        return (n_words_hyp, 0)
    cached_ed = BeamEditDistance(words_ref)
    shifts = 0
    input_words = words_hyp
    checked_candidates = 0
    while True:
        (delta, new_input_words, checked_candidates) = _shift(input_words, words_ref, cached_ed, checked_candidates)
        if (checked_candidates >= _MAX_SHIFT_CANDIDATES):
            break
        if (delta <= 0):
            break
        shifts += 1
        input_words = new_input_words
    (edit_distance, trace) = cached_ed(input_words)
    total_edits = (shifts + edit_distance)
    return (total_edits, n_words_ref)
"""""", """""" 
    n_words_ref = len(words_ref)
    n_words_hyp = len(words_hyp)
    if (n_words_ref == 0):
        trace = (_OP_DEL * n_words_hyp)
        return (n_words_hyp, 0)
    cached_ed = _shift(words_ref)
    shifts = 0
    input_words = words_hyp
    checked_candidates = 0
    while True:
        (delta, new_input_words, checked_candidates) = BeamEditDistance(input_words, words_ref, cached_ed, checked_candidates)
        if (checked_candidates >= _MAX_SHIFT_CANDIDATES):
            break
        if (delta <= 0):
            break
        shifts += 1
        input_words = new_input_words
    (edit_distance, trace) = cached_ed(input_words)
    total_edits = (shifts + edit_distance)
    return (total_edits, n_words_ref)
""""""]",1
"temp_checkout, temp_repo = temp_repo, temp_checkout
@contextlib.contextmanager
def temp_common():
    """"""Initialize a repository in the current path and return a CC. This is
    used to test CC functionality that should work identically regardless of
    local or remote.
    """"""","["""""" 
    with temp_repo() as (repo_path, _):
        with temp_checkout() as (working_path, _):
            cc = svn.common.CommonClient(working_path, svn.constants.LT_PATH)
            (yield (repo_path, working_path, cc))
"""""", """""" 
    with temp_checkout() as (repo_path, _):
        with temp_repo() as (working_path, _):
            cc = svn.common.CommonClient(working_path, svn.constants.LT_PATH)
            (yield (repo_path, working_path, cc))
""""""]",1
"blend, grayscale = grayscale, blend
def contrast_list(var, images):
    """"""
    Perform color contrast on the given list of images.
    Args:
        var (float): variance.
        images (list): list of images to perform color contrast.
    Returns:
        (array): image that performed color contrast.
    """"""","["""""" 
    alpha = (1.0 + np.random.uniform((- var), var))
    out_images = []
    for image in images:
        img_gray = grayscale(image)
        img_gray.fill(np.mean(img_gray[0]))
        out_images.append(blend(image, img_gray, alpha))
    return out_images
"""""", """""" 
    alpha = (1.0 + np.random.uniform((- var), var))
    out_images = []
    for image in images:
        img_gray = blend(image)
        img_gray.fill(np.mean(img_gray[0]))
        out_images.append(grayscale(image, img_gray, alpha))
    return out_images
""""""]",1
"__img_seq_setup, __get_input_type = __get_input_type, __img_seq_setup
def setup_input(args):
    """"""
    Input type can be 
        an image file
        a video file
        a folder with image files
        a folder with bbox (json) files
        ""webcam""
    
    """"""","["""""" 
    image_exts = ('jpg', 'png', 'jpeg', 'bmp')
    video_exts = ('mp4', 'avi', 'mov')
    input_type = __get_input_type(args)
    if (input_type == 'video'):
        cap = cv2.VideoCapture(args.input_path)
        assert cap.isOpened(), f'Failed in opening video: {args.input_path}'
        __video_setup(args)
        return (input_type, cap)
    elif (input_type == 'webcam'):
        cap = cv2.VideoCapture(0)
        return (input_type, cap)
    elif (input_type == 'image_dir'):
        image_list = gnu.get_all_files(args.input_path, image_exts, 'relative')
        image_list = [osp.join(args.input_path, image_name) for image_name in image_list]
        __img_seq_setup(args)
        return (input_type, image_list)
    elif (input_type == 'bbox_dir'):
        __img_seq_setup(args)
        json_files = gnu.get_all_files(args.input_path, '.json', 'relative')
        input_data = list()
        for json_file in json_files:
            json_path = osp.join(args.input_path, json_file)
            (image_path, body_bbox_list, hand_bbox_list) = load_info_from_json(json_path)
            input_data.append(dict(image_path=image_path, hand_bbox_list=hand_bbox_list, body_bbox_list=body_bbox_list))
        return (input_type, input_data)
    else:
        assert False, 'Unknown input type'
"""""", """""" 
    image_exts = ('jpg', 'png', 'jpeg', 'bmp')
    video_exts = ('mp4', 'avi', 'mov')
    input_type = __img_seq_setup(args)
    if (input_type == 'video'):
        cap = cv2.VideoCapture(args.input_path)
        assert cap.isOpened(), f'Failed in opening video: {args.input_path}'
        __video_setup(args)
        return (input_type, cap)
    elif (input_type == 'webcam'):
        cap = cv2.VideoCapture(0)
        return (input_type, cap)
    elif (input_type == 'image_dir'):
        image_list = gnu.get_all_files(args.input_path, image_exts, 'relative')
        image_list = [osp.join(args.input_path, image_name) for image_name in image_list]
        __get_input_type(args)
        return (input_type, image_list)
    elif (input_type == 'bbox_dir'):
        __get_input_type(args)
        json_files = gnu.get_all_files(args.input_path, '.json', 'relative')
        input_data = list()
        for json_file in json_files:
            json_path = osp.join(args.input_path, json_file)
            (image_path, body_bbox_list, hand_bbox_list) = load_info_from_json(json_path)
            input_data.append(dict(image_path=image_path, hand_bbox_list=hand_bbox_list, body_bbox_list=body_bbox_list))
        return (input_type, input_data)
    else:
        assert False, 'Unknown input type'
""""""]",1
"create_task, ParlaiParser = ParlaiParser, create_task
def get_teacher(config) -> Teacher:
    """"""
    Return teacher for use in drawing passages for QA.
    """"""","["""""" 
    parser = ParlaiParser(True, False)
    opt = parser.parse_args(list(chain.from_iterable(((('--' + k), v) for (k, v) in config.teacher.items()))))
    agent = RepeatLabelAgent(opt)
    return create_task(opt, agent).get_task_agent()
"""""", """""" 
    parser = create_task(True, False)
    opt = parser.parse_args(list(chain.from_iterable(((('--' + k), v) for (k, v) in config.teacher.items()))))
    agent = RepeatLabelAgent(opt)
    return ParlaiParser(opt, agent).get_task_agent()
""""""]",1
"_merge_qconfig_dict, get_qconfig_dict = get_qconfig_dict, _merge_qconfig_dict
def get_qconfig_dict_sub_modules(module: torch.nn.Module, sub_module_names: List[str], qconfig):
    """"""
    Get a qconfig_dict that collects all qconfig_dicts from specified sub modules
    """"""","["""""" 
    ret = {}
    for name in sub_module_names:
        sub_module = getattr(module, name)
        sub_qd = get_qconfig_dict(sub_module, qconfig)
        if (sub_qd is not None):
            ret = _merge_qconfig_dict(ret, sub_qd, prefix=name)
    if (not ret):
        ret = None
    return ret
"""""", """""" 
    ret = {}
    for name in sub_module_names:
        sub_module = getattr(module, name)
        sub_qd = _merge_qconfig_dict(sub_module, qconfig)
        if (sub_qd is not None):
            ret = get_qconfig_dict(ret, sub_qd, prefix=name)
    if (not ret):
        ret = None
    return ret
""""""]",1
"StlinkDetectLinuxGeneric, StlinkDetectWindows = StlinkDetectWindows, StlinkDetectLinuxGeneric
def create_mbed_detector(**kwargs):
    """"""Factory used to create host OS specific mbed-lstools object

    :param kwargs: keyword arguments to pass along to the constructors
    @return Returns MbedLsTools object or None if host OS is not supported

    """"""","["""""" 
    host_os = platform.system()
    if (host_os == 'Windows'):
        from .windows import StlinkDetectWindows
        return StlinkDetectWindows(**kwargs)
    elif (host_os == 'Linux'):
        from .linux import StlinkDetectLinuxGeneric
        return StlinkDetectLinuxGeneric(**kwargs)
    elif (host_os == 'Darwin'):
        from .darwin import StlinkDetectDarwin
        return StlinkDetectDarwin(**kwargs)
    else:
        return None
"""""", """""" 
    host_os = platform.system()
    if (host_os == 'Windows'):
        from .windows import StlinkDetectWindows
        return StlinkDetectLinuxGeneric(**kwargs)
    elif (host_os == 'Linux'):
        from .linux import StlinkDetectLinuxGeneric
        return StlinkDetectWindows(**kwargs)
    elif (host_os == 'Darwin'):
        from .darwin import StlinkDetectDarwin
        return StlinkDetectDarwin(**kwargs)
    else:
        return None
""""""]",1
"_msvc_scripterror_policy_lookup, debug = debug, _msvc_scripterror_policy_lookup
def msvc_set_scripterror_policy(MSVC_SCRIPTERROR_POLICY=None):
    """""" Set the default policy when msvc batch file execution errors are detected.

    Args:
        MSVC_SCRIPTERROR_POLICY:
           string representing the policy behavior
           when msvc batch file execution errors are detected or None

    Returns:
        The previous policy is returned when the MSVC_SCRIPTERROR_POLICY argument
        is not None. The active policy is returned when the MSVC_SCRIPTERROR_POLICY
        argument is None.

    """"""","["""""" 
    global _MSVC_SCRIPTERROR_POLICY_DEF
    prev_policy = _MSVC_SCRIPTERROR_POLICY_DEF.symbol
    policy = MSVC_SCRIPTERROR_POLICY
    if (policy is not None):
        _MSVC_SCRIPTERROR_POLICY_DEF = _msvc_scripterror_policy_lookup(policy)
    debug('prev_policy=%s, set_policy=%s, policy.symbol=%s, policy.value=%s', repr(prev_policy), repr(policy), repr(_MSVC_SCRIPTERROR_POLICY_DEF.symbol), repr(_MSVC_SCRIPTERROR_POLICY_DEF.value))
    return prev_policy
"""""", """""" 
    global _MSVC_SCRIPTERROR_POLICY_DEF
    prev_policy = _MSVC_SCRIPTERROR_POLICY_DEF.symbol
    policy = MSVC_SCRIPTERROR_POLICY
    if (policy is not None):
        _MSVC_SCRIPTERROR_POLICY_DEF = debug(policy)
    _msvc_scripterror_policy_lookup('prev_policy=%s, set_policy=%s, policy.symbol=%s, policy.value=%s', repr(prev_policy), repr(policy), repr(_MSVC_SCRIPTERROR_POLICY_DEF.symbol), repr(_MSVC_SCRIPTERROR_POLICY_DEF.value))
    return prev_policy
""""""]",1
"_run_wait_hook, get_browser = get_browser, _run_wait_hook
def assert_element_present(element):
    """"""Assert element is present in the DOM

    Parameters:
    element : element
    """"""","["""""" 
    _add_step('Assert element is present')
    _run_wait_hook()
    assert get_browser().element_is_present(element), f'element {element} is not present'
"""""", """""" 
    _add_step('Assert element is present')
    get_browser()
    assert _run_wait_hook().element_is_present(element), f'element {element} is not present'
""""""]",1
"get_dependencies, get_inputs = get_inputs, get_dependencies
def _install_action_function(_env, node):
    """"""Install files using the install or copy commands""""""","["""""" 
    return {'outputs': get_outputs(node), 'rule': get_rule(node, 'INSTALL'), 'inputs': get_inputs(node), 'implicit': get_dependencies(node)}
"""""", """""" 
    return {'outputs': get_outputs(node), 'rule': get_rule(node, 'INSTALL'), 'inputs': get_dependencies(node), 'implicit': get_inputs(node)}
""""""]",1
"squareform, pdist = pdist, squareform
def recover_closest_one_dataset(feature_matrix_all, image_paths, save_path, n_image_samples=10, n_closest=3):
    """"""
    Provide sample recoveries.

    Args:
        feature_matrix_all: np.ndarray [n_samples x embed_dim], full data embedding of test samples.
        image_paths:        list [n_samples], list of datapaths corresponding to <feature_matrix_all>
        save_path:          str, where to store sample image.
        n_image_samples:    Number of sample recoveries.
        n_closest:          Number of closest recoveries to show.
    Returns:
        Nothing!
    """"""","["""""" 
    image_paths = np.array([x[0] for x in image_paths])
    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)
    closest_feature_idxs = squareform(pdist(feature_matrix_all)).argsort(1)[:, :(n_closest + 1)]
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
"""""", """""" 
    image_paths = np.array([x[0] for x in image_paths])
    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)
    closest_feature_idxs = pdist(squareform(feature_matrix_all)).argsort(1)[:, :(n_closest + 1)]
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
""""""]",1
"ConvertCompletionDataToVimData, ToUnicode = ToUnicode, ConvertCompletionDataToVimData
def _FilterToMatchingCompletions(completed_item, completions):
    """"""Filter to completions matching the item Vim said was completed""""""","["""""" 
    match_keys = ['word', 'abbr', 'menu', 'info']
    matched_completions = []
    for completion in completions:
        item = ConvertCompletionDataToVimData(completion)

        def matcher(key):
            return (ToUnicode(completed_item.get(key, '')) == ToUnicode(item.get(key, '')))
        if all((matcher(i) for i in match_keys)):
            matched_completions.append(completion.get('extra_data', {}))
    return matched_completions
"""""", """""" 
    match_keys = ['word', 'abbr', 'menu', 'info']
    matched_completions = []
    for completion in completions:
        item = ToUnicode(completion)

        def matcher(key):
            return (ConvertCompletionDataToVimData(completed_item.get(key, '')) == ConvertCompletionDataToVimData(item.get(key, '')))
        if all((matcher(i) for i in match_keys)):
            matched_completions.append(completion.get('extra_data', {}))
    return matched_completions
""""""]",1
"model, accuracy = accuracy, model
def evaluate(test_loader, model, criterion, n_iter=(- 1), verbose=False, device='cuda'):
    """"""
    Standard evaluation loop.
    """"""","["""""" 
    n_iter = (len(test_loader) if (n_iter == (- 1)) else n_iter)
    modulo = (0 if verbose else (- 1))
    batch_time = AverageMeter('Time', ':6.3f')
    losses = AverageMeter('Loss', ':.4e')
    top1 = AverageMeter('Acc@1', ':6.2f')
    top5 = AverageMeter('Acc@5', ':6.2f')
    progress = ProgressMeter(len(test_loader), batch_time, losses, top1, top5, prefix='Test: ')
    model.eval()
    with torch.no_grad():
        end = time.time()
        for (i, (input, target)) in enumerate(test_loader):
            if (i >= n_iter):
                break
            input = (input.cuda() if (device == 'cuda') else input)
            target = (target.cuda() if (device == 'cuda') else target)
            output = model(input)
            loss = criterion(output, target)
            (acc1, acc5) = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((i % 10) == modulo):
                progress.print(i)
        return top1.avg
"""""", """""" 
    n_iter = (len(test_loader) if (n_iter == (- 1)) else n_iter)
    modulo = (0 if verbose else (- 1))
    batch_time = AverageMeter('Time', ':6.3f')
    losses = AverageMeter('Loss', ':.4e')
    top1 = AverageMeter('Acc@1', ':6.2f')
    top5 = AverageMeter('Acc@5', ':6.2f')
    progress = ProgressMeter(len(test_loader), batch_time, losses, top1, top5, prefix='Test: ')
    accuracy.eval()
    with torch.no_grad():
        end = time.time()
        for (i, (input, target)) in enumerate(test_loader):
            if (i >= n_iter):
                break
            input = (input.cuda() if (device == 'cuda') else input)
            target = (target.cuda() if (device == 'cuda') else target)
            output = accuracy(input)
            loss = criterion(output, target)
            (acc1, acc5) = model(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((i % 10) == modulo):
                progress.print(i)
        return top1.avg
""""""]",1
"get_box, get_smpl_color = get_smpl_color, get_box
def vis_frame_smpl(frame, im_res, smpl_output, opt, vis_thres):
    """"""
    frame: frame image
    im_res: result dict
    smpl_output: predictions

    return rendered image
    """"""","["""""" 
    from .render_pytorch3d import render_mesh
    img = frame.copy()
    (height, width) = img.shape[:2]
    img_size = (height, width)
    focal = np.array([1000, 1000])
    all_transl = smpl_output['transl'].detach()
    vertices = smpl_output['pred_vertices'].detach()
    smpl_faces = smpl_output['smpl_faces']
    for (n_human, human) in enumerate(im_res['result']):
        kp_preds = human['keypoints']
        kp_scores = human['kp_score']
        score = human['bbox_score']
        if (score < 0.3):
            continue
        bbox = human['crop_box']
        bbox_w = bbox[2]
        bbox = [bbox[0], (bbox[0] + bbox[2]), bbox[1], (bbox[1] + bbox[3])]
        if (opt.pose_track or opt.tracking):
            while isinstance(human['idx'], list):
                human['idx'].sort()
                human['idx'] = human['idx'][0]
            color = get_smpl_color(int(abs(human['idx'])))
        else:
            color = [int((0.65098039 * 255)), int((0.74117647 * 255)), int((0.85882353 * 255))]
        if opt.showbox:
            if ('crop_box' not in human.keys()):
                from trackers.PoseFlow.poseflow_infer import get_box
                keypoints = []
                for n in range(kp_scores.shape[0]):
                    keypoints.append(float(kp_preds[(n, 0)]))
                    keypoints.append(float(kp_preds[(n, 1)]))
                    keypoints.append(float(kp_scores[n]))
                bbox = get_box(keypoints, height, width)
            cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), color, 2)
            if opt.tracking:
                cv2.putText(img, str(human['idx']), (int(bbox[0]), int((bbox[2] + 26))), DEFAULT_FONT, 1, BLACK, 2)
        transl = all_transl[[n_human]]
        vert = vertices[[n_human]]
        img = vis_smpl_3d(vert, transl, img, bbox_w, smpl_faces, render_mesh, color=color)
    return img
"""""", """""" 
    from .render_pytorch3d import render_mesh
    img = frame.copy()
    (height, width) = img.shape[:2]
    img_size = (height, width)
    focal = np.array([1000, 1000])
    all_transl = smpl_output['transl'].detach()
    vertices = smpl_output['pred_vertices'].detach()
    smpl_faces = smpl_output['smpl_faces']
    for (n_human, human) in enumerate(im_res['result']):
        kp_preds = human['keypoints']
        kp_scores = human['kp_score']
        score = human['bbox_score']
        if (score < 0.3):
            continue
        bbox = human['crop_box']
        bbox_w = bbox[2]
        bbox = [bbox[0], (bbox[0] + bbox[2]), bbox[1], (bbox[1] + bbox[3])]
        if (opt.pose_track or opt.tracking):
            while isinstance(human['idx'], list):
                human['idx'].sort()
                human['idx'] = human['idx'][0]
            color = get_box(int(abs(human['idx'])))
        else:
            color = [int((0.65098039 * 255)), int((0.74117647 * 255)), int((0.85882353 * 255))]
        if opt.showbox:
            if ('crop_box' not in human.keys()):
                from trackers.PoseFlow.poseflow_infer import get_box
                keypoints = []
                for n in range(kp_scores.shape[0]):
                    keypoints.append(float(kp_preds[(n, 0)]))
                    keypoints.append(float(kp_preds[(n, 1)]))
                    keypoints.append(float(kp_scores[n]))
                bbox = get_smpl_color(keypoints, height, width)
            cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), color, 2)
            if opt.tracking:
                cv2.putText(img, str(human['idx']), (int(bbox[0]), int((bbox[2] + 26))), DEFAULT_FONT, 1, BLACK, 2)
        transl = all_transl[[n_human]]
        vert = vertices[[n_human]]
        img = vis_smpl_3d(vert, transl, img, bbox_w, smpl_faces, render_mesh, color=color)
    return img
""""""]",1
"CurrentLineContents, CurrentColumn = CurrentColumn, CurrentLineContents
def CurrentLineContentsAndCodepointColumn():
    """"""Returns the line contents as a unicode string and the 0-based current
  column as a codepoint offset. If the current column is outside the line,
  returns the column position at the end of the line.""""""","["""""" 
    line = CurrentLineContents()
    byte_column = CurrentColumn()
    column = (ByteOffsetToCodepointOffset(line, (byte_column + 1)) - 1)
    return (line, column)
"""""", """""" 
    line = CurrentColumn()
    byte_column = CurrentLineContents()
    column = (ByteOffsetToCodepointOffset(line, (byte_column + 1)) - 1)
    return (line, column)
""""""]",1
"get_browser, _step = _step, get_browser
def select_option_by_text(element, text):
    """"""Select an option from a select dropdown by text.

    Parameters:
    element : element
    text : value
    """"""","["""""" 
    element = get_browser().find(element)
    with _step(f""Select option '{text}' from element {element.name}""):
        element.select.select_by_visible_text(text)
"""""", """""" 
    element = _step().find(element)
    with get_browser(f""Select option '{text}' from element {element.name}""):
        element.select.select_by_visible_text(text)
""""""]",1
"conv2d_fixed_padding, projection_shortcut = projection_shortcut, conv2d_fixed_padding
def _building_block_v2(inputs, filters, training, projection_shortcut, strides, data_format):
    """"""A single block for ResNet v2, without a bottleneck.

  Batch normalization then ReLu then convolution as described by:
    Identity Mappings in Deep Residual Networks
    https://arxiv.org/pdf/1603.05027.pdf
    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Jul 2016.

  Args:
    inputs: A tensor of size [batch, channels, height_in, width_in] or
      [batch, height_in, width_in, channels] depending on data_format.
    filters: The number of filters for the convolutions.
    training: A Boolean for whether the model is in training or inference
      mode. Needed for batch normalization.
    projection_shortcut: The function to use for projection shortcuts
      (typically a 1x1 convolution when downsampling the input).
    strides: The block's stride. If greater than 1, this block will ultimately
      downsample the input.
    data_format: The input format ('channels_last' or 'channels_first').

  Returns:
    The output tensor of the block; shape should match inputs.
  """"""","["""""" 
    shortcut = inputs
    inputs = batch_norm(inputs, training, data_format)
    inputs = tf.nn.relu(inputs)
    if (projection_shortcut is not None):
        shortcut = projection_shortcut(inputs)
    inputs = conv2d_fixed_padding(inputs=inputs, filters=filters, kernel_size=3, strides=strides, data_format=data_format)
    inputs = batch_norm(inputs, training, data_format)
    inputs = tf.nn.relu(inputs)
    inputs = conv2d_fixed_padding(inputs=inputs, filters=filters, kernel_size=3, strides=1, data_format=data_format)
    return (inputs + shortcut)
"""""", """""" 
    shortcut = inputs
    inputs = batch_norm(inputs, training, data_format)
    inputs = tf.nn.relu(inputs)
    if (conv2d_fixed_padding is not None):
        shortcut = conv2d_fixed_padding(inputs)
    inputs = projection_shortcut(inputs=inputs, filters=filters, kernel_size=3, strides=strides, data_format=data_format)
    inputs = batch_norm(inputs, training, data_format)
    inputs = tf.nn.relu(inputs)
    inputs = projection_shortcut(inputs=inputs, filters=filters, kernel_size=3, strides=1, data_format=data_format)
    return (inputs + shortcut)
""""""]",1
"_parse_installed_version, _is_version_valid = _is_version_valid, _parse_installed_version
def check_dependency(dependency_name: str, dependency_spec: DependencySpec, namespace: str=None, saved_versions: Dict[(str, LooseVersion)]=None) -> Tuple[(bool, LooseVersion)]:
    """"""
    Check if dependency defined by given DependencySpec is valid
    :param dependency_name: name of dependency to check
    :param dependency_spec: specification of dependency to check
    :param namespace: k8s namespace where server component of checked dependency is located
    :param saved_versions: dict containing saved versions from previous dependency check. If provided, saved version
    will be used instead of running command to check version of dependency
    :return: a tuple of validation status and installed version
    """"""","["""""" 
    if namespace:
        for (i, arg) in enumerate(dependency_spec.version_command_args):
            dependency_spec.version_command_args[i] = arg.replace(NAMESPACE_PLACEHOLDER, namespace)
    if (saved_versions and saved_versions.get(dependency_name)):
        log.debug(f'Reading {dependency_name} version from saved verify result.')
        return (_is_version_valid(saved_versions[dependency_name], dependency_spec.expected_version, dependency_spec.match_exact_version), saved_versions[dependency_name])
    try:
        (output, exit_code, log_output) = dependency_spec.version_command(dependency_spec.version_command_args)
        if (exit_code != 0):
            raise RuntimeError
    except RuntimeError as e:
        raise RuntimeError(Texts.VERSION_CMD_FAIL_MSG.format(version_cmd=dependency_spec.version_command, version_cmd_args=dependency_spec.version_command_args, output=log_output)) from e
    if dependency_spec.version_field:
        installed_version = _parse_installed_version(output, version_field=dependency_spec.version_field)
    else:
        installed_version = LooseVersion(output.strip())
    return (_is_version_valid(installed_version=installed_version, expected_version=dependency_spec.expected_version, match_exact_version=dependency_spec.match_exact_version), installed_version)
"""""", """""" 
    if namespace:
        for (i, arg) in enumerate(dependency_spec.version_command_args):
            dependency_spec.version_command_args[i] = arg.replace(NAMESPACE_PLACEHOLDER, namespace)
    if (saved_versions and saved_versions.get(dependency_name)):
        log.debug(f'Reading {dependency_name} version from saved verify result.')
        return (_parse_installed_version(saved_versions[dependency_name], dependency_spec.expected_version, dependency_spec.match_exact_version), saved_versions[dependency_name])
    try:
        (output, exit_code, log_output) = dependency_spec.version_command(dependency_spec.version_command_args)
        if (exit_code != 0):
            raise RuntimeError
    except RuntimeError as e:
        raise RuntimeError(Texts.VERSION_CMD_FAIL_MSG.format(version_cmd=dependency_spec.version_command, version_cmd_args=dependency_spec.version_command_args, output=log_output)) from e
    if dependency_spec.version_field:
        installed_version = _is_version_valid(output, version_field=dependency_spec.version_field)
    else:
        installed_version = LooseVersion(output.strip())
    return (_parse_installed_version(installed_version=installed_version, expected_version=dependency_spec.expected_version, match_exact_version=dependency_spec.match_exact_version), installed_version)
""""""]",1
"_verify_step, get_browser = get_browser, _verify_step
def verify_url_is_not(url):
    """"""Verify the current URL is not `url`

    Parameters:
    url : value
    """"""","["""""" 
    msg = f""Verify URL is not '{url}'""
    err = f""expected URL to not be '{url}'""
    with _verify_step(msg, err) as s:
        s.condition = (get_browser().current_url != url)
"""""", """""" 
    msg = f""Verify URL is not '{url}'""
    err = f""expected URL to not be '{url}'""
    with get_browser(msg, err) as s:
        s.condition = (_verify_step().current_url != url)
""""""]",1
"config_exists, get_config_file_paths = get_config_file_paths, config_exists
def check():
    """"""
    check existing setup
    """"""","["""""" 
    CONFIG_FILE_PATH = get_config_file_paths()['USER_CONFIG_FILE_PATH']
    if config_exists():
        with open(CONFIG_FILE_PATH) as config_file:
            contents = yaml.load(config_file)
            click.echo(('Name: ' + contents['name']))
            click.echo(('Email: ' + contents['email']))
            click.echo(('Github username: ' + contents['github']['username']))
            return
    else:
        click.echo(chalk.red('The configuration file does not exist. Please type ""yoda setup new"" to create a new one'))
        return
"""""", """""" 
    CONFIG_FILE_PATH = config_exists()['USER_CONFIG_FILE_PATH']
    if get_config_file_paths():
        with open(CONFIG_FILE_PATH) as config_file:
            contents = yaml.load(config_file)
            click.echo(('Name: ' + contents['name']))
            click.echo(('Email: ' + contents['email']))
            click.echo(('Github username: ' + contents['github']['username']))
            return
    else:
        click.echo(chalk.red('The configuration file does not exist. Please type ""yoda setup new"" to create a new one'))
        return
""""""]",1
"Identifiers, _extract_sequence_identifier = _extract_sequence_identifier, Identifiers
def get_identifiers(description: str) -> Identifiers:
    """"""Computes extra MSA features from the description.""""""","["""""" 
    sequence_identifier = _extract_sequence_identifier(description)
    if (sequence_identifier is None):
        return Identifiers()
    else:
        return _parse_sequence_identifier(sequence_identifier)
"""""", """""" 
    sequence_identifier = Identifiers(description)
    if (sequence_identifier is None):
        return _extract_sequence_identifier()
    else:
        return _parse_sequence_identifier(sequence_identifier)
""""""]",1
"pack_adjustment_phase_orders, encode_all_powers_action = encode_all_powers_action, pack_adjustment_phase_orders
def power_prob_distributions_to_tensors_joint(all_power_prob_distributions: JointPolicy, max_actions: int, x_possible_actions: torch.Tensor, x_in_adj_phase: bool, x_power: torch.Tensor) -> Tuple[(torch.Tensor, torch.Tensor)]:
    """"""Converting the policies to 2 tensors  orders and probs.

        orders (1, max_actions x MAX_SEQ_LEN)
        probs (1, max_actions)
    """"""","["""""" 
    orders_tensors = torch.full((max_actions, N_SCS), EOS_IDX, dtype=torch.long)
    probs_tensor = torch.zeros(max_actions)
    if x_in_adj_phase:
        tmp = orders_tensors.new_full((max_actions, len(POWERS), N_SCS), EOS_IDX)
        for (i, (joint_action, prob)) in enumerate(all_power_prob_distributions):
            for (power_idx, power) in enumerate(POWERS):
                if (not joint_action.get(power)):
                    continue
                action = joint_action[power]
                (action_tensor, good) = encode_power_actions(action, x_possible_actions[power_idx], x_in_adj_phase, max_seq_len=N_SCS)
                assert good, (power, action, x_possible_actions[power_idx])
                tmp[(i, power_idx)] = action_tensor
            probs_tensor[i] = prob
        orders_tensors = pack_adjustment_phase_orders(tmp)
    else:
        for (i, (joint_action, prob)) in enumerate(all_power_prob_distributions):
            (orders_tensors[i], valid_mask) = encode_all_powers_action(joint_action, x_possible_actions, x_power, x_in_adj_phase)
            assert valid_mask.all(), all_power_prob_distributions
            probs_tensor[i] = prob
    return (orders_tensors.unsqueeze(0), probs_tensor.unsqueeze(0))
"""""", """""" 
    orders_tensors = torch.full((max_actions, N_SCS), EOS_IDX, dtype=torch.long)
    probs_tensor = torch.zeros(max_actions)
    if x_in_adj_phase:
        tmp = orders_tensors.new_full((max_actions, len(POWERS), N_SCS), EOS_IDX)
        for (i, (joint_action, prob)) in enumerate(all_power_prob_distributions):
            for (power_idx, power) in enumerate(POWERS):
                if (not joint_action.get(power)):
                    continue
                action = joint_action[power]
                (action_tensor, good) = encode_power_actions(action, x_possible_actions[power_idx], x_in_adj_phase, max_seq_len=N_SCS)
                assert good, (power, action, x_possible_actions[power_idx])
                tmp[(i, power_idx)] = action_tensor
            probs_tensor[i] = prob
        orders_tensors = encode_all_powers_action(tmp)
    else:
        for (i, (joint_action, prob)) in enumerate(all_power_prob_distributions):
            (orders_tensors[i], valid_mask) = pack_adjustment_phase_orders(joint_action, x_possible_actions, x_power, x_in_adj_phase)
            assert valid_mask.all(), all_power_prob_distributions
            probs_tensor[i] = prob
    return (orders_tensors.unsqueeze(0), probs_tensor.unsqueeze(0))
""""""]",1
"get_msvc_version_prefix, _MSVC_EXTENDED_VERSION_COMPONENTS_DEFINITION = _MSVC_EXTENDED_VERSION_COMPONENTS_DEFINITION, get_msvc_version_prefix
def msvc_extended_version_components(version):
    """"""
    Decompose an msvc version or msvc toolset version into components.

    Args:
        version: str
            version specification

    Returns:
        None or MSVCExtendedVersionComponents namedtuple:
    """"""","["""""" 
    if (not version):
        return None
    m = re_extended_version.match(version)
    if (not m):
        return None
    msvc_toolset_version = m.group('version')
    msvc_toolset_comps = tuple(msvc_toolset_version.split('.'))
    msvc_verstr = get_msvc_version_prefix(msvc_toolset_version)
    if (not msvc_verstr):
        return None
    msvc_suffix = (m.group('suffix') if m.group('suffix') else '')
    msvc_version = (msvc_verstr + msvc_suffix)
    vs_def = Config.MSVC_VERSION_SUFFIX.get(msvc_version)
    if (not vs_def):
        return None
    msvc_vernum = float(msvc_verstr)
    msvc_comps = tuple(msvc_verstr.split('.'))
    (msvc_major, msvc_minor) = [int(x) for x in msvc_comps]
    msvc_extended_version_components_def = _MSVC_EXTENDED_VERSION_COMPONENTS_DEFINITION(msvc_version=msvc_version, msvc_verstr=msvc_verstr, msvc_suffix=msvc_suffix, msvc_vernum=msvc_vernum, msvc_major=msvc_major, msvc_minor=msvc_minor, msvc_comps=msvc_comps, msvc_toolset_version=msvc_toolset_version, msvc_toolset_comps=msvc_toolset_comps, version=version)
    return msvc_extended_version_components_def
"""""", """""" 
    if (not version):
        return None
    m = re_extended_version.match(version)
    if (not m):
        return None
    msvc_toolset_version = m.group('version')
    msvc_toolset_comps = tuple(msvc_toolset_version.split('.'))
    msvc_verstr = _MSVC_EXTENDED_VERSION_COMPONENTS_DEFINITION(msvc_toolset_version)
    if (not msvc_verstr):
        return None
    msvc_suffix = (m.group('suffix') if m.group('suffix') else '')
    msvc_version = (msvc_verstr + msvc_suffix)
    vs_def = Config.MSVC_VERSION_SUFFIX.get(msvc_version)
    if (not vs_def):
        return None
    msvc_vernum = float(msvc_verstr)
    msvc_comps = tuple(msvc_verstr.split('.'))
    (msvc_major, msvc_minor) = [int(x) for x in msvc_comps]
    msvc_extended_version_components_def = get_msvc_version_prefix(msvc_version=msvc_version, msvc_verstr=msvc_verstr, msvc_suffix=msvc_suffix, msvc_vernum=msvc_vernum, msvc_major=msvc_major, msvc_minor=msvc_minor, msvc_comps=msvc_comps, msvc_toolset_version=msvc_toolset_version, msvc_toolset_comps=msvc_toolset_comps, version=version)
    return msvc_extended_version_components_def
""""""]",1
"InvalidOsError, get_os_version = get_os_version, InvalidOsError
def check_os():
    """""" Check if user's OS is supported by nctl. """"""","["""""" 
    try:
        (os_name, os_version) = get_os_version()
        if (os_name == ''):
            raise InvalidOsError(Texts.UNKNOWN_OS_ERROR_MSG)
    except InvalidOsError:
        raise
    except Exception as exe:
        raise InvalidOsError(Texts.GET_OS_VERSION_ERROR_MSG) from exe
    log.info(f'Detected OS: {os_name} {os_version}')
    if (os_name not in SUPPORTED_OS_MAP):
        raise InvalidOsError(Texts.UNSUPPORTED_OS_ERROR_MSG.format(os_name=os_name, os_version=os_version))
    if (not _is_version_valid(os_version, SUPPORTED_OS_MAP[os_name])):
        raise InvalidOsError(Texts.INVALID_OS_VERSION_ERROR_MSG.format(os_name=os_name, os_version=os_version))
"""""", """""" 
    try:
        (os_name, os_version) = InvalidOsError()
        if (os_name == ''):
            raise get_os_version(Texts.UNKNOWN_OS_ERROR_MSG)
    except get_os_version:
        raise
    except Exception as exe:
        raise get_os_version(Texts.GET_OS_VERSION_ERROR_MSG) from exe
    log.info(f'Detected OS: {os_name} {os_version}')
    if (os_name not in SUPPORTED_OS_MAP):
        raise get_os_version(Texts.UNSUPPORTED_OS_ERROR_MSG.format(os_name=os_name, os_version=os_version))
    if (not _is_version_valid(os_version, SUPPORTED_OS_MAP[os_name])):
        raise get_os_version(Texts.INVALID_OS_VERSION_ERROR_MSG.format(os_name=os_name, os_version=os_version))
""""""]",1
"tempdir, create_files = create_files, tempdir
def test_lint_namespace_package_under_dir(initialized_linter: PyLinter) -> None:
    """"""Regression test for https://github.com/PyCQA/pylint/issues/1667""""""","["""""" 
    linter = initialized_linter
    with tempdir():
        create_files(['outer/namespace/__init__.py', 'outer/namespace/module.py'])
        linter.check(['outer.namespace'])
    assert (not linter.stats.by_msg)
"""""", """""" 
    linter = initialized_linter
    with create_files():
        tempdir(['outer/namespace/__init__.py', 'outer/namespace/module.py'])
        linter.check(['outer.namespace'])
    assert (not linter.stats.by_msg)
""""""]",1
"makeExternalImage, makeExternalLink = makeExternalLink, makeExternalImage
def replaceExternalLinks(text):
    """"""
    https://www.mediawiki.org/wiki/Help:Links#External_links
    [URL anchor text]
    """"""","["""""" 
    s = ''
    cur = 0
    for m in ExtLinkBracketedRegex.finditer(text):
        s += text[cur:m.start()]
        cur = m.end()
        url = m.group(1)
        label = m.group(3)
        m = EXT_IMAGE_REGEX.match(label)
        if m:
            label = makeExternalImage(label)
        s += makeExternalLink(url, label)
    return (s + text[cur:])
"""""", """""" 
    s = ''
    cur = 0
    for m in ExtLinkBracketedRegex.finditer(text):
        s += text[cur:m.start()]
        cur = m.end()
        url = m.group(1)
        label = m.group(3)
        m = EXT_IMAGE_REGEX.match(label)
        if m:
            label = makeExternalLink(label)
        s += makeExternalImage(url, label)
    return (s + text[cur:])
""""""]",1
"Catalog, Component = Component, Catalog
def test():
    """"""
    Test method to demonstrate utility.
    """"""","["""""" 
    insar = Component('insar')
    reference = {}
    reference['hdf5'] = 'reference.h5'
    reference['output'] = 'reference.raw'
    secondary = {}
    secondary['hdf5'] = 'secondary.h5'
    secondary['output'] = 'secondary.raw'
    insar['reference'] = reference
    insar['secondary'] = secondary
    insar['doppler method'] = 'useDEFAULT'
    insar['sensor name'] = 'COSMO_SKYMED'
    insar['range looks'] = 3
    insar['dem'] = Catalog('dem.xml')
    insar.writeXML('test.xml', root='insarApp')
"""""", """""" 
    insar = Catalog('insar')
    reference = {}
    reference['hdf5'] = 'reference.h5'
    reference['output'] = 'reference.raw'
    secondary = {}
    secondary['hdf5'] = 'secondary.h5'
    secondary['output'] = 'secondary.raw'
    insar['reference'] = reference
    insar['secondary'] = secondary
    insar['doppler method'] = 'useDEFAULT'
    insar['sensor name'] = 'COSMO_SKYMED'
    insar['range looks'] = 3
    insar['dem'] = Component('dem.xml')
    insar.writeXML('test.xml', root='insarApp')
""""""]",1
"merge_base, find_revision_sha = find_revision_sha, merge_base
def is_ancestor(repo, rev1, rev2):
    """"""return true if rev1 is an ancestor of rev2""""""","["""""" 
    sha1 = find_revision_sha(repo, rev1)
    sha2 = find_revision_sha(repo, rev2)
    return (True if (sha1 in merge_base(repo, sha1, sha2)) else False)
"""""", """""" 
    sha1 = merge_base(repo, rev1)
    sha2 = merge_base(repo, rev2)
    return (True if (sha1 in find_revision_sha(repo, sha1, sha2)) else False)
""""""]",1
"func, wraps = wraps, func
def s3_request(func):
    """"""
    Wrapper function for s3 requests in order to create more helpful error
    messages.
    """"""","["""""" 

    @wraps(func)
    def wrapper(url, *args, **kwargs):
        try:
            return func(url, *args, **kwargs)
        except ClientError as exc:
            if (int(exc.response['Error']['Code']) == 404):
                raise EnvironmentError('file {} not found'.format(url))
            else:
                raise
    return wrapper
"""""", """""" 

    @func(wraps)
    def wrapper(url, *args, **kwargs):
        try:
            return wraps(url, *args, **kwargs)
        except ClientError as exc:
            if (int(exc.response['Error']['Code']) == 404):
                raise EnvironmentError('file {} not found'.format(url))
            else:
                raise
    return wrapper
""""""]",1
"defaultdict, is_unvote_draw_msg = is_unvote_draw_msg, defaultdict
def get_gamejson_draw_state(game_json: GameJson, until_time: Optional[Timestamp], metadata: Metadata) -> Optional[CurrentDrawState]:
    """"""
    Returns dict with boolean flag corresponding to which players have currently voted for a draw
    up through and including time until_time.
    """"""","["""""" 
    if (not metadata['opt'].get('include_draw_state')):
        return None
    drawstate: CurrentDrawState = defaultdict((lambda : False))
    if (until_time is None):
        return drawstate
    for phase_data in game_json.values():
        for message in phase_data['messages']:
            if (message[MessageObjectPart.TIME_SENT] > until_time):
                break
            if is_draw_msg(message):
                drawstate[message[MessageObjectPart.SENDER]] = True
            elif is_unvote_draw_msg(message):
                drawstate[message[MessageObjectPart.SENDER]] = False
    return drawstate
"""""", """""" 
    if (not metadata['opt'].get('include_draw_state')):
        return None
    drawstate: CurrentDrawState = is_unvote_draw_msg((lambda : False))
    if (until_time is None):
        return drawstate
    for phase_data in game_json.values():
        for message in phase_data['messages']:
            if (message[MessageObjectPart.TIME_SENT] > until_time):
                break
            if is_draw_msg(message):
                drawstate[message[MessageObjectPart.SENDER]] = True
            elif defaultdict(message):
                drawstate[message[MessageObjectPart.SENDER]] = False
    return drawstate
""""""]",1
"_step, get_browser = get_browser, _step
def delete_all_cookies():
    """"""Delete all cookies from the current session.
    Note: this only deletes cookies from the current domain.
    """"""","["""""" 
    with _step('Delete all cookies'):
        get_browser().delete_all_cookies()
"""""", """""" 
    with get_browser('Delete all cookies'):
        _step().delete_all_cookies()
""""""]",1
"isprojective, tarjan = tarjan, isprojective
def istree(sequence, proj=False, multiroot=False):
    """"""
    Checks if the arcs form an valid dependency tree.

    Args:
        sequence (list[int]):
            A list of head indices.
        proj (bool):
            If ``True``, requires the tree to be projective. Default: ``False``.
        multiroot (bool):
            If ``False``, requires the tree to contain only a single root. Default: ``True``.

    Returns:
        ``True`` if the arcs form an valid tree, ``False`` otherwise.

    Examples:
        >>> istree([3, 0, 0, 3], multiroot=True)
        True
        >>> istree([3, 0, 0, 3], proj=True)
        False
    """"""","["""""" 
    if (proj and (not isprojective(sequence))):
        return False
    n_roots = sum(((head == 0) for head in sequence))
    if (n_roots == 0):
        return False
    if ((not multiroot) and (n_roots > 1)):
        return False
    if any(((i == head) for (i, head) in enumerate(sequence, 1))):
        return False
    return (next(tarjan(sequence), None) is None)
"""""", """""" 
    if (proj and (not tarjan(sequence))):
        return False
    n_roots = sum(((head == 0) for head in sequence))
    if (n_roots == 0):
        return False
    if ((not multiroot) and (n_roots > 1)):
        return False
    if any(((i == head) for (i, head) in enumerate(sequence, 1))):
        return False
    return (next(isprojective(sequence), None) is None)
""""""]",1
"random, delayed = delayed, random
def selective_search(img, mode='single', random=False):
    """"""
        Selective Search in Python
    """"""","["""""" 
    strategy = load_strategy(mode)
    vault = Parallel(n_jobs=1)((delayed(selective_search_one)(img, color, k, sim) for (color, k, sim) in strategy))
    boxes = [x for (x, _) in vault]
    priorities = [y for (_, y) in vault]
    boxes = [item for sublist in boxes for item in sublist]
    priorities = [item for sublist in priorities for item in sublist]
    if random:
        rand_list = [random() for i in range(len(priorities))]
        priorities = [(p * r) for (p, r) in zip(priorities, rand_list)]
        boxes = [b for (_, b) in sorted(zip(priorities, boxes))]
    boxes = list(dict.fromkeys(boxes))
    return boxes
"""""", """""" 
    strategy = load_strategy(mode)
    vault = Parallel(n_jobs=1)((random(selective_search_one)(img, color, k, sim) for (color, k, sim) in strategy))
    boxes = [x for (x, _) in vault]
    priorities = [y for (_, y) in vault]
    boxes = [item for sublist in boxes for item in sublist]
    priorities = [item for sublist in priorities for item in sublist]
    if delayed:
        rand_list = [delayed() for i in range(len(priorities))]
        priorities = [(p * r) for (p, r) in zip(priorities, rand_list)]
        boxes = [b for (_, b) in sorted(zip(priorities, boxes))]
    boxes = list(dict.fromkeys(boxes))
    return boxes
""""""]",1
"PdbStructure, _add_restraints = _add_restraints, PdbStructure
def get_initial_energies(pdb_strs: Sequence[str], stiffness: float=0.0, restraint_set: str='non_hydrogen', exclude_residues: Optional[Sequence[int]]=None):
    """"""Returns initial potential energies for a sequence of PDBs.

    Assumes the input PDBs are ready for minimization, and all have the same
    topology.
    Allows time to be saved by not pdbfixing / rebuilding the system.

    Args:
      pdb_strs: List of PDB strings.
      stiffness: kcal/mol A**2, spring constant of heavy atom restraining
          potential.
      restraint_set: Which atom types to restrain.
      exclude_residues: An optional list of zero-indexed residues to exclude from
          restraints.

    Returns:
      A list of initial energies in the same order as pdb_strs.
    """"""","["""""" 
    exclude_residues = (exclude_residues or [])
    openmm_pdbs = [openmm_app.PDBFile(PdbStructure(io.StringIO(p))) for p in pdb_strs]
    force_field = openmm_app.ForceField('amber99sb.xml')
    system = force_field.createSystem(openmm_pdbs[0].topology, constraints=openmm_app.HBonds)
    stiffness = ((stiffness * ENERGY) / (LENGTH ** 2))
    if (stiffness > ((0 * ENERGY) / (LENGTH ** 2))):
        _add_restraints(system, openmm_pdbs[0], stiffness, restraint_set, exclude_residues)
    simulation = openmm_app.Simulation(openmm_pdbs[0].topology, system, openmm.LangevinIntegrator(0, 0.01, 0.0), openmm.Platform.getPlatformByName('CPU'))
    energies = []
    for pdb in openmm_pdbs:
        try:
            simulation.context.setPositions(pdb.positions)
            state = simulation.context.getState(getEnergy=True)
            energies.append(state.getPotentialEnergy().value_in_unit(ENERGY))
        except Exception as e:
            logging.error('Error getting initial energy, returning large value %s', e)
            energies.append(unit.Quantity(1e+20, ENERGY))
    return energies
"""""", """""" 
    exclude_residues = (exclude_residues or [])
    openmm_pdbs = [openmm_app.PDBFile(_add_restraints(io.StringIO(p))) for p in pdb_strs]
    force_field = openmm_app.ForceField('amber99sb.xml')
    system = force_field.createSystem(openmm_pdbs[0].topology, constraints=openmm_app.HBonds)
    stiffness = ((stiffness * ENERGY) / (LENGTH ** 2))
    if (stiffness > ((0 * ENERGY) / (LENGTH ** 2))):
        PdbStructure(system, openmm_pdbs[0], stiffness, restraint_set, exclude_residues)
    simulation = openmm_app.Simulation(openmm_pdbs[0].topology, system, openmm.LangevinIntegrator(0, 0.01, 0.0), openmm.Platform.getPlatformByName('CPU'))
    energies = []
    for pdb in openmm_pdbs:
        try:
            simulation.context.setPositions(pdb.positions)
            state = simulation.context.getState(getEnergy=True)
            energies.append(state.getPotentialEnergy().value_in_unit(ENERGY))
        except Exception as e:
            logging.error('Error getting initial energy, returning large value %s', e)
            energies.append(unit.Quantity(1e+20, ENERGY))
    return energies
""""""]",1
"crf_log_norm, crf_sequence_score = crf_sequence_score, crf_log_norm
def crf_log_likelihood(inputs, tag_indices, sequence_lengths, transition_params=None):
    """"""Computes the log-likelihood of tag sequences in a CRF.

    Args:
      inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials
    to use as input to the CRF layer.
      tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which
    we compute the log-likelihood.
      sequence_lengths: A [batch_size] vector of true sequence lengths.
      transition_params: A [num_tags, num_tags] transition matrix, (Default value = None)

    Returns:
      log_likelihood: A [batch_size] `Tensor` containing the log-likelihood of
      each example, given the sequence of tag indices.
      transition_params: A [num_tags, num_tags] transition matrix. This is
      either provided by the caller or created in this function.

    """"""","["""""" 
    num_tags = inputs.shape[2]
    tag_indices = tf.cast(tag_indices, dtype=tf.int32)
    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)
    if (transition_params is None):
        initializer = tf.keras.initializers.GlorotUniform()
        transition_params = tf.Variable(initializer([num_tags, num_tags]), 'transitions')
    sequence_scores = crf_sequence_score(inputs, tag_indices, sequence_lengths, transition_params)
    log_norm = crf_log_norm(inputs, sequence_lengths, transition_params)
    log_likelihood = (sequence_scores - log_norm)
    return (log_likelihood, transition_params)
"""""", """""" 
    num_tags = inputs.shape[2]
    tag_indices = tf.cast(tag_indices, dtype=tf.int32)
    sequence_lengths = tf.cast(sequence_lengths, dtype=tf.int32)
    if (transition_params is None):
        initializer = tf.keras.initializers.GlorotUniform()
        transition_params = tf.Variable(initializer([num_tags, num_tags]), 'transitions')
    sequence_scores = crf_log_norm(inputs, tag_indices, sequence_lengths, transition_params)
    log_norm = crf_sequence_score(inputs, sequence_lengths, transition_params)
    log_likelihood = (sequence_scores - log_norm)
    return (log_likelihood, transition_params)
""""""]",1
"setup_script_registry, _SupercommandParser = _SupercommandParser, setup_script_registry
def superscript_main(args=None):
    """"""
    Superscript is a loader for all the other scripts.
    """"""","["""""" 
    setup_script_registry()
    parser = _SupercommandParser(False, False, formatter_class=_SuperscriptHelpFormatter)
    parser.add_argument('--helpall', action='helpall', help='List all commands, including advanced ones.')
    parser.add_argument('--version', action='version', version=get_version_string(), help='Prints version info and exit.')
    parser.set_defaults(super_command=None)
    subparsers = parser.add_subparsers(parser_class=_SubcommandParser, title='Commands', metavar='COMMAND')
    hparser = subparsers.add_parser('help', aliases=['h'], help=argparse.SUPPRESS, description='List the main commands.')
    hparser.set_defaults(super_command='help')
    hparser = subparsers.add_parser('helpall', help=argparse.SUPPRESS, description='List all commands, including advanced ones.')
    hparser.set_defaults(super_command='helpall')
    for (script_name, registration) in SCRIPT_REGISTRY.items():
        logging.verbose(f'Discovered command {script_name}')
        script_parser = registration.klass.setup_args()
        if (script_parser is None):
            script_parser = ParlaiParser(False, False)
        help_ = (argparse.SUPPRESS if registration.hidden else script_parser.description)
        subparser = subparsers.add_parser(script_name, aliases=registration.aliases, help=help_, description=script_parser.description, formatter_class=CustomHelpFormatter)
        subparser.set_defaults(super_command=script_name, _subparser=subparser)
        subparser.set_defaults(**script_parser._defaults)
        for action in script_parser._actions:
            subparser._add_action(action)
        for action_group in script_parser._action_groups:
            subparser._action_groups.append(action_group)
    try:
        import argcomplete
        argcomplete.autocomplete(parser)
    except ModuleNotFoundError:
        pass
    opt = parser.parse_args(args)
    cmd = opt.pop('super_command')
    if (cmd == 'helpall'):
        parser.print_helpall()
    elif (cmd == 'versioninfo'):
        exit(0)
    elif ((cmd == 'help') or (cmd is None)):
        parser.print_help()
    elif (cmd is not None):
        return SCRIPT_REGISTRY[cmd].klass._run_from_parser_and_opt(opt, parser)
"""""", """""" 
    _SupercommandParser()
    parser = setup_script_registry(False, False, formatter_class=_SuperscriptHelpFormatter)
    parser.add_argument('--helpall', action='helpall', help='List all commands, including advanced ones.')
    parser.add_argument('--version', action='version', version=get_version_string(), help='Prints version info and exit.')
    parser.set_defaults(super_command=None)
    subparsers = parser.add_subparsers(parser_class=_SubcommandParser, title='Commands', metavar='COMMAND')
    hparser = subparsers.add_parser('help', aliases=['h'], help=argparse.SUPPRESS, description='List the main commands.')
    hparser.set_defaults(super_command='help')
    hparser = subparsers.add_parser('helpall', help=argparse.SUPPRESS, description='List all commands, including advanced ones.')
    hparser.set_defaults(super_command='helpall')
    for (script_name, registration) in SCRIPT_REGISTRY.items():
        logging.verbose(f'Discovered command {script_name}')
        script_parser = registration.klass.setup_args()
        if (script_parser is None):
            script_parser = ParlaiParser(False, False)
        help_ = (argparse.SUPPRESS if registration.hidden else script_parser.description)
        subparser = subparsers.add_parser(script_name, aliases=registration.aliases, help=help_, description=script_parser.description, formatter_class=CustomHelpFormatter)
        subparser.set_defaults(super_command=script_name, _subparser=subparser)
        subparser.set_defaults(**script_parser._defaults)
        for action in script_parser._actions:
            subparser._add_action(action)
        for action_group in script_parser._action_groups:
            subparser._action_groups.append(action_group)
    try:
        import argcomplete
        argcomplete.autocomplete(parser)
    except ModuleNotFoundError:
        pass
    opt = parser.parse_args(args)
    cmd = opt.pop('super_command')
    if (cmd == 'helpall'):
        parser.print_helpall()
    elif (cmd == 'versioninfo'):
        exit(0)
    elif ((cmd == 'help') or (cmd is None)):
        parser.print_help()
    elif (cmd is not None):
        return SCRIPT_REGISTRY[cmd].klass._run_from_parser_and_opt(opt, parser)
""""""]",1
"_ConvertSourcesToFilterHierarchy, _IdlFilesHandledNonNatively = _IdlFilesHandledNonNatively, _ConvertSourcesToFilterHierarchy
def _AdjustSourcesAndConvertToFilterHierarchy(spec, options, gyp_dir, sources, excluded_sources, list_excluded, version):
    """"""Adjusts the list of sources and excluded sources.

  Also converts the sets to lists.

  Arguments:
    spec: The target dictionary containing the properties of the target.
    options: Global generator options.
    gyp_dir: The path to the gyp file being processed.
    sources: A set of sources to be included for this project.
    excluded_sources: A set of sources to be excluded for this project.
    version: A MSVSVersion object.
  Returns:
    A trio of (list of sources, list of excluded sources,
               path of excluded IDL file)
  """"""","["""""" 
    excluded_sources.update(OrderedSet(spec.get('sources_excluded', [])))
    sources.update(excluded_sources)
    sources = _FixPaths(sources)
    excluded_sources = _FixPaths(excluded_sources)
    excluded_idl = _IdlFilesHandledNonNatively(spec, sources)
    precompiled_related = _GetPrecompileRelatedFiles(spec)
    fully_excluded = [i for i in excluded_sources if (i not in precompiled_related)]
    sources = [i.split('\\') for i in sources]
    sources = _ConvertSourcesToFilterHierarchy(sources, excluded=fully_excluded, list_excluded=list_excluded, msvs_version=version)
    if version.UsesVcxproj():
        while (all([isinstance(s, MSVSProject.Filter) for s in sources]) and (len({s.name for s in sources}) == 1)):
            assert all([(len(s.contents) == 1) for s in sources])
            sources = [s.contents[0] for s in sources]
    else:
        while ((len(sources) == 1) and isinstance(sources[0], MSVSProject.Filter)):
            sources = sources[0].contents
    return (sources, excluded_sources, excluded_idl)
"""""", """""" 
    excluded_sources.update(OrderedSet(spec.get('sources_excluded', [])))
    sources.update(excluded_sources)
    sources = _FixPaths(sources)
    excluded_sources = _FixPaths(excluded_sources)
    excluded_idl = _ConvertSourcesToFilterHierarchy(spec, sources)
    precompiled_related = _GetPrecompileRelatedFiles(spec)
    fully_excluded = [i for i in excluded_sources if (i not in precompiled_related)]
    sources = [i.split('\\') for i in sources]
    sources = _IdlFilesHandledNonNatively(sources, excluded=fully_excluded, list_excluded=list_excluded, msvs_version=version)
    if version.UsesVcxproj():
        while (all([isinstance(s, MSVSProject.Filter) for s in sources]) and (len({s.name for s in sources}) == 1)):
            assert all([(len(s.contents) == 1) for s in sources])
            sources = [s.contents[0] for s in sources]
    else:
        while ((len(sources) == 1) and isinstance(sources[0], MSVSProject.Filter)):
            sources = sources[0].contents
    return (sources, excluded_sources, excluded_idl)
""""""]",1
"_dataclass_from_dict, _dataclass_list_from_dict_list = _dataclass_list_from_dict_list, _dataclass_from_dict
def load_dataclass(f: IO, cls: Type[_X], binary: bool=False) -> _X:
    """"""
    Loads to a @dataclass or collection hierarchy including dataclasses
    from a json recursively.
    Call it like load_dataclass(f, typing.List[FrameAnnotationAnnotation]).
    raises KeyError if json has keys not mapping to the dataclass fields.

    Args:
        f: Either a path to a file, or a file opened for writing.
        cls: The class of the loaded dataclass.
        binary: Set to True if `f` is a file handle, else False.
    """"""","["""""" 
    if binary:
        asdict = json.loads(f.read().decode('utf8'))
    else:
        asdict = json.load(f)
    if isinstance(asdict, list):
        cls = get_args(cls)[0]
        res = list(_dataclass_list_from_dict_list(asdict, cls))
    else:
        res = _dataclass_from_dict(asdict, cls)
    return res
"""""", """""" 
    if binary:
        asdict = json.loads(f.read().decode('utf8'))
    else:
        asdict = json.load(f)
    if isinstance(asdict, list):
        cls = get_args(cls)[0]
        res = list(_dataclass_from_dict(asdict, cls))
    else:
        res = _dataclass_list_from_dict_list(asdict, cls)
    return res
""""""]",1
"set_gpu, set_gpu_memory_growth = set_gpu_memory_growth, set_gpu
def nice_gpu():
    """"""Use GPU nicely.""""""","["""""" 
    set_gpu_memory_growth()
    set_gpu()
"""""", """""" 
    set_gpu()
    set_gpu_memory_growth()
""""""]",1
"__init_xsl_stylesheet, __extend_targets_sources = __extend_targets_sources, __init_xsl_stylesheet
def DocbookPdf(env, target, source=None, *args, **kw):
    """"""
    A pseudo-Builder, providing a Docbook toolchain for PDF output.
    """"""","["""""" 
    (target, source) = __extend_targets_sources(target, source)
    __init_xsl_stylesheet(kw, env, '$DOCBOOK_DEFAULT_XSL_PDF', ['fo', 'docbook.xsl'])
    __builder = __select_builder(__lxml_builder, __xsltproc_builder)
    result = []
    for (t, s) in zip(target, source):
        (t, stem) = __ensure_suffix_stem(t, '.pdf')
        xsl = __builder.__call__(env, (stem + '.fo'), s, **kw)
        result.extend(xsl)
        env.Depends(xsl, kw['DOCBOOK_XSL'])
        result.extend(__fop_builder.__call__(env, t, xsl, **kw))
    return result
"""""", """""" 
    (target, source) = __init_xsl_stylesheet(target, source)
    __extend_targets_sources(kw, env, '$DOCBOOK_DEFAULT_XSL_PDF', ['fo', 'docbook.xsl'])
    __builder = __select_builder(__lxml_builder, __xsltproc_builder)
    result = []
    for (t, s) in zip(target, source):
        (t, stem) = __ensure_suffix_stem(t, '.pdf')
        xsl = __builder.__call__(env, (stem + '.fo'), s, **kw)
        result.extend(xsl)
        env.Depends(xsl, kw['DOCBOOK_XSL'])
        result.extend(__fop_builder.__call__(env, t, xsl, **kw))
    return result
""""""]",1
"AverageMeter, model = model, AverageMeter
def validate(val_loader, model, criterion, opt):
    """"""validation""""""","["""""" 
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()
    model.eval()
    with torch.no_grad():
        end = time.time()
        for (idx, (input, target)) in enumerate(val_loader):
            input = input.float()
            if torch.cuda.is_available():
                input = input.cuda()
                target = target.cuda()
            output = model(input)
            loss = criterion(output, target)
            (acc1, acc5) = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((idx % opt.print_freq) == 0):
                print('Test: [{0}/{1}]\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\tLoss {loss.val:.4f} ({loss.avg:.4f})\tAcc@1 {top1.val:.3f} ({top1.avg:.3f})\tAcc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(idx, len(val_loader), batch_time=batch_time, loss=losses, top1=top1, top5=top5))
        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))
    return (top1.avg, top5.avg, losses.avg)
"""""", """""" 
    batch_time = model()
    losses = model()
    top1 = model()
    top5 = model()
    AverageMeter.eval()
    with torch.no_grad():
        end = time.time()
        for (idx, (input, target)) in enumerate(val_loader):
            input = input.float()
            if torch.cuda.is_available():
                input = input.cuda()
                target = target.cuda()
            output = AverageMeter(input)
            loss = criterion(output, target)
            (acc1, acc5) = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((idx % opt.print_freq) == 0):
                print('Test: [{0}/{1}]\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\tLoss {loss.val:.4f} ({loss.avg:.4f})\tAcc@1 {top1.val:.3f} ({top1.avg:.3f})\tAcc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(idx, len(val_loader), batch_time=batch_time, loss=losses, top1=top1, top5=top5))
        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))
    return (top1.avg, top5.avg, losses.avg)
""""""]",1
"_screenshot_on_step, _run_wait_hook = _run_wait_hook, _screenshot_on_step
def assert_element_attribute_is_not(element, attribute, value):
    """"""Assert the value of element attribute is not `value`

    Parameters:
    element : element
    attribute : value
    value : value
    """"""","["""""" 
    element = get_browser().find(element, timeout=0)
    step_message = f'Assert element {element.name} attribute {attribute} value is not {value}'
    _add_step(step_message)
    _run_wait_hook()
    attr_value = element.get_attribute(attribute)
    msg = f'expected element {element.name} attribute {attribute} value to not be {value}'
    assert (attr_value != value), msg
    _screenshot_on_step()
"""""", """""" 
    element = get_browser().find(element, timeout=0)
    step_message = f'Assert element {element.name} attribute {attribute} value is not {value}'
    _add_step(step_message)
    _screenshot_on_step()
    attr_value = element.get_attribute(attribute)
    msg = f'expected element {element.name} attribute {attribute} value to not be {value}'
    assert (attr_value != value), msg
    _run_wait_hook()
""""""]",1
"normalize_numpy, normalize_torch = normalize_torch, normalize_numpy
def normalize(targets: Union[(np.ndarray, Tensor)], threshold: Union[(np.ndarray, Tensor, float)]) -> Union[(np.ndarray, Tensor)]:
    """"""Normalize the targets by using the cumulative density function.""""""","["""""" 
    if isinstance(targets, Tensor):
        return normalize_torch(targets, threshold)
    if isinstance(targets, np.ndarray):
        return normalize_numpy(targets, threshold)
    raise ValueError(f'Targets must be either Tensor or Numpy array. Received {type(targets)}')
"""""", """""" 
    if isinstance(targets, Tensor):
        return normalize_numpy(targets, threshold)
    if isinstance(targets, np.ndarray):
        return normalize_torch(targets, threshold)
    raise ValueError(f'Targets must be either Tensor or Numpy array. Received {type(targets)}')
""""""]",1
"tabulate, handle_error = handle_error, tabulate
def list_unitialized_experiments_in_cli(verbosity_lvl: int, all_users: bool, name: str, headers: List[str], listed_runs_kinds: List[RunKinds]=None, count: int=None, brief: bool=False):
    """"""
    Display a list of selected runs in the cli.

    :param verbosity_lvl: level at which error messages should be logged or displayed
    :param all_users: whether to display runs regardless of their owner or not
    :param name: regular expression to which names of the shown runs have to match
    :param headers: headers which will be displayed on top of a table shown in the cli
    :param count: number of rows displayed on a list. If not given - content of a list is not limited
    """"""","["""""" 
    if (not listed_runs_kinds):
        listed_runs_kinds = [RunKinds.TRAINING, RunKinds.JUPYTER]
    try:
        namespace = (None if all_users else get_kubectl_current_context_namespace())
        creating_experiments = Experiment.list(namespace=namespace, state=ExperimentStatus.CREATING, run_kinds_filter=listed_runs_kinds, name_filter=name)
        runs = Run.list(namespace=namespace, name_filter=name, run_kinds_filter=listed_runs_kinds)
        names_of_experiment_with_runs = set()
        for run in runs:
            names_of_experiment_with_runs.add(run.experiment_name)
        uninitialized_experiments = [experiment for experiment in creating_experiments if (experiment.name not in names_of_experiment_with_runs)]
        displayed_items_count = (count if count else len(uninitialized_experiments))
        click.echo(tabulate([uninitialized_experiment_cli_representation(experiment) for experiment in uninitialized_experiments][(- displayed_items_count):], headers=headers, tablefmt=TBLT_TABLE_FORMAT))
    except InvalidRegularExpressionError:
        handle_error(logger, Texts.INVALID_REGEX_ERROR_MSG, Texts.INVALID_REGEX_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
    except Exception:
        handle_error(logger, Texts.OTHER_ERROR_MSG, Texts.OTHER_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
"""""", """""" 
    if (not listed_runs_kinds):
        listed_runs_kinds = [RunKinds.TRAINING, RunKinds.JUPYTER]
    try:
        namespace = (None if all_users else get_kubectl_current_context_namespace())
        creating_experiments = Experiment.list(namespace=namespace, state=ExperimentStatus.CREATING, run_kinds_filter=listed_runs_kinds, name_filter=name)
        runs = Run.list(namespace=namespace, name_filter=name, run_kinds_filter=listed_runs_kinds)
        names_of_experiment_with_runs = set()
        for run in runs:
            names_of_experiment_with_runs.add(run.experiment_name)
        uninitialized_experiments = [experiment for experiment in creating_experiments if (experiment.name not in names_of_experiment_with_runs)]
        displayed_items_count = (count if count else len(uninitialized_experiments))
        click.echo(handle_error([uninitialized_experiment_cli_representation(experiment) for experiment in uninitialized_experiments][(- displayed_items_count):], headers=headers, tablefmt=TBLT_TABLE_FORMAT))
    except InvalidRegularExpressionError:
        tabulate(logger, Texts.INVALID_REGEX_ERROR_MSG, Texts.INVALID_REGEX_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
    except Exception:
        tabulate(logger, Texts.OTHER_ERROR_MSG, Texts.OTHER_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
""""""]",1
"DopIQ, createSensor = createSensor, DopIQ
def unpack(hdf5, slcname, multiple=False, orbtype='PRC'):
    """"""
    Unpack HDF5 to binary SLC file.
    """"""","["""""" 
    if multiple:
        print('Trying multiple sub-dirs - ESA convention ...')
        imgname = glob.glob(os.path.join(hdf5, '*', 'DAT*'))
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LEA*'))
        if ((len(imgname) == 0) or (len(ldrname) == 0)):
            print('Did not find ESA style files in sub-dirs. Trying RPAC style in sub-dirs ....')
            imgname = glob.glob(os.path.join(hdf5, '*', 'IMA*'))
            ldrname = glob.glob(os.path.join(hdf5, '*', 'SAR*'))
            if ((len(imgname) == 0) or (len(ldrname) == 0)):
                print('Did not find RPAC style files in sub-dirs. Trying RPAC style in same-dir ....')
                imgname = glob.glob(os.path.join(hdf5, 'IMA*'))
                ldrname = glob.glob(os.path.join(hdf5, 'SAR*'))
    else:
        imgname = [glob.glob(os.path.join(hdf5, 'DAT*'))[0]]
        ldrname = [glob.glob(os.path.join(hdf5, 'LEA*'))[0]]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = createSensor('ERS')
    obj.configure()
    obj._imageFileList = imgname
    obj._leaderFileList = ldrname
    if (orbtype == 'ODR'):
        obj._orbitType = 'ODR'
        obj._orbitDir = '/Users/agram/orbit/ODR/ERS1'
    if (orbtype == 'PRC'):
        obj._orbitType = 'PRC'
        obj._orbitDir = '/Users/agram/orbit/PRC/ERS1'
    obj.output = os.path.join(slcname, (date + '.raw'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = DopIQ()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    print(coef)
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
"""""", """""" 
    if multiple:
        print('Trying multiple sub-dirs - ESA convention ...')
        imgname = glob.glob(os.path.join(hdf5, '*', 'DAT*'))
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LEA*'))
        if ((len(imgname) == 0) or (len(ldrname) == 0)):
            print('Did not find ESA style files in sub-dirs. Trying RPAC style in sub-dirs ....')
            imgname = glob.glob(os.path.join(hdf5, '*', 'IMA*'))
            ldrname = glob.glob(os.path.join(hdf5, '*', 'SAR*'))
            if ((len(imgname) == 0) or (len(ldrname) == 0)):
                print('Did not find RPAC style files in sub-dirs. Trying RPAC style in same-dir ....')
                imgname = glob.glob(os.path.join(hdf5, 'IMA*'))
                ldrname = glob.glob(os.path.join(hdf5, 'SAR*'))
    else:
        imgname = [glob.glob(os.path.join(hdf5, 'DAT*'))[0]]
        ldrname = [glob.glob(os.path.join(hdf5, 'LEA*'))[0]]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = DopIQ('ERS')
    obj.configure()
    obj._imageFileList = imgname
    obj._leaderFileList = ldrname
    if (orbtype == 'ODR'):
        obj._orbitType = 'ODR'
        obj._orbitDir = '/Users/agram/orbit/ODR/ERS1'
    if (orbtype == 'PRC'):
        obj._orbitType = 'PRC'
        obj._orbitDir = '/Users/agram/orbit/PRC/ERS1'
    obj.output = os.path.join(slcname, (date + '.raw'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = createSensor()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    print(coef)
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
""""""]",1
"_, _write_element = _write_element, _
def lint_read_errors(report: ProjectReport, out=sys.stdout) -> Iterable[str]:
    """"""Lint for read errors.""""""","["""""" 
    bad_files = []
    if report.read_errors:
        out.write('# ')
        out.write(_('READ ERRORS'))
        out.write('\n\n')
        out.write(_('Could not read:'))
        out.write('\n')
        for file_ in report.read_errors:
            bad_files.append(file_)
            _write_element(file_, out=out)
        out.write('\n\n')
    return bad_files
"""""", """""" 
    bad_files = []
    if report.read_errors:
        out.write('# ')
        out.write(_write_element('READ ERRORS'))
        out.write('\n\n')
        out.write(_write_element('Could not read:'))
        out.write('\n')
        for file_ in report.read_errors:
            bad_files.append(file_)
            _(file_, out=out)
        out.write('\n\n')
    return bad_files
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_selected_option_by_value(element, value):
    """"""Verify an element has a selected option by the option value

    Parameters:
    element : element
    value : value
    """"""","["""""" 
    element = get_browser().find(element)
    with _verify_step(f'Verify selected option value of element {element.name} is {value}') as s:
        selected_option_value = element.select.first_selected_option.value
        s.error = f'Expected selected option in element {element.name} to be {value} but was {selected_option_value}'
        s.condition = (selected_option_value == value)
"""""", """""" 
    element = _verify_step().find(element)
    with get_browser(f'Verify selected option value of element {element.name} is {value}') as s:
        selected_option_value = element.select.first_selected_option.value
        s.error = f'Expected selected option in element {element.name} to be {value} but was {selected_option_value}'
        s.condition = (selected_option_value == value)
""""""]",1
"runCmd, coherence = coherence, runCmd
def runCoherence(self):
    """"""estimate coherence
    """"""","["""""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    numberRangeLooks = (self._insar.numberRangeLooks1 * self._insar.numberRangeLooks2)
    numberAzimuthLooks = (self._insar.numberAzimuthLooks1 * self._insar.numberAzimuthLooks2)
    if ((numberRangeLooks * numberAzimuthLooks) >= 9):
        cmd = ""imageMath.py -e='sqrt(b_0*b_1);abs(a)/(b_0+(b_0==0))/(b_1+(b_1==0))*(b_0!=0)*(b_1!=0)' --a={} --b={} -o {} -t float -s BIL"".format(self._insar.multilookDifferentialInterferogram, self._insar.multilookAmplitude, self._insar.multilookCoherence)
        runCmd(cmd)
    else:
        coherence(self._insar.multilookAmplitude, self._insar.multilookDifferentialInterferogram, self._insar.multilookCoherence, method='cchz_wave', windowSize=5)
    os.chdir('../')
    catalog.printToLog(logger, 'runCoherence')
    self._insar.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    numberRangeLooks = (self._insar.numberRangeLooks1 * self._insar.numberRangeLooks2)
    numberAzimuthLooks = (self._insar.numberAzimuthLooks1 * self._insar.numberAzimuthLooks2)
    if ((numberRangeLooks * numberAzimuthLooks) >= 9):
        cmd = ""imageMath.py -e='sqrt(b_0*b_1);abs(a)/(b_0+(b_0==0))/(b_1+(b_1==0))*(b_0!=0)*(b_1!=0)' --a={} --b={} -o {} -t float -s BIL"".format(self._insar.multilookDifferentialInterferogram, self._insar.multilookAmplitude, self._insar.multilookCoherence)
        coherence(cmd)
    else:
        runCmd(self._insar.multilookAmplitude, self._insar.multilookDifferentialInterferogram, self._insar.multilookCoherence, method='cchz_wave', windowSize=5)
    os.chdir('../')
    catalog.printToLog(logger, 'runCoherence')
    self._insar.procDoc.addAllFromCatalog(catalog)
""""""]",1
"is_rollout_joint_action, game_from_view_of = game_from_view_of, is_rollout_joint_action
def get_prob_under_nucleus_sampling(message_handler: ParlaiMessageHandler, game: pydipcc.Game, timestamp: int, pseudoorders_per_phase_time_sent: Dict[(Tuple[(str, int)], RolloutJointAction)]) -> Optional[float]:
    """"""
    This function determines if the message at a given timestamp in a given game coul
    be produced by a given agent under nucleus scoring. It returns true of the message could
    not be produced.

    It does this by rolling back the game to right before the message in question was produced,
    generating pseudo orders if relevant, and then calculating the probability of the message
    in question under nucleus scoring according the agent's conditional distribution over messages.

    Since nucleus scoring truncates the probability of unlikely messages to 0, we say the message
    ""in nucleus"" by the agent if its probability under nucleus scoring is nonzero.
    """"""","["""""" 
    game_at_time_end = game.rolled_back_to_timestamp_end(timestamp)
    game_at_time_start = game.rolled_back_to_timestamp_start(timestamp)
    message_data = game_at_time_end.messages[timestamp]
    sender = message_data[MessageObjectPart.SENDER]
    recipient = message_data[MessageObjectPart.RECIPIENT]
    curr_phase = message_data[MessageObjectPart.PHASE]
    game_at_time_start_with_perspective = game_from_view_of(game_at_time_start, sender)
    if message_handler.expects_pseudo_orders():
        pseudo_orders = pseudoorders_per_phase_time_sent[(message_data[MessageObjectPart.PHASE], message_data[MessageObjectPart.TIME_SENT])]
        if ((not is_movement_phase(curr_phase)) and (len(pseudo_orders.keys()) == 1)):
            return None
        else:
            pseudo_orders = sorted(pseudo_orders.items(), key=(lambda psuedo_order: sort_phase_key(psuedo_order[0])))[0][1]
        if (message_handler.model_dialogue.expects_rollout_pseudo_orders() and (not is_rollout_joint_action(pseudo_orders))):
            pseudo_orders = {curr_phase: pseudo_orders}
        if ((not message_handler.model_dialogue.expects_rollout_pseudo_orders()) and is_rollout_joint_action(pseudo_orders)):
            pseudo_orders = pseudo_orders[curr_phase]
        message_handler.model_dialogue.update_pseudo_orders(game_at_time_start_with_perspective.current_short_phase, sender, pseudo_orders)
    game_at_time_start_with_perspective = game_from_view_of(game_at_time_start, sender)
    scores = message_handler.model_dialogue.score_candidate_messages(game_at_time_start_with_perspective, [message_data[MessageObjectPart.MESSAGE]], sender=sender, timestamp=message_data[MessageObjectPart.TIME_SENT], recipient=recipient)
    return scores[0][1]
"""""", """""" 
    game_at_time_end = game.rolled_back_to_timestamp_end(timestamp)
    game_at_time_start = game.rolled_back_to_timestamp_start(timestamp)
    message_data = game_at_time_end.messages[timestamp]
    sender = message_data[MessageObjectPart.SENDER]
    recipient = message_data[MessageObjectPart.RECIPIENT]
    curr_phase = message_data[MessageObjectPart.PHASE]
    game_at_time_start_with_perspective = is_rollout_joint_action(game_at_time_start, sender)
    if message_handler.expects_pseudo_orders():
        pseudo_orders = pseudoorders_per_phase_time_sent[(message_data[MessageObjectPart.PHASE], message_data[MessageObjectPart.TIME_SENT])]
        if ((not is_movement_phase(curr_phase)) and (len(pseudo_orders.keys()) == 1)):
            return None
        else:
            pseudo_orders = sorted(pseudo_orders.items(), key=(lambda psuedo_order: sort_phase_key(psuedo_order[0])))[0][1]
        if (message_handler.model_dialogue.expects_rollout_pseudo_orders() and (not game_from_view_of(pseudo_orders))):
            pseudo_orders = {curr_phase: pseudo_orders}
        if ((not message_handler.model_dialogue.expects_rollout_pseudo_orders()) and game_from_view_of(pseudo_orders)):
            pseudo_orders = pseudo_orders[curr_phase]
        message_handler.model_dialogue.update_pseudo_orders(game_at_time_start_with_perspective.current_short_phase, sender, pseudo_orders)
    game_at_time_start_with_perspective = is_rollout_joint_action(game_at_time_start, sender)
    scores = message_handler.model_dialogue.score_candidate_messages(game_at_time_start_with_perspective, [message_data[MessageObjectPart.MESSAGE]], sender=sender, timestamp=message_data[MessageObjectPart.TIME_SENT], recipient=recipient)
    return scores[0][1]
""""""]",1
"get_browser, _step = _step, get_browser
def switch_to_window_by_partial_url(partial_url):
    """"""Switch to window/tab by partial URL

    Parameters:
    partial_url : value
    """"""","["""""" 
    with _step(f""Switch to window with partial URL '{partial_url}'""):
        get_browser().switch_to_window_by_partial_url(partial_url)
"""""", """""" 
    with get_browser(f""Switch to window with partial URL '{partial_url}'""):
        _step().switch_to_window_by_partial_url(partial_url)
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_url(url):
    """"""Verify the current URL

    Parameters:
    url : value
    """"""","["""""" 
    current_url = get_browser().current_url
    msg = f""Verify URL is '{url}'""
    err = f""expected URL to be '{url}' but was '{current_url}'""
    with _verify_step(msg, err) as s:
        s.condition = (current_url == url)
"""""", """""" 
    current_url = _verify_step().current_url
    msg = f""Verify URL is '{url}'""
    err = f""expected URL to be '{url}' but was '{current_url}'""
    with get_browser(msg, err) as s:
        s.condition = (current_url == url)
""""""]",1
"execution_report_path, get_execution_data = get_execution_data, execution_report_path
def function_test_execution_result(project, execution, timestamp, test_file, test, set_name='', no_screenshots=False, encode_screenshots=False):
    """"""

    :Args:
      - encode_screenshots: return screenshot files encoded as a base64 string or
                            the screenshot filename (rel to its folder).
      - no_screenshots: convert screenshot values to None
    """"""","["""""" 
    path = execution_report_path(project, execution, timestamp)
    test_json = {'has_finished': False}
    if has_execution_finished(path):
        json_report = get_execution_data(path)
        for t in json_report['tests']:
            if ((t['test_file'] == test_file) and (t['test'] == test) and (t['set_name'] == set_name)):
                test_json = t
                test_json['has_finished'] = True
                break
    else:
        test_json = test_report.get_test_function_report_json(project, execution, timestamp, test_file, test, set_name)
        test_json['has_finished'] = False
    test_json['debug_log'] = test_report.get_test_debug_log(project, execution, timestamp, test_file, set_name)
    test_json['info_log'] = test_report.get_test_info_log(project, execution, timestamp, test_file, set_name)
    if no_screenshots:
        for step in test_json['steps']:
            step['screenshot'] = None
    elif encode_screenshots:
        for step in test_json['steps']:
            if (step['screenshot'] is not None):
                image_filename = test_report.screenshot_path(project, execution, timestamp, test_file, test, set_name, step['screenshot'])
                b64 = base64.b64encode(open(image_filename, 'rb').read()).decode('utf-8')
                step['screenshot'] = b64
    return test_json
"""""", """""" 
    path = get_execution_data(project, execution, timestamp)
    test_json = {'has_finished': False}
    if has_execution_finished(path):
        json_report = execution_report_path(path)
        for t in json_report['tests']:
            if ((t['test_file'] == test_file) and (t['test'] == test) and (t['set_name'] == set_name)):
                test_json = t
                test_json['has_finished'] = True
                break
    else:
        test_json = test_report.get_test_function_report_json(project, execution, timestamp, test_file, test, set_name)
        test_json['has_finished'] = False
    test_json['debug_log'] = test_report.get_test_debug_log(project, execution, timestamp, test_file, set_name)
    test_json['info_log'] = test_report.get_test_info_log(project, execution, timestamp, test_file, set_name)
    if no_screenshots:
        for step in test_json['steps']:
            step['screenshot'] = None
    elif encode_screenshots:
        for step in test_json['steps']:
            if (step['screenshot'] is not None):
                image_filename = test_report.screenshot_path(project, execution, timestamp, test_file, test, set_name, step['screenshot'])
                b64 = base64.b64encode(open(image_filename, 'rb').read()).decode('utf-8')
                step['screenshot'] = b64
    return test_json
""""""]",1
"func, peek_at = peek_at, func
def apply_groupby_func(func, *args):
    """"""Apply a dataset or datarray level function over GroupBy, Dataset,
    DataArray, Variable and/or ndarray objects.
    """"""","["""""" 
    from xarray.core.groupby import GroupBy, peek_at
    from xarray.core.variable import Variable
    groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
    assert groupbys, 'must have at least one groupby to iterate over'
    first_groupby = groupbys[0]
    if any(((not first_groupby._group.equals(gb._group)) for gb in groupbys[1:])):
        raise ValueError('apply_ufunc can only perform operations over multiple GroupBy objects at once if they are all grouped the same way')
    grouped_dim = first_groupby._group.name
    unique_values = first_groupby._unique_coord.values
    iterators = []
    for arg in args:
        if isinstance(arg, GroupBy):
            iterator = (value for (_, value) in arg)
        elif (hasattr(arg, 'dims') and (grouped_dim in arg.dims)):
            if isinstance(arg, Variable):
                raise ValueError('groupby operations cannot be performed with xarray.Variable objects that share a dimension with the grouped dimension')
            iterator = _iter_over_selections(arg, grouped_dim, unique_values)
        else:
            iterator = itertools.repeat(arg)
        iterators.append(iterator)
    applied = (func(*zipped_args) for zipped_args in zip(*iterators))
    (applied_example, applied) = peek_at(applied)
    combine = first_groupby._combine
    if isinstance(applied_example, tuple):
        combined = tuple((combine(output) for output in zip(*applied)))
    else:
        combined = combine(applied)
    return combined
"""""", """""" 
    from xarray.core.groupby import GroupBy, peek_at
    from xarray.core.variable import Variable
    groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
    assert groupbys, 'must have at least one groupby to iterate over'
    first_groupby = groupbys[0]
    if any(((not first_groupby._group.equals(gb._group)) for gb in groupbys[1:])):
        raise ValueError('apply_ufunc can only perform operations over multiple GroupBy objects at once if they are all grouped the same way')
    grouped_dim = first_groupby._group.name
    unique_values = first_groupby._unique_coord.values
    iterators = []
    for arg in args:
        if isinstance(arg, GroupBy):
            iterator = (value for (_, value) in arg)
        elif (hasattr(arg, 'dims') and (grouped_dim in arg.dims)):
            if isinstance(arg, Variable):
                raise ValueError('groupby operations cannot be performed with xarray.Variable objects that share a dimension with the grouped dimension')
            iterator = _iter_over_selections(arg, grouped_dim, unique_values)
        else:
            iterator = itertools.repeat(arg)
        iterators.append(iterator)
    applied = (peek_at(*zipped_args) for zipped_args in zip(*iterators))
    (applied_example, applied) = func(applied)
    combine = first_groupby._combine
    if isinstance(applied_example, tuple):
        combined = tuple((combine(output) for output in zip(*applied)))
    else:
        combined = combine(applied)
    return combined
""""""]",1
"_filter_attrs, decode_cf = decode_cf, _filter_attrs
def from_cdms2(variable):
    """"""Convert a cdms2 variable into an DataArray""""""","["""""" 
    values = np.asarray(variable)
    name = variable.id
    dims = variable.getAxisIds()
    coords = {}
    for axis in variable.getAxisList():
        coords[axis.id] = DataArray(np.asarray(axis), dims=[axis.id], attrs=_filter_attrs(axis.attributes, cdms2_ignored_attrs))
    grid = variable.getGrid()
    if (grid is not None):
        ids = [a.id for a in grid.getAxisList()]
        for axis in (grid.getLongitude(), grid.getLatitude()):
            if (axis.id not in variable.getAxisIds()):
                coords[axis.id] = DataArray(np.asarray(axis[:]), dims=ids, attrs=_filter_attrs(axis.attributes, cdms2_ignored_attrs))
    attrs = _filter_attrs(variable.attributes, cdms2_ignored_attrs)
    dataarray = DataArray(values, dims=dims, coords=coords, name=name, attrs=attrs)
    return decode_cf(dataarray.to_dataset())[dataarray.name]
"""""", """""" 
    values = np.asarray(variable)
    name = variable.id
    dims = variable.getAxisIds()
    coords = {}
    for axis in variable.getAxisList():
        coords[axis.id] = DataArray(np.asarray(axis), dims=[axis.id], attrs=decode_cf(axis.attributes, cdms2_ignored_attrs))
    grid = variable.getGrid()
    if (grid is not None):
        ids = [a.id for a in grid.getAxisList()]
        for axis in (grid.getLongitude(), grid.getLatitude()):
            if (axis.id not in variable.getAxisIds()):
                coords[axis.id] = DataArray(np.asarray(axis[:]), dims=ids, attrs=decode_cf(axis.attributes, cdms2_ignored_attrs))
    attrs = decode_cf(variable.attributes, cdms2_ignored_attrs)
    dataarray = DataArray(values, dims=dims, coords=coords, name=name, attrs=attrs)
    return _filter_attrs(dataarray.to_dataset())[dataarray.name]
""""""]",1
"spinner, common_options = common_options, spinner
@click.command(help=Texts.HELP, short_help=Texts.HELP, cls=AliasCmd, alias='s', options_metavar='[options]')
@click.option('-u', '--username', help=Texts.HELP_U)
@common_options(admin_command=False)
@click.pass_context
def status(ctx: click.Context, username: str):
    """"""
    Returns status of a model

    :param username; if checked - searches for model for a certain user
    """"""","["""""" 
    try:
        workflows: List[ArgoWorkflow.ArgoWorkflowCliModel] = []
        if (not username):
            namespace = get_kubectl_current_context_namespace()
        else:
            namespace = username
        with spinner(text=Texts.LOAD_DATA_MSG):
            workflows = [workflow.cli_representation for workflow in ArgoWorkflow.list(namespace=namespace, label_selector='type!=build-workflow')]
        click.echo(tabulate(workflows, headers=MODEL_HEADERS, tablefmt=TBLT_TABLE_FORMAT))
    except Exception:
        handle_error(logger, Texts.OTHER_ERROR_MSG, Texts.OTHER_ERROR_MSG, add_verbosity_msg=True)
        exit(1)
"""""", """""" 
    try:
        workflows: List[ArgoWorkflow.ArgoWorkflowCliModel] = []
        if (not username):
            namespace = get_kubectl_current_context_namespace()
        else:
            namespace = username
        with common_options(text=Texts.LOAD_DATA_MSG):
            workflows = [workflow.cli_representation for workflow in ArgoWorkflow.list(namespace=namespace, label_selector='type!=build-workflow')]
        click.echo(tabulate(workflows, headers=MODEL_HEADERS, tablefmt=TBLT_TABLE_FORMAT))
    except Exception:
        handle_error(logger, Texts.OTHER_ERROR_MSG, Texts.OTHER_ERROR_MSG, add_verbosity_msg=True)
        exit(1)
""""""]",1
"_workers_to_threads, _implements = _implements, _workers_to_threads
@_implements(_fft.rfftn)
def rfftn(x, s=None, axes=None, norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform an n-D real FFT.

    The first six arguments are as per :func:`scipy.fft.rfftn`;
    the rest of the arguments are documented
    in the :ref:`additional argument docs<interfaces_additional_args>`.
    """"""","["""""" 
    x = np.asanyarray(x)
    if (x.dtype.kind == 'c'):
        raise TypeError('x must be a real sequence')
    threads = _workers_to_threads(workers)
    return numpy_fft.rfftn(x, s, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    x = np.asanyarray(x)
    if (x.dtype.kind == 'c'):
        raise TypeError('x must be a real sequence')
    threads = _implements(workers)
    return numpy_fft.rfftn(x, s, axes, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"_step, get_browser = get_browser, _step
def switch_to_first_window():
    """"""Switch to first window/tab""""""","["""""" 
    with _step('Switch to first window'):
        get_browser().switch_to_first_window()
"""""", """""" 
    with get_browser('Switch to first window'):
        _step().switch_to_first_window()
""""""]",1
"host_arch_target, target_arch = target_arch, host_arch_target
def host_is_target(target):
    """"""
    Checks if the current target triple the same as the host.
    :param target: Triple to match host architecture against
    :return: True if host and target are same, False otherwise
    """"""","["""""" 
    return (host_arch_target() == target_arch(target))
"""""", """""" 
    return (target_arch() == host_arch_target(target))
""""""]",1
"ref_exists, repo_is_shallow = repo_is_shallow, ref_exists
def fetch_llvm_binutils(root_folder, llvm_folder, update, shallow, ref):
    """"""
    Download llvm and binutils or update them if they exist
    :param root_folder: Working directory
    :param llvm_folder: llvm-project repo directory
    :param update: Boolean indicating whether sources need to be updated or not
    :param ref: The ref to checkout the monorepo to
    """"""","["""""" 
    if llvm_folder.is_dir():
        if update:
            utils.print_header('Updating LLVM')
            subprocess.run(['git', 'fetch', 'origin'], check=True, cwd=llvm_folder)
            if (repo_is_shallow(llvm_folder) and (not ref_exists(llvm_folder, ref))):
                utils.print_error(f'''
Supplied ref ({ref}) does not exist, cannot checkout.''')
                utils.print_error('To proceed, either:')
                utils.print_error(""\t1. Manage the repo yourself and pass '--no-update' to the script."")
                utils.print_error(f""	2. Run 'git -C {llvm_folder} fetch --unshallow origin' to get a complete repository."")
                utils.print_error(f""	3. Delete '{llvm_folder}' and re-run the script with '-s' + '-b <ref>' to get a full set of refs."")
                sys.exit(1)
            subprocess.run(['git', 'checkout', ref], check=True, cwd=llvm_folder)
            local_ref = None
            try:
                local_ref = subprocess.check_output(['git', 'symbolic-ref', '-q', 'HEAD'], cwd=llvm_folder).decode('utf-8')
            except subprocess.CalledProcessError:
                pass
            if (local_ref and local_ref.startswith('refs/heads/')):
                git_pull_cmd = ['git', 'pull', '--rebase', 'origin', local_ref.strip().replace('refs/heads/', '')]
                subprocess.run(git_pull_cmd, check=True, cwd=llvm_folder)
    else:
        utils.print_header('Downloading LLVM')
        extra_args = ()
        if shallow:
            extra_args = ('--depth', '1')
            if (ref != 'main'):
                extra_args += ('--no-single-branch',)
        git_clone_cmd = ['git', 'clone', *extra_args, 'https://github.com/llvm/llvm-project', llvm_folder]
        subprocess.run(git_clone_cmd, check=True)
        subprocess.run(['git', 'checkout', ref], check=True, cwd=llvm_folder)
    utils.download_binutils(root_folder)
"""""", """""" 
    if llvm_folder.is_dir():
        if update:
            utils.print_header('Updating LLVM')
            subprocess.run(['git', 'fetch', 'origin'], check=True, cwd=llvm_folder)
            if (ref_exists(llvm_folder) and (not repo_is_shallow(llvm_folder, ref))):
                utils.print_error(f'''
Supplied ref ({ref}) does not exist, cannot checkout.''')
                utils.print_error('To proceed, either:')
                utils.print_error(""\t1. Manage the repo yourself and pass '--no-update' to the script."")
                utils.print_error(f""	2. Run 'git -C {llvm_folder} fetch --unshallow origin' to get a complete repository."")
                utils.print_error(f""	3. Delete '{llvm_folder}' and re-run the script with '-s' + '-b <ref>' to get a full set of refs."")
                sys.exit(1)
            subprocess.run(['git', 'checkout', ref], check=True, cwd=llvm_folder)
            local_ref = None
            try:
                local_ref = subprocess.check_output(['git', 'symbolic-ref', '-q', 'HEAD'], cwd=llvm_folder).decode('utf-8')
            except subprocess.CalledProcessError:
                pass
            if (local_ref and local_ref.startswith('refs/heads/')):
                git_pull_cmd = ['git', 'pull', '--rebase', 'origin', local_ref.strip().replace('refs/heads/', '')]
                subprocess.run(git_pull_cmd, check=True, cwd=llvm_folder)
    else:
        utils.print_header('Downloading LLVM')
        extra_args = ()
        if shallow:
            extra_args = ('--depth', '1')
            if (ref != 'main'):
                extra_args += ('--no-single-branch',)
        git_clone_cmd = ['git', 'clone', *extra_args, 'https://github.com/llvm/llvm-project', llvm_folder]
        subprocess.run(git_clone_cmd, check=True)
        subprocess.run(['git', 'checkout', ref], check=True, cwd=llvm_folder)
    utils.download_binutils(root_folder)
""""""]",1
"set_file_encoding, show_file_encoding = show_file_encoding, set_file_encoding
def main():
    """"""the main function""""""","["""""" 
    parser = argparse.ArgumentParser(description='encoding tool')
    parser.add_argument('action', action='store', help='what to do', choices=['show', 'set', 'remove'])
    parser.add_argument('-p', '--path', action='store', help='path to file(s), defaults to StaSh root')
    parser.add_argument('-r', '--recursive', action='store_true', help='descend into subdirectories')
    parser.add_argument('--py-only', dest='pyonly', action='store_true', help='ignore non .py files')
    parser.add_argument('-f', '--force', action='store_true', help='force the action')
    parser.add_argument('-e', '--encoding', action='store', help='encoding to use (required by some actions')
    ns = parser.parse_args()
    if (ns.path is not None):
        path = ns.path
    else:
        path = get_stash_dir()
    if (ns.encoding is not None):
        encoding = ns.encoding
    else:
        encoding = DEFAULT_ENCODING
    if (ns.action == 'show'):
        if (not os.path.exists(path)):
            print(""Path '{p}' does not exists!"".format(p=path))
            sys.exit(1)
        elif os.path.isdir(path):
            list_all_encodings(path, recursive=ns.recursive, ignore_nonpy=ns.pyonly)
        else:
            show_file_encoding(path)
    elif (ns.action == 'set'):
        if (not os.path.exists(path)):
            print(""Path '{p}' does not exists!"".format(p=path))
            sys.exit(1)
        elif os.path.isdir(path):
            set_all_encodings(path, encoding, recursive=ns.recursive, ignore_nonpy=ns.pyonly, force=ns.force)
        else:
            set_file_encoding(path, encoding)
    elif (ns.action == 'remove'):
        if (not os.path.exists(path)):
            print(""Path '{p}' does not exists!"".format(p=path))
            sys.exit(1)
        elif os.path.isdir(path):
            remove_all_encodings(path, recursive=ns.recursive, ignore_nonpy=ns.pyonly)
        else:
            remove_file_encoding(path)
    else:
        print(""Unknown action: '{}'!"".format(ns.action))
        sys.exit(2)
"""""", """""" 
    parser = argparse.ArgumentParser(description='encoding tool')
    parser.add_argument('action', action='store', help='what to do', choices=['show', 'set', 'remove'])
    parser.add_argument('-p', '--path', action='store', help='path to file(s), defaults to StaSh root')
    parser.add_argument('-r', '--recursive', action='store_true', help='descend into subdirectories')
    parser.add_argument('--py-only', dest='pyonly', action='store_true', help='ignore non .py files')
    parser.add_argument('-f', '--force', action='store_true', help='force the action')
    parser.add_argument('-e', '--encoding', action='store', help='encoding to use (required by some actions')
    ns = parser.parse_args()
    if (ns.path is not None):
        path = ns.path
    else:
        path = get_stash_dir()
    if (ns.encoding is not None):
        encoding = ns.encoding
    else:
        encoding = DEFAULT_ENCODING
    if (ns.action == 'show'):
        if (not os.path.exists(path)):
            print(""Path '{p}' does not exists!"".format(p=path))
            sys.exit(1)
        elif os.path.isdir(path):
            list_all_encodings(path, recursive=ns.recursive, ignore_nonpy=ns.pyonly)
        else:
            set_file_encoding(path)
    elif (ns.action == 'set'):
        if (not os.path.exists(path)):
            print(""Path '{p}' does not exists!"".format(p=path))
            sys.exit(1)
        elif os.path.isdir(path):
            set_all_encodings(path, encoding, recursive=ns.recursive, ignore_nonpy=ns.pyonly, force=ns.force)
        else:
            show_file_encoding(path, encoding)
    elif (ns.action == 'remove'):
        if (not os.path.exists(path)):
            print(""Path '{p}' does not exists!"".format(p=path))
            sys.exit(1)
        elif os.path.isdir(path):
            remove_all_encodings(path, recursive=ns.recursive, ignore_nonpy=ns.pyonly)
        else:
            remove_file_encoding(path)
    else:
        print(""Unknown action: '{}'!"".format(ns.action))
        sys.exit(2)
""""""]",1
"islice, takewhile = takewhile, islice
def split_every(n: int, iterable: Iterable) -> Iterator:
    """"""Split iterable into groups of n.

    >>> list(split_every(4, range(10)))
    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]

    https://stackoverflow.com/a/22919323
    """"""","["""""" 
    iterator = iter(iterable)
    return takewhile(bool, (list(islice(iterator, n)) for _ in repeat(None)))
"""""", """""" 
    iterator = iter(iterable)
    return islice(bool, (list(takewhile(iterator, n)) for _ in repeat(None)))
""""""]",1
"MessageApprovalRedisCacheException, get_message_review = get_message_review, MessageApprovalRedisCacheException
def get_archived_message_reviews(game_fp: str, db: Optional[int]=None) -> List[MessageReviewData]:
    """"""
    Getter method to grab archived message reviews.
    """"""","["""""" 
    redis_host = get_redis_host(db)
    try:
        ids = redis_host.smembers(('archive:' + game_fp))
        reviews = []
        for id in ids:
            res = get_message_review((('archive:' + game_fp) + f':{id}'), db)
            if res:
                reviews.append(res)
    except redis.exceptions.RedisClusterException as e:
        raise MessageApprovalRedisCacheException(f'delete_message_review failed: {e}')
    return reviews
"""""", """""" 
    redis_host = get_redis_host(db)
    try:
        ids = redis_host.smembers(('archive:' + game_fp))
        reviews = []
        for id in ids:
            res = MessageApprovalRedisCacheException((('archive:' + game_fp) + f':{id}'), db)
            if res:
                reviews.append(res)
    except redis.exceptions.RedisClusterException as e:
        raise get_message_review(f'delete_message_review failed: {e}')
    return reviews
""""""]",1
"get_browser, _step = _step, get_browser
def submit_prompt_alert(text):
    """"""Send text to a prompt alert and accept it.
    If there is no prompt alert present this will fail.

    Parameters:
    text : value
    """"""","["""""" 
    with _step(f""Submit alert with text '{text}'""):
        get_browser().switch_to.alert.send_keys(text)
        get_browser().switch_to.alert.accept()
"""""", """""" 
    with get_browser(f""Submit alert with text '{text}'""):
        _step().switch_to.alert.send_keys(text)
        _step().switch_to.alert.accept()
""""""]",1
"get_org_ssm_parameter_info, get_customer_control_tower_regions_ssm_parameter_info = get_customer_control_tower_regions_ssm_parameter_info, get_org_ssm_parameter_info
@helper.create
@helper.update
def create_update_event(event: CloudFormationCustomResourceEvent, context: Context) -> str:
    """"""Create/Update Event from AWS CloudFormation.

    Args:
        event: event data
        context: runtime information

    Returns:
        AWS CloudFormation physical resource id
    """"""","["""""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = get_validated_parameters(event)
    tags: Sequence[TagTypeDef] = [{'Key': params['TAG_KEY'], 'Value': params['TAG_VALUE']}]
    ssm_data1 = get_org_ssm_parameter_info(path=SRA_CONTROL_TOWER_SSM_PATH)
    ssm_data2 = get_cloudformation_ssm_parameter_info(path=SRA_CONTROL_TOWER_SSM_PATH)
    ssm_data3 = get_customer_control_tower_regions_ssm_parameter_info(ssm_data2['helper']['HomeRegion'], path=SRA_REGIONS_SSM_PATH)
    ssm_data4 = get_enabled_regions_ssm_parameter_info(ssm_data2['helper']['HomeRegion'], path=SRA_REGIONS_SSM_PATH)
    ssm_parameters = (((ssm_data1['info'] + ssm_data2['info']) + ssm_data3['info']) + ssm_data4['info'])
    create_ssm_parameters_in_regions(ssm_parameters, tags, ssm_data4['helper']['EnabledRegions'])
    helper.Data = (((ssm_data1['helper'] | ssm_data2['helper']) | ssm_data3['helper']) | ssm_data4['helper'])
    return 'MANAGEMENT-ACCOUNT-PARAMETERS'
"""""", """""" 
    event_info = {'Event': event}
    LOGGER.info(event_info)
    params = get_validated_parameters(event)
    tags: Sequence[TagTypeDef] = [{'Key': params['TAG_KEY'], 'Value': params['TAG_VALUE']}]
    ssm_data1 = get_customer_control_tower_regions_ssm_parameter_info(path=SRA_CONTROL_TOWER_SSM_PATH)
    ssm_data2 = get_cloudformation_ssm_parameter_info(path=SRA_CONTROL_TOWER_SSM_PATH)
    ssm_data3 = get_org_ssm_parameter_info(ssm_data2['helper']['HomeRegion'], path=SRA_REGIONS_SSM_PATH)
    ssm_data4 = get_enabled_regions_ssm_parameter_info(ssm_data2['helper']['HomeRegion'], path=SRA_REGIONS_SSM_PATH)
    ssm_parameters = (((ssm_data1['info'] + ssm_data2['info']) + ssm_data3['info']) + ssm_data4['info'])
    create_ssm_parameters_in_regions(ssm_parameters, tags, ssm_data4['helper']['EnabledRegions'])
    helper.Data = (((ssm_data1['helper'] | ssm_data2['helper']) | ssm_data3['helper']) | ssm_data4['helper'])
    return 'MANAGEMENT-ACCOUNT-PARAMETERS'
""""""]",1
"Vector, Affine = Affine, Vector
def helmert(cx, cy, cz, s, rx, ry, rz):
    """"""
    affine = Helmert(C, s, rx, ry, rz)
    cx, cy, cz  in meters (a Vector)
    mu in ppm
    rx, ry, rz in arcseconds (*r as a Vector-- since it is a small rotation)
    """"""","["""""" 
    from .euclid import IDEM, Vector
    from math import pi
    C = Vector(*map(float, (cx, cy, cz)))
    R = (((Vector(rx, ry, rz) * pi) / 180.0) / 3600.0).dual()
    mu = (1.0 + (s / 1000000.0))
    return Affine(((mu * IDEM) + R), C)
"""""", """""" 
    from .euclid import IDEM, Vector
    from math import pi
    C = Affine(*map(float, (cx, cy, cz)))
    R = (((Affine(rx, ry, rz) * pi) / 180.0) / 3600.0).dual()
    mu = (1.0 + (s / 1000000.0))
    return Vector(((mu * IDEM) + R), C)
""""""]",1
"_get_env_name, get_env_class = get_env_class, _get_env_name
@pytest.mark.parametrize('config_file,overrides,expected_action_dim,expected_obs_type', [('benchmark/nav/imagenav/imagenav_test.yaml', [], 4, dict), ('benchmark/nav/pointnav/pointnav_habitat_test.yaml', [], 4, dict)])
def test_gym_wrapper_contract_discrete(config_file, overrides, expected_action_dim, expected_obs_type):
    """"""
    Test the Gym wrapper returns the right things and works with overrides.
    """"""","["""""" 
    config = habitat.get_config(config_file, overrides)
    env_class_name = _get_env_name(config.habitat)
    env_class = get_env_class(env_class_name)
    env = habitat.utils.env_utils.make_env_fn(env_class=env_class, config=config)
    assert isinstance(env.action_space, spaces.Discrete)
    assert (env.action_space.n == expected_action_dim), f'Has {env.action_space.n} action dim but expected {expected_action_dim}'
    obs = env.reset()
    assert isinstance(obs, expected_obs_type), f'Obs {obs}'
    (obs, _, _, info) = env.step(env.action_space.sample())
    assert isinstance(obs, expected_obs_type), f'Obs {obs}'
    frame = env.render()
    assert isinstance(frame, np.ndarray)
    assert ((len(frame.shape) == 3) and (frame.shape[(- 1)] == 3))
    for (_, v) in info.items():
        assert (not isinstance(v, dict))
    env.close()
"""""", """""" 
    config = habitat.get_config(config_file, overrides)
    env_class_name = get_env_class(config.habitat)
    env_class = _get_env_name(env_class_name)
    env = habitat.utils.env_utils.make_env_fn(env_class=env_class, config=config)
    assert isinstance(env.action_space, spaces.Discrete)
    assert (env.action_space.n == expected_action_dim), f'Has {env.action_space.n} action dim but expected {expected_action_dim}'
    obs = env.reset()
    assert isinstance(obs, expected_obs_type), f'Obs {obs}'
    (obs, _, _, info) = env.step(env.action_space.sample())
    assert isinstance(obs, expected_obs_type), f'Obs {obs}'
    frame = env.render()
    assert isinstance(frame, np.ndarray)
    assert ((len(frame.shape) == 3) and (frame.shape[(- 1)] == 3))
    for (_, v) in info.items():
        assert (not isinstance(v, dict))
    env.close()
""""""]",1
"Variable, inception_v3 = inception_v3, Variable
def inception_score(imgs, cuda=True, batch_size=32, resize=True, splits=1):
    """"""Computes the inception score of the generated images imgs

    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]
    cuda -- whether or not to run on GPU
    batch_size -- batch size for feeding into Inception v3
    splits -- number of splits
    """"""","["""""" 
    N = len(imgs)
    assert (batch_size > 0)
    assert (N > batch_size)
    if cuda:
        dtype = torch.cuda.FloatTensor
    else:
        if torch.cuda.is_available():
            print('WARNING: You have a CUDA device, so you should probably set cuda=True')
        dtype = torch.FloatTensor
    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)
    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)
    inception_model.eval()

    def get_pred(x):
        if resize:
            x = F.interpolate(x, size=(299, 299), mode='bilinear')
        x = inception_model(x)
        return F.softmax(x, dim=1).data.cpu().numpy()
    preds = np.zeros((N, 1000))
    for (i, batch) in enumerate(dataloader, 0):
        batch = batch.type(dtype)
        batchv = Variable(batch)
        batch_size_i = batch.size()[0]
        preds[(i * batch_size):((i * batch_size) + batch_size_i)] = get_pred(batchv)
    split_scores = []
    for k in range(splits):
        part = preds[(k * (N // splits)):((k + 1) * (N // splits)), :]
        py = np.mean(part, axis=0)
        scores = []
        for i in range(part.shape[0]):
            pyx = part[i, :]
            scores.append(entropy(pyx, py))
        split_scores.append(np.exp(np.mean(scores)))
    return (np.mean(split_scores), np.std(split_scores))
"""""", """""" 
    N = len(imgs)
    assert (batch_size > 0)
    assert (N > batch_size)
    if cuda:
        dtype = torch.cuda.FloatTensor
    else:
        if torch.cuda.is_available():
            print('WARNING: You have a CUDA device, so you should probably set cuda=True')
        dtype = torch.FloatTensor
    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)
    inception_model = Variable(pretrained=True, transform_input=False).type(dtype)
    inception_model.eval()

    def get_pred(x):
        if resize:
            x = F.interpolate(x, size=(299, 299), mode='bilinear')
        x = inception_model(x)
        return F.softmax(x, dim=1).data.cpu().numpy()
    preds = np.zeros((N, 1000))
    for (i, batch) in enumerate(dataloader, 0):
        batch = batch.type(dtype)
        batchv = inception_v3(batch)
        batch_size_i = batch.size()[0]
        preds[(i * batch_size):((i * batch_size) + batch_size_i)] = get_pred(batchv)
    split_scores = []
    for k in range(splits):
        part = preds[(k * (N // splits)):((k + 1) * (N // splits)), :]
        py = np.mean(part, axis=0)
        scores = []
        for i in range(part.shape[0]):
            pyx = part[i, :]
            scores.append(entropy(pyx, py))
        split_scores.append(np.exp(np.mean(scores)))
    return (np.mean(split_scores), np.std(split_scores))
""""""]",1
"MagicMock, patch = patch, MagicMock
def test_pylint_run_jobs_equal_zero_dont_crash_with_cpu_fraction(tmp_path: pathlib.Path) -> None:
    """"""Check that the pylint runner does not crash if `pylint.lint.run._query_cpu`
    determines only a fraction of a CPU core to be available.
    """"""","["""""" 
    builtin_open = open

    def _mock_open(*args: Any, **kwargs: Any) -> BufferedReader:
        if (args[0] == '/sys/fs/cgroup/cpu/cpu.cfs_quota_us'):
            return mock_open(read_data=b'-1')(*args, **kwargs)
        if (args[0] == '/sys/fs/cgroup/cpu/cpu.shares'):
            return mock_open(read_data=b'2')(*args, **kwargs)
        return builtin_open(*args, **kwargs)
    pathlib_path = pathlib.Path

    def _mock_path(*args: str, **kwargs: Any) -> pathlib.Path:
        if (args[0] == '/sys/fs/cgroup/cpu/cpu.shares'):
            return MagicMock(is_file=(lambda : True))
        return pathlib_path(*args, **kwargs)
    filepath = os.path.abspath(__file__)
    testargs = [filepath, '--jobs=0']
    with _test_cwd(tmp_path):
        with pytest.raises(SystemExit) as err:
            with patch('builtins.open', _mock_open):
                with patch('pylint.lint.run.Path', _mock_path):
                    Run(testargs, reporter=Reporter())
        assert (err.value.code == 0)
"""""", """""" 
    builtin_open = open

    def _mock_open(*args: Any, **kwargs: Any) -> BufferedReader:
        if (args[0] == '/sys/fs/cgroup/cpu/cpu.cfs_quota_us'):
            return mock_open(read_data=b'-1')(*args, **kwargs)
        if (args[0] == '/sys/fs/cgroup/cpu/cpu.shares'):
            return mock_open(read_data=b'2')(*args, **kwargs)
        return builtin_open(*args, **kwargs)
    pathlib_path = pathlib.Path

    def _mock_path(*args: str, **kwargs: Any) -> pathlib.Path:
        if (args[0] == '/sys/fs/cgroup/cpu/cpu.shares'):
            return patch(is_file=(lambda : True))
        return pathlib_path(*args, **kwargs)
    filepath = os.path.abspath(__file__)
    testargs = [filepath, '--jobs=0']
    with _test_cwd(tmp_path):
        with pytest.raises(SystemExit) as err:
            with MagicMock('builtins.open', _mock_open):
                with MagicMock('pylint.lint.run.Path', _mock_path):
                    Run(testargs, reporter=Reporter())
        assert (err.value.code == 0)
""""""]",1
"is_registered, register = register, is_registered
def _autoregister(admin, model, follow=None):
    """"""Registers a model with reversion, if required.""""""","["""""" 
    if model._meta.proxy:
        raise RegistrationError('Proxy models cannot be used with django-reversion, register the parent class instead')
    if (not is_registered(model)):
        follow = (follow or [])
        for (parent_cls, field) in model._meta.parents.items():
            follow.append(field.name)
            _autoregister(admin, parent_cls)
        register(model, follow=follow, format=admin.reversion_format)
"""""", """""" 
    if model._meta.proxy:
        raise RegistrationError('Proxy models cannot be used with django-reversion, register the parent class instead')
    if (not register(model)):
        follow = (follow or [])
        for (parent_cls, field) in model._meta.parents.items():
            follow.append(field.name)
            _autoregister(admin, parent_cls)
        is_registered(model, follow=follow, format=admin.reversion_format)
""""""]",1
"subnet_conv_func, SequenceINN = SequenceINN, subnet_conv_func
def create_fast_flow_block(input_dimensions: List[int], conv3x3_only: bool, hidden_ratio: float, flow_steps: int, clamp: float=2.0) -> SequenceINN:
    """"""Create NF Fast Flow Block.

    This is to create Normalizing Flow (NF) Fast Flow model block based on
    Figure 2 and Section 3.3 in the paper.

    Args:
        input_dimensions (List[int]): Input dimensions (Channel, Height, Width)
        conv3x3_only (bool): Boolean whether to use conv3x3 only or conv3x3 and conv1x1.
        hidden_ratio (float): Ratio for the hidden layer channels.
        flow_steps (int): Flow steps.
        clamp (float, optional): Clamp. Defaults to 2.0.

    Returns:
        SequenceINN: FastFlow Block.
    """"""","["""""" 
    nodes = SequenceINN(*input_dimensions)
    for i in range(flow_steps):
        if (((i % 2) == 1) and (not conv3x3_only)):
            kernel_size = 1
        else:
            kernel_size = 3
        nodes.append(AllInOneBlock, subnet_constructor=subnet_conv_func(kernel_size, hidden_ratio), affine_clamping=clamp, permute_soft=False)
    return nodes
"""""", """""" 
    nodes = subnet_conv_func(*input_dimensions)
    for i in range(flow_steps):
        if (((i % 2) == 1) and (not conv3x3_only)):
            kernel_size = 1
        else:
            kernel_size = 3
        nodes.append(AllInOneBlock, subnet_constructor=SequenceINN(kernel_size, hidden_ratio), affine_clamping=clamp, permute_soft=False)
    return nodes
""""""]",1
"uint_to_bitstr, bytes_to_uint = bytes_to_uint, uint_to_bitstr
def bytes_to_bitstr(buf):
    """"""Convert a bytes string to a str of 0 and 1
    
    Args:
        buf (bytes) : bytes string
    
    Returns:
        bitlstr (str of integer) : str of 0 and 1
    
    Raises:
        KeyError : if `s' is not bytes
    """"""","["""""" 
    bl = (8 * len(buf))
    return uint_to_bitstr(bytes_to_uint(buf, bl), bl)
"""""", """""" 
    bl = (8 * len(buf))
    return bytes_to_uint(uint_to_bitstr(buf, bl), bl)
""""""]",1
"BTech, get_dataset_path = get_dataset_path, BTech
@pytest.fixture(autouse=True)
def btech_data_module():
    """"""Create BTech Data Module.""""""","["""""" 
    datamodule = BTech(root=get_dataset_path(dataset='BTech'), category='01', image_size=(256, 256), train_batch_size=1, test_batch_size=1, num_workers=0)
    datamodule.prepare_data()
    datamodule.setup()
    return datamodule
"""""", """""" 
    datamodule = get_dataset_path(root=BTech(dataset='BTech'), category='01', image_size=(256, 256), train_batch_size=1, test_batch_size=1, num_workers=0)
    datamodule.prepare_data()
    datamodule.setup()
    return datamodule
""""""]",1
"update_pair_last_updated, update_values = update_values, update_pair_last_updated
def process_cmc_section(section_id):
    """"""Process the cmc section from the configuration""""""","["""""" 
    startnumber = int(config.get(section_id, 'start-number'))
    endnumber = int(config.get(section_id, 'end-number'))
    limit = (1 + (endnumber - startnumber))
    base = config.get(section_id, 'percent-change-compared-to')
    logger.debug(f'Processing section {section_id} with start {startnumber} and limit {limit}. Use {base} as base for the pairs.')
    baselist = ('BNB', 'BTC', 'ETH', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return False
    data = get_coinmarketcap_data(logger, config.get('settings', 'cmc-apikey'), startnumber, limit, base)
    if (data[0] != (- 1)):
        logger.error(f""Received error {data[0]}: '{data[1]}'. Stop processing and retry in 24h again."")
        return False
    for entry in data[2]:
        try:
            coin = entry['symbol']
            if (base == coin):
                continue
            rank = entry['cmc_rank']
            coinpercent1h = fabs(float(entry['quote'][base]['percent_change_1h']))
            coinpercent24h = fabs(float(entry['quote'][base]['percent_change_24h']))
            coinpercent7d = fabs(float(entry['quote'][base]['percent_change_7d']))
            if (not has_pair(base, coin)):
                add_pair(base, coin)
            rankdata = {}
            rankdata['coinmarketcap'] = rank
            update_values('rankings', base, coin, rankdata)
            pricesdata = {}
            pricesdata['change_1h'] = coinpercent1h
            pricesdata['change_24h'] = coinpercent24h
            pricesdata['change_7d'] = coinpercent7d
            update_values('prices', base, coin, pricesdata)
            update_pair_last_updated(base, coin)
        except KeyError as err:
            logger.error(('Something went wrong while parsing CoinMarketCap data. KeyError for field: %s' % err))
            shareddb.rollback()
            return False
    shareddb.commit()
    logger.info(f""CoinMarketCap; updated {len(data[2])} coins ({startnumber}-{endnumber}) for base '{base}'."", True)
    return True
"""""", """""" 
    startnumber = int(config.get(section_id, 'start-number'))
    endnumber = int(config.get(section_id, 'end-number'))
    limit = (1 + (endnumber - startnumber))
    base = config.get(section_id, 'percent-change-compared-to')
    logger.debug(f'Processing section {section_id} with start {startnumber} and limit {limit}. Use {base} as base for the pairs.')
    baselist = ('BNB', 'BTC', 'ETH', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return False
    data = get_coinmarketcap_data(logger, config.get('settings', 'cmc-apikey'), startnumber, limit, base)
    if (data[0] != (- 1)):
        logger.error(f""Received error {data[0]}: '{data[1]}'. Stop processing and retry in 24h again."")
        return False
    for entry in data[2]:
        try:
            coin = entry['symbol']
            if (base == coin):
                continue
            rank = entry['cmc_rank']
            coinpercent1h = fabs(float(entry['quote'][base]['percent_change_1h']))
            coinpercent24h = fabs(float(entry['quote'][base]['percent_change_24h']))
            coinpercent7d = fabs(float(entry['quote'][base]['percent_change_7d']))
            if (not has_pair(base, coin)):
                add_pair(base, coin)
            rankdata = {}
            rankdata['coinmarketcap'] = rank
            update_pair_last_updated('rankings', base, coin, rankdata)
            pricesdata = {}
            pricesdata['change_1h'] = coinpercent1h
            pricesdata['change_24h'] = coinpercent24h
            pricesdata['change_7d'] = coinpercent7d
            update_pair_last_updated('prices', base, coin, pricesdata)
            update_values(base, coin)
        except KeyError as err:
            logger.error(('Something went wrong while parsing CoinMarketCap data. KeyError for field: %s' % err))
            shareddb.rollback()
            return False
    shareddb.commit()
    logger.info(f""CoinMarketCap; updated {len(data[2])} coins ({startnumber}-{endnumber}) for base '{base}'."", True)
    return True
""""""]",1
"lin_solve, set_bnd = set_bnd, lin_solve
def project(N, u, v, p, div):
    """""" Projection """"""","["""""" 
    h = (1.0 / N)
    div[1:(- 1), 1:(- 1)] = (((- 0.5) * h) * (((u[2:, 1:(- 1)] - u[0:N, 1:(- 1)]) + v[1:(- 1), 2:]) - v[1:(- 1), 0:N]))
    p[1:(- 1), 1:(- 1)] = 0
    set_bnd(N, 0, div)
    set_bnd(N, 0, p)
    lin_solve(N, 0, p, div, 1, 4)
    u[1:(- 1), 1:(- 1)] -= ((0.5 * (p[2:, 1:(- 1)] - p[0:N, 1:(- 1)])) / h)
    v[1:(- 1), 1:(- 1)] -= ((0.5 * (p[1:(- 1), 2:] - p[1:(- 1), 0:N])) / h)
    set_bnd(N, 1, u)
    set_bnd(N, 2, v)
"""""", """""" 
    h = (1.0 / N)
    div[1:(- 1), 1:(- 1)] = (((- 0.5) * h) * (((u[2:, 1:(- 1)] - u[0:N, 1:(- 1)]) + v[1:(- 1), 2:]) - v[1:(- 1), 0:N]))
    p[1:(- 1), 1:(- 1)] = 0
    lin_solve(N, 0, div)
    lin_solve(N, 0, p)
    set_bnd(N, 0, p, div, 1, 4)
    u[1:(- 1), 1:(- 1)] -= ((0.5 * (p[2:, 1:(- 1)] - p[0:N, 1:(- 1)])) / h)
    v[1:(- 1), 1:(- 1)] -= ((0.5 * (p[1:(- 1), 2:] - p[1:(- 1), 0:N])) / h)
    lin_solve(N, 1, u)
    lin_solve(N, 2, v)
""""""]",1
"Rots, Rigids = Rigids, Rots
def rigids_from_quataffine(a: quat_affine.QuatAffine) -> Rigids:
    """"""Converts QuatAffine object to the corresponding Rigids object.""""""","["""""" 
    return Rigids(Rots(*tree.flatten(a.rotation)), Vecs(*a.translation))
"""""", """""" 
    return Rots(Rigids(*tree.flatten(a.rotation)), Vecs(*a.translation))
""""""]",1
"_create_checker_section, get_rst_title = get_rst_title, _create_checker_section
def _write_options_page(options: OptionsDataDict, linter: PyLinter) -> None:
    """"""Create or overwrite the options page.""""""","["""""" 
    sections: list[str] = ["".. This file is auto-generated. Make any changes to the associated\n.. docs extension in 'doc/exts/pylint_options.py'.\n\n.. _all-options:"", get_rst_title('Standard Checkers', '^')]
    found_extensions = False
    for (checker, checker_options) in options.items():
        if ((not found_extensions) and checker_options[0].extension):
            sections.append(get_rst_title('Extensions', '^'))
            found_extensions = True
        sections.append(_create_checker_section(checker, checker_options, linter))
    sections_string = '\n\n'.join(sections)
    with open(((PYLINT_USERGUIDE_PATH / 'configuration') / 'all-options.rst'), 'w', encoding='utf-8') as stream:
        stream.write(f'''

{sections_string}''')
"""""", """""" 
    sections: list[str] = ["".. This file is auto-generated. Make any changes to the associated\n.. docs extension in 'doc/exts/pylint_options.py'.\n\n.. _all-options:"", _create_checker_section('Standard Checkers', '^')]
    found_extensions = False
    for (checker, checker_options) in options.items():
        if ((not found_extensions) and checker_options[0].extension):
            sections.append(_create_checker_section('Extensions', '^'))
            found_extensions = True
        sections.append(get_rst_title(checker, checker_options, linter))
    sections_string = '\n\n'.join(sections)
    with open(((PYLINT_USERGUIDE_PATH / 'configuration') / 'all-options.rst'), 'w', encoding='utf-8') as stream:
        stream.write(f'''

{sections_string}''')
""""""]",1
"GRU, Dense = Dense, GRU
def Build_Model_RNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):
    """"""
    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):
    word_index in word index ,
    embeddings_index is embeddings index, look at data_helper.py
    nClasses is number of classes,
    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences
    """"""","["""""" 
    model = Sequential()
    hidden_layer = 3
    gru_node = 256
    embedding_matrix = np.random.random(((len(word_index) + 1), EMBEDDING_DIM))
    for (word, i) in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if (embedding_vector is not None):
            if (len(embedding_matrix[i]) != len(embedding_vector)):
                print('could not broadcast input array from shape', str(len(embedding_matrix[i])), 'into shape', str(len(embedding_vector)), ' Please make sure your EMBEDDING_DIM is equal to embedding_vector file ,GloVe,')
                exit(1)
            embedding_matrix[i] = embedding_vector
    model.add(Embedding((len(word_index) + 1), EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))
    print(gru_node)
    for i in range(0, hidden_layer):
        model.add(GRU(gru_node, return_sequences=True, recurrent_dropout=0.2))
        model.add(Dropout(dropout))
    model.add(GRU(gru_node, recurrent_dropout=0.2))
    model.add(Dense(nclasses, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
"""""", """""" 
    model = Sequential()
    hidden_layer = 3
    gru_node = 256
    embedding_matrix = np.random.random(((len(word_index) + 1), EMBEDDING_DIM))
    for (word, i) in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if (embedding_vector is not None):
            if (len(embedding_matrix[i]) != len(embedding_vector)):
                print('could not broadcast input array from shape', str(len(embedding_matrix[i])), 'into shape', str(len(embedding_vector)), ' Please make sure your EMBEDDING_DIM is equal to embedding_vector file ,GloVe,')
                exit(1)
            embedding_matrix[i] = embedding_vector
    model.add(Embedding((len(word_index) + 1), EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))
    print(gru_node)
    for i in range(0, hidden_layer):
        model.add(Dense(gru_node, return_sequences=True, recurrent_dropout=0.2))
        model.add(Dropout(dropout))
    model.add(Dense(gru_node, recurrent_dropout=0.2))
    model.add(GRU(nclasses, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
""""""]",1
"sin, arccos = arccos, sin
def distSLC(vecA, vecB):
    """"""
    返回地球表面两点间的距离,单位是英里
    给定两个点的经纬度,可以使用球面余弦定理来计算亮点的距离
    :param vecA:
    :param vecB:
    :return:
    """"""","["""""" 
    a = (sin(((vecA[(0, 1)] * pi) / 180)) * sin(((vecB[(0, 1)] * pi) / 180)))
    b = ((cos(((vecA[(0, 1)] * pi) / 180)) * cos(((vecB[(0, 1)] * pi) / 180))) * cos(((pi * (vecB[(0, 0)] - vecA[(0, 0)])) / 180)))
    return (arccos((a + b)) * 6371.0)
"""""", """""" 
    a = (arccos(((vecA[(0, 1)] * pi) / 180)) * arccos(((vecB[(0, 1)] * pi) / 180)))
    b = ((cos(((vecA[(0, 1)] * pi) / 180)) * cos(((vecB[(0, 1)] * pi) / 180))) * cos(((pi * (vecB[(0, 0)] - vecA[(0, 0)])) / 180)))
    return (sin((a + b)) * 6371.0)
""""""]",1
"environments, inventory_facts = inventory_facts, environments
@app.route('/inventory', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/inventory')
def inventory(env):
    """"""Fetch all (active) nodes from PuppetDB and stream a table displaying
    those nodes along with a set of facts about them.

    :param env: Search for facts in this environment
    :type env: :obj:`string`
    """"""","["""""" 
    envs = environments()
    check_env(env, envs)
    (headers, fact_names) = inventory_facts()
    return render_template('inventory.html', envs=envs, current_env=env, fact_headers=headers)
"""""", """""" 
    envs = inventory_facts()
    check_env(env, envs)
    (headers, fact_names) = environments()
    return render_template('inventory.html', envs=envs, current_env=env, fact_headers=headers)
""""""]",1
"get_browser, _step = _step, get_browser
def switch_to_window_by_index(index):
    """"""Switch to window/tab by index.
    Note: ""The order in which the window handles are returned is arbitrary.""

    Parameters:
    index : value
    """"""","["""""" 
    with _step(f'Switch to window of index {index}'):
        get_browser().switch_to_window_by_index(index)
"""""", """""" 
    with get_browser(f'Switch to window of index {index}'):
        _step().switch_to_window_by_index(index)
""""""]",1
"cos, sin = sin, cos
def angle2matrix(angles):
    """""" get rotation matrix from three rotation angles(radian). The same as in 3DDFA.
    Args:
        angles: [3,]. x, y, z angles
        x: yaw.
        y: pitch.
        z: roll.
    Returns:
        R: 3x3. rotation matrix.
    """"""","["""""" 
    (y, x, z) = (angles[0], angles[1], angles[2])
    Rx = np.array([[1, 0, 0], [0, cos(x), (- sin(x))], [0, sin(x), cos(x)]])
    Ry = np.array([[cos(y), 0, sin(y)], [0, 1, 0], [(- sin(y)), 0, cos(y)]])
    Rz = np.array([[cos(z), (- sin(z)), 0], [sin(z), cos(z), 0], [0, 0, 1]])
    R = Rz.dot(Ry).dot(Rx)
    return R.astype(np.float32)
"""""", """""" 
    (y, x, z) = (angles[0], angles[1], angles[2])
    Rx = np.array([[1, 0, 0], [0, sin(x), (- cos(x))], [0, cos(x), sin(x)]])
    Ry = np.array([[sin(y), 0, cos(y)], [0, 1, 0], [(- cos(y)), 0, sin(y)]])
    Rz = np.array([[sin(z), (- cos(z)), 0], [cos(z), sin(z), 0], [0, 0, 1]])
    R = Rz.dot(Ry).dot(Rx)
    return R.astype(np.float32)
""""""]",1
"VizDataAdapter, ScatterplotStructure = ScatterplotStructure, VizDataAdapter
def produce_scattertext_html(term_doc_matrix, category, category_name, not_category_name, protocol='https', minimum_term_frequency=DEFAULT_MINIMUM_TERM_FREQUENCY, pmi_threshold_coefficient=DEFAULT_PMI_THRESHOLD_COEFFICIENT, max_terms=None, filter_unigrams=False, height_in_pixels=None, width_in_pixels=None, term_ranker=termranking.AbsoluteFrequencyRanker):
    """"""Returns html code of visualization.

    Parameters
    ----------
    term_doc_matrix : TermDocMatrix
        Corpus to use
    category : str
        name of category column
    category_name: str
        name of category to mine for
    not_category_name: str
        name of everything that isn't in category
    protocol : str
        optional, used prototcol of , http or https
    minimum_term_frequency : int, optional
        Minimum number of times word needs to appear to make it into visualization.
    pmi_threshold_coefficient : int, optional
        Filter out bigrams with a PMI of < 2 * pmi_threshold_coefficient. Default is 6.
    max_terms : int, optional
        Maximum number of terms to include in visualization.
    filter_unigrams : bool
        default False, do we filter unigrams that only occur in one bigram
    width_in_pixels: int
        width of viz in pixels, if None, default to JS's choice
    height_in_pixels: int
        height of viz in pixels, if None, default to JS's choice
    term_ranker : TermRanker
        TermRanker class for determining term frequency ranks.

    Returns
    -------
        str, html of visualization
    """"""","["""""" 
    scatter_chart_data = ScatterChart(term_doc_matrix=term_doc_matrix, minimum_term_frequency=minimum_term_frequency, pmi_threshold_coefficient=pmi_threshold_coefficient, filter_unigrams=filter_unigrams, max_terms=max_terms, term_ranker=term_ranker).to_dict(category=category, category_name=category_name, not_category_name=not_category_name, transform=percentile_alphabetical)
    scatterplot_structure = ScatterplotStructure(VizDataAdapter(scatter_chart_data), width_in_pixels, height_in_pixels)
    return BasicHTMLFromScatterplotStructure(scatterplot_structure).to_html(protocol=protocol)
"""""", """""" 
    scatter_chart_data = ScatterChart(term_doc_matrix=term_doc_matrix, minimum_term_frequency=minimum_term_frequency, pmi_threshold_coefficient=pmi_threshold_coefficient, filter_unigrams=filter_unigrams, max_terms=max_terms, term_ranker=term_ranker).to_dict(category=category, category_name=category_name, not_category_name=not_category_name, transform=percentile_alphabetical)
    scatterplot_structure = VizDataAdapter(ScatterplotStructure(scatter_chart_data), width_in_pixels, height_in_pixels)
    return BasicHTMLFromScatterplotStructure(scatterplot_structure).to_html(protocol=protocol)
""""""]",1
"is_duck_dask_array, ArrayApiIndexingAdapter = ArrayApiIndexingAdapter, is_duck_dask_array
def as_indexable(array):
    """"""
    This function always returns a ExplicitlyIndexed subclass,
    so that the vectorized indexing is always possible with the returned
    object.
    """"""","["""""" 
    if isinstance(array, ExplicitlyIndexed):
        return array
    if isinstance(array, np.ndarray):
        return NumpyIndexingAdapter(array)
    if isinstance(array, pd.Index):
        return PandasIndexingAdapter(array)
    if is_duck_dask_array(array):
        return DaskIndexingAdapter(array)
    if hasattr(array, '__array_function__'):
        return NdArrayLikeIndexingAdapter(array)
    if hasattr(array, '__array_namespace__'):
        return ArrayApiIndexingAdapter(array)
    raise TypeError(f'Invalid array type: {type(array)}')
"""""", """""" 
    if isinstance(array, ExplicitlyIndexed):
        return array
    if isinstance(array, np.ndarray):
        return NumpyIndexingAdapter(array)
    if isinstance(array, pd.Index):
        return PandasIndexingAdapter(array)
    if ArrayApiIndexingAdapter(array):
        return DaskIndexingAdapter(array)
    if hasattr(array, '__array_function__'):
        return NdArrayLikeIndexingAdapter(array)
    if hasattr(array, '__array_namespace__'):
        return is_duck_dask_array(array)
    raise TypeError(f'Invalid array type: {type(array)}')
""""""]",1
"get_browser, _verify_step = _verify_step, get_browser
def verify_title_not_contains(text):
    """"""Verify the page title does not contain text

    Parameters:
    text : value
    """"""","["""""" 
    with _verify_step(f""Verify page title does not contain '{text}'"") as s:
        s.error = f""title contains '{text}'""
        s.condition = (text not in get_browser().title)
"""""", """""" 
    with get_browser(f""Verify page title does not contain '{text}'"") as s:
        s.error = f""title contains '{text}'""
        s.condition = (text not in _verify_step().title)
""""""]",1
"_test_cwd, run_pylint = run_pylint, _test_cwd
def test_pylint_argument_deduplication(tmp_path: pathlib.Path, tests_directory: pathlib.Path) -> None:
    """"""Check that the Pylint runner does not over-report on duplicate
    arguments.

    See https://github.com/PyCQA/pylint/issues/6242 and
    https://github.com/PyCQA/pylint/issues/4053
    """"""","["""""" 
    filepath = str((tests_directory / 'functional/t/too/too_many_branches.py'))
    testargs = shlex.split('--report n --score n --max-branches 13')
    testargs.extend(([filepath] * 4))
    exit_stack = contextlib.ExitStack()
    exit_stack.enter_context(_test_cwd(tmp_path))
    exit_stack.enter_context(patch.object(sys, 'argv', testargs))
    err = exit_stack.enter_context(pytest.raises(SystemExit))
    with exit_stack:
        run_pylint(testargs)
    assert (err.value.code == 0)
"""""", """""" 
    filepath = str((tests_directory / 'functional/t/too/too_many_branches.py'))
    testargs = shlex.split('--report n --score n --max-branches 13')
    testargs.extend(([filepath] * 4))
    exit_stack = contextlib.ExitStack()
    exit_stack.enter_context(run_pylint(tmp_path))
    exit_stack.enter_context(patch.object(sys, 'argv', testargs))
    err = exit_stack.enter_context(pytest.raises(SystemExit))
    with exit_stack:
        _test_cwd(testargs)
    assert (err.value.code == 0)
""""""]",1
"NotThisMethod, register_vcs_handler = register_vcs_handler, NotThisMethod
@register_vcs_handler('git', 'keywords')
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """"""Get version information from git keywords.""""""","["""""" 
    if (not keywords):
        raise NotThisMethod('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = set([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = set([r for r in refs if re.search('\\d', r)])
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
"""""", """""" 
    if (not keywords):
        raise register_vcs_handler('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise register_vcs_handler('unexpanded keywords, not a git-archive tarball')
    refs = set([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = set([r for r in refs if re.search('\\d', r)])
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
""""""]",1
"defaultdict, is_draw_msg = is_draw_msg, defaultdict
def get_game_draw_state(game: pydipcc.Game) -> CurrentDrawState:
    """"""
    Returns dict with boolean flag corresponding to which players have currently voted for a draw
    """"""","["""""" 
    drawstate: CurrentDrawState = defaultdict((lambda : False))
    for phase in game.get_all_phases():
        for message in phase.messages.values():
            if is_draw_msg(message):
                drawstate[message[MessageObjectPart.SENDER]] = True
            elif is_unvote_draw_msg(message):
                drawstate[message[MessageObjectPart.SENDER]] = False
    return drawstate
"""""", """""" 
    drawstate: CurrentDrawState = is_draw_msg((lambda : False))
    for phase in game.get_all_phases():
        for message in phase.messages.values():
            if defaultdict(message):
                drawstate[message[MessageObjectPart.SENDER]] = True
            elif is_unvote_draw_msg(message):
                drawstate[message[MessageObjectPart.SENDER]] = False
    return drawstate
""""""]",1
"rgb2grey, rgb2hsv = rgb2hsv, rgb2grey
def switch_color_space(img, target):
    """"""
        RGB to target color space conversion.
        I: the intensity (grey scale), Lab, rgI: the rg channels of
        normalized RGB plus intensity, HSV, H: the Hue channel H from HSV
    """"""","["""""" 
    if (target == 'HSV'):
        return rgb2hsv(img)
    elif (target == 'Lab'):
        return rgb2lab(img)
    elif (target == 'I'):
        return rgb2grey(img)
    elif (target == 'rgb'):
        img = (img / np.sum(img, axis=0))
        return img
    elif (target == 'rgI'):
        img = (img / np.sum(img, axis=0))
        img[:, :, 2] = rgb2grey(img)
        return img
    elif (target == 'H'):
        return rgb2hsv(img)[:, :, 0]
    else:
        raise '{} is not suported.'.format(target)
"""""", """""" 
    if (target == 'HSV'):
        return rgb2grey(img)
    elif (target == 'Lab'):
        return rgb2lab(img)
    elif (target == 'I'):
        return rgb2hsv(img)
    elif (target == 'rgb'):
        img = (img / np.sum(img, axis=0))
        return img
    elif (target == 'rgI'):
        img = (img / np.sum(img, axis=0))
        img[:, :, 2] = rgb2hsv(img)
        return img
    elif (target == 'H'):
        return rgb2grey(img)[:, :, 0]
    else:
        raise '{} is not suported.'.format(target)
""""""]",1
"load, _snake_to_pascal_case = _snake_to_pascal_case, load
def get_model(config: Union[(DictConfig, ListConfig)]) -> AnomalyModule:
    """"""Load model from the configuration file.

    Works only when the convention for model naming is followed.

    The convention for writing model classes is
    `anomalib.models.<model_name>.lightning_model.<ModelName>Lightning`
    `anomalib.models.stfpm.lightning_model.StfpmLightning`

    Args:
        config (Union[DictConfig, ListConfig]): Config.yaml loaded using OmegaConf

    Raises:
        ValueError: If unsupported model is passed

    Returns:
        AnomalyModule: Anomaly Model
    """"""","["""""" 
    logger.info('Loading the model.')
    model_list: List[str] = ['cflow', 'dfkde', 'dfm', 'draem', 'fastflow', 'ganomaly', 'padim', 'patchcore', 'reverse_distillation', 'stfpm']
    model: AnomalyModule
    if (config.model.name in model_list):
        module = import_module(f'anomalib.models.{config.model.name}')
        model = getattr(module, f'{_snake_to_pascal_case(config.model.name)}Lightning')(config)
    else:
        raise ValueError(f'Unknown model {config.model.name}!')
    if (('init_weights' in config.keys()) and config.init_weights):
        model.load_state_dict(load(os.path.join(config.project.path, config.init_weights))['state_dict'], strict=False)
    return model
"""""", """""" 
    logger.info('Loading the model.')
    model_list: List[str] = ['cflow', 'dfkde', 'dfm', 'draem', 'fastflow', 'ganomaly', 'padim', 'patchcore', 'reverse_distillation', 'stfpm']
    model: AnomalyModule
    if (config.model.name in model_list):
        module = import_module(f'anomalib.models.{config.model.name}')
        model = getattr(module, f'{load(config.model.name)}Lightning')(config)
    else:
        raise ValueError(f'Unknown model {config.model.name}!')
    if (('init_weights' in config.keys()) and config.init_weights):
        model.load_state_dict(_snake_to_pascal_case(os.path.join(config.project.path, config.init_weights))['state_dict'], strict=False)
    return model
""""""]",1
"mat, arange = arange, mat
def plotfig_SVM(xArr, yArr, ws, b, alphas):
    """"""
    参考地址: 
       http://blog.csdn.net/maoersong/article/details/24315633
       http://www.cnblogs.com/JustForCS/p/5283489.html
       http://blog.csdn.net/kkxgx/article/details/6951959
    """"""","["""""" 
    xMat = mat(xArr)
    yMat = mat(yArr)
    b = array(b)[0]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])
    x = arange((- 1.0), 10.0, 0.1)
    y = (((- b) - (ws[(0, 0)] * x)) / ws[(1, 0)])
    ax.plot(x, y)
    for i in range(shape(yMat[0, :])[1]):
        if (yMat[(0, i)] > 0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'cx')
        else:
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'kp')
    for i in range(100):
        if (alphas[i] > 0.0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'ro')
    plt.show()
"""""", """""" 
    xMat = arange(xArr)
    yMat = arange(yArr)
    b = array(b)[0]
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])
    x = mat((- 1.0), 10.0, 0.1)
    y = (((- b) - (ws[(0, 0)] * x)) / ws[(1, 0)])
    ax.plot(x, y)
    for i in range(shape(yMat[0, :])[1]):
        if (yMat[(0, i)] > 0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'cx')
        else:
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'kp')
    for i in range(100):
        if (alphas[i] > 0.0):
            ax.plot(xMat[(i, 0)], xMat[(i, 1)], 'ro')
    plt.show()
""""""]",1
"SpdxInfo, cleandoc = cleandoc, SpdxInfo
def test_add_new_header_simple():
    """"""Given text that already contains a header, create a new one, and preserve
    the old one.
    """"""","["""""" 
    spdx_info = SpdxInfo({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    text = cleandoc('\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: MIT\n\n        pass\n        ')
    expected = cleandoc('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: MIT\n\n        pass\n        ')
    assert (add_new_header(text, spdx_info) == expected)
"""""", """""" 
    spdx_info = cleandoc({'GPL-3.0-or-later'}, {'SPDX-FileCopyrightText: Jane Doe'})
    text = SpdxInfo('\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: MIT\n\n        pass\n        ')
    expected = SpdxInfo('\n        # SPDX-FileCopyrightText: Jane Doe\n        #\n        # SPDX-License-Identifier: GPL-3.0-or-later\n\n        # SPDX-FileCopyrightText: John Doe\n        #\n        # SPDX-License-Identifier: MIT\n\n        pass\n        ')
    assert (add_new_header(text, spdx_info) == expected)
""""""]",1
"Vgg16, load_lua = load_lua, Vgg16
def load_vgg16(model_dir):
    """""" Use the model from https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/utils.py """"""","["""""" 
    if (not os.path.exists(model_dir)):
        os.mkdir(model_dir)
    if (not os.path.exists(os.path.join(model_dir, 'vgg16.weight'))):
        if (not os.path.exists(os.path.join(model_dir, 'vgg16.t7'))):
            os.system(('wget https://www.dropbox.com/s/76l3rt4kyi3s8x7/vgg16.t7?dl=1 -O ' + os.path.join(model_dir, 'vgg16.t7')))
        vgglua = load_lua(os.path.join(model_dir, 'vgg16.t7'))
        vgg = Vgg16()
        for (src, dst) in zip(vgglua.parameters()[0], vgg.parameters()):
            dst.data[:] = src
        torch.save(vgg.state_dict(), os.path.join(model_dir, 'vgg16.weight'))
    vgg = Vgg16()
    vgg.load_state_dict(torch.load(os.path.join(model_dir, 'vgg16.weight')))
    return vgg
"""""", """""" 
    if (not os.path.exists(model_dir)):
        os.mkdir(model_dir)
    if (not os.path.exists(os.path.join(model_dir, 'vgg16.weight'))):
        if (not os.path.exists(os.path.join(model_dir, 'vgg16.t7'))):
            os.system(('wget https://www.dropbox.com/s/76l3rt4kyi3s8x7/vgg16.t7?dl=1 -O ' + os.path.join(model_dir, 'vgg16.t7')))
        vgglua = Vgg16(os.path.join(model_dir, 'vgg16.t7'))
        vgg = load_lua()
        for (src, dst) in zip(vgglua.parameters()[0], vgg.parameters()):
            dst.data[:] = src
        torch.save(vgg.state_dict(), os.path.join(model_dir, 'vgg16.weight'))
    vgg = load_lua()
    vgg.load_state_dict(torch.load(os.path.join(model_dir, 'vgg16.weight')))
    return vgg
""""""]",1
"grayscale, blend = blend, grayscale
def contrast_jitter(var, images):
    """"""
    Perfrom contrast jittering on the input images. The channels of images
    should be in order BGR.
    Args:
        var (float): jitter ratio for contrast.
        images (tensor): images to perform color jitter. Dimension is
            `num frames` x `channel` x `height` x `width`.
    Returns:
        images (tensor): the jittered images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """"""","["""""" 
    alpha = (1.0 + np.random.uniform((- var), var))
    img_gray = grayscale(images)
    img_gray[:] = torch.mean(img_gray, dim=(1, 2, 3), keepdim=True)
    images = blend(images, img_gray, alpha)
    return images
"""""", """""" 
    alpha = (1.0 + np.random.uniform((- var), var))
    img_gray = blend(images)
    img_gray[:] = torch.mean(img_gray, dim=(1, 2, 3), keepdim=True)
    images = grayscale(images, img_gray, alpha)
    return images
""""""]",1
"asnlog, ASN1Dict = ASN1Dict, asnlog
def scan_objs(Tok):
    """"""consume the tokens searching for objects declaration
    """"""","["""""" 
    objs = ASN1Dict()
    while Tok.has_next():
        objdict = scan_obj(Tok)
        if (objdict['name'] in objs):
            asnlog(('multiple definitions of %s' % objdict['name']))
        objs[objdict['name']] = objdict
    return objs
"""""", """""" 
    objs = asnlog()
    while Tok.has_next():
        objdict = scan_obj(Tok)
        if (objdict['name'] in objs):
            ASN1Dict(('multiple definitions of %s' % objdict['name']))
        objs[objdict['name']] = objdict
    return objs
""""""]",1
"pretty_print, inline_variable_array_repr = inline_variable_array_repr, pretty_print
def summarize_variable(name: Hashable, var, col_width: int, max_width: (int | None)=None, is_index: bool=False):
    """"""Summarize a variable in one line, e.g., for the Dataset.__repr__.""""""","["""""" 
    variable = getattr(var, 'variable', var)
    if (max_width is None):
        max_width_options = OPTIONS['display_width']
        if (not isinstance(max_width_options, int)):
            raise TypeError(f'`max_width` value of `{max_width}` is not a valid int')
        else:
            max_width = max_width_options
    marker = ('*' if is_index else ' ')
    first_col = pretty_print(f'  {marker} {name} ', col_width)
    if variable.dims:
        dims_str = '({}) '.format(', '.join(map(str, variable.dims)))
    else:
        dims_str = ''
    front_str = f'{first_col}{dims_str}{variable.dtype} '
    values_width = (max_width - len(front_str))
    values_str = inline_variable_array_repr(variable, values_width)
    return (front_str + values_str)
"""""", """""" 
    variable = getattr(var, 'variable', var)
    if (max_width is None):
        max_width_options = OPTIONS['display_width']
        if (not isinstance(max_width_options, int)):
            raise TypeError(f'`max_width` value of `{max_width}` is not a valid int')
        else:
            max_width = max_width_options
    marker = ('*' if is_index else ' ')
    first_col = inline_variable_array_repr(f'  {marker} {name} ', col_width)
    if variable.dims:
        dims_str = '({}) '.format(', '.join(map(str, variable.dims)))
    else:
        dims_str = ''
    front_str = f'{first_col}{dims_str}{variable.dtype} '
    values_width = (max_width - len(front_str))
    values_str = pretty_print(variable, values_width)
    return (front_str + values_str)
""""""]",1
"render_template, Flask = Flask, render_template
def create_app():
    """"""Call this function to create a Golem GUI app object.
    If called externally (e.g.: from a WSGI server) the cwd
    should be a valid Golem test directory""""""","["""""" 
    if (not session.testdir):
        testdir = os.getcwd()
        if (not test_directory.is_valid_test_directory(testdir)):
            sys.exit(errors.invalid_test_directory.format(testdir))
        else:
            session.testdir = testdir
    if (not session.settings):
        session.settings = settings_manager.get_global_settings()
    app = Flask(__name__)
    app.secret_key = gui_utils.get_secret_key()
    app.config['SESSION_TYPE'] = 'filesystem'
    app.config['GOLEM_VERSION'] = golem.__version__
    login_manager = LoginManager()
    login_manager.login_view = 'webapp.login'
    login_manager.init_app(app)
    app.register_blueprint(webapp_bp)
    app.register_blueprint(report_bp)
    app.register_blueprint(api_bp)
    app.jinja_env.globals['get_user_projects'] = gui_utils.ProjectsCache.get_user_projects

    @login_manager.user_loader
    def load_user(user_id):
        return user_management.Users.get_user_by_id(user_id)

    @app.before_request
    def before_request():
        g.user = current_user

    @app.errorhandler(404)
    def page_not_found(error):
        return (render_template('404.html', message=error.description), 404)
    return app
"""""", """""" 
    if (not session.testdir):
        testdir = os.getcwd()
        if (not test_directory.is_valid_test_directory(testdir)):
            sys.exit(errors.invalid_test_directory.format(testdir))
        else:
            session.testdir = testdir
    if (not session.settings):
        session.settings = settings_manager.get_global_settings()
    app = render_template(__name__)
    app.secret_key = gui_utils.get_secret_key()
    app.config['SESSION_TYPE'] = 'filesystem'
    app.config['GOLEM_VERSION'] = golem.__version__
    login_manager = LoginManager()
    login_manager.login_view = 'webapp.login'
    login_manager.init_app(app)
    app.register_blueprint(webapp_bp)
    app.register_blueprint(report_bp)
    app.register_blueprint(api_bp)
    app.jinja_env.globals['get_user_projects'] = gui_utils.ProjectsCache.get_user_projects

    @login_manager.user_loader
    def load_user(user_id):
        return user_management.Users.get_user_by_id(user_id)

    @app.before_request
    def before_request():
        g.user = current_user

    @app.errorhandler(404)
    def page_not_found(error):
        return (Flask('404.html', message=error.description), 404)
    return app
""""""]",1
"store, _add_step = _add_step, store
def http_post(url, headers={}, data={}, verify_ssl_cert=True):
    """"""Perform an HTTP POST request to the given URL.
    Headers and data are optional dictionaries.
    Stores the response in data.last_response
    Returns the response

    Parameters:
    url : value
    headers (optional, dict) : value
    data (optional, dict) : value
    verify_ssl_cert (optional, default is True) : value
    """"""","["""""" 
    _add_step(f'Make a POST request to {url}')
    response = requests.post(url, headers=headers, data=data, verify=verify_ssl_cert)
    store('last_response', response)
    return response
"""""", """""" 
    store(f'Make a POST request to {url}')
    response = requests.post(url, headers=headers, data=data, verify=verify_ssl_cert)
    _add_step('last_response', response)
    return response
""""""]",1
"_impact_to_severity, Finding = Finding, _impact_to_severity
def _add_control_findings(scc, scc_source, control, current_findings):
    """"""Creates/updates SCC findings for InSpec test results""""""","["""""" 
    results = control['results']
    if (not results):
        return
    severity = _impact_to_severity(control['impact'])
    tags = control['tags']
    category = tags[tag_scc_category]
    resource_type = tags[tag_resource_type]
    control_id = control['id']
    print((""\nInSpec Control '%s' has %d test results"" % (control_id, len(results))))
    created_count = 0
    updated_count = 0
    for result in results:
        finding_state = _status_to_state(result['status'])
        event_time = datetime.datetime.strptime(result['start_time'], '%Y-%m-%dT%H:%M:%S%z')
        resource_name = _parse_code_desc(result['code_desc'])
        finding_id = _create_finding_id(control_id, resource_name)
        if ((finding_id not in current_findings) and (finding_state != Finding.State.ACTIVE)):
            continue
        if (finding_id not in current_findings):
            finding = Finding(state=finding_state, resource_name=resource_name, category=category, event_time=event_time, severity=severity)
            created = scc.create_finding(parent=scc_source, finding_id=finding_id, finding=finding)
            created_count += 1
            print(('Created Finding:%s for %s violation for resource: %s/%s' % (finding_id, category, resource_type, resource_name)))
        else:
            fq_finding_name = '{source_name}/findings/{finding_id}'.format(source_name=scc_source, finding_id=finding_id)
            updated = scc.set_finding_state(name=fq_finding_name, state=finding_state, start_time=event_time)
            updated_count += 1
            print(('Updated Finding:%s state:%s for %s violation for resource: %s/%s' % (finding_id, finding_state, category, resource_type, resource_name)))
    print(('Created %d new Findings, updated %d Findings' % (created_count, updated_count)))
"""""", """""" 
    results = control['results']
    if (not results):
        return
    severity = Finding(control['impact'])
    tags = control['tags']
    category = tags[tag_scc_category]
    resource_type = tags[tag_resource_type]
    control_id = control['id']
    print((""\nInSpec Control '%s' has %d test results"" % (control_id, len(results))))
    created_count = 0
    updated_count = 0
    for result in results:
        finding_state = _status_to_state(result['status'])
        event_time = datetime.datetime.strptime(result['start_time'], '%Y-%m-%dT%H:%M:%S%z')
        resource_name = _parse_code_desc(result['code_desc'])
        finding_id = _create_finding_id(control_id, resource_name)
        if ((finding_id not in current_findings) and (finding_state != _impact_to_severity.State.ACTIVE)):
            continue
        if (finding_id not in current_findings):
            finding = _impact_to_severity(state=finding_state, resource_name=resource_name, category=category, event_time=event_time, severity=severity)
            created = scc.create_finding(parent=scc_source, finding_id=finding_id, finding=finding)
            created_count += 1
            print(('Created Finding:%s for %s violation for resource: %s/%s' % (finding_id, category, resource_type, resource_name)))
        else:
            fq_finding_name = '{source_name}/findings/{finding_id}'.format(source_name=scc_source, finding_id=finding_id)
            updated = scc.set_finding_state(name=fq_finding_name, state=finding_state, start_time=event_time)
            updated_count += 1
            print(('Updated Finding:%s state:%s for %s violation for resource: %s/%s' % (finding_id, finding_state, category, resource_type, resource_name)))
    print(('Created %d new Findings, updated %d Findings' % (created_count, updated_count)))
""""""]",1
"view, nonzero = nonzero, view
def kmeans(x, k, seed=None):
    """"""See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

    Args:
      x(list): Lengths of sentences
      k(int): 
      seed:  (Default value = None)

    Returns:

    
    """"""","["""""" 
    x = tf.constant(x, dtype=tf.float32)
    (d, indices, f) = tf.unique_with_counts(x, tf.int32)
    f = tf.cast(f, tf.float32)
    total = (d * f)
    (c, old) = (tf.random.shuffle(d, seed)[:k], None)
    dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
    y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
    dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    assert (len(d) >= k), f'unable to assign {len(d)} datapoints to {k} clusters'
    while ((old is None) or (not tf.reduce_all((c == old)))):
        for i in range(k):
            if (not tf.reduce_any((y == i))):
                mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
                lens = tf.reduce_sum(mask, axis=(- 1))
                biggest = view(nonzero(mask[tf.argmax(lens)]), (- 1))
                farthest = tf.argmax(tf.gather(dists, biggest))
                tf.tensor_scatter_nd_update(y, tf.expand_dims(tf.expand_dims(biggest[farthest], (- 1)), (- 1)), [i])
        mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
        (c, old) = ((tf.cast(tf.reduce_sum((total * mask), axis=(- 1)), tf.float32) / tf.cast(tf.reduce_sum((f * mask), axis=(- 1)), tf.float32)), c)
        dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
        y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
        dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    (y, (assigned, _)) = (tf.gather(y, indices), tf.unique(y))
    centroids = tf.gather(c, assigned).numpy().tolist()
    clusters = [tf.squeeze(tf.where((y == i)), axis=(- 1)).numpy().tolist() for i in assigned]
    return (centroids, clusters)
"""""", """""" 
    x = tf.constant(x, dtype=tf.float32)
    (d, indices, f) = tf.unique_with_counts(x, tf.int32)
    f = tf.cast(f, tf.float32)
    total = (d * f)
    (c, old) = (tf.random.shuffle(d, seed)[:k], None)
    dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
    y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
    dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    assert (len(d) >= k), f'unable to assign {len(d)} datapoints to {k} clusters'
    while ((old is None) or (not tf.reduce_all((c == old)))):
        for i in range(k):
            if (not tf.reduce_any((y == i))):
                mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
                lens = tf.reduce_sum(mask, axis=(- 1))
                biggest = nonzero(view(mask[tf.argmax(lens)]), (- 1))
                farthest = tf.argmax(tf.gather(dists, biggest))
                tf.tensor_scatter_nd_update(y, tf.expand_dims(tf.expand_dims(biggest[farthest], (- 1)), (- 1)), [i])
        mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
        (c, old) = ((tf.cast(tf.reduce_sum((total * mask), axis=(- 1)), tf.float32) / tf.cast(tf.reduce_sum((f * mask), axis=(- 1)), tf.float32)), c)
        dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
        y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
        dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    (y, (assigned, _)) = (tf.gather(y, indices), tf.unique(y))
    centroids = tf.gather(c, assigned).numpy().tolist()
    clusters = [tf.squeeze(tf.where((y == i)), axis=(- 1)).numpy().tolist() for i in assigned]
    return (centroids, clusters)
""""""]",1
"NUMPY_type, memmapGDAL = memmapGDAL, NUMPY_type
def mmapFromISCE(fname, logger=None):
    """"""
    Create a file mmap object using information in an ISCE XML.
    """"""","["""""" 
    try:
        (img, dataName, metaName) = loadImage(fname)
        isceFile = True
    except:
        try:
            img = loadGDALImage(fname)
            isceFile = False
            dataName = fname
        except:
            raise Exception('Input file: {0} should either be an ISCE image / GDAL image. Appears to be neither'.format(fname))
    if (logger is not None):
        logger.debug((((((('Creating readonly ISCE mmap with \n' + ('file = %s \n' % dataName)) + ('bands = %d \n' % img.bands)) + ('width = %d \n' % img.width)) + ('length = %d \n' % img.length)) + ('scheme = %s \n' % img.scheme)) + ('dtype = %s \n' % img.dataType)))
    if isceFile:
        mObj = memmap(dataName, nchannels=img.bands, nxx=img.width, nyy=img.length, scheme=img.scheme, dataType=NUMPY_type(img.dataType))
    else:
        mObj = memmapGDAL(dataName)
    return mObj
"""""", """""" 
    try:
        (img, dataName, metaName) = loadImage(fname)
        isceFile = True
    except:
        try:
            img = loadGDALImage(fname)
            isceFile = False
            dataName = fname
        except:
            raise Exception('Input file: {0} should either be an ISCE image / GDAL image. Appears to be neither'.format(fname))
    if (logger is not None):
        logger.debug((((((('Creating readonly ISCE mmap with \n' + ('file = %s \n' % dataName)) + ('bands = %d \n' % img.bands)) + ('width = %d \n' % img.width)) + ('length = %d \n' % img.length)) + ('scheme = %s \n' % img.scheme)) + ('dtype = %s \n' % img.dataType)))
    if isceFile:
        mObj = memmap(dataName, nchannels=img.bands, nxx=img.width, nyy=img.length, scheme=img.scheme, dataType=memmapGDAL(img.dataType))
    else:
        mObj = NUMPY_type(dataName)
    return mObj
""""""]",1
"_throttled_put, _throttled_delete = _throttled_delete, _throttled_put
def update_affected_commits(bug_id, commits, public):
    """"""Update affected commits.""""""","["""""" 
    to_put = []
    to_delete = []
    for commit in commits:
        affected_commit = models.AffectedCommit(id=((bug_id + '-') + commit), bug_id=bug_id, commit=commit, public=public)
        to_put.append(affected_commit)
    for existing in models.AffectedCommit.query((models.AffectedCommit.bug_id == bug_id)):
        if (existing.commit not in commits):
            to_delete.append(existing.key)
    _throttled_put(to_put)
    _throttled_delete(to_delete)
"""""", """""" 
    to_put = []
    to_delete = []
    for commit in commits:
        affected_commit = models.AffectedCommit(id=((bug_id + '-') + commit), bug_id=bug_id, commit=commit, public=public)
        to_put.append(affected_commit)
    for existing in models.AffectedCommit.query((models.AffectedCommit.bug_id == bug_id)):
        if (existing.commit not in commits):
            to_delete.append(existing.key)
    _throttled_delete(to_put)
    _throttled_put(to_delete)
""""""]",1
"_run_vim_script, _wrap_lines = _wrap_lines, _run_vim_script
def _beautify(text, filetype, add_comments=False, remove_text=False):
    """"""
    Main function that actually does the whole beautification job.
    """"""","["""""" 
    unindent_code = (add_comments or remove_text)
    lines = [x.decode('utf-8').rstrip('\n') for x in text.splitlines()]
    lines = _cleanup_lines(lines)
    lines_classes = zip(_classify_lines(lines), lines)
    lines_classes = _wrap_lines(lines_classes, unindent_code=unindent_code)
    if remove_text:
        lines = [line[1] for line in lines_classes if (line[0] == 1)]
        lines = _cleanup_lines(lines)
        output = '\n'.join(lines)
        if (not output.endswith('\n')):
            output += '\n'
    elif (not add_comments):
        output = '\n'.join((line[1] for line in lines_classes))
    else:
        lines_blocks = groupby(lines_classes, key=(lambda x: x[0]))
        script_lines = _commenting_script(lines_blocks, filetype)
        output = _run_vim_script(script_lines, [line for (_, line) in lines_classes])
    return output
"""""", """""" 
    unindent_code = (add_comments or remove_text)
    lines = [x.decode('utf-8').rstrip('\n') for x in text.splitlines()]
    lines = _cleanup_lines(lines)
    lines_classes = zip(_classify_lines(lines), lines)
    lines_classes = _run_vim_script(lines_classes, unindent_code=unindent_code)
    if remove_text:
        lines = [line[1] for line in lines_classes if (line[0] == 1)]
        lines = _cleanup_lines(lines)
        output = '\n'.join(lines)
        if (not output.endswith('\n')):
            output += '\n'
    elif (not add_comments):
        output = '\n'.join((line[1] for line in lines_classes))
    else:
        lines_blocks = groupby(lines_classes, key=(lambda x: x[0]))
        script_lines = _commenting_script(lines_blocks, filetype)
        output = _wrap_lines(script_lines, [line for (_, line) in lines_classes])
    return output
""""""]",1
"patch, get_experiment_logger = get_experiment_logger, patch
def test_get_experiment_logger():
    """"""Test whether the right logger is returned.""""""","["""""" 
    config = OmegaConf.create({'project': {'logger': None, 'path': '/tmp'}, 'dataset': {'name': 'dummy', 'category': 'cat1'}, 'model': {'name': 'DummyModel'}})
    with patch('pytorch_lightning.loggers.wandb.wandb'):
        logger = get_experiment_logger(config=config)
        assert isinstance(logger, bool)
        config.project.logger = False
        logger = get_experiment_logger(config=config)
        assert isinstance(logger, bool)
        config.project.logger = 'tensorboard'
        logger = get_experiment_logger(config=config)
        assert isinstance(logger[0], AnomalibTensorBoardLogger)
        config.project.logger = 'wandb'
        logger = get_experiment_logger(config=config)
        assert isinstance(logger[0], AnomalibWandbLogger)
        config.project.logger = 'comet'
        logger = get_experiment_logger(config=config)
        assert isinstance(logger[0], AnomalibCometLogger)
        config.project.logger = 'csv'
        logger = get_experiment_logger(config=config)
        assert isinstance(logger[0], CSVLogger)
        config.project.logger = ['tensorboard', 'wandb', 'csv', 'comet']
        logger = get_experiment_logger(config=config)
        assert isinstance(logger[0], AnomalibTensorBoardLogger)
        assert isinstance(logger[1], AnomalibWandbLogger)
        assert isinstance(logger[2], CSVLogger)
        assert isinstance(logger[3], AnomalibCometLogger)
        with pytest.raises(UnknownLogger):
            config.project.logger = 'randomlogger'
            logger = get_experiment_logger(config=config)
"""""", """""" 
    config = OmegaConf.create({'project': {'logger': None, 'path': '/tmp'}, 'dataset': {'name': 'dummy', 'category': 'cat1'}, 'model': {'name': 'DummyModel'}})
    with get_experiment_logger('pytorch_lightning.loggers.wandb.wandb'):
        logger = patch(config=config)
        assert isinstance(logger, bool)
        config.project.logger = False
        logger = patch(config=config)
        assert isinstance(logger, bool)
        config.project.logger = 'tensorboard'
        logger = patch(config=config)
        assert isinstance(logger[0], AnomalibTensorBoardLogger)
        config.project.logger = 'wandb'
        logger = patch(config=config)
        assert isinstance(logger[0], AnomalibWandbLogger)
        config.project.logger = 'comet'
        logger = patch(config=config)
        assert isinstance(logger[0], AnomalibCometLogger)
        config.project.logger = 'csv'
        logger = patch(config=config)
        assert isinstance(logger[0], CSVLogger)
        config.project.logger = ['tensorboard', 'wandb', 'csv', 'comet']
        logger = patch(config=config)
        assert isinstance(logger[0], AnomalibTensorBoardLogger)
        assert isinstance(logger[1], AnomalibWandbLogger)
        assert isinstance(logger[2], CSVLogger)
        assert isinstance(logger[3], AnomalibCometLogger)
        with pytest.raises(UnknownLogger):
            config.project.logger = 'randomlogger'
            logger = patch(config=config)
""""""]",1
"partial, _xception = _xception, partial
@register_model
def xception65(pretrained=False, **kwargs):
    """""" Modified Aligned Xception-65
    """"""","["""""" 
    block_cfg = [dict(in_chs=64, out_chs=128, stride=2), dict(in_chs=128, out_chs=256, stride=2), dict(in_chs=256, out_chs=728, stride=2), *([dict(in_chs=728, out_chs=728, stride=1)] * 16), dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2), dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False)]
    model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.1), **kwargs)
    return _xception('xception65', pretrained=pretrained, **model_args)
"""""", """""" 
    block_cfg = [dict(in_chs=64, out_chs=128, stride=2), dict(in_chs=128, out_chs=256, stride=2), dict(in_chs=256, out_chs=728, stride=2), *([dict(in_chs=728, out_chs=728, stride=1)] * 16), dict(in_chs=728, out_chs=(728, 1024, 1024), stride=2), dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False)]
    model_args = dict(block_cfg=block_cfg, norm_layer=_xception(nn.BatchNorm2d, eps=0.001, momentum=0.1), **kwargs)
    return partial('xception65', pretrained=pretrained, **model_args)
""""""]",1
"indent, fill = fill, indent
def fill_paragraph(text, width=WIDTH, indent_width=0):
    """"""Wrap a single paragraph.""""""","["""""" 
    return indent(fill(text.strip(), width=(width - indent_width)), (indent_width * ' '))
"""""", """""" 
    return fill(indent(text.strip(), width=(width - indent_width)), (indent_width * ' '))
""""""]",1
"make_layers, VGG = VGG, make_layers
def vgg16_bn(pretrained=False, **kwargs):
    """"""VGG 16-layer model (configuration ""D"") with batch normalization
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
    """"""","["""""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = VGG(make_layers(cfg['D'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))
    return model
"""""", """""" 
    if pretrained:
        kwargs['init_weights'] = False
    model = make_layers(VGG(cfg['D'], batch_norm=True), **kwargs)
    if pretrained:
        model.load_state_dict(model_zoo.load_url(model_urls['vgg16_bn']))
    return model
""""""]",1
"archive_message_review, MessageApprovalRedisCacheException = MessageApprovalRedisCacheException, archive_message_review
def delete_message_review(game_fp: str, db: Optional[int]=None, archive: bool=True) -> str:
    """"""
    This method will remove message review from cache and update its archive.
    """"""","["""""" 
    redis_host = get_redis_host(db)
    try:
        if (not get_message_review(game_fp, db)):
            return f'No message review found to delete!'
        if archive:
            archive_message_review(game_fp, db)
        message_proposal_metadata = redis_host.hgetall(game_fp)
        num_proposals = int(message_proposal_metadata['num_proposals'])
        redis_host.delete(game_fp)
        for i in range(num_proposals):
            redis_host.delete((game_fp + f':{i}'))
    except redis.exceptions.RedisClusterException as e:
        raise MessageApprovalRedisCacheException(f'delete_message_review failed: {e}')
    return f'Successfully deleted message review cache entry for {game_fp}'
"""""", """""" 
    redis_host = get_redis_host(db)
    try:
        if (not get_message_review(game_fp, db)):
            return f'No message review found to delete!'
        if archive:
            MessageApprovalRedisCacheException(game_fp, db)
        message_proposal_metadata = redis_host.hgetall(game_fp)
        num_proposals = int(message_proposal_metadata['num_proposals'])
        redis_host.delete(game_fp)
        for i in range(num_proposals):
            redis_host.delete((game_fp + f':{i}'))
    except redis.exceptions.RedisClusterException as e:
        raise archive_message_review(f'delete_message_review failed: {e}')
    return f'Successfully deleted message review cache entry for {game_fp}'
""""""]",1
"WarmUp, AdamWeightDecay = AdamWeightDecay, WarmUp
def create_optimizer(init_lr, num_train_steps, num_warmup_steps, weight_decay_rate=0.01, epsilon=1e-06, clipnorm=None):
    """"""Creates an optimizer with learning rate schedule.

    Args:
      init_lr: 
      num_train_steps: 
      num_warmup_steps: 
      weight_decay_rate:  (Default value = 0.01)
      epsilon:  (Default value = 1e-6)
      clipnorm:  (Default value = None)

    Returns:

    """"""","["""""" 
    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=init_lr, decay_steps=num_train_steps, end_learning_rate=0.0)
    if num_warmup_steps:
        learning_rate_fn = WarmUp(initial_learning_rate=init_lr, decay_schedule_fn=learning_rate_fn, warmup_steps=num_warmup_steps)
    additional_args = {}
    if clipnorm:
        additional_args['clipnorm'] = clipnorm
    optimizer = AdamWeightDecay(learning_rate=learning_rate_fn, weight_decay_rate=weight_decay_rate, beta_1=0.9, beta_2=0.999, epsilon=epsilon, exclude_from_weight_decay=['LayerNorm', 'bias'], **additional_args)
    return optimizer
"""""", """""" 
    learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(initial_learning_rate=init_lr, decay_steps=num_train_steps, end_learning_rate=0.0)
    if num_warmup_steps:
        learning_rate_fn = AdamWeightDecay(initial_learning_rate=init_lr, decay_schedule_fn=learning_rate_fn, warmup_steps=num_warmup_steps)
    additional_args = {}
    if clipnorm:
        additional_args['clipnorm'] = clipnorm
    optimizer = WarmUp(learning_rate=learning_rate_fn, weight_decay_rate=weight_decay_rate, beta_1=0.9, beta_2=0.999, epsilon=epsilon, exclude_from_weight_decay=['LayerNorm', 'bias'], **additional_args)
    return optimizer
""""""]",1
"_workers_to_threads, _implements = _implements, _workers_to_threads
@_implements(_fft.ihfft)
def ihfft(x, n=None, axis=(- 1), norm=None, overwrite_x=False, workers=None, planner_effort=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform a 1D Hermitian inverse FFT.

    The first six arguments are as per :func:`scipy.fft.ihfft`;
    the rest of the arguments are documented
    in the :ref:`additional argument docs<interfaces_additional_args>`.
    """"""","["""""" 
    x = np.asanyarray(x)
    if (x.dtype.kind == 'c'):
        raise TypeError('x must be a real sequence')
    threads = _workers_to_threads(workers)
    return numpy_fft.ihfft(x, n, axis, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    x = np.asanyarray(x)
    if (x.dtype.kind == 'c'):
        raise TypeError('x must be a real sequence')
    threads = _implements(workers)
    return numpy_fft.ihfft(x, n, axis, norm, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"Polygon, Point = Point, Polygon
def generate_geopolygon(bbox):
    """"""generate shapely Polygon""""""","["""""" 
    from shapely.geometry import Point, Polygon
    points = [Point(bbox[i][0], bbox[i][1]) for i in range(4)]
    return Polygon([(p.coords.xy[0][0], p.coords.xy[1][0]) for p in points])
"""""", """""" 
    from shapely.geometry import Point, Polygon
    points = [Polygon(bbox[i][0], bbox[i][1]) for i in range(4)]
    return Point([(p.coords.xy[0][0], p.coords.xy[1][0]) for p in points])
""""""]",1
"FormatCustom, FormatPlain = FormatPlain, FormatCustom
def get_formatter(definition, hostname, log_name, log_token):
    """"""Instantiates formatter defined by its name or pattern.
    """"""","["""""" 
    if (definition == 'plain'):
        return FormatPlain(log_token).format_line
    elif (definition == 'syslog'):
        return FormatSyslog(hostname, log_name, log_token).format_line
    if (definition and (definition.find('$line') >= 0)):
        return FormatCustom(definition, hostname, log_name, log_token).format_line
    return None
"""""", """""" 
    if (definition == 'plain'):
        return FormatCustom(log_token).format_line
    elif (definition == 'syslog'):
        return FormatSyslog(hostname, log_name, log_token).format_line
    if (definition and (definition.find('$line') >= 0)):
        return FormatPlain(definition, hostname, log_name, log_token).format_line
    return None
""""""]",1
"trace_and_save_torchscript, get_model_attributes = get_model_attributes, trace_and_save_torchscript
@ExportFactory.register('_dynamic_')
def export_to_torchscript_dynamic(args, task, model, inputs, output_base_dir, *, export_format=None, **kwargs):
    """"""Task returns the model based on the given export_format
    The code will try to access `task.get_{model_name}_model()` to get the model
    for export, `model_name` is extracted from `export_format` without `torchscript_`
    prefix.
    """"""","["""""" 
    assert hasattr(task, 'get_model_by_name')
    assert export_format.startswith('torchscript_')
    model_name = export_format[len('torchscript_'):]
    model = task.get_model_by_name(model_name, model)
    model_attrs = get_model_attributes(model)
    if ((model_attrs is not None) and ('data' in model_attrs)):
        inputs = model_attrs['data']
    print(f'Converting to {model_name}...')
    output_dir = os.path.join(output_base_dir, export_format)
    torch_script_path = trace_and_save_torchscript(model, inputs, output_dir, use_get_traceable=bool(args.use_get_traceable), trace_type=args.trace_type, opt_for_mobile=args.opt_for_mobile, model_attrs=model_attrs, save_for_lite_interpreter=args.save_for_lite_interpreter, save_bundle_input=args.save_bundle_input)
    return torch_script_path
"""""", """""" 
    assert hasattr(task, 'get_model_by_name')
    assert export_format.startswith('torchscript_')
    model_name = export_format[len('torchscript_'):]
    model = task.get_model_by_name(model_name, model)
    model_attrs = trace_and_save_torchscript(model)
    if ((model_attrs is not None) and ('data' in model_attrs)):
        inputs = model_attrs['data']
    print(f'Converting to {model_name}...')
    output_dir = os.path.join(output_base_dir, export_format)
    torch_script_path = get_model_attributes(model, inputs, output_dir, use_get_traceable=bool(args.use_get_traceable), trace_type=args.trace_type, opt_for_mobile=args.opt_for_mobile, model_attrs=model_attrs, save_for_lite_interpreter=args.save_for_lite_interpreter, save_bundle_input=args.save_bundle_input)
    return torch_script_path
""""""]",1
"extract_tarball, get_md5sum = get_md5sum, extract_tarball
def download_file(source_path, dest_path, extract_to=None, expected_md5=None):
    """"""Downloading utility.

    Downloads the specified test to the system location specified by the SACREBLEU environment variable.

    :param source_path: the remote uri to download
    :param dest_path: where to save the file
    :param extract_to: for tarballs, where to extract to
    :param expected_md5: the MD5 sum
    :return: the set of processed file names
    """"""","["""""" 
    import urllib.request
    import ssl
    outdir = os.path.dirname(dest_path)
    os.makedirs(outdir, exist_ok=True)
    lockfile = f'{dest_path}.lock'
    with portalocker.Lock(lockfile, timeout=60):
        if ((not os.path.exists(dest_path)) or (os.path.getsize(dest_path) == 0)):
            sacrelogger.info(f'Downloading {source_path} to {dest_path}')
            md5 = hashlib.md5()
            try:
                with urllib.request.urlopen(source_path) as f, open(dest_path, 'wb') as out:
                    out.write(f.read())
            except ssl.SSLError:
                sacrelogger.error('An SSL error was encountered in downloading the files. If you\'re on a Mac, you may need to run the ""Install Certificates.command"" file located in the ""Python 3"" folder, often found under /Applications')
                sys.exit(1)
            if (expected_md5 is not None):
                cur_md5 = get_md5sum(dest_path)
                if (cur_md5 != expected_md5):
                    sacrelogger.error(f'Fatal: MD5 sum of downloaded file was incorrect (got {cur_md5}, expected {expected_md5}).')
                    sacrelogger.error(f'Please manually delete {dest_path!r} and rerun the command.')
                    sacrelogger.error(f'If the problem persists, the tarball may have changed, in which case, please contact the SacreBLEU maintainer.')
                    sys.exit(1)
            if (extract_to is not None):
                extract_tarball(dest_path, extract_to)
"""""", """""" 
    import urllib.request
    import ssl
    outdir = os.path.dirname(dest_path)
    os.makedirs(outdir, exist_ok=True)
    lockfile = f'{dest_path}.lock'
    with portalocker.Lock(lockfile, timeout=60):
        if ((not os.path.exists(dest_path)) or (os.path.getsize(dest_path) == 0)):
            sacrelogger.info(f'Downloading {source_path} to {dest_path}')
            md5 = hashlib.md5()
            try:
                with urllib.request.urlopen(source_path) as f, open(dest_path, 'wb') as out:
                    out.write(f.read())
            except ssl.SSLError:
                sacrelogger.error('An SSL error was encountered in downloading the files. If you\'re on a Mac, you may need to run the ""Install Certificates.command"" file located in the ""Python 3"" folder, often found under /Applications')
                sys.exit(1)
            if (expected_md5 is not None):
                cur_md5 = extract_tarball(dest_path)
                if (cur_md5 != expected_md5):
                    sacrelogger.error(f'Fatal: MD5 sum of downloaded file was incorrect (got {cur_md5}, expected {expected_md5}).')
                    sacrelogger.error(f'Please manually delete {dest_path!r} and rerun the command.')
                    sacrelogger.error(f'If the problem persists, the tarball may have changed, in which case, please contact the SacreBLEU maintainer.')
                    sys.exit(1)
            if (extract_to is not None):
                get_md5sum(dest_path, extract_to)
""""""]",1
"tabulate, handle_error = handle_error, tabulate
@click.command(help=Texts.HELP, short_help=Texts.HELP, cls=AliasCmd, alias='v', options_metavar='[options]')
@common_options(verify_dependencies=False, verify_config_path=False)
@click.pass_context
def version(ctx: click.Context):
    """""" Returns the version of the installed nctl application. """"""","["""""" 
    platform_version: Optional[str] = Texts.INITIAL_PLATFORM_VERSION
    error_msg = ''
    platform_version_fail = False
    try:
        platform_version = NAUTAConfigMap(config_map_request_timeout=PLATFORM_VERSION_REQUEST_TIMEOUT).platform_version
        if (not platform_version):
            platform_version_fail = True
            raise ValueError(Texts.KUBECTL_INT_ERROR_MSG)
    except KubernetesError:
        error_msg = Texts.KUBECTL_INT_ERROR_MSG
        platform_version_fail = True
    except Exception:
        error_msg = Texts.OTHER_ERROR_MSG
        platform_version_fail = True
    version_table: List[list] = [[Texts.TABLE_APP_ROW_NAME, VERSION], [Texts.TABLE_PLATFORM_ROW_NAME, platform_version]]
    click.echo(tabulate(version_table, headers=Texts.TABLE_HEADERS, tablefmt=TBLT_TABLE_FORMAT))
    if platform_version_fail:
        handle_error(logger, error_msg, error_msg, add_verbosity_msg=(ctx.obj.verbosity == 0))
"""""", """""" 
    platform_version: Optional[str] = Texts.INITIAL_PLATFORM_VERSION
    error_msg = ''
    platform_version_fail = False
    try:
        platform_version = NAUTAConfigMap(config_map_request_timeout=PLATFORM_VERSION_REQUEST_TIMEOUT).platform_version
        if (not platform_version):
            platform_version_fail = True
            raise ValueError(Texts.KUBECTL_INT_ERROR_MSG)
    except KubernetesError:
        error_msg = Texts.KUBECTL_INT_ERROR_MSG
        platform_version_fail = True
    except Exception:
        error_msg = Texts.OTHER_ERROR_MSG
        platform_version_fail = True
    version_table: List[list] = [[Texts.TABLE_APP_ROW_NAME, VERSION], [Texts.TABLE_PLATFORM_ROW_NAME, platform_version]]
    click.echo(handle_error(version_table, headers=Texts.TABLE_HEADERS, tablefmt=TBLT_TABLE_FORMAT))
    if platform_version_fail:
        tabulate(logger, error_msg, error_msg, add_verbosity_msg=(ctx.obj.verbosity == 0))
""""""]",1
"_default_effort, _default_threads = _default_threads, _default_effort
def fft(x, n=None, axis=(- 1), overwrite_x=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""
    Perform an 1D FFT.

    The first three arguments are as per :func:`scipy.fftpack.fft`;
    the rest of the arguments are documented in the
    :ref:`additional argument docs<interfaces_additional_args>`.

    Warning: `scipy.fftpack` is considered legacy, new code should
    use `scipy.fft` instead.

    """"""","["""""" 
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return numpy_fft.fft(x, n, axis, None, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
"""""", """""" 
    planner_effort = _default_threads(planner_effort)
    threads = _default_effort(threads)
    return numpy_fft.fft(x, n, axis, None, overwrite_x, planner_effort, threads, auto_align_input, auto_contiguous)
""""""]",1
"_normalize, regex_match = regex_match, _normalize
def has_answer(answers, text, tokenizer, match_type) -> bool:
    """"""Check if a document contains an answer string.
    If `match_type` is string, token matching is done between the text and answer.
    If `match_type` is regex, we search the whole text with the regex.
    """"""","["""""" 
    text = _normalize(text)
    if (match_type == 'string'):
        text = tokenizer.tokenize(text).words(uncased=True)
        for single_answer in answers:
            single_answer = _normalize(single_answer)
            single_answer = tokenizer.tokenize(single_answer)
            single_answer = single_answer.words(uncased=True)
            for i in range(0, ((len(text) - len(single_answer)) + 1)):
                if (single_answer == text[i:(i + len(single_answer))]):
                    return True
    elif (match_type == 'regex'):
        for single_answer in answers:
            single_answer = _normalize(single_answer)
            if regex_match(text, single_answer):
                return True
    return False
"""""", """""" 
    text = regex_match(text)
    if (match_type == 'string'):
        text = tokenizer.tokenize(text).words(uncased=True)
        for single_answer in answers:
            single_answer = regex_match(single_answer)
            single_answer = tokenizer.tokenize(single_answer)
            single_answer = single_answer.words(uncased=True)
            for i in range(0, ((len(text) - len(single_answer)) + 1)):
                if (single_answer == text[i:(i + len(single_answer))]):
                    return True
    elif (match_type == 'regex'):
        for single_answer in answers:
            single_answer = regex_match(single_answer)
            if _normalize(text, single_answer):
                return True
    return False
""""""]",1
"get_data_folder, ImageFolderInstance = ImageFolderInstance, get_data_folder
def get_imagenet_dataloader(dataset='imagenet', batch_size=128, num_workers=16, is_instance=False):
    """"""
    Data Loader for imagenet
    """"""","["""""" 
    if (dataset == 'imagenet'):
        data_folder = get_data_folder()
    else:
        raise NotImplementedError('dataset not supported: {}'.format(dataset))
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    test_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])
    train_folder = os.path.join(data_folder, 'train')
    test_folder = os.path.join(data_folder, 'val')
    if is_instance:
        train_set = ImageFolderInstance(train_folder, transform=train_transform)
        n_data = len(train_set)
    else:
        train_set = datasets.ImageFolder(train_folder, transform=train_transform)
    test_set = datasets.ImageFolder(test_folder, transform=test_transform)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=(num_workers // 2), pin_memory=True)
    if is_instance:
        return (train_loader, test_loader, n_data)
    else:
        return (train_loader, test_loader)
"""""", """""" 
    if (dataset == 'imagenet'):
        data_folder = ImageFolderInstance()
    else:
        raise NotImplementedError('dataset not supported: {}'.format(dataset))
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    train_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), normalize])
    test_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), normalize])
    train_folder = os.path.join(data_folder, 'train')
    test_folder = os.path.join(data_folder, 'val')
    if is_instance:
        train_set = get_data_folder(train_folder, transform=train_transform)
        n_data = len(train_set)
    else:
        train_set = datasets.ImageFolder(train_folder, transform=train_transform)
    test_set = datasets.ImageFolder(test_folder, transform=test_transform)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=(num_workers // 2), pin_memory=True)
    if is_instance:
        return (train_loader, test_loader, n_data)
    else:
        return (train_loader, test_loader)
""""""]",1
"MaxNLocator, Patch = Patch, MaxNLocator
def plot_split_indices(cv, cv_args, X, y, groups, n_splits, image_file_path=None):
    """"""Create a sample plot for indices of a cross-validation object.""""""","["""""" 
    (fig, ax) = plt.subplots(figsize=(12, 4))
    cmap_data = plt.cm.tab20
    cmap_cv = plt.cm.coolwarm
    lw = 10
    marker_size = 200
    for (split_idx, (train_idx, test_idx)) in enumerate(cv.split(X=X, y=y, groups=groups)):
        indices = np.array(([np.nan] * len(X)))
        indices[test_idx] = 1
        indices[train_idx] = 0
        ax.scatter(range(len(X)), ([(split_idx + 0.5)] * len(X)), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=(- 0.4), vmax=1.4, s=marker_size)
    ax.scatter(range(len(X)), ([(split_idx + 1.5)] * len(X)), c=groups, marker='_', lw=lw, cmap=cmap_data, s=marker_size)
    yticklabels = (list(range(n_splits)) + ['group'])
    ax.set(yticks=(np.arange((n_splits + 1)) + 0.5), yticklabels=yticklabels, ylabel='CV iteration', ylim=[(n_splits + 1.2), (- 0.2)], xlim=[(- 0.5), (len(indices) - 0.5)])
    ax.legend([Patch(color=cmap_cv(0.2)), Patch(color=cmap_cv(0.8))], ['Training set', 'Testing set'], loc=(1.02, 0.8), fontsize=13)
    ax.set_title('{}\n{}'.format(type(cv).__name__, cv_args), fontsize=15)
    ax.xaxis.set_major_locator(MaxNLocator(min_n_ticks=len(X), integer=True))
    ax.set_xlabel(xlabel='Sample index', fontsize=13)
    ax.set_ylabel(ylabel='CV iteration', fontsize=13)
    ax.tick_params(axis='both', which='major', labelsize=13)
    ax.tick_params(axis='both', which='minor', labelsize=13)
    plt.tight_layout()
    if image_file_path:
        plt.savefig(image_file_path, bbox_inches='tight')
    plt.show()
"""""", """""" 
    (fig, ax) = plt.subplots(figsize=(12, 4))
    cmap_data = plt.cm.tab20
    cmap_cv = plt.cm.coolwarm
    lw = 10
    marker_size = 200
    for (split_idx, (train_idx, test_idx)) in enumerate(cv.split(X=X, y=y, groups=groups)):
        indices = np.array(([np.nan] * len(X)))
        indices[test_idx] = 1
        indices[train_idx] = 0
        ax.scatter(range(len(X)), ([(split_idx + 0.5)] * len(X)), c=indices, marker='_', lw=lw, cmap=cmap_cv, vmin=(- 0.4), vmax=1.4, s=marker_size)
    ax.scatter(range(len(X)), ([(split_idx + 1.5)] * len(X)), c=groups, marker='_', lw=lw, cmap=cmap_data, s=marker_size)
    yticklabels = (list(range(n_splits)) + ['group'])
    ax.set(yticks=(np.arange((n_splits + 1)) + 0.5), yticklabels=yticklabels, ylabel='CV iteration', ylim=[(n_splits + 1.2), (- 0.2)], xlim=[(- 0.5), (len(indices) - 0.5)])
    ax.legend([MaxNLocator(color=cmap_cv(0.2)), MaxNLocator(color=cmap_cv(0.8))], ['Training set', 'Testing set'], loc=(1.02, 0.8), fontsize=13)
    ax.set_title('{}\n{}'.format(type(cv).__name__, cv_args), fontsize=15)
    ax.xaxis.set_major_locator(Patch(min_n_ticks=len(X), integer=True))
    ax.set_xlabel(xlabel='Sample index', fontsize=13)
    ax.set_ylabel(ylabel='CV iteration', fontsize=13)
    ax.tick_params(axis='both', which='major', labelsize=13)
    ax.tick_params(axis='both', which='minor', labelsize=13)
    plt.tight_layout()
    if image_file_path:
        plt.savefig(image_file_path, bbox_inches='tight')
    plt.show()
""""""]",1
"cleandoc, main = main, cleandoc
def test_annotate_force_dot_license_identical_to_explicit_license(fake_repository, stringio, mock_date_today):
    """"""For backwards compatibility, --force-dot-license should have identical
    results as --explicit-license.
    """"""","["""""" 
    files = [(fake_repository / 'foo.py'), (fake_repository / 'bar.py')]
    for path in files:
        path.write_text('pass')
    expected = cleandoc('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    for (arg, path) in zip(('--force-dot-license', '--explicit-license'), files):
        main(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', arg, str(path)], out=stringio)
    for path in files:
        assert (path.with_name(f'{path.name}.license').read_text().strip() == expected)
        assert (path.read_text() == 'pass')
"""""", """""" 
    files = [(fake_repository / 'foo.py'), (fake_repository / 'bar.py')]
    for path in files:
        path.write_text('pass')
    expected = main('\n        SPDX-FileCopyrightText: 2018 Jane Doe\n\n        SPDX-License-Identifier: GPL-3.0-or-later\n        ')
    for (arg, path) in zip(('--force-dot-license', '--explicit-license'), files):
        cleandoc(['annotate', '--license', 'GPL-3.0-or-later', '--copyright', 'Jane Doe', arg, str(path)], out=stringio)
    for path in files:
        assert (path.with_name(f'{path.name}.license').read_text().strip() == expected)
        assert (path.read_text() == 'pass')
""""""]",1
"friend_name_exists, create_folder = create_folder, friend_name_exists
def setup():
    """"""
    create new setup
    :return:
    """"""","["""""" 
    create_folder(PEOPLE_CONFIG_FOLDER_PATH)
    click.echo(chalk.blue('Enter their name:'))
    name = input().strip().lower()
    if friend_name_exists(name):
        click.echo(chalk.red('A configuration with this friend name already exists.Please type ""yoda people --help""'))
    click.echo(chalk.blue('Input their DOB (YYYY-MM-DD):'))
    incorrect_date_format = True
    while incorrect_date_format:
        dob = input().strip()
        try:
            date_str = datetime.datetime.strptime(dob, '%Y-%m-%d').strftime('%Y-%m-%d')
            if (date_str != dob):
                raise ValueError
            incorrect_date_format = False
        except ValueError:
            click.echo(chalk.red('Incorrect data format, should be YYYY-MM-DD. Please repeat:'))
    click.echo(chalk.blue('Enter their Mobile Number:'))
    mobile = input().strip()
    if os.path.isfile(PEOPLE_CONFIG_FILE_PATH):
        setup_data = dict(name=name, mobile=mobile, dob=dob)
        append_data_into_file(setup_data, PEOPLE_CONFIG_FILE_PATH)
    else:
        setup_data = dict(entries=[dict(name=name, mobile=mobile, dob=dob)])
        input_data(setup_data, PEOPLE_CONFIG_FILE_PATH)
    input_data(dict(entries=[]), get_friends_file_path(name))
"""""", """""" 
    friend_name_exists(PEOPLE_CONFIG_FOLDER_PATH)
    click.echo(chalk.blue('Enter their name:'))
    name = input().strip().lower()
    if create_folder(name):
        click.echo(chalk.red('A configuration with this friend name already exists.Please type ""yoda people --help""'))
    click.echo(chalk.blue('Input their DOB (YYYY-MM-DD):'))
    incorrect_date_format = True
    while incorrect_date_format:
        dob = input().strip()
        try:
            date_str = datetime.datetime.strptime(dob, '%Y-%m-%d').strftime('%Y-%m-%d')
            if (date_str != dob):
                raise ValueError
            incorrect_date_format = False
        except ValueError:
            click.echo(chalk.red('Incorrect data format, should be YYYY-MM-DD. Please repeat:'))
    click.echo(chalk.blue('Enter their Mobile Number:'))
    mobile = input().strip()
    if os.path.isfile(PEOPLE_CONFIG_FILE_PATH):
        setup_data = dict(name=name, mobile=mobile, dob=dob)
        append_data_into_file(setup_data, PEOPLE_CONFIG_FILE_PATH)
    else:
        setup_data = dict(entries=[dict(name=name, mobile=mobile, dob=dob)])
        input_data(setup_data, PEOPLE_CONFIG_FILE_PATH)
    input_data(dict(entries=[]), get_friends_file_path(name))
""""""]",1
"_Xfftn, _default_threads = _default_threads, _Xfftn
def fftn(a, s=None, axes=None, norm=None, overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True):
    """"""Perform an n-D FFT.

    The first four arguments are as per :func:`numpy.fft.fftn`;
    the rest of the arguments are documented
    in the :ref:`additional arguments docs<interfaces_additional_args>`.
    """"""","["""""" 
    calling_func = 'fftn'
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
"""""", """""" 
    calling_func = 'fftn'
    planner_effort = _default_effort(planner_effort)
    threads = _Xfftn(threads)
    return _default_threads(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, calling_func, **_norm_args(norm))
""""""]",1
"StringToCMakeTargetName, NormjoinPathForceCMakeSource = NormjoinPathForceCMakeSource, StringToCMakeTargetName
def WriteActions(target_name, actions, extra_sources, extra_deps, path_to_gyp, output):
    """"""Write CMake for the 'actions' in the target.

  Args:
    target_name: the name of the CMake target being generated.
    actions: the Gyp 'actions' dict for this target.
    extra_sources: [(<cmake_src>, <src>)] to append with generated source files.
    extra_deps: [<cmake_taget>] to append with generated targets.
    path_to_gyp: relative path from CMakeLists.txt being generated to
        the Gyp file in which the target being generated is defined.
  """"""","["""""" 
    for action in actions:
        action_name = StringToCMakeTargetName(action['action_name'])
        action_target_name = f'{target_name}__{action_name}'
        inputs = action['inputs']
        inputs_name = (action_target_name + '__input')
        SetVariableList(output, inputs_name, [NormjoinPathForceCMakeSource(path_to_gyp, dep) for dep in inputs])
        outputs = action['outputs']
        cmake_outputs = [NormjoinPathForceCMakeSource(path_to_gyp, out) for out in outputs]
        outputs_name = (action_target_name + '__output')
        SetVariableList(output, outputs_name, cmake_outputs)
        dirs = {dir for dir in (os.path.dirname(o) for o in outputs) if dir}
        if int(action.get('process_outputs_as_sources', False)):
            extra_sources.extend(zip(cmake_outputs, outputs))
        output.write('add_custom_command(OUTPUT ')
        WriteVariable(output, outputs_name)
        output.write('\n')
        if (len(dirs) > 0):
            for directory in dirs:
                output.write('  COMMAND ${CMAKE_COMMAND} -E make_directory ')
                output.write(directory)
                output.write('\n')
        output.write('  COMMAND ')
        output.write(gyp.common.EncodePOSIXShellList(action['action']))
        output.write('\n')
        output.write('  DEPENDS ')
        WriteVariable(output, inputs_name)
        output.write('\n')
        output.write('  WORKING_DIRECTORY ${CMAKE_CURRENT_LIST_DIR}/')
        output.write(path_to_gyp)
        output.write('\n')
        output.write('  COMMENT ')
        if ('message' in action):
            output.write(action['message'])
        else:
            output.write(action_target_name)
        output.write('\n')
        output.write('  VERBATIM\n')
        output.write(')\n')
        output.write('add_custom_target(')
        output.write(action_target_name)
        output.write('\n  DEPENDS ')
        WriteVariable(output, outputs_name)
        output.write('\n  SOURCES ')
        WriteVariable(output, inputs_name)
        output.write('\n)\n')
        extra_deps.append(action_target_name)
"""""", """""" 
    for action in actions:
        action_name = NormjoinPathForceCMakeSource(action['action_name'])
        action_target_name = f'{target_name}__{action_name}'
        inputs = action['inputs']
        inputs_name = (action_target_name + '__input')
        SetVariableList(output, inputs_name, [StringToCMakeTargetName(path_to_gyp, dep) for dep in inputs])
        outputs = action['outputs']
        cmake_outputs = [StringToCMakeTargetName(path_to_gyp, out) for out in outputs]
        outputs_name = (action_target_name + '__output')
        SetVariableList(output, outputs_name, cmake_outputs)
        dirs = {dir for dir in (os.path.dirname(o) for o in outputs) if dir}
        if int(action.get('process_outputs_as_sources', False)):
            extra_sources.extend(zip(cmake_outputs, outputs))
        output.write('add_custom_command(OUTPUT ')
        WriteVariable(output, outputs_name)
        output.write('\n')
        if (len(dirs) > 0):
            for directory in dirs:
                output.write('  COMMAND ${CMAKE_COMMAND} -E make_directory ')
                output.write(directory)
                output.write('\n')
        output.write('  COMMAND ')
        output.write(gyp.common.EncodePOSIXShellList(action['action']))
        output.write('\n')
        output.write('  DEPENDS ')
        WriteVariable(output, inputs_name)
        output.write('\n')
        output.write('  WORKING_DIRECTORY ${CMAKE_CURRENT_LIST_DIR}/')
        output.write(path_to_gyp)
        output.write('\n')
        output.write('  COMMENT ')
        if ('message' in action):
            output.write(action['message'])
        else:
            output.write(action_target_name)
        output.write('\n')
        output.write('  VERBATIM\n')
        output.write(')\n')
        output.write('add_custom_target(')
        output.write(action_target_name)
        output.write('\n  DEPENDS ')
        WriteVariable(output, outputs_name)
        output.write('\n  SOURCES ')
        WriteVariable(output, inputs_name)
        output.write('\n)\n')
        extra_deps.append(action_target_name)
""""""]",1
"end_of_chunk, start_of_chunk = start_of_chunk, end_of_chunk
def get_entities(seq, suffix=False):
    """"""Gets entities from sequence.

    Args:
      seq(list): sequence of labels.
      suffix:  (Default value = False)

    Returns:
      list: list of (chunk_type, chunk_start, chunk_end).
      Example:

    >>> from seqeval.metrics.sequence_labeling import get_entities
        >>> seq = ['B-PER', 'I-PER', 'O', 'B-LOC']
        >>> get_entities(seq)
        [('PER', 0, 2), ('LOC', 3, 4)]
    """"""","["""""" 
    if any((isinstance(s, list) for s in seq)):
        seq = [item for sublist in seq for item in (sublist + ['O'])]
    prev_tag = 'O'
    prev_type = ''
    begin_offset = 0
    chunks = []
    for (i, chunk) in enumerate((seq + ['O'])):
        if suffix:
            tag = chunk[(- 1)]
            type_ = chunk[:(- 2)]
        else:
            tag = chunk[0]
            type_ = chunk[2:]
        if end_of_chunk(prev_tag, tag, prev_type, type_):
            chunks.append((prev_type, begin_offset, i))
        if start_of_chunk(prev_tag, tag, prev_type, type_):
            begin_offset = i
        prev_tag = tag
        prev_type = type_
    return chunks
"""""", """""" 
    if any((isinstance(s, list) for s in seq)):
        seq = [item for sublist in seq for item in (sublist + ['O'])]
    prev_tag = 'O'
    prev_type = ''
    begin_offset = 0
    chunks = []
    for (i, chunk) in enumerate((seq + ['O'])):
        if suffix:
            tag = chunk[(- 1)]
            type_ = chunk[:(- 2)]
        else:
            tag = chunk[0]
            type_ = chunk[2:]
        if start_of_chunk(prev_tag, tag, prev_type, type_):
            chunks.append((prev_type, begin_offset, i))
        if end_of_chunk(prev_tag, tag, prev_type, type_):
            begin_offset = i
        prev_tag = tag
        prev_type = type_
    return chunks
""""""]",1
"print_error, print_info = print_info, print_error
def check_import():
    """"""
    Try to import the aeneas package and return ``True`` if that fails.
    """"""","["""""" 
    try:
        import aeneas
        print_success(u'aeneas         OK')
        return False
    except ImportError:
        print_error(u'aeneas         ERROR')
        print_info(u'  Unable to load the aeneas Python package')
        print_info(u'  This error is probably caused by:')
        print_info(u'    A. you did not download/git-clone the aeneas package properly; or')
        print_info(u'    B. you did not install the required Python packages:')
        print_info(u'      1. BeautifulSoup4')
        print_info(u'      2. lxml')
        print_info(u'      3. numpy')
    except Exception as e:
        print_error(e)
    return True
"""""", """""" 
    try:
        import aeneas
        print_success(u'aeneas         OK')
        return False
    except ImportError:
        print_info(u'aeneas         ERROR')
        print_error(u'  Unable to load the aeneas Python package')
        print_error(u'  This error is probably caused by:')
        print_error(u'    A. you did not download/git-clone the aeneas package properly; or')
        print_error(u'    B. you did not install the required Python packages:')
        print_error(u'      1. BeautifulSoup4')
        print_error(u'      2. lxml')
        print_error(u'      3. numpy')
    except Exception as e:
        print_info(e)
    return True
""""""]",1
"common_options, list_runs_in_cli = list_runs_in_cli, common_options
@click.command(name='list', help=Texts.HELP, short_help=Texts.HELP, cls=AliasCmd, alias='ls', options_metavar='[options]')
@click.option('-a', '--all-users', is_flag=True, help=Texts.HELP_A)
@click.option('-n', '--name', type=str, help=Texts.HELP_N)
@click.option('-s', '--status', type=click.Choice([status.name for status in RunStatus]), help=Texts.HELP_S)
@click.option('-u', '--uninitialized', is_flag=True, help=Texts.HELP_U)
@click.option('-c', '--count', type=click.IntRange(min=1), help=Texts.HELP_C)
@click.option('-b', '--brief', is_flag=True, help=Texts.HELP_B)
@common_options()
@click.pass_context
def list_inference_instances(ctx: click.Context, all_users: bool, name: str, status: str, uninitialized: bool, count: int, brief: bool):
    """""" List inference instances. """"""","["""""" 
    status = (RunStatus[status] if status else None)
    if brief:
        table_headers = [RUN_INFERENCE_NAME, RUN_SUBMISSION_DATE, RUN_SUBMITTER, RUN_STATUS]
    else:
        table_headers = [RUN_INFERENCE_NAME, RUN_PARAMETERS, RUN_SUBMISSION_DATE, RUN_START_DATE, RUN_DURATION, RUN_SUBMITTER, RUN_STATUS, RUN_TEMPLATE_NAME, RUN_TEMPLATE_VERSION]
    if uninitialized:
        list_unitialized_experiments_in_cli(verbosity_lvl=ctx.obj.verbosity, all_users=all_users, name=name, headers=table_headers, listed_runs_kinds=LISTED_RUNS_KINDS, count=count, brief=brief)
    else:
        list_runs_in_cli(ctx.obj.verbosity, all_users, name, LISTED_RUNS_KINDS, table_headers, status=status, with_metrics=False, count=count, brief=brief)
"""""", """""" 
    status = (RunStatus[status] if status else None)
    if brief:
        table_headers = [RUN_INFERENCE_NAME, RUN_SUBMISSION_DATE, RUN_SUBMITTER, RUN_STATUS]
    else:
        table_headers = [RUN_INFERENCE_NAME, RUN_PARAMETERS, RUN_SUBMISSION_DATE, RUN_START_DATE, RUN_DURATION, RUN_SUBMITTER, RUN_STATUS, RUN_TEMPLATE_NAME, RUN_TEMPLATE_VERSION]
    if uninitialized:
        list_unitialized_experiments_in_cli(verbosity_lvl=ctx.obj.verbosity, all_users=all_users, name=name, headers=table_headers, listed_runs_kinds=LISTED_RUNS_KINDS, count=count, brief=brief)
    else:
        common_options(ctx.obj.verbosity, all_users, name, LISTED_RUNS_KINDS, table_headers, status=status, with_metrics=False, count=count, brief=brief)
""""""]",1
"list_unitialized_experiments_in_cli, common_options = common_options, list_unitialized_experiments_in_cli
@click.command(name='list', short_help=Texts.SHORT_HELP, cls=AliasCmd, alias='ls', options_metavar='[options]')
@click.option('-a', '--all-users', is_flag=True, help=Texts.HELP_A)
@click.option('-n', '--name', type=str, help=Texts.HELP_N)
@click.option('-s', '--status', type=click.Choice([status.name for status in RunStatus]), help=Texts.HELP_S)
@click.option('-u', '--uninitialized', is_flag=True, help=Texts.HELP_U)
@click.option('-c', '--count', type=click.IntRange(min=1), help=Texts.HELP_C)
@click.option('-b', '--brief', is_flag=True, help=Texts.HELP_B)
@common_options()
@click.pass_context
def list_experiments(ctx: click.Context, all_users: bool, name: str, status: str, uninitialized: bool, count: int, brief: bool):
    """""" List experiments. """"""","["""""" 
    status = (RunStatus[status] if status else None)
    if brief:
        list_headers = [RUN_NAME, RUN_SUBMISSION_DATE, RUN_SUBMITTER, RUN_STATUS]
    else:
        list_headers = EXPERIMENTS_LIST_HEADERS
    if uninitialized:
        list_unitialized_experiments_in_cli(verbosity_lvl=ctx.obj.verbosity, all_users=all_users, name=name, headers=list_headers, count=count, brief=brief)
    else:
        list_runs_in_cli(ctx.obj.verbosity, all_users, name, LISTED_RUNS_KINDS, list_headers, with_metrics=True, status=status, count=count, brief=brief)
"""""", """""" 
    status = (RunStatus[status] if status else None)
    if brief:
        list_headers = [RUN_NAME, RUN_SUBMISSION_DATE, RUN_SUBMITTER, RUN_STATUS]
    else:
        list_headers = EXPERIMENTS_LIST_HEADERS
    if uninitialized:
        common_options(verbosity_lvl=ctx.obj.verbosity, all_users=all_users, name=name, headers=list_headers, count=count, brief=brief)
    else:
        list_runs_in_cli(ctx.obj.verbosity, all_users, name, LISTED_RUNS_KINDS, list_headers, with_metrics=True, status=status, count=count, brief=brief)
""""""]",1
"print_warning, check_import = check_import, print_warning
def main():
    """""" The entry point for this module """"""","["""""" 
    if check_import():
        sys.exit(1)
    from aeneas.diagnostics import Diagnostics
    (errors, warnings, c_ext_warnings) = Diagnostics.check_all()
    if errors:
        sys.exit(1)
    if c_ext_warnings:
        print_warning(u'All required dependencies are met but at least one Python C extension is not available')
        print_warning(u'You can still run aeneas but it will be slower')
        print_warning(u'Enjoy running aeneas!')
        sys.exit(2)
    else:
        print_success(u'All required dependencies are met and all available Python C extensions are working')
        print_success(u'Enjoy running aeneas!')
        sys.exit(0)
"""""", """""" 
    if print_warning():
        sys.exit(1)
    from aeneas.diagnostics import Diagnostics
    (errors, warnings, c_ext_warnings) = Diagnostics.check_all()
    if errors:
        sys.exit(1)
    if c_ext_warnings:
        check_import(u'All required dependencies are met but at least one Python C extension is not available')
        check_import(u'You can still run aeneas but it will be slower')
        check_import(u'Enjoy running aeneas!')
        sys.exit(2)
    else:
        print_success(u'All required dependencies are met and all available Python C extensions are working')
        print_success(u'Enjoy running aeneas!')
        sys.exit(0)
""""""]",1
"tqdm, EncodedDataset = EncodedDataset, tqdm
def encode(args, sentences, sort_input=False):
    """"""Create an EncodedDataset from a list of sentences

    Parameters:
    sentences (list[list[string]]): list of elements. Each element is a list
                                    that contains either a single sentence
                                    or two sentences
    sort_input (bool): if true, sort sentences by number of tokens in them

    Returns:
    dataset (EncodedDataset): an object that contains the contextual
                              representations of the input sentences
    """"""","["""""" 
    print('Language Models: {}'.format(args.lm))
    model = build_model_by_name(args.lm, args)
    if sort_input:
        sorted(sentences, key=(lambda k: len(' '.join(k).split())))
    encoded_sents = []
    for current_batch in tqdm(_batchify(sentences, args.batch_size)):
        (embeddings, sent_lens, tokenized_sents) = model.get_contextual_embeddings(current_batch)
        agg_embeddings = _aggregate_layers(embeddings)
        sent_embeddings = [agg_embeddings[i, :l] for (i, l) in enumerate(sent_lens)]
        encoded_sents.extend(list(zip(sent_embeddings, sent_lens, tokenized_sents)))
    dataset = EncodedDataset(encoded_sents)
    return dataset
"""""", """""" 
    print('Language Models: {}'.format(args.lm))
    model = build_model_by_name(args.lm, args)
    if sort_input:
        sorted(sentences, key=(lambda k: len(' '.join(k).split())))
    encoded_sents = []
    for current_batch in EncodedDataset(_batchify(sentences, args.batch_size)):
        (embeddings, sent_lens, tokenized_sents) = model.get_contextual_embeddings(current_batch)
        agg_embeddings = _aggregate_layers(embeddings)
        sent_embeddings = [agg_embeddings[i, :l] for (i, l) in enumerate(sent_lens)]
        encoded_sents.extend(list(zip(sent_embeddings, sent_lens, tokenized_sents)))
    dataset = tqdm(encoded_sents)
    return dataset
""""""]",1
"get_or_abort, render_template = render_template, get_or_abort
@app.route('/node/<node_name>', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/node/<node_name>')
def node(env, node_name):
    """"""Display a dashboard for a node showing as much data as we have on that
    node. This includes facts and reports but not Resources as that is too
    heavy to do within a single request.

    :param env: Ensure that the node, facts and reports are in this environment
    :type env: :obj:`string`
    """"""","["""""" 
    envs = environments()
    check_env(env, envs)
    query = AndOperator()
    if (env != '*'):
        query.add(EqualsOperator('environment', env))
    query.add(EqualsOperator('certname', node_name))
    node = get_or_abort(puppetdb.node, node_name)
    return render_template('node.html', node=node, envs=envs, current_env=env, columns=REPORTS_COLUMNS[:2])
"""""", """""" 
    envs = environments()
    check_env(env, envs)
    query = AndOperator()
    if (env != '*'):
        query.add(EqualsOperator('environment', env))
    query.add(EqualsOperator('certname', node_name))
    node = render_template(puppetdb.node, node_name)
    return get_or_abort('node.html', node=node, envs=envs, current_env=env, columns=REPORTS_COLUMNS[:2])
""""""]",1
"input_data, get_input = get_input, input_data
def add_to_lending_list(params):
    """"""
    add anything to the reading list
    """"""","["""""" 
    click.echo(chalk.blue('Did you lend or borrow this item? (l/b)'))

    def add_to_list(_status, _item, _person, _enddate):
        setup_data = dict(status=_status, item=_item, person=_person, enddate=_enddate)
        if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
            append_data_into_file(setup_data, LENDLIST_CONFIG_FILE_PATH)
        else:
            setup_data = dict(entries=[setup_data])
            create_folder(os.path.join(LIFE_CONFIG_FOLDER_PATH, 'lendlist'))
            input_data(setup_data, LENDLIST_CONFIG_FILE_PATH)
        click.echo(chalk.blue((('Added ' + _item) + ' to your lease list!')))

    def status_check(_status):
        if (_status == 'l'):
            _status = 'Lent'
            click.echo(chalk.blue('What item did you lend?'))
            _item = get_input()
            click.echo(chalk.blue('Who did you lend it to?'))
            _person = get_input()
            while True:
                try:
                    click.echo(chalk.blue('When do you need it back? (dd/mm/yyyy)'))
                    date_entry = get_input()
                    (day, month, year) = map(int, date_entry.split('/'))
                    _enddate = datetime.date(year, month, day)
                    break
                except:
                    click.echo(chalk.red('Invalid date input!'))
            add_to_list(_status, _item, _person, _enddate)
        elif (_status == 'b'):
            _status = 'Borrowed'
            click.echo(chalk.blue('What item did you borrow?'))
            _item = get_input()
            click.echo(chalk.blue('Who did you borrow it from?'))
            _person = get_input()
            while True:
                try:
                    click.echo(chalk.blue('When do you need to give it back? (dd/mm/yy)'))
                    date_entry = get_input()
                    (day, month, year) = map(int, date_entry.split('/'))
                    _enddate = datetime.date(year, month, day)
                    break
                except:
                    click.echo(chalk.red('Invalid date input!'))
            add_to_list(_status, _item, _person, _enddate)
        else:
            click.echo(chalk.red('Input not recognised! Type l for lent and b for borrowed.'))
            _status = get_input().lower()
            status_check(_status)
    _status = get_input().lower()
    status_check(_status)
"""""", """""" 
    click.echo(chalk.blue('Did you lend or borrow this item? (l/b)'))

    def add_to_list(_status, _item, _person, _enddate):
        setup_data = dict(status=_status, item=_item, person=_person, enddate=_enddate)
        if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
            append_data_into_file(setup_data, LENDLIST_CONFIG_FILE_PATH)
        else:
            setup_data = dict(entries=[setup_data])
            create_folder(os.path.join(LIFE_CONFIG_FOLDER_PATH, 'lendlist'))
            get_input(setup_data, LENDLIST_CONFIG_FILE_PATH)
        click.echo(chalk.blue((('Added ' + _item) + ' to your lease list!')))

    def status_check(_status):
        if (_status == 'l'):
            _status = 'Lent'
            click.echo(chalk.blue('What item did you lend?'))
            _item = input_data()
            click.echo(chalk.blue('Who did you lend it to?'))
            _person = input_data()
            while True:
                try:
                    click.echo(chalk.blue('When do you need it back? (dd/mm/yyyy)'))
                    date_entry = input_data()
                    (day, month, year) = map(int, date_entry.split('/'))
                    _enddate = datetime.date(year, month, day)
                    break
                except:
                    click.echo(chalk.red('Invalid date input!'))
            add_to_list(_status, _item, _person, _enddate)
        elif (_status == 'b'):
            _status = 'Borrowed'
            click.echo(chalk.blue('What item did you borrow?'))
            _item = input_data()
            click.echo(chalk.blue('Who did you borrow it from?'))
            _person = input_data()
            while True:
                try:
                    click.echo(chalk.blue('When do you need to give it back? (dd/mm/yy)'))
                    date_entry = input_data()
                    (day, month, year) = map(int, date_entry.split('/'))
                    _enddate = datetime.date(year, month, day)
                    break
                except:
                    click.echo(chalk.red('Invalid date input!'))
            add_to_list(_status, _item, _person, _enddate)
        else:
            click.echo(chalk.red('Input not recognised! Type l for lent and b for borrowed.'))
            _status = input_data().lower()
            status_check(_status)
    _status = input_data().lower()
    status_check(_status)
""""""]",1
"_EscapeCommandLineArgumentForMSBuild, _EscapeEnvironmentVariableExpansion = _EscapeEnvironmentVariableExpansion, _EscapeCommandLineArgumentForMSBuild
def _EscapeCppDefineForMSBuild(s):
    """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""","["""""" 
    s = _EscapeEnvironmentVariableExpansion(s)
    s = _EscapeCommandLineArgumentForMSBuild(s)
    s = _EscapeMSBuildSpecialCharacters(s)
    s = s.replace('#', ('\\%03o' % ord('#')))
    return s
"""""", """""" 
    s = _EscapeCommandLineArgumentForMSBuild(s)
    s = _EscapeEnvironmentVariableExpansion(s)
    s = _EscapeMSBuildSpecialCharacters(s)
    s = s.replace('#', ('\\%03o' % ord('#')))
    return s
""""""]",1
"make_dataset, env_class = env_class, make_dataset
def make_env_fn(config: 'DictConfig', env_class: Union[(Type[Env], Type[RLEnv])]) -> Union[(Env, RLEnv)]:
    """"""Creates an env of type env_class with specified config and rank.
    This is to be passed in as an argument when creating VectorEnv.

    Args:
        config: root exp config that has core env config node as well as
            env-specific config node.
        env_class: class type of the env to be created.

    Returns:
        env object created according to specification.
    """"""","["""""" 
    if ('habitat' in config):
        config = config.habitat
    dataset = make_dataset(config.dataset.type, config=config.dataset)
    env = env_class(config=config, dataset=dataset)
    env.seed(config.seed)
    return env
"""""", """""" 
    if ('habitat' in config):
        config = config.habitat
    dataset = env_class(config.dataset.type, config=config.dataset)
    env = make_dataset(config=config, dataset=dataset)
    env.seed(config.seed)
    return env
""""""]",1
"jsonify, get_or_abort = get_or_abort, jsonify
@app.route('/daily_reports_chart.json', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/daily_reports_chart.json')
def daily_reports_chart(env):
    """"""Return JSON data to generate a bar chart of daily runs.

    If certname is passed as GET argument, the data will target that
    node only.
    """"""","["""""" 
    certname = request.args.get('certname')
    result = get_or_abort(get_daily_reports_chart, db=puppetdb, env=env, days_number=app.config['DAILY_REPORTS_CHART_DAYS'], certname=certname)
    return jsonify(result=result)
"""""", """""" 
    certname = request.args.get('certname')
    result = jsonify(get_daily_reports_chart, db=puppetdb, env=env, days_number=app.config['DAILY_REPORTS_CHART_DAYS'], certname=certname)
    return get_or_abort(result=result)
""""""]",1
"_size_to_index, scatter_log_softmax = scatter_log_softmax, _size_to_index
def variadic_log_softmax(input, size):
    """"""
    Compute log softmax over categories with variadic sizes.

    Suppose there are :math:`N` samples, and the numbers of categories in all samples are summed to :math:`B`.

    Parameters:
        input (Tensor): input of shape :math:`(B, ...)`
        size (LongTensor): number of categories of shape :math:`(N,)`
    """"""","["""""" 
    index2sample = _size_to_index(size)
    index2sample = index2sample.view(([(- 1)] + ([1] * (input.ndim - 1))))
    index2sample = index2sample.expand_as(input)
    log_likelihood = scatter_log_softmax(input, index2sample, dim=0)
    return log_likelihood
"""""", """""" 
    index2sample = scatter_log_softmax(size)
    index2sample = index2sample.view(([(- 1)] + ([1] * (input.ndim - 1))))
    index2sample = index2sample.expand_as(input)
    log_likelihood = _size_to_index(input, index2sample, dim=0)
    return log_likelihood
""""""]",1
"enableWarningClass, suppressWarningClass = suppressWarningClass, enableWarningClass
def process_warn_strings(arguments):
    """"""Process requests to enable/disable warnings.

    The requests are strings passed to the --warn option or the
    SetOption('warn') function.

    An argument to this option should be of the form ""warning-class""
    or ""no-warning-class"".  The warning class is munged and has
    the suffix ""Warning"" added in order to get an actual class name
    from the classes above, which we need to pass to the
    {enable,disable}WarningClass() functions.

    For example, ""deprecated"" will enable the DeprecatedWarning class.
    ""no-dependency"" will disable the DependencyWarning class.

    As a special case, --warn=all and --warn=no-all will enable or
    disable (respectively) the base class of all SCons warnings.
    """"""","["""""" 

    def _classmunge(s):
        'Convert a warning argument to SConsCase.\n\n        The result is CamelCase, except ""Scons"" is changed to ""SCons""\n        '
        s = s.replace('-', ' ').title().replace(' ', '')
        return s.replace('Scons', 'SCons')
    for arg in arguments:
        enable = True
        if arg.startswith('no-'):
            enable = False
            arg = arg[len('no-'):]
        if (arg == 'all'):
            class_name = 'SConsWarning'
        else:
            class_name = (_classmunge(arg) + 'Warning')
        try:
            clazz = globals()[class_name]
        except KeyError:
            sys.stderr.write((""No warning type: '%s'\n"" % arg))
        else:
            if enable:
                enableWarningClass(clazz)
            elif issubclass(clazz, MandatoryDeprecatedWarning):
                fmt = ""Can not disable mandataory warning: '%s'\n""
                sys.stderr.write((fmt % arg))
            else:
                suppressWarningClass(clazz)
"""""", """""" 

    def _classmunge(s):
        'Convert a warning argument to SConsCase.\n\n        The result is CamelCase, except ""Scons"" is changed to ""SCons""\n        '
        s = s.replace('-', ' ').title().replace(' ', '')
        return s.replace('Scons', 'SCons')
    for arg in arguments:
        enable = True
        if arg.startswith('no-'):
            enable = False
            arg = arg[len('no-'):]
        if (arg == 'all'):
            class_name = 'SConsWarning'
        else:
            class_name = (_classmunge(arg) + 'Warning')
        try:
            clazz = globals()[class_name]
        except KeyError:
            sys.stderr.write((""No warning type: '%s'\n"" % arg))
        else:
            if enable:
                suppressWarningClass(clazz)
            elif issubclass(clazz, MandatoryDeprecatedWarning):
                fmt = ""Can not disable mandataory warning: '%s'\n""
                sys.stderr.write((fmt % arg))
            else:
                enableWarningClass(clazz)
""""""]",1
"QueryForm, get_or_abort_except_client_errors = get_or_abort_except_client_errors, QueryForm
@app.route('/query', methods=('GET', 'POST'), defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/query', methods=('GET', 'POST'))
def query(env):
    """"""Allows to execute raw, user created queries against PuppetDB. This will return
    the JSON of the response or a message telling you what went wrong why nothing was returned.

    :param env: Serves no purpose for the query data but is required for the select field in
     the environment block
    :type env: :obj:`string`
    """"""","["""""" 
    if (not app.config['ENABLE_QUERY']):
        log.warning('Access to query interface disabled by administrator.')
        abort(403)
    envs = environments()
    if (env != app.config['DEFAULT_ENVIRONMENT']):
        check_env(env, envs)
    form = QueryForm(meta={'csrf_secret': app.config['SECRET_KEY'], 'csrf_context': session})
    if form.validate_on_submit():
        if (form.endpoints.data not in ENABLED_QUERY_ENDPOINTS):
            log.warning('Access to query endpoint %s disabled by administrator.', form.endpoints.data)
            abort(403)
        query = form.query.data.strip()
        if ((form.endpoints.data != 'pql') and (not query.startswith('['))):
            query = f'[{query}]'
        try:
            result = get_or_abort_except_client_errors(puppetdb._query, form.endpoints.data, query=query)
            zero_results = (len(result) == 0)
            result = (result if (not zero_results) else None)
            if form.rawjson.data:
                return render_template('query.html', form=form, zero_results=zero_results, result=result, columns=None, envs=envs, current_env=env)
            else:
                rows = []
                if (not zero_results):
                    columns = result[0].keys()
                    for items in result:
                        rows.append(list(items.values()))
                else:
                    columns = []
                return render_template('query.html', form=form, zero_results=zero_results, result=rows, columns=columns, envs=envs, current_env=env)
        except HTTPError as e:
            error_text = e.response.text
            return render_template('query.html', form=form, error_text=error_text, envs=envs, current_env=env)
    return render_template('query.html', form=form, envs=envs, current_env=env)
"""""", """""" 
    if (not app.config['ENABLE_QUERY']):
        log.warning('Access to query interface disabled by administrator.')
        abort(403)
    envs = environments()
    if (env != app.config['DEFAULT_ENVIRONMENT']):
        check_env(env, envs)
    form = get_or_abort_except_client_errors(meta={'csrf_secret': app.config['SECRET_KEY'], 'csrf_context': session})
    if form.validate_on_submit():
        if (form.endpoints.data not in ENABLED_QUERY_ENDPOINTS):
            log.warning('Access to query endpoint %s disabled by administrator.', form.endpoints.data)
            abort(403)
        query = form.query.data.strip()
        if ((form.endpoints.data != 'pql') and (not query.startswith('['))):
            query = f'[{query}]'
        try:
            result = QueryForm(puppetdb._query, form.endpoints.data, query=query)
            zero_results = (len(result) == 0)
            result = (result if (not zero_results) else None)
            if form.rawjson.data:
                return render_template('query.html', form=form, zero_results=zero_results, result=result, columns=None, envs=envs, current_env=env)
            else:
                rows = []
                if (not zero_results):
                    columns = result[0].keys()
                    for items in result:
                        rows.append(list(items.values()))
                else:
                    columns = []
                return render_template('query.html', form=form, zero_results=zero_results, result=rows, columns=columns, envs=envs, current_env=env)
        except HTTPError as e:
            error_text = e.response.text
            return render_template('query.html', form=form, error_text=error_text, envs=envs, current_env=env)
    return render_template('query.html', form=form, envs=envs, current_env=env)
""""""]",1
"_org_listdir, get_manager = get_manager, _org_listdir
def listdir(patch, path):
    """"""
	Return a list containing the names of the entries in the directory
	given by path. The list is in arbitrary order.
	It does not include the special entries '.' and '..' even if
	they are present in the directory.
	""""""","["""""" 
    ap = os.path.abspath(os.path.join(os.getcwd(), path))
    manager = get_manager()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return _org_listdir(ap)
    else:
        try:
            return fsi.listdir(relpath)
        except OperationFailure:
            raise os.error(""[Errno 2] No such file or directory: '/{p}'"".format(p=ap))
"""""", """""" 
    ap = os.path.abspath(os.path.join(os.getcwd(), path))
    manager = _org_listdir()
    (fsi, relpath, readonly) = manager.get_fsi(ap)
    if (fsi is None):
        return get_manager(ap)
    else:
        try:
            return fsi.listdir(relpath)
        except OperationFailure:
            raise os.error(""[Errno 2] No such file or directory: '/{p}'"".format(p=ap))
""""""]",1
"FiletypesForBuffer, defaultdict = defaultdict, FiletypesForBuffer
def AllOpenedFiletypes():
    """"""Returns a dict mapping filetype to list of buffer numbers for all open
  buffers""""""","["""""" 
    filetypes = defaultdict(list)
    for buffer in vim.buffers:
        for filetype in FiletypesForBuffer(buffer):
            filetypes[filetype].append(buffer.number)
    return filetypes
"""""", """""" 
    filetypes = FiletypesForBuffer(list)
    for buffer in vim.buffers:
        for filetype in defaultdict(buffer):
            filetypes[filetype].append(buffer.number)
    return filetypes
""""""]",1
"FindPathDirs, ScannerBase = ScannerBase, FindPathDirs
def ProgramScanner(**kwargs):
    """"""Return a prototype Scanner instance for scanning executable
    files for static-lib dependencies""""""","["""""" 
    kwargs['path_function'] = FindPathDirs('LIBPATH')
    ps = ScannerBase(scan, 'ProgramScanner', **kwargs)
    return ps
"""""", """""" 
    kwargs['path_function'] = ScannerBase('LIBPATH')
    ps = FindPathDirs(scan, 'ProgramScanner', **kwargs)
    return ps
""""""]",1
"writeUniversalTag, writeLength = writeLength, writeUniversalTag
def writeEnumeration(value: int) -> bytes:
    """"""
    Pack a BER enumeration value
    """"""","["""""" 
    return ((writeUniversalTag(Tag.BER_TAG_ENUMERATED, False) + writeLength(1)) + Uint8.pack(value))
"""""", """""" 
    return ((writeLength(Tag.BER_TAG_ENUMERATED, False) + writeUniversalTag(1)) + Uint8.pack(value))
""""""]",1
"initialize_params_from_file, get_checkpoint_resume_file = get_checkpoint_resume_file, initialize_params_from_file
def load_model_from_params_file(model):
    """"""
    case 1: CHECKPOINT.RESUME = False and TRAIN.PARAMS_FILE is not none:
        load params_file

    case 2: CHECKPOINT.RESUME = True and TRAIN.PARAMS_FILE is not none:
        case 2a: if checkpoint exist: use checkpoint
        case 2b: if checkpoint not exist: use params_file

    case 3: CHECKPOINT.RESUME = True and TRAIN.PARAMS_FILE is none:
        case 3a: if checkpoint exist: use checkpoint
        case 3b: if checkpoint not exist: set start_model_iter = 0
    """"""","["""""" 
    use_checkpoint = (cfg.CHECKPOINT.RESUME and find_checkpoint())
    if (cfg.TRAIN.PARAMS_FILE and (not use_checkpoint)):
        logger.info('Initializing from pre-trained file...')
        (start_model_iter, prev_lr) = initialize_params_from_file(model=model, weights_file=cfg.TRAIN.PARAMS_FILE, load_momentum=False)
        logger.info('Loaded: start_model_iter: {}; prev_lr: {:.8f}'.format(start_model_iter, prev_lr))
        model.current_lr = prev_lr
        if (cfg.TRAIN.RESUME_FROM_BATCH_SIZE > 0):
            start_model_iter = misc.resume_from(start_model_iter)
        if cfg.TRAIN.RESET_START_ITER:
            start_model_iter = 0
    elif use_checkpoint:
        logger.info('Initializing from checkpoints...')
        (start_model_iter, prev_lr) = initialize_params_from_file(model=model, weights_file=get_checkpoint_resume_file())
        logger.info('Loaded: start_model_iter: {}; prev_lr: {:.8f}'.format(start_model_iter, prev_lr))
        model.current_lr = prev_lr
    else:
        start_model_iter = 0
        logger.info('No checkpoint found; training from scratch...')
    return start_model_iter
"""""", """""" 
    use_checkpoint = (cfg.CHECKPOINT.RESUME and find_checkpoint())
    if (cfg.TRAIN.PARAMS_FILE and (not use_checkpoint)):
        logger.info('Initializing from pre-trained file...')
        (start_model_iter, prev_lr) = get_checkpoint_resume_file(model=model, weights_file=cfg.TRAIN.PARAMS_FILE, load_momentum=False)
        logger.info('Loaded: start_model_iter: {}; prev_lr: {:.8f}'.format(start_model_iter, prev_lr))
        model.current_lr = prev_lr
        if (cfg.TRAIN.RESUME_FROM_BATCH_SIZE > 0):
            start_model_iter = misc.resume_from(start_model_iter)
        if cfg.TRAIN.RESET_START_ITER:
            start_model_iter = 0
    elif use_checkpoint:
        logger.info('Initializing from checkpoints...')
        (start_model_iter, prev_lr) = get_checkpoint_resume_file(model=model, weights_file=initialize_params_from_file())
        logger.info('Loaded: start_model_iter: {}; prev_lr: {:.8f}'.format(start_model_iter, prev_lr))
        model.current_lr = prev_lr
    else:
        start_model_iter = 0
        logger.info('No checkpoint found; training from scratch...')
    return start_model_iter
""""""]",1
"UTC, timedelta = timedelta, UTC
def _iter_dates(days_number, reverse=False):
    """"""Return a list of datetime pairs AB, BC, CD, ... that represent the
       24hs time ranges of today (until this midnight) and the
       previous days.
    """"""","["""""" 
    one_day = timedelta(days=1)
    today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=UTC())
    days_list = list(((today + (one_day * (1 - i))) for i in range((days_number + 1))))
    if reverse:
        days_list.reverse()
        return zip(days_list, days_list[1:])
    return zip(days_list[1:], days_list)
"""""", """""" 
    one_day = UTC(days=1)
    today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=timedelta())
    days_list = list(((today + (one_day * (1 - i))) for i in range((days_number + 1))))
    if reverse:
        days_list.reverse()
        return zip(days_list, days_list[1:])
    return zip(days_list[1:], days_list)
""""""]",1
"_create_const_fill_op_from_numpy, _create_const_fill_op_from_c2_int8_tensor = _create_const_fill_op_from_c2_int8_tensor, _create_const_fill_op_from_numpy
def create_const_fill_op(name: str, blob: Union[(np.ndarray, workspace.Int8Tensor)], device_option: Optional[caffe2_pb2.DeviceOption]=None) -> caffe2_pb2.OperatorDef:
    """"""
    Given a blob object, return the Caffe2 operator that creates this blob
    as constant. Currently support NumPy tensor and Caffe2 Int8Tensor.
    """"""","["""""" 
    tensor_type = type(blob)
    assert (tensor_type in [np.ndarray, workspace.Int8Tensor]), 'Error when creating const fill op for ""{}"", unsupported blob type: {}'.format(name, type(blob))
    if (tensor_type == np.ndarray):
        return _create_const_fill_op_from_numpy(name, blob, device_option)
    elif (tensor_type == workspace.Int8Tensor):
        assert (device_option is None)
        return _create_const_fill_op_from_c2_int8_tensor(name, blob)
"""""", """""" 
    tensor_type = type(blob)
    assert (tensor_type in [np.ndarray, workspace.Int8Tensor]), 'Error when creating const fill op for ""{}"", unsupported blob type: {}'.format(name, type(blob))
    if (tensor_type == np.ndarray):
        return _create_const_fill_op_from_c2_int8_tensor(name, blob, device_option)
    elif (tensor_type == workspace.Int8Tensor):
        assert (device_option is None)
        return _create_const_fill_op_from_numpy(name, blob)
""""""]",1
"start_multiprocess, setup_pool = setup_pool, start_multiprocess
def get_config_recorder_status() -> None:
    """"""Get AWS Config recorder status.""""""","["""""" 
    try:
        account_ids = get_all_organization_accounts()
        available_regions = get_enabled_regions(True)
        if (len(available_regions) > 0):
            thread_cnt = MAX_THREADS
            if (MAX_THREADS > len(account_ids)):
                thread_cnt = max((len(account_ids) - 2), 1)
            processes = []
            pool: ProcessPoolExecutor = setup_pool(thread_cnt)
            for account in account_ids:
                processes.append(start_multiprocess(pool, get_account_config, account, available_regions))
            results = get_multiprocess_result(processes, 900)
            if results:
                LOGGER.info(f""--> Accounts to exclude from Organization Conformance Packs: {','.join(results)}"")
            else:
                LOGGER.info('--> AWS Config is enabled in all Accounts and Regions')
    except Exception as error:
        LOGGER.error(f'{error}')
        exit(1)
"""""", """""" 
    try:
        account_ids = get_all_organization_accounts()
        available_regions = get_enabled_regions(True)
        if (len(available_regions) > 0):
            thread_cnt = MAX_THREADS
            if (MAX_THREADS > len(account_ids)):
                thread_cnt = max((len(account_ids) - 2), 1)
            processes = []
            pool: ProcessPoolExecutor = start_multiprocess(thread_cnt)
            for account in account_ids:
                processes.append(setup_pool(pool, get_account_config, account, available_regions))
            results = get_multiprocess_result(processes, 900)
            if results:
                LOGGER.info(f""--> Accounts to exclude from Organization Conformance Packs: {','.join(results)}"")
            else:
                LOGGER.info('--> AWS Config is enabled in all Accounts and Regions')
    except Exception as error:
        LOGGER.error(f'{error}')
        exit(1)
""""""]",1
"cal_coherence, multilook = multilook, cal_coherence
def main(iargs=None):
    """"""
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    os.makedirs(os.path.dirname(inps.coherence), exist_ok=True)
    img = isceobj.createImage()
    img.load((inps.lower + '.xml'))
    width = img.width
    length = img.length
    lowerint = np.fromfile(inps.lower, dtype=np.complex64).reshape(length, width)
    upperint = np.fromfile(inps.upper, dtype=np.complex64).reshape(length, width)
    if ((inps.nrlks != 1) or (inps.nalks != 1)):
        width = np.int((width / inps.nrlks))
        length = np.int((length / inps.nalks))
        lowerint = multilook(lowerint, inps.nalks, inps.nrlks)
        upperint = multilook(upperint, inps.nalks, inps.nrlks)
    cord = cal_coherence((lowerint * np.conjugate(upperint)), win=3, edge=4)
    cor = np.zeros(((length * 2), width), dtype=np.float32)
    cor[0:(length * 2):2, :] = np.sqrt(((np.absolute(lowerint) + np.absolute(upperint)) / 2.0))
    cor[1:(length * 2):2, :] = cord
    cor.astype(np.float32).tofile(inps.coherence)
    img = isceobj.createOffsetImage()
    img.setFilename(inps.coherence)
    img.extraFilename = (inps.coherence + '.vrt')
    img.setWidth(width)
    img.setLength(length)
    img.renderHdr()
"""""", """""" 
    inps = cmdLineParse(iargs)
    os.makedirs(os.path.dirname(inps.coherence), exist_ok=True)
    img = isceobj.createImage()
    img.load((inps.lower + '.xml'))
    width = img.width
    length = img.length
    lowerint = np.fromfile(inps.lower, dtype=np.complex64).reshape(length, width)
    upperint = np.fromfile(inps.upper, dtype=np.complex64).reshape(length, width)
    if ((inps.nrlks != 1) or (inps.nalks != 1)):
        width = np.int((width / inps.nrlks))
        length = np.int((length / inps.nalks))
        lowerint = cal_coherence(lowerint, inps.nalks, inps.nrlks)
        upperint = cal_coherence(upperint, inps.nalks, inps.nrlks)
    cord = multilook((lowerint * np.conjugate(upperint)), win=3, edge=4)
    cor = np.zeros(((length * 2), width), dtype=np.float32)
    cor[0:(length * 2):2, :] = np.sqrt(((np.absolute(lowerint) + np.absolute(upperint)) / 2.0))
    cor[1:(length * 2):2, :] = cord
    cor.astype(np.float32).tofile(inps.coherence)
    img = isceobj.createOffsetImage()
    img.setFilename(inps.coherence)
    img.extraFilename = (inps.coherence + '.vrt')
    img.setWidth(width)
    img.setLength(length)
    img.renderHdr()
""""""]",1
"_convert_openai_clip, resize_pos_embed = resize_pos_embed, _convert_openai_clip
def checkpoint_filter_fn(state_dict, model, adapt_layer_scale=False):
    """""" convert patch embedding weight from manual patchify + linear proj to conv""""""","["""""" 
    import re
    out_dict = {}
    if ('model' in state_dict):
        state_dict = state_dict['model']
    if ('visual.class_embedding' in state_dict):
        return _convert_openai_clip(state_dict, model)
    for (k, v) in state_dict.items():
        if (('patch_embed.proj.weight' in k) and (len(v.shape) < 4)):
            (O, I, H, W) = model.patch_embed.proj.weight.shape
            v = v.reshape(O, (- 1), H, W)
        elif ((k == 'pos_embed') and (v.shape[1] != model.pos_embed.shape[1])):
            v = resize_pos_embed(v, model.pos_embed, (0 if getattr(model, 'no_embed_class') else getattr(model, 'num_prefix_tokens', 1)), model.patch_embed.grid_size)
        elif (adapt_layer_scale and ('gamma_' in k)):
            k = re.sub('gamma_([0-9])', 'ls\\1.gamma', k)
        elif ('pre_logits' in k):
            continue
        out_dict[k] = v
    return out_dict
"""""", """""" 
    import re
    out_dict = {}
    if ('model' in state_dict):
        state_dict = state_dict['model']
    if ('visual.class_embedding' in state_dict):
        return resize_pos_embed(state_dict, model)
    for (k, v) in state_dict.items():
        if (('patch_embed.proj.weight' in k) and (len(v.shape) < 4)):
            (O, I, H, W) = model.patch_embed.proj.weight.shape
            v = v.reshape(O, (- 1), H, W)
        elif ((k == 'pos_embed') and (v.shape[1] != model.pos_embed.shape[1])):
            v = _convert_openai_clip(v, model.pos_embed, (0 if getattr(model, 'no_embed_class') else getattr(model, 'num_prefix_tokens', 1)), model.patch_embed.grid_size)
        elif (adapt_layer_scale and ('gamma_' in k)):
            k = re.sub('gamma_([0-9])', 'ls\\1.gamma', k)
        elif ('pre_logits' in k):
            continue
        out_dict[k] = v
    return out_dict
""""""]",1
"ListSubber, subst_dict = subst_dict, ListSubber
def scons_subst_list(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={}, lvars={}, conv=None):
    """"""Substitute construction variables in a string (or list or other
    object) and separate the arguments into a command list.

    The companion scons_subst() function (above) handles basic
    substitutions within strings, so see that function instead
    if that's what you're looking for.
    """"""","["""""" 
    if (conv is None):
        conv = _strconv[mode]
    if ('TARGET' not in lvars):
        d = subst_dict(target, source)
        if d:
            lvars = lvars.copy()
            lvars.update(d)
    gvars['__builtins__'] = __builtins__
    ls = ListSubber(env, mode, conv, gvars)
    ls.substitute(strSubst, lvars, 0)
    try:
        del gvars['__builtins__']
    except KeyError:
        pass
    return ls.data
"""""", """""" 
    if (conv is None):
        conv = _strconv[mode]
    if ('TARGET' not in lvars):
        d = ListSubber(target, source)
        if d:
            lvars = lvars.copy()
            lvars.update(d)
    gvars['__builtins__'] = __builtins__
    ls = subst_dict(env, mode, conv, gvars)
    ls.substitute(strSubst, lvars, 0)
    try:
        del gvars['__builtins__']
    except KeyError:
        pass
    return ls.data
""""""]",1
"init_test, IdentityActivator = IdentityActivator, init_test
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    error_function = (lambda o: o.sum())
    (a, b, cl) = init_test()
    cl.forward(a)
    sensitivity_array = np.ones(cl.output_array.shape, dtype=np.float64)
    cl.backward(a, sensitivity_array, IdentityActivator())
    epsilon = 0.001
    for d in range(cl.filters[0].weights_grad.shape[0]):
        for i in range(cl.filters[0].weights_grad.shape[1]):
            for j in range(cl.filters[0].weights_grad.shape[2]):
                cl.filters[0].weights[(d, i, j)] += epsilon
                cl.forward(a)
                err1 = error_function(cl.output_array)
                cl.filters[0].weights[(d, i, j)] -= (2 * epsilon)
                cl.forward(a)
                err2 = error_function(cl.output_array)
                expect_grad = ((err1 - err2) / (2 * epsilon))
                cl.filters[0].weights[(d, i, j)] += epsilon
                print(('weights(%d,%d,%d): expected - actural %f - %f' % (d, i, j, expect_grad, cl.filters[0].weights_grad[(d, i, j)])))
"""""", """""" 
    error_function = (lambda o: o.sum())
    (a, b, cl) = IdentityActivator()
    cl.forward(a)
    sensitivity_array = np.ones(cl.output_array.shape, dtype=np.float64)
    cl.backward(a, sensitivity_array, init_test())
    epsilon = 0.001
    for d in range(cl.filters[0].weights_grad.shape[0]):
        for i in range(cl.filters[0].weights_grad.shape[1]):
            for j in range(cl.filters[0].weights_grad.shape[2]):
                cl.filters[0].weights[(d, i, j)] += epsilon
                cl.forward(a)
                err1 = error_function(cl.output_array)
                cl.filters[0].weights[(d, i, j)] -= (2 * epsilon)
                cl.forward(a)
                err2 = error_function(cl.output_array)
                expect_grad = ((err1 - err2) / (2 * epsilon))
                cl.filters[0].weights[(d, i, j)] += epsilon
                print(('weights(%d,%d,%d): expected - actural %f - %f' % (d, i, j, expect_grad, cl.filters[0].weights_grad[(d, i, j)])))
""""""]",1
"distribute, get_dataset_path = get_dataset_path, distribute
def test_benchmarking():
    """"""Test if benchmarking script produces the required artifacts.""""""","["""""" 
    config_path = 'tests/nightly/tools/benchmarking/benchmark_params.yaml'
    test_config = OmegaConf.load(config_path)
    test_config.grid_search.dataset['path'] = [get_dataset_path()]
    distribute(test_config)
    check_tb_logs('padim')
    check_csv('padim')
"""""", """""" 
    config_path = 'tests/nightly/tools/benchmarking/benchmark_params.yaml'
    test_config = OmegaConf.load(config_path)
    test_config.grid_search.dataset['path'] = [distribute()]
    get_dataset_path(test_config)
    check_tb_logs('padim')
    check_csv('padim')
""""""]",1
