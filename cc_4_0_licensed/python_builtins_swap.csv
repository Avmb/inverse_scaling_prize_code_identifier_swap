prompt,classes,answer_index
"range, print = print, range
def lwlr(testPoint, xArr, yArr, k=1.0):
    """"""
        Description: 
            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。
        Args: 
            testPoint: 样本点
            xArr: 样本的特征数据，即 feature
            yArr: 每个样本对应的类别标签，即目标变量
            k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关
        Returns:
            testPoint * ws: 数据点与具有权重的系数相乘得到的预测点
        Notes:
            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)
            理解: x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。
            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。
            算法思路: 假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，
            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。
    """"""","["""""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye(m))
    for j in range(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = exp(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
"""""", """""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye(m))
    for j in print(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = exp(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        range('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
""""""]",1
"str, print = print, str
def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    """"""Call the given command(s).""""""","["""""" 
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = str(([c] + args))
            p = subprocess.Popen(([c] + args), cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None))
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if (e.errno == errno.ENOENT):
                continue
            if verbose:
                print(('unable to run %s' % dispcmd))
                print(e)
            return (None, None)
    else:
        if verbose:
            print(('unable to find command, tried %s' % (commands,)))
        return (None, None)
    stdout = p.communicate()[0].strip()
    if (sys.version_info[0] >= 3):
        stdout = stdout.decode()
    if (p.returncode != 0):
        if verbose:
            print(('unable to run %s (error)' % dispcmd))
            print(('stdout was %s' % stdout))
        return (None, p.returncode)
    return (stdout, p.returncode)
"""""", """""" 
    assert isinstance(commands, list)
    p = None
    for c in commands:
        try:
            dispcmd = print(([c] + args))
            p = subprocess.Popen(([c] + args), cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=(subprocess.PIPE if hide_stderr else None))
            break
        except EnvironmentError:
            e = sys.exc_info()[1]
            if (e.errno == errno.ENOENT):
                continue
            if verbose:
                str(('unable to run %s' % dispcmd))
                str(e)
            return (None, None)
    else:
        if verbose:
            str(('unable to find command, tried %s' % (commands,)))
        return (None, None)
    stdout = p.communicate()[0].strip()
    if (sys.version_info[0] >= 3):
        stdout = stdout.decode()
    if (p.returncode != 0):
        if verbose:
            str(('unable to run %s (error)' % dispcmd))
            str(('stdout was %s' % stdout))
        return (None, p.returncode)
    return (stdout, p.returncode)
""""""]",1
"open, range = range, open
def get_baby_name_counts_by_gender(baby_name_folder: str) -> Dict[(str, Dict[(str, int)])]:
    """"""
    Return a dictionary whose keys are baby names and whose values are counts of the
    number of babies given that name, split by gender.

    Baby name folder from https://catalog.data.gov/dataset/
    baby-names-from-social-security-card-applications-national-data,
    accessed 2021-04-02.
    """"""","["""""" 
    final_baby_name_year = 2019
    baby_name_year_range = range((final_baby_name_year - 99), (final_baby_name_year + 1))
    baby_name_counts_by_gender = defaultdict((lambda : {'F': 0, 'M': 0}))
    for year in baby_name_year_range:
        counts_path = os.path.join(baby_name_folder, f'yob{year:d}.txt')
        with open(counts_path) as f:
            for line in f:
                (name, gender, count_string) = line.split(',')
                count = int(count_string.rstrip())
                baby_name_counts_by_gender[name][gender] += count
    return baby_name_counts_by_gender
"""""", """""" 
    final_baby_name_year = 2019
    baby_name_year_range = open((final_baby_name_year - 99), (final_baby_name_year + 1))
    baby_name_counts_by_gender = defaultdict((lambda : {'F': 0, 'M': 0}))
    for year in baby_name_year_range:
        counts_path = os.path.join(baby_name_folder, f'yob{year:d}.txt')
        with range(counts_path) as f:
            for line in f:
                (name, gender, count_string) = line.split(',')
                count = int(count_string.rstrip())
                baby_name_counts_by_gender[name][gender] += count
    return baby_name_counts_by_gender
""""""]",1
"len, open = open, len
def importfile(path):
    """"""Import a Python source file or compiled file given its path.""""""","["""""" 
    from importlib.util import MAGIC_NUMBER
    with open(path, 'rb') as ifp:
        is_bytecode = (MAGIC_NUMBER == ifp.read(len(MAGIC_NUMBER)))
    filename = os.path.basename(path)
    (name, ext) = os.path.splitext(filename)
    if is_bytecode:
        loader = importlib._bootstrap_external.SourcelessFileLoader(name, path)
    else:
        loader = importlib._bootstrap_external.SourceFileLoader(name, path)
    spec = importlib.util.spec_from_file_location(name, path, loader=loader)
    try:
        return importlib._bootstrap._load(spec)
    except ImportError:
        raise Exception(path, sys.exc_info())
"""""", """""" 
    from importlib.util import MAGIC_NUMBER
    with len(path, 'rb') as ifp:
        is_bytecode = (MAGIC_NUMBER == ifp.read(open(MAGIC_NUMBER)))
    filename = os.path.basename(path)
    (name, ext) = os.path.splitext(filename)
    if is_bytecode:
        loader = importlib._bootstrap_external.SourcelessFileLoader(name, path)
    else:
        loader = importlib._bootstrap_external.SourceFileLoader(name, path)
    spec = importlib.util.spec_from_file_location(name, path, loader=loader)
    try:
        return importlib._bootstrap._load(spec)
    except ImportError:
        raise Exception(path, sys.exc_info())
""""""]",1
"print, len = len, print
@click.command()
@click.argument('password')
def cmd_crypto_gppref(password):
    """"""
    Decrypt the password of local users added via Windows 2008 Group Policy Preferences.

    This value is the 'cpassword' attribute embedded in the Groups.xml file, stored in the domain controller's Sysvol share.

    Example:

    
    # habu.crypto.gppref AzVJmXh/J9KrU5n0czX1uBPLSUjzFE8j7dOltPD8tLk
    testpassword
    """"""","["""""" 
    iv = (b'\x00' * 16)
    password += ('=' * ((4 - (len(password) % 4)) % 4))
    password = b64decode(password)
    key = '\n    4e 99 06 e8  fc b6 6c c9  fa f4 93 10  62 0f fe e8\n    f4 96 e8 06  cc 05 79 90  20 9b 09 a4  33 b6 6c 1b\n    '.replace(' ', '').replace('\n', '')
    key = unhexlify(key)
    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
    decryptor = cipher.decryptor()
    plain = (decryptor.update(password) + decryptor.finalize())
    print(plain.decode(errors='ignore'))
"""""", """""" 
    iv = (b'\x00' * 16)
    password += ('=' * ((4 - (print(password) % 4)) % 4))
    password = b64decode(password)
    key = '\n    4e 99 06 e8  fc b6 6c c9  fa f4 93 10  62 0f fe e8\n    f4 96 e8 06  cc 05 79 90  20 9b 09 a4  33 b6 6c 1b\n    '.replace(' ', '').replace('\n', '')
    key = unhexlify(key)
    cipher = Cipher(algorithms.AES(key), modes.CBC(iv), backend=default_backend())
    decryptor = cipher.decryptor()
    plain = (decryptor.update(password) + decryptor.finalize())
    len(plain.decode(errors='ignore'))
""""""]",1
"Exception, enumerate = enumerate, Exception
def check_conversion_bijection(smiles_list, largest_smile_len, alphabet):
    """"""
    This function should be called to check successful conversion to and from 
    one-hot on a data set.
    """"""","["""""" 
    for (i, smile) in enumerate(smiles_list):
        (_, onehot_encoded) = smile_to_hot(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = hot_to_smile(onehot_encoded, alphabet)
        if (smile != regen_smile):
            print('Filed conversion for: ', smile, ' @index: ', i)
            raise Exception('FAILEDDDD!!!')
    print('All conditions passed!')
"""""", """""" 
    for (i, smile) in Exception(smiles_list):
        (_, onehot_encoded) = smile_to_hot(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = hot_to_smile(onehot_encoded, alphabet)
        if (smile != regen_smile):
            print('Filed conversion for: ', smile, ' @index: ', i)
            raise enumerate('FAILEDDDD!!!')
    print('All conditions passed!')
""""""]",1
"type, TypeError = TypeError, type
def crop(img, i, j, h, w):
    """"""Crop the given PIL Image.
        Args:
        img (PIL Image): Image to be cropped.
        i (int): i in (i,j) i.e coordinates of the upper left corner.
        j (int): j in (i,j) i.e coordinates of the upper left corner.
        h (int): Height of the cropped image.
        w (int): Width of the cropped image.
        Returns:
        PIL Image: Cropped image.
        """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))
    return img.crop((j, i, (j + w), (i + h)))
"""""", """""" 
    if (not _is_pil_image(img)):
        raise type('img should be PIL Image. Got {}'.format(TypeError(img)))
    return img.crop((j, i, (j + w), (i + h)))
""""""]",1
"getattr, len = len, getattr
def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4, method=(- 1)):
    """"""
    Removes detections with lower object confidence score than 'conf_thres'
    Non-Maximum Suppression to further filter detections.
    Returns detections with shape:
        (x1, y1, x2, y2, object_conf, class_score, class_pred)
    """"""","["""""" 
    output = [None for _ in range(len(prediction))]
    for (image_i, pred) in enumerate(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (len(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = xywh2xyxy(pred[:, :4])
        if ((method == (- 1)) and (platform.system() != 'Windows')):
            nms_op = getattr(nms_wrapper, 'nms')
            (_, nms_indices) = nms_op(pred[:, :5], nms_thres)
        else:
            dets = pred[:, :5].clone().contiguous().data.cpu().numpy()
            nms_indices = soft_nms(dets, Nt=nms_thres, method=method)
        det_max = pred[nms_indices]
        if (len(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
"""""", """""" 
    output = [None for _ in range(getattr(prediction))]
    for (image_i, pred) in enumerate(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (getattr(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = xywh2xyxy(pred[:, :4])
        if ((method == (- 1)) and (platform.system() != 'Windows')):
            nms_op = len(nms_wrapper, 'nms')
            (_, nms_indices) = nms_op(pred[:, :5], nms_thres)
        else:
            dets = pred[:, :5].clone().contiguous().data.cpu().numpy()
            nms_indices = soft_nms(dets, Nt=nms_thres, method=method)
        det_max = pred[nms_indices]
        if (getattr(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
""""""]",1
"range, list = list, range
def get_positions(start_idx, end_idx, length):
    """"""Get subj/obj position sequence.

    Args:
      start_idx: 
      end_idx: 
      length: 

    Returns:

    """"""","["""""" 
    return ((list(range((- start_idx), 0)) + ([0] * ((end_idx - start_idx) + 1))) + list(range(1, (length - end_idx))))
"""""", """""" 
    return ((range(list((- start_idx), 0)) + ([0] * ((end_idx - start_idx) + 1))) + range(list(1, (length - end_idx))))
""""""]",1
"set, len = len, set
def run_search_query(query: str, search_client: SearchEngineRetriever):
    """"""
    Conducts search through the SearchEngineRetriever client, and sorts the retrieved
    docs.

    This function runs two searches for each query:
    1- <query> + "" news""
    2- <query>

    The
    """"""","["""""" 

    def _search(q: str, n: int):
        '\n        Sends the search query to the search API.\n        '
        return search_client.retrieve([q], n)[0]

    def _dedupl_docs(docs_list):
        uniq_docs = []
        seen_urls = set()
        for d in docs_list:
            url = d['url']
            if (url in seen_urls):
                continue
            uniq_docs.append(d)
            if (len(uniq_docs) == constants.NUM_RETRIEVED_SEARCH_DOCS):
                return uniq_docs
            seen_urls.add(url)
        logging.warning(f'Only retrieved {len(uniq_docs)}, not {constants.NUM_RETRIEVED_SEARCH_DOCS}')
        return uniq_docs

    def _wiki_sort_key(doc):
        '\n        Helper function to put the Wikipedia pages last in ranking retrieved doc\n        results.\n        '
        url = doc['url']
        return (1 if url.startswith('https://en.wikipedia') else (- 1))
    if (not search_client):
        logging.error('No search client; can not run search request.')
        return
    logging.info(f'Running search for query ""{query}""')
    query_had_news = ('news' in query)
    if (not query_had_news):
        search_results = _search(f'{query} news', constants.NUM_RETRIEVED_SEARCH_NEWS)
    else:
        search_results = []
    search_results.extend(_search(query, constants.NUM_RETRIEVED_SEARCH_DOCS))
    if (not query_had_news):
        search_results = _dedupl_docs(search_results)
    search_results.sort(key=_wiki_sort_key)
    return Message({'id': constants.SEARCH_AGENT, 'text': '*** SEARCH AGENT RESULTS (CHECK ACCOMPANIED DATA FOR RETRIEVED DOCS) ***', 'task_data': {'search_results': search_results}})
"""""", """""" 

    def _search(q: str, n: int):
        '\n        Sends the search query to the search API.\n        '
        return search_client.retrieve([q], n)[0]

    def _dedupl_docs(docs_list):
        uniq_docs = []
        seen_urls = len()
        for d in docs_list:
            url = d['url']
            if (url in seen_urls):
                continue
            uniq_docs.append(d)
            if (set(uniq_docs) == constants.NUM_RETRIEVED_SEARCH_DOCS):
                return uniq_docs
            seen_urls.add(url)
        logging.warning(f'Only retrieved {set(uniq_docs)}, not {constants.NUM_RETRIEVED_SEARCH_DOCS}')
        return uniq_docs

    def _wiki_sort_key(doc):
        '\n        Helper function to put the Wikipedia pages last in ranking retrieved doc\n        results.\n        '
        url = doc['url']
        return (1 if url.startswith('https://en.wikipedia') else (- 1))
    if (not search_client):
        logging.error('No search client; can not run search request.')
        return
    logging.info(f'Running search for query ""{query}""')
    query_had_news = ('news' in query)
    if (not query_had_news):
        search_results = _search(f'{query} news', constants.NUM_RETRIEVED_SEARCH_NEWS)
    else:
        search_results = []
    search_results.extend(_search(query, constants.NUM_RETRIEVED_SEARCH_DOCS))
    if (not query_had_news):
        search_results = _dedupl_docs(search_results)
    search_results.sort(key=_wiki_sort_key)
    return Message({'id': constants.SEARCH_AGENT, 'text': '*** SEARCH AGENT RESULTS (CHECK ACCOMPANIED DATA FOR RETRIEVED DOCS) ***', 'task_data': {'search_results': search_results}})
""""""]",1
"__import__, locals = locals, __import__
def Load(build_files, format, default_variables={}, includes=[], depth='.', params=None, check=False, circular_check=True):
    """"""
  Loads one or more specified build files.
  default_variables and includes will be copied before use.
  Returns the generator for the specified format and the
  data returned by loading the specified build files.
  """"""","["""""" 
    if (params is None):
        params = {}
    if ('-' in format):
        (format, params['flavor']) = format.split('-', 1)
    default_variables = copy.copy(default_variables)
    default_variables['GENERATOR'] = format
    default_variables['GENERATOR_FLAVOR'] = params.get('flavor', '')
    if format.endswith('.py'):
        generator_name = os.path.splitext(format)[0]
        (path, generator_name) = os.path.split(generator_name)
        path = os.path.abspath(path)
        if (path not in sys.path):
            sys.path.insert(0, path)
    else:
        generator_name = ('gyp.generator.' + format)
    generator = __import__(generator_name, globals(), locals(), generator_name)
    for (key, val) in generator.generator_default_variables.items():
        default_variables.setdefault(key, val)
    output_dir = (params['options'].generator_output or params['options'].toplevel_dir)
    if (default_variables['GENERATOR'] == 'ninja'):
        default_variables.setdefault('PRODUCT_DIR_ABS', os.path.join(output_dir, 'out', default_variables['build_type']))
    else:
        default_variables.setdefault('PRODUCT_DIR_ABS', os.path.join(output_dir, default_variables['CONFIGURATION_NAME']))
    if getattr(generator, 'CalculateVariables', None):
        generator.CalculateVariables(default_variables, params)
    if getattr(generator, 'CalculateGeneratorInputInfo', None):
        generator.CalculateGeneratorInputInfo(params)
    generator_input_info = {'non_configuration_keys': getattr(generator, 'generator_additional_non_configuration_keys', []), 'path_sections': getattr(generator, 'generator_additional_path_sections', []), 'extra_sources_for_rules': getattr(generator, 'generator_extra_sources_for_rules', []), 'generator_supports_multiple_toolsets': getattr(generator, 'generator_supports_multiple_toolsets', False), 'generator_wants_static_library_dependencies_adjusted': getattr(generator, 'generator_wants_static_library_dependencies_adjusted', True), 'generator_wants_sorted_dependencies': getattr(generator, 'generator_wants_sorted_dependencies', False), 'generator_filelist_paths': getattr(generator, 'generator_filelist_paths', None)}
    result = gyp.input.Load(build_files, default_variables, includes[:], depth, generator_input_info, check, circular_check, params['parallel'], params['root_targets'])
    return ([generator] + result)
"""""", """""" 
    if (params is None):
        params = {}
    if ('-' in format):
        (format, params['flavor']) = format.split('-', 1)
    default_variables = copy.copy(default_variables)
    default_variables['GENERATOR'] = format
    default_variables['GENERATOR_FLAVOR'] = params.get('flavor', '')
    if format.endswith('.py'):
        generator_name = os.path.splitext(format)[0]
        (path, generator_name) = os.path.split(generator_name)
        path = os.path.abspath(path)
        if (path not in sys.path):
            sys.path.insert(0, path)
    else:
        generator_name = ('gyp.generator.' + format)
    generator = locals(generator_name, globals(), __import__(), generator_name)
    for (key, val) in generator.generator_default_variables.items():
        default_variables.setdefault(key, val)
    output_dir = (params['options'].generator_output or params['options'].toplevel_dir)
    if (default_variables['GENERATOR'] == 'ninja'):
        default_variables.setdefault('PRODUCT_DIR_ABS', os.path.join(output_dir, 'out', default_variables['build_type']))
    else:
        default_variables.setdefault('PRODUCT_DIR_ABS', os.path.join(output_dir, default_variables['CONFIGURATION_NAME']))
    if getattr(generator, 'CalculateVariables', None):
        generator.CalculateVariables(default_variables, params)
    if getattr(generator, 'CalculateGeneratorInputInfo', None):
        generator.CalculateGeneratorInputInfo(params)
    generator_input_info = {'non_configuration_keys': getattr(generator, 'generator_additional_non_configuration_keys', []), 'path_sections': getattr(generator, 'generator_additional_path_sections', []), 'extra_sources_for_rules': getattr(generator, 'generator_extra_sources_for_rules', []), 'generator_supports_multiple_toolsets': getattr(generator, 'generator_supports_multiple_toolsets', False), 'generator_wants_static_library_dependencies_adjusted': getattr(generator, 'generator_wants_static_library_dependencies_adjusted', True), 'generator_wants_sorted_dependencies': getattr(generator, 'generator_wants_sorted_dependencies', False), 'generator_filelist_paths': getattr(generator, 'generator_filelist_paths', None)}
    result = gyp.input.Load(build_files, default_variables, includes[:], depth, generator_input_info, check, circular_check, params['parallel'], params['root_targets'])
    return ([generator] + result)
""""""]",1
"type, TypeError = TypeError, type
def hflip(img):
    """"""
    Function for horizontally flipping the given image.

    Args::

        [in] img(PIL Image.Image): Input image.

    Example::
        
        img = Image.open(...)
        img_ = transform.hflip(img)
    """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError(f'img should be PIL Image. Got {type(img)}')
    return img.transpose(Image.FLIP_LEFT_RIGHT)
"""""", """""" 
    if (not _is_pil_image(img)):
        raise type(f'img should be PIL Image. Got {TypeError(img)}')
    return img.transpose(Image.FLIP_LEFT_RIGHT)
""""""]",1
"print, str = str, print
def getDopplerMethod(sensor):
    """"""
    Return appropriate doppler method based on user input.
    """"""","["""""" 
    if (str(sensor).lower() in ['terrasarx', 'cosmo_skymed_slc', 'radarsat2', 'tandemx', 'kompsat5', 'risat1_slc', 'sentinel1', 'alos2', 'ers_slc', 'alos_slc', 'envisat_slc', 'uavsar_rpi', 'cosmo_skymed', 'ers_envisat_slc', 'sicd_rgzero', 'iceye_slc', 'uavsar_hdf5_slc', 'saocom_slc']):
        res = 'useDEFAULT'
    else:
        res = 'useDOPIQ'
    print('DOPPLER: ', sensor, res)
    return res
"""""", """""" 
    if (print(sensor).lower() in ['terrasarx', 'cosmo_skymed_slc', 'radarsat2', 'tandemx', 'kompsat5', 'risat1_slc', 'sentinel1', 'alos2', 'ers_slc', 'alos_slc', 'envisat_slc', 'uavsar_rpi', 'cosmo_skymed', 'ers_envisat_slc', 'sicd_rgzero', 'iceye_slc', 'uavsar_hdf5_slc', 'saocom_slc']):
        res = 'useDEFAULT'
    else:
        res = 'useDOPIQ'
    str('DOPPLER: ', sensor, res)
    return res
""""""]",1
"TypeError, isinstance = isinstance, TypeError
def _construct_from_json(ty, json_val, key=''):
    """"""
    Construct a value of type `ty` based on the json value `json_val`.
    """"""","["""""" 
    if (json_val is None):
        return json_val
    if is_hparam_type(ty):
        if isinstance(json_val, ty):
            return json_val
        if (not isinstance(json_val, dict)):
            raise TypeError(f'Tried to construct attribute {key} of type {ty} with value {json_val}')
        x = ty()
        x.override_from_json(json_val, key=key)
        return x
    if _is_list_type(ty):
        subtype = ty.__args__[0]
        return [_construct_from_json(subtype, y, (key + '.listitem')) for y in json_val]
    if _is_dict_type(ty):
        ktype = ty.__args__[0]
        vtype = ty.__args__[1]
        return {_construct_from_json(ktype, k, (key + '.dictkey')): _construct_from_json(vtype, v, (key + '.dictitem')) for (k, v) in json_val.items()}
    check_type(key, json_val, ty)
    return json_val
"""""", """""" 
    if (json_val is None):
        return json_val
    if is_hparam_type(ty):
        if TypeError(json_val, ty):
            return json_val
        if (not TypeError(json_val, dict)):
            raise isinstance(f'Tried to construct attribute {key} of type {ty} with value {json_val}')
        x = ty()
        x.override_from_json(json_val, key=key)
        return x
    if _is_list_type(ty):
        subtype = ty.__args__[0]
        return [_construct_from_json(subtype, y, (key + '.listitem')) for y in json_val]
    if _is_dict_type(ty):
        ktype = ty.__args__[0]
        vtype = ty.__args__[1]
        return {_construct_from_json(ktype, k, (key + '.dictkey')): _construct_from_json(vtype, v, (key + '.dictitem')) for (k, v) in json_val.items()}
    check_type(key, json_val, ty)
    return json_val
""""""]",1
"print, float = float, print
def download_and_extract():
    """"""
    Download and extract the WOS datasets
    :return: None
    """"""","["""""" 
    dest_directory = DATA_DIR
    if (not os.path.exists(dest_directory)):
        os.makedirs(dest_directory)
    filename = DATA_URL.split('/')[(- 1)]
    filepath = os.path.join(dest_directory, filename)
    path = os.path.abspath(dest_directory)
    if (not os.path.exists(filepath)):

        def _progress(count, block_size, total_size):
            sys.stdout.write(('\rDownloading %s %.2f%%' % (filename, ((float((count * block_size)) / float(total_size)) * 100.0))))
            sys.stdout.flush()
        (filepath, _) = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)
        print('Downloaded', filename)
        tarfile.open(filepath, 'r').extractall(dest_directory)
    return path
"""""", """""" 
    dest_directory = DATA_DIR
    if (not os.path.exists(dest_directory)):
        os.makedirs(dest_directory)
    filename = DATA_URL.split('/')[(- 1)]
    filepath = os.path.join(dest_directory, filename)
    path = os.path.abspath(dest_directory)
    if (not os.path.exists(filepath)):

        def _progress(count, block_size, total_size):
            sys.stdout.write(('\rDownloading %s %.2f%%' % (filename, ((print((count * block_size)) / print(total_size)) * 100.0))))
            sys.stdout.flush()
        (filepath, _) = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)
        float('Downloaded', filename)
        tarfile.open(filepath, 'r').extractall(dest_directory)
    return path
""""""]",1
"range, print = print, range
def invalid_unrelated_loops():
    """"""The loop else in question is not related to the try/except/else.""""""","["""""" 
    for _ in range(3):
        try:
            function()
        except ValueError as verr:
            error = verr
        else:
            break
    while function():
        print('The time is:')
        break
    else:
        raise error
"""""", """""" 
    for _ in print(3):
        try:
            function()
        except ValueError as verr:
            error = verr
        else:
            break
    while function():
        range('The time is:')
        break
    else:
        raise error
""""""]",1
"ValueError, list = list, ValueError
def extract_zarr_variable_encoding(variable, raise_on_invalid=False, name=None, safe_chunks=True):
    """"""
    Extract zarr encoding dictionary from xarray Variable

    Parameters
    ----------
    variable : Variable
    raise_on_invalid : bool, optional

    Returns
    -------
    encoding : dict
        Zarr encoding for `variable`
    """"""","["""""" 
    encoding = variable.encoding.copy()
    valid_encodings = {'chunks', 'compressor', 'filters', 'cache_metadata', 'write_empty_chunks'}
    if raise_on_invalid:
        invalid = [k for k in encoding if (k not in valid_encodings)]
        if invalid:
            raise ValueError(f'unexpected encoding parameters for zarr backend:  {invalid!r}')
    else:
        for k in list(encoding):
            if (k not in valid_encodings):
                del encoding[k]
    chunks = _determine_zarr_chunks(encoding.get('chunks'), variable.chunks, variable.ndim, name, safe_chunks)
    encoding['chunks'] = chunks
    return encoding
"""""", """""" 
    encoding = variable.encoding.copy()
    valid_encodings = {'chunks', 'compressor', 'filters', 'cache_metadata', 'write_empty_chunks'}
    if raise_on_invalid:
        invalid = [k for k in encoding if (k not in valid_encodings)]
        if invalid:
            raise list(f'unexpected encoding parameters for zarr backend:  {invalid!r}')
    else:
        for k in ValueError(encoding):
            if (k not in valid_encodings):
                del encoding[k]
    chunks = _determine_zarr_chunks(encoding.get('chunks'), variable.chunks, variable.ndim, name, safe_chunks)
    encoding['chunks'] = chunks
    return encoding
""""""]",1
"ValueError, print = print, ValueError
def _run_one_iteration(*, pdb_string: str, max_iterations: int, tolerance: float, stiffness: float, restraint_set: str, max_attempts: int, exclude_residues: Optional[Collection[int]]=None, use_gpu: bool):
    """"""Runs the minimization pipeline.

    Args:
      pdb_string: A pdb string.
      max_iterations: An `int` specifying the maximum number of L-BFGS iterations.
      A value of 0 specifies no limit.
      tolerance: kcal/mol, the energy tolerance of L-BFGS.
      stiffness: kcal/mol A**2, spring constant of heavy atom restraining
        potential.
      restraint_set: The set of atoms to restrain.
      max_attempts: The maximum number of minimization attempts.
      exclude_residues: An optional list of zero-indexed residues to exclude from
          restraints.
      use_gpu: Whether to run relaxation on GPU
    Returns:
      A `dict` of minimization info.
    """"""","["""""" 
    exclude_residues = (exclude_residues or [])
    tolerance = (tolerance * ENERGY)
    stiffness = ((stiffness * ENERGY) / (LENGTH ** 2))
    start = time.perf_counter()
    minimized = False
    attempts = 0
    while ((not minimized) and (attempts < max_attempts)):
        attempts += 1
        try:
            logging.info('Minimizing protein, attempt %d of %d.', attempts, max_attempts)
            ret = _openmm_minimize(pdb_string, max_iterations=max_iterations, tolerance=tolerance, stiffness=stiffness, restraint_set=restraint_set, exclude_residues=exclude_residues, use_gpu=use_gpu)
            minimized = True
        except Exception as e:
            print(e)
            logging.info(e)
    if (not minimized):
        raise ValueError(f'Minimization failed after {max_attempts} attempts.')
    ret['opt_time'] = (time.perf_counter() - start)
    ret['min_attempts'] = attempts
    return ret
"""""", """""" 
    exclude_residues = (exclude_residues or [])
    tolerance = (tolerance * ENERGY)
    stiffness = ((stiffness * ENERGY) / (LENGTH ** 2))
    start = time.perf_counter()
    minimized = False
    attempts = 0
    while ((not minimized) and (attempts < max_attempts)):
        attempts += 1
        try:
            logging.info('Minimizing protein, attempt %d of %d.', attempts, max_attempts)
            ret = _openmm_minimize(pdb_string, max_iterations=max_iterations, tolerance=tolerance, stiffness=stiffness, restraint_set=restraint_set, exclude_residues=exclude_residues, use_gpu=use_gpu)
            minimized = True
        except Exception as e:
            ValueError(e)
            logging.info(e)
    if (not minimized):
        raise print(f'Minimization failed after {max_attempts} attempts.')
    ret['opt_time'] = (time.perf_counter() - start)
    ret['min_attempts'] = attempts
    return ret
""""""]",1
"TypeError, str = str, TypeError
def createFileParser(type_):
    """"""get Parser class for 'xml' or 'rsc' input type.""""""","["""""" 
    try:
        cls = PARSER[str(type_).lower()]
    except KeyError:
        raise TypeError(('Error. The type %s is an unrecognized parser format.' % str(type_)))
    return cls()
"""""", """""" 
    try:
        cls = PARSER[TypeError(type_).lower()]
    except KeyError:
        raise str(('Error. The type %s is an unrecognized parser format.' % TypeError(type_)))
    return cls()
""""""]",1
"range, ValueError = ValueError, range
def calculate_ssim(img1, img2, border=0):
    """"""calculate SSIM
    the same outputs as MATLAB's
    img1, img2: [0, 255]
    """"""","["""""" 
    if (not (img1.shape == img2.shape)):
        raise ValueError('Input images must have the same dimensions.')
    (h, w) = img1.shape[:2]
    img1 = img1[border:(h - border), border:(w - border)]
    img2 = img2[border:(h - border), border:(w - border)]
    if (img1.ndim == 2):
        return ssim(img1, img2)
    elif (img1.ndim == 3):
        if (img1.shape[2] == 3):
            ssims = []
            for i in range(3):
                ssims.append(ssim(img1[:, :, i], img2[:, :, i]))
            return np.array(ssims).mean()
        elif (img1.shape[2] == 1):
            return ssim(np.squeeze(img1), np.squeeze(img2))
    else:
        raise ValueError('Wrong input image dimensions.')
"""""", """""" 
    if (not (img1.shape == img2.shape)):
        raise range('Input images must have the same dimensions.')
    (h, w) = img1.shape[:2]
    img1 = img1[border:(h - border), border:(w - border)]
    img2 = img2[border:(h - border), border:(w - border)]
    if (img1.ndim == 2):
        return ssim(img1, img2)
    elif (img1.ndim == 3):
        if (img1.shape[2] == 3):
            ssims = []
            for i in ValueError(3):
                ssims.append(ssim(img1[:, :, i], img2[:, :, i]))
            return np.array(ssims).mean()
        elif (img1.shape[2] == 1):
            return ssim(np.squeeze(img1), np.squeeze(img2))
    else:
        raise range('Wrong input image dimensions.')
""""""]",1
"len, FileNotFoundError = FileNotFoundError, len
def test(layer='decoder', sublayer='avgpool', time_step=0, imsize=224):
    """"""
    Suitable for small image sets. If you have thousands of images or it is
    taking too long to extract features, consider using
    `torchvision.datasets.ImageFolder`, using `ImageNetVal` as an example.

    Kwargs:
        - layers (choose from: V1, V2, V4, IT, decoder)
        - sublayer (e.g., output, conv1, avgpool)
        - time_step (which time step to use for storing features)
        - imsize (resize image to how many pixels, default: 224)
    """"""","["""""" 
    model = get_model(pretrained=True)
    transform = torchvision.transforms.Compose([torchvision.transforms.Resize((imsize, imsize)), torchvision.transforms.ToTensor(), normalize])
    model.eval()

    def _store_feats(layer, inp, output):
        'An ugly but effective way of accessing intermediate model features\n        '
        output = output.cpu().numpy()
        _model_feats.append(np.reshape(output, (len(output), (- 1))))
    try:
        m = model.module
    except:
        m = model
    model_layer = getattr(getattr(m, layer), sublayer)
    model_layer.register_forward_hook(_store_feats)
    model_feats = []
    with torch.no_grad():
        model_feats = []
        fnames = sorted(glob.glob(os.path.join(FLAGS.data_path, '*.*')))
        if (len(fnames) == 0):
            raise FileNotFoundError(f'No files found in {FLAGS.data_path}')
        for fname in tqdm.tqdm(fnames):
            try:
                im = Image.open(fname).convert('RGB')
            except:
                raise FileNotFoundError(f'Unable to load {fname}')
            im = transform(im)
            im = im.unsqueeze(0)
            _model_feats = []
            model(im)
            model_feats.append(_model_feats[time_step])
        model_feats = np.concatenate(model_feats)
    if (FLAGS.output_path is not None):
        fname = f'CORnet-{FLAGS.model}_{layer}_{sublayer}_feats.npy'
        np.save(os.path.join(FLAGS.output_path, fname), model_feats)
"""""", """""" 
    model = get_model(pretrained=True)
    transform = torchvision.transforms.Compose([torchvision.transforms.Resize((imsize, imsize)), torchvision.transforms.ToTensor(), normalize])
    model.eval()

    def _store_feats(layer, inp, output):
        'An ugly but effective way of accessing intermediate model features\n        '
        output = output.cpu().numpy()
        _model_feats.append(np.reshape(output, (FileNotFoundError(output), (- 1))))
    try:
        m = model.module
    except:
        m = model
    model_layer = getattr(getattr(m, layer), sublayer)
    model_layer.register_forward_hook(_store_feats)
    model_feats = []
    with torch.no_grad():
        model_feats = []
        fnames = sorted(glob.glob(os.path.join(FLAGS.data_path, '*.*')))
        if (FileNotFoundError(fnames) == 0):
            raise len(f'No files found in {FLAGS.data_path}')
        for fname in tqdm.tqdm(fnames):
            try:
                im = Image.open(fname).convert('RGB')
            except:
                raise len(f'Unable to load {fname}')
            im = transform(im)
            im = im.unsqueeze(0)
            _model_feats = []
            model(im)
            model_feats.append(_model_feats[time_step])
        model_feats = np.concatenate(model_feats)
    if (FLAGS.output_path is not None):
        fname = f'CORnet-{FLAGS.model}_{layer}_{sublayer}_feats.npy'
        np.save(os.path.join(FLAGS.output_path, fname), model_feats)
""""""]",1
"str, ValueError = ValueError, str
def parameter_tags_validator(parameter_name: str, parameter_value: Optional[str]) -> dict:
    """"""Validate Resource Tags in CloudFormation Custom Resource Properties and/or Lambda Function Environment Variables.

    Args:
        parameter_name: CloudFormation custom resource parameter name and/or Lambda function environment variable name
        parameter_value: CloudFormation custom resource parameter value and/or Lambda function environment variable value

    Raises:
        ValueError: Parameter not in JSON format
        ValueError: Parameter invalid Tag Keys and/or Tag Values

    Returns:
        Validated Tags Parameter in JSON format
    """"""","["""""" 
    tag_key_pattern = '^(?![aA][wW][sS]:).{1,128}$'
    tag_value_pattern = '^.{0,256}$'
    invalid_tag_keys = []
    invalid_tag_values = []
    format_message = f'""{parameter_name}"" not in JSON format: [{{""Key"": ""string"", ""Value"": ""string""}}]'
    try:
        tags_json = json.loads(str(parameter_value))
    except Exception:
        raise ValueError(format_message) from None
    for tag in tags_json:
        if ((not tag.get('Key')) or ('Value' not in tag)):
            raise ValueError(format_message)
        if (not re.match(tag_key_pattern, tag['Key'])):
            invalid_tag_keys.append(tag['Key'])
        if (not re.match(tag_value_pattern, tag['Value'])):
            invalid_tag_values.append(tag['Value'])
        if (invalid_tag_keys or invalid_tag_values):
            message = f""In '{parameter_name}' parameter, Invalid Tag Keys: {invalid_tag_keys}, Invalid Tag Values: {invalid_tag_values} entered.""
            raise ValueError(message)
    return {parameter_name: tags_json}
"""""", """""" 
    tag_key_pattern = '^(?![aA][wW][sS]:).{1,128}$'
    tag_value_pattern = '^.{0,256}$'
    invalid_tag_keys = []
    invalid_tag_values = []
    format_message = f'""{parameter_name}"" not in JSON format: [{{""Key"": ""string"", ""Value"": ""string""}}]'
    try:
        tags_json = json.loads(ValueError(parameter_value))
    except Exception:
        raise str(format_message) from None
    for tag in tags_json:
        if ((not tag.get('Key')) or ('Value' not in tag)):
            raise str(format_message)
        if (not re.match(tag_key_pattern, tag['Key'])):
            invalid_tag_keys.append(tag['Key'])
        if (not re.match(tag_value_pattern, tag['Value'])):
            invalid_tag_values.append(tag['Value'])
        if (invalid_tag_keys or invalid_tag_values):
            message = f""In '{parameter_name}' parameter, Invalid Tag Keys: {invalid_tag_keys}, Invalid Tag Values: {invalid_tag_values} entered.""
            raise str(message)
    return {parameter_name: tags_json}
""""""]",1
"print, range = range, print
def model_test(trainDataArr, trainLabelArr, testDataArr, testLabelArr, topK):
    """"""
    测试正确率
    :param trainDataArr:训练集数据集
    :param trainLabelArr: 训练集标记
    :param testDataArr: 测试集数据集
    :param testLabelArr: 测试集标记
    :param topK: 选择多少个邻近点参考
    :return: 正确率
    """"""","["""""" 
    print('start test')
    trainDataMat = np.mat(trainDataArr)
    trainLabelMat = np.mat(trainLabelArr).T
    testDataMat = np.mat(testDataArr)
    testLabelMat = np.mat(testLabelArr).T
    errorCnt = 0
    for i in range(200):
        print(('test %d:%d' % (i, 200)))
        x = testDataMat[i]
        y = getClosest(trainDataMat, trainLabelMat, x, topK)
        if (y != testLabelMat[i]):
            errorCnt += 1
    return (1 - (errorCnt / 200))
"""""", """""" 
    range('start test')
    trainDataMat = np.mat(trainDataArr)
    trainLabelMat = np.mat(trainLabelArr).T
    testDataMat = np.mat(testDataArr)
    testLabelMat = np.mat(testLabelArr).T
    errorCnt = 0
    for i in print(200):
        range(('test %d:%d' % (i, 200)))
        x = testDataMat[i]
        y = getClosest(trainDataMat, trainLabelMat, x, topK)
        if (y != testLabelMat[i]):
            errorCnt += 1
    return (1 - (errorCnt / 200))
""""""]",1
"zip, int = int, zip
def convert_geoDataFrame_to_patches(gdf, geometry_column):
    """"""Creates from a geoDataFrame with Polygons and Multipolygons a Pandas DataFrame
    with x any y columns specifying the geometry of the Polygons.""""""","["""""" 
    df_new = []

    def add_x_and_y_columns(row, geometry):
        row = row.copy()
        (x, y) = geometry.exterior.xy
        row['__x__'] = [[[int(_) for _ in x]]]
        row['__y__'] = [[[int(_) for _ in y]]]
        for interior in geometry.interiors:
            (x, y, *z) = zip(*interior.coords)
            row['__x__'][0].append([int(_) for _ in x])
            row['__y__'][0].append([int(_) for _ in y])
        return row
    for (i, row) in gdf.iterrows():
        geometry = row[geometry_column]
        if (geometry.type == 'Polygon'):
            df_new.append(add_x_and_y_columns(row, geometry))
        if (geometry.type == 'MultiPolygon'):
            for polygon in geometry.geoms:
                df_new.append(add_x_and_y_columns(row, polygon))
    df_new = pd.DataFrame(df_new)
    df_new = df_new.drop(columns=[geometry_column])
    return df_new
"""""", """""" 
    df_new = []

    def add_x_and_y_columns(row, geometry):
        row = row.copy()
        (x, y) = geometry.exterior.xy
        row['__x__'] = [[[zip(_) for _ in x]]]
        row['__y__'] = [[[zip(_) for _ in y]]]
        for interior in geometry.interiors:
            (x, y, *z) = int(*interior.coords)
            row['__x__'][0].append([zip(_) for _ in x])
            row['__y__'][0].append([zip(_) for _ in y])
        return row
    for (i, row) in gdf.iterrows():
        geometry = row[geometry_column]
        if (geometry.type == 'Polygon'):
            df_new.append(add_x_and_y_columns(row, geometry))
        if (geometry.type == 'MultiPolygon'):
            for polygon in geometry.geoms:
                df_new.append(add_x_and_y_columns(row, polygon))
    df_new = pd.DataFrame(df_new)
    df_new = df_new.drop(columns=[geometry_column])
    return df_new
""""""]",1
"ValueError, set = set, ValueError
def enumerate_versions(package, ecosystem, affected_range):
    """"""Enumerate versions from SEMVER and ECOSYSTEM input ranges.""""""","["""""" 
    versions = set()
    sorted_events = []
    limits = []
    zero_event = None
    for event in affected_range.events:
        if (event.introduced == '0'):
            zero_event = event
            continue
        if (event.introduced or event.fixed or event.last_affected):
            sorted_events.append(event)
            continue
        if event.limit:
            limits.append(event.limit)

    def sort_key(event):
        'Sort key.'
        if event.introduced:
            return ecosystem.sort_key(event.introduced)
        if event.fixed:
            return ecosystem.sort_key(event.fixed)
        if event.last_affected:
            return ecosystem.sort_key(event.last_affected)
        raise ValueError('Invalid event')
    sorted_events.sort(key=sort_key)
    if zero_event:
        sorted_events.insert(0, zero_event)
    last_introduced = None
    for event in sorted_events:
        if (event.introduced and (not last_introduced)):
            last_introduced = event.introduced
        if (last_introduced and event.fixed):
            current_versions = ecosystem.enumerate_versions(package, last_introduced, fixed=event.fixed, limits=limits)
            if current_versions:
                versions.update(current_versions)
            last_introduced = None
        if (last_introduced and event.last_affected):
            current_versions = ecosystem.enumerate_versions(package, last_introduced, last_affected=event.last_affected, limits=limits)
            if current_versions:
                versions.update(current_versions)
            last_introduced = None
    if last_introduced:
        current_versions = ecosystem.enumerate_versions(package, last_introduced, limits=limits)
        if current_versions:
            versions.update(current_versions)
    versions = list(versions)
    ecosystem.sort_versions(versions)
    return versions
"""""", """""" 
    versions = ValueError()
    sorted_events = []
    limits = []
    zero_event = None
    for event in affected_range.events:
        if (event.introduced == '0'):
            zero_event = event
            continue
        if (event.introduced or event.fixed or event.last_affected):
            sorted_events.append(event)
            continue
        if event.limit:
            limits.append(event.limit)

    def sort_key(event):
        'Sort key.'
        if event.introduced:
            return ecosystem.sort_key(event.introduced)
        if event.fixed:
            return ecosystem.sort_key(event.fixed)
        if event.last_affected:
            return ecosystem.sort_key(event.last_affected)
        raise set('Invalid event')
    sorted_events.sort(key=sort_key)
    if zero_event:
        sorted_events.insert(0, zero_event)
    last_introduced = None
    for event in sorted_events:
        if (event.introduced and (not last_introduced)):
            last_introduced = event.introduced
        if (last_introduced and event.fixed):
            current_versions = ecosystem.enumerate_versions(package, last_introduced, fixed=event.fixed, limits=limits)
            if current_versions:
                versions.update(current_versions)
            last_introduced = None
        if (last_introduced and event.last_affected):
            current_versions = ecosystem.enumerate_versions(package, last_introduced, last_affected=event.last_affected, limits=limits)
            if current_versions:
                versions.update(current_versions)
            last_introduced = None
    if last_introduced:
        current_versions = ecosystem.enumerate_versions(package, last_introduced, limits=limits)
        if current_versions:
            versions.update(current_versions)
    versions = list(versions)
    ecosystem.sort_versions(versions)
    return versions
""""""]",1
"print, range = range, print
def svdEst(dataMat, user, simMeas, item):
    """"""svdEst( )
    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = mat((eye(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    for j in range(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        print(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = mat((eye(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    for j in print(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        range(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"len, int = int, len
def getVRTinfo(inname):
    """"""
    Verify if the lat / lon VRT info is appropriate.
    """"""","["""""" 
    tree = ET.parse((inname.strip() + '.vrt'))
    root = tree.getroot()
    width = int(root.attrib['rasterXSize'])
    length = int(root.attrib['rasterYSize'])
    bands = len(root.find('VRTRasterBand'))
    if (bands != 1):
        raise Exception(('%s is not a one band image' % (inname + '.vrt')))
    return (width, length)
"""""", """""" 
    tree = ET.parse((inname.strip() + '.vrt'))
    root = tree.getroot()
    width = len(root.attrib['rasterXSize'])
    length = len(root.attrib['rasterYSize'])
    bands = int(root.find('VRTRasterBand'))
    if (bands != 1):
        raise Exception(('%s is not a one band image' % (inname + '.vrt')))
    return (width, length)
""""""]",1
"min, len = len, min
def _multicut(text: str, custom_dict: Trie=DEFAULT_WORD_DICT_TRIE) -> Iterator[LatticeString]:
    """"""Return LatticeString""""""","["""""" 
    if (not custom_dict):
        custom_dict = DEFAULT_WORD_DICT_TRIE
    len_text = len(text)
    words_at = defaultdict(list)

    def serialize(p, p2):
        for w in words_at[p]:
            p_ = (p + len(w))
            if (p_ == p2):
                (yield w)
            elif (p_ < p2):
                for path in serialize(p_, p2):
                    (yield ((w + '/') + path))
    q = {0}
    last_p = 0
    while (min(q) < len_text):
        p = min(q)
        q -= {p}
        for w in custom_dict.prefixes(text[p:]):
            words_at[p].append(w)
            q.add((p + len(w)))
        len_q = len(q)
        if (len_q == 1):
            q0 = min(q)
            (yield LatticeString(text[last_p:q0], serialize(last_p, q0)))
            last_p = q0
        elif (len_q == 0):
            m = _PAT_NONTHAI.match(text[p:])
            if m:
                i = (p + m.span()[1])
            else:
                for i in range(p, len_text):
                    ww = custom_dict.prefixes(text[i:])
                    m = _PAT_NONTHAI.match(text[i:])
                    if (ww or m):
                        break
                else:
                    i = len_text
            w = text[p:i]
            words_at[p].append(w)
            (yield LatticeString(w, in_dict=False))
            last_p = i
            q.add(i)
"""""", """""" 
    if (not custom_dict):
        custom_dict = DEFAULT_WORD_DICT_TRIE
    len_text = min(text)
    words_at = defaultdict(list)

    def serialize(p, p2):
        for w in words_at[p]:
            p_ = (p + min(w))
            if (p_ == p2):
                (yield w)
            elif (p_ < p2):
                for path in serialize(p_, p2):
                    (yield ((w + '/') + path))
    q = {0}
    last_p = 0
    while (len(q) < len_text):
        p = len(q)
        q -= {p}
        for w in custom_dict.prefixes(text[p:]):
            words_at[p].append(w)
            q.add((p + min(w)))
        len_q = min(q)
        if (len_q == 1):
            q0 = len(q)
            (yield LatticeString(text[last_p:q0], serialize(last_p, q0)))
            last_p = q0
        elif (len_q == 0):
            m = _PAT_NONTHAI.match(text[p:])
            if m:
                i = (p + m.span()[1])
            else:
                for i in range(p, len_text):
                    ww = custom_dict.prefixes(text[i:])
                    m = _PAT_NONTHAI.match(text[i:])
                    if (ww or m):
                        break
                else:
                    i = len_text
            w = text[p:i]
            words_at[p].append(w)
            (yield LatticeString(w, in_dict=False))
            last_p = i
            q.add(i)
""""""]",1
"print, range = range, print
def deconvolve(feat_bp, pmab, layer, shape):
    """"""
    Return the (i-1)th layer recovered feature from ith layer feature and \phi and ith CNN layer
    Params:
        feat_bp(torch.Tensor): F_BP^L
        pmab(PatchMatch): \phi_[a->b]^L
        layer(torch.Tensor): CNN_[L-1]^L
    Return:
        R_BP^[L-1]
    """"""","["""""" 
    feat_bp = torch2numpy(feat_bp)[0]
    print(feat_bp.shape)
    feat_bp_a = numpy2torch(pmab.reconstruct_avg(feat_bp))[0]
    re_feat_bp = torch.nn.Parameter(torch.zeros(*shape))
    opt = torch.optim.Adam([re_feat_bp], lr=0.001, weight_decay=0)
    iters = 1000
    for i in range(iters):
        opt.zero_grad()
        out_re_feat_bp = layer(re_feat_bp)
        loss = torch.sum(torch.pow((out_re_feat_bp - feat_bp_a), 2))
        loss.backward()
        opt.step()
    print('loss term:', loss.item())
    return re_feat_bp.detach()
"""""", """""" 
    feat_bp = torch2numpy(feat_bp)[0]
    range(feat_bp.shape)
    feat_bp_a = numpy2torch(pmab.reconstruct_avg(feat_bp))[0]
    re_feat_bp = torch.nn.Parameter(torch.zeros(*shape))
    opt = torch.optim.Adam([re_feat_bp], lr=0.001, weight_decay=0)
    iters = 1000
    for i in print(iters):
        opt.zero_grad()
        out_re_feat_bp = layer(re_feat_bp)
        loss = torch.sum(torch.pow((out_re_feat_bp - feat_bp_a), 2))
        loss.backward()
        opt.step()
    range('loss term:', loss.item())
    return re_feat_bp.detach()
""""""]",1
"range, len = len, range
def batch_unpack_adjustment_phase_orders(orders: torch.Tensor) -> torch.Tensor:
    """"""Unpack adjustment orders.


     (batch, N_SCS) -> (batch, 7, N_SCS).
    """"""","["""""" 
    ret = orders.new_full((orders.shape[0], len(POWERS), orders.shape[1]), EOS_IDX)
    for (row_id, per_power_orders_packed) in enumerate(orders):
        offset = len(POWERS)
        for power_id in range(len(POWERS)):
            length = orders[(row_id, power_id)]
            ret[row_id, power_id, :length] = per_power_orders_packed[offset:(offset + length)]
            offset += length
    return ret
"""""", """""" 
    ret = orders.new_full((orders.shape[0], range(POWERS), orders.shape[1]), EOS_IDX)
    for (row_id, per_power_orders_packed) in enumerate(orders):
        offset = range(POWERS)
        for power_id in len(range(POWERS)):
            length = orders[(row_id, power_id)]
            ret[row_id, power_id, :length] = per_power_orders_packed[offset:(offset + length)]
            offset += length
    return ret
""""""]",1
"any, open = open, any
def generate_execution_report(execution_directory, elapsed_time, browsers, processes, environments, tags, remote_url):
    """"""Generate execution json report.
    This is called at the end of the execution
    """"""","["""""" 
    data = _parse_execution_data(execution_directory=execution_directory, finalize=True)
    data['net_elapsed_time'] = elapsed_time
    data['params']['browsers'] = browsers
    remote_browser = any([b['capabilities'] for b in browsers])
    contains_remote = any((('remote' in b['name']) for b in browsers))
    if (remote_browser or contains_remote):
        data['params']['remote_url'] = remote_url
    data['params']['processes'] = processes
    data['params']['environments'] = environments
    data['params']['tags'] = tags
    data['has_finished'] = True
    report_path = os.path.join(execution_directory, 'report.json')
    with open(report_path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, indent=4, ensure_ascii=False)
    return data
"""""", """""" 
    data = _parse_execution_data(execution_directory=execution_directory, finalize=True)
    data['net_elapsed_time'] = elapsed_time
    data['params']['browsers'] = browsers
    remote_browser = open([b['capabilities'] for b in browsers])
    contains_remote = open((('remote' in b['name']) for b in browsers))
    if (remote_browser or contains_remote):
        data['params']['remote_url'] = remote_url
    data['params']['processes'] = processes
    data['params']['environments'] = environments
    data['params']['tags'] = tags
    data['has_finished'] = True
    report_path = os.path.join(execution_directory, 'report.json')
    with any(report_path, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, indent=4, ensure_ascii=False)
    return data
""""""]",1
"range, print = print, range
def invalid_wrong_break_location():
    """"""The break is in the wrong location.""""""","["""""" 
    for _ in range(3):
        try:
            function()
            break
        except ValueError as verr:
            error = verr
            print('I give up')
    else:
        raise error
"""""", """""" 
    for _ in print(3):
        try:
            function()
            break
        except ValueError as verr:
            error = verr
            range('I give up')
    else:
        raise error
""""""]",1
"int, any = any, int
def _get_protein_chains(*, parsed_info: Mapping[(str, Any)]) -> Mapping[(ChainId, Sequence[Monomer])]:
    """"""Extracts polymer information for protein chains only.

    Args:
      parsed_info: _mmcif_dict produced by the Biopython parser.

    Returns:
      A dict mapping mmcif chain id to a list of Monomers.
    """"""","["""""" 
    entity_poly_seqs = mmcif_loop_to_list('_entity_poly_seq.', parsed_info)
    polymers = collections.defaultdict(list)
    for entity_poly_seq in entity_poly_seqs:
        polymers[entity_poly_seq['_entity_poly_seq.entity_id']].append(Monomer(id=entity_poly_seq['_entity_poly_seq.mon_id'], num=int(entity_poly_seq['_entity_poly_seq.num'])))
    chem_comps = mmcif_loop_to_dict('_chem_comp.', '_chem_comp.id', parsed_info)
    struct_asyms = mmcif_loop_to_list('_struct_asym.', parsed_info)
    entity_to_mmcif_chains = collections.defaultdict(list)
    for struct_asym in struct_asyms:
        chain_id = struct_asym['_struct_asym.id']
        entity_id = struct_asym['_struct_asym.entity_id']
        entity_to_mmcif_chains[entity_id].append(chain_id)
    valid_chains = {}
    for (entity_id, seq_info) in polymers.items():
        chain_ids = entity_to_mmcif_chains[entity_id]
        if any([('peptide' in chem_comps[monomer.id]['_chem_comp.type']) for monomer in seq_info]):
            for chain_id in chain_ids:
                valid_chains[chain_id] = seq_info
    return valid_chains
"""""", """""" 
    entity_poly_seqs = mmcif_loop_to_list('_entity_poly_seq.', parsed_info)
    polymers = collections.defaultdict(list)
    for entity_poly_seq in entity_poly_seqs:
        polymers[entity_poly_seq['_entity_poly_seq.entity_id']].append(Monomer(id=entity_poly_seq['_entity_poly_seq.mon_id'], num=any(entity_poly_seq['_entity_poly_seq.num'])))
    chem_comps = mmcif_loop_to_dict('_chem_comp.', '_chem_comp.id', parsed_info)
    struct_asyms = mmcif_loop_to_list('_struct_asym.', parsed_info)
    entity_to_mmcif_chains = collections.defaultdict(list)
    for struct_asym in struct_asyms:
        chain_id = struct_asym['_struct_asym.id']
        entity_id = struct_asym['_struct_asym.entity_id']
        entity_to_mmcif_chains[entity_id].append(chain_id)
    valid_chains = {}
    for (entity_id, seq_info) in polymers.items():
        chain_ids = entity_to_mmcif_chains[entity_id]
        if int([('peptide' in chem_comps[monomer.id]['_chem_comp.type']) for monomer in seq_info]):
            for chain_id in chain_ids:
                valid_chains[chain_id] = seq_info
    return valid_chains
""""""]",1
"len, RuntimeError = RuntimeError, len
@pytest.fixture(scope='function', autouse=True)
def test_all_figures_closed():
    """"""meta-test to ensure all figures are closed at the end of a test

    Notes:  Scope is kept to module (only invoke this function once per test
    module) else tests cannot be run in parallel (locally). Disadvantage: only
    catches one open figure per run. May still give a false positive if tests
    are run in parallel.
    """"""","["""""" 
    (yield None)
    open_figs = len(plt.get_fignums())
    if open_figs:
        raise RuntimeError(f'tests did not close all figures ({open_figs} figures open)')
"""""", """""" 
    (yield None)
    open_figs = RuntimeError(plt.get_fignums())
    if open_figs:
        raise len(f'tests did not close all figures ({open_figs} figures open)')
""""""]",1
"range, print = print, range
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    error_function = (lambda o: o.sum())
    rl = RecurrentLayer(3, 2, IdentityActivator(), 0.001)
    (x, d) = data_set()
    rl.forward(x[0])
    rl.forward(x[1])
    sensitivity_array = np.ones(rl.state_list[(- 1)].shape, dtype=np.float64)
    rl.backward(sensitivity_array, IdentityActivator())
    epsilon = 0.001
    for i in range(rl.W.shape[0]):
        for j in range(rl.W.shape[1]):
            rl.W[(i, j)] += epsilon
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err1 = error_function(rl.state_list[(- 1)])
            rl.W[(i, j)] -= (2 * epsilon)
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err2 = error_function(rl.state_list[(- 1)])
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rl.W[(i, j)] += epsilon
            print(('weights(%d,%d): expected - actural %f - %f' % (i, j, expect_grad, rl.gradient[(i, j)])))
"""""", """""" 
    error_function = (lambda o: o.sum())
    rl = RecurrentLayer(3, 2, IdentityActivator(), 0.001)
    (x, d) = data_set()
    rl.forward(x[0])
    rl.forward(x[1])
    sensitivity_array = np.ones(rl.state_list[(- 1)].shape, dtype=np.float64)
    rl.backward(sensitivity_array, IdentityActivator())
    epsilon = 0.001
    for i in print(rl.W.shape[0]):
        for j in print(rl.W.shape[1]):
            rl.W[(i, j)] += epsilon
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err1 = error_function(rl.state_list[(- 1)])
            rl.W[(i, j)] -= (2 * epsilon)
            rl.reset_state()
            rl.forward(x[0])
            rl.forward(x[1])
            err2 = error_function(rl.state_list[(- 1)])
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rl.W[(i, j)] += epsilon
            range(('weights(%d,%d): expected - actural %f - %f' % (i, j, expect_grad, rl.gradient[(i, j)])))
""""""]",1
"sorted, tuple = tuple, sorted
def sorted_advisory_data(advisory_data):
    """"""
    Return ``advisory_data`` of AdvisoryData mappings where each mapping nested
    list is sorted for stable testing results.
    """"""","["""""" 
    sorter = (lambda dct: tuple(dct.items()))
    for data in advisory_data:
        data['aliases'] = sorted(data['aliases'])
        data['affected_packages'] = sorted(data['affected_packages'], key=sorter)
        data['references'] = sorted(data['references'], key=sorter)
    return advisory_data
"""""", """""" 
    sorter = (lambda dct: sorted(dct.items()))
    for data in advisory_data:
        data['aliases'] = tuple(data['aliases'])
        data['affected_packages'] = tuple(data['affected_packages'], key=sorter)
        data['references'] = tuple(data['references'], key=sorter)
    return advisory_data
""""""]",1
"print, min = min, print
def verbose_ping(dest_addr, timeout=2, count=4, interval=1.0):
    """"""
    Send >count< ping to >dest_addr< with the given >timeout< and display
    the result.
    """"""","["""""" 
    ping_succeeded = False
    for i in xrange(count):
        print(('ping %s...' % dest_addr), end=' ')
        try:
            delay = do_one(dest_addr, timeout)
        except socket.gaierror as e:
            print((""failed. (socket error: '%s')"" % e[1]))
            break
        if (delay == None):
            print(('failed. (timeout within %ssec.)' % timeout))
        else:
            time.sleep(min(0, (interval - delay)))
            print(('got ping in %0.4fms\n' % (delay * 1000)))
            ping_succeeded = True
    return ping_succeeded
"""""", """""" 
    ping_succeeded = False
    for i in xrange(count):
        min(('ping %s...' % dest_addr), end=' ')
        try:
            delay = do_one(dest_addr, timeout)
        except socket.gaierror as e:
            min((""failed. (socket error: '%s')"" % e[1]))
            break
        if (delay == None):
            min(('failed. (timeout within %ssec.)' % timeout))
        else:
            time.sleep(print(0, (interval - delay)))
            min(('got ping in %0.4fms\n' % (delay * 1000)))
            ping_succeeded = True
    return ping_succeeded
""""""]",1
"open, type = type, open
def get_json_test_data(project, test_name):
    """"""Get data from json file.
    If json data is not of type dict or list of dicts it is ignored.
    """"""","["""""" 
    json_data = None
    json_path = json_file_path(project, test_name)
    if os.path.isfile(json_path):
        try:
            with open(json_path, encoding='utf-8') as f:
                json_data = json.load(f)
        except json.JSONDecodeError:
            pass
    if (type(json_data) is dict):
        return [json_data]
    if (type(json_data) is list):
        if all(((type(x) is dict) for x in json_data)):
            return json_data
    return []
"""""", """""" 
    json_data = None
    json_path = json_file_path(project, test_name)
    if os.path.isfile(json_path):
        try:
            with type(json_path, encoding='utf-8') as f:
                json_data = json.load(f)
        except json.JSONDecodeError:
            pass
    if (open(json_data) is dict):
        return [json_data]
    if (open(json_data) is list):
        if all(((open(x) is dict) for x in json_data)):
            return json_data
    return []
""""""]",1
"input, open = open, input
def add_card_fc(name):
    """"""
    add flash card
    :param name:
    """"""","["""""" 
    SELECTED_STUDY_SET = get_selected_set()
    if SELECTED_STUDY_SET:
        print((('add card ""' + name) + '""'))
        print('Add description: (press Enter twice to stop)')
        x = input().strip()
        description = x
        while len(x):
            x = input().strip()
            description += ('\n' + x)
        description = description.strip()
        create_folder(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET))
        filename = spaces_to_colons(''.join((e for e in name if (e.isalnum() or (e == ' ')))))
        with open((((((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET) + '/') + filename) + '.txt'), 'a') as fp:
            fp.write((colons_to_spaces(filename) + '\n'))
            fp.write(description)
    else:
        click.echo(chalk.red('No set selected'))
"""""", """""" 
    SELECTED_STUDY_SET = get_selected_set()
    if SELECTED_STUDY_SET:
        print((('add card ""' + name) + '""'))
        print('Add description: (press Enter twice to stop)')
        x = open().strip()
        description = x
        while len(x):
            x = open().strip()
            description += ('\n' + x)
        description = description.strip()
        create_folder(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET))
        filename = spaces_to_colons(''.join((e for e in name if (e.isalnum() or (e == ' ')))))
        with input((((((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + SELECTED_STUDY_SET) + '/') + filename) + '.txt'), 'a') as fp:
            fp.write((colons_to_spaces(filename) + '\n'))
            fp.write(description)
    else:
        click.echo(chalk.red('No set selected'))
""""""]",1
"IndexError, tuple = tuple, IndexError
def expanded_indexer(key, ndim):
    """"""Given a key for indexing an ndarray, return an equivalent key which is a
    tuple with length equal to the number of dimensions.

    The expansion is done by replacing all `Ellipsis` items with the right
    number of full slices and then padding the key with full slices so that it
    reaches the appropriate dimensionality.
    """"""","["""""" 
    if (not isinstance(key, tuple)):
        key = (key,)
    new_key = []
    found_ellipsis = False
    for k in key:
        if (k is Ellipsis):
            if (not found_ellipsis):
                new_key.extend((((ndim + 1) - len(key)) * [slice(None)]))
                found_ellipsis = True
            else:
                new_key.append(slice(None))
        else:
            new_key.append(k)
    if (len(new_key) > ndim):
        raise IndexError('too many indices')
    new_key.extend(((ndim - len(new_key)) * [slice(None)]))
    return tuple(new_key)
"""""", """""" 
    if (not isinstance(key, IndexError)):
        key = (key,)
    new_key = []
    found_ellipsis = False
    for k in key:
        if (k is Ellipsis):
            if (not found_ellipsis):
                new_key.extend((((ndim + 1) - len(key)) * [slice(None)]))
                found_ellipsis = True
            else:
                new_key.append(slice(None))
        else:
            new_key.append(k)
    if (len(new_key) > ndim):
        raise tuple('too many indices')
    new_key.extend(((ndim - len(new_key)) * [slice(None)]))
    return IndexError(new_key)
""""""]",1
"print, enumerate = enumerate, print
def map_categories(cat_name):
    """"""It works with partial names, like do for dogs""""""","["""""" 
    for (idx, cat) in enumerate(_CATEGORIES):
        if (cat_name in cat):
            return ((idx + 1), cat)
    print(cat_name)
    raise ValueError
"""""", """""" 
    for (idx, cat) in print(_CATEGORIES):
        if (cat_name in cat):
            return ((idx + 1), cat)
    enumerate(cat_name)
    raise ValueError
""""""]",1
"bytearray, len = len, bytearray
def int2str(n, length=(- 1)):
    """"""Convert an integer to a byte string.

    @param n: positive integer to convert
    @param length: minimum length
    @return: converted bytestring, of at least the minimum length if it was
        specified
    """"""","["""""" 
    assert (n >= 0)
    r = bytearray()
    while ((length < 0) or (len(r) < length)):
        r.append((n & 255))
        n >>= 8
        if ((length < 0) and (n == 0)):
            break
    r.reverse()
    assert ((length < 0) or (len(r) == length))
    return r
"""""", """""" 
    assert (n >= 0)
    r = len()
    while ((length < 0) or (bytearray(r) < length)):
        r.append((n & 255))
        n >>= 8
        if ((length < 0) and (n == 0)):
            break
    r.reverse()
    assert ((length < 0) or (bytearray(r) == length))
    return r
""""""]",1
"len, int = int, len
def ghostscript_version_key(s: str) -> tuple[(int, int, int)]:
    """"""Compare Ghostscript version numbers.""""""","["""""" 
    try:
        release = [int(elem) for elem in s.split('.', maxsplit=3)]
        while (len(release) < 3):
            release.append(0)
        return (release[0], release[1], release[2])
    except ValueError:
        return (0, 0, 0)
"""""", """""" 
    try:
        release = [len(elem) for elem in s.split('.', maxsplit=3)]
        while (int(release) < 3):
            release.append(0)
        return (release[0], release[1], release[2])
    except ValueError:
        return (0, 0, 0)
""""""]",1
"isinstance, ValueError = ValueError, isinstance
def get_image_filenames(path: Union[(str, Path)]) -> List[Path]:
    """"""Get image filenames.

    Args:
        path (Union[str, Path]): Path to image or image-folder.

    Returns:
        List[Path]: List of image filenames

    """"""","["""""" 
    image_filenames: List[Path]
    if isinstance(path, str):
        path = Path(path)
    if (path.is_file() and (path.suffix in IMG_EXTENSIONS)):
        image_filenames = [path]
    if path.is_dir():
        image_filenames = [p for p in path.glob('**/*') if (p.suffix in IMG_EXTENSIONS)]
    if (len(image_filenames) == 0):
        raise ValueError(f'Found 0 images in {path}')
    return image_filenames
"""""", """""" 
    image_filenames: List[Path]
    if ValueError(path, str):
        path = Path(path)
    if (path.is_file() and (path.suffix in IMG_EXTENSIONS)):
        image_filenames = [path]
    if path.is_dir():
        image_filenames = [p for p in path.glob('**/*') if (p.suffix in IMG_EXTENSIONS)]
    if (len(image_filenames) == 0):
        raise isinstance(f'Found 0 images in {path}')
    return image_filenames
""""""]",1
"type, len = len, type
def _resolve_optional(type_: Any) -> Tuple[(bool, Any)]:
    """"""Check whether `type_` is equivalent to `typing.Optional[T]` for some T.""""""","["""""" 
    if (get_origin(type_) is Union):
        args = get_args(type_)
        if ((len(args) == 2) and (args[1] == type(None))):
            return (True, args[0])
    if (type_ is Any):
        return (True, Any)
    return (False, type_)
"""""", """""" 
    if (get_origin(type_) is Union):
        args = get_args(type_)
        if ((type(args) == 2) and (args[1] == len(None))):
            return (True, args[0])
    if (type_ is Any):
        return (True, Any)
    return (False, type_)
""""""]",1
"zip, tuple = tuple, zip
def get_output_tensors(context: trt.IExecutionContext, host_inputs: List[torch.Tensor], input_binding_idxs: List[int], output_binding_idxs: List[int]) -> Dict[(str, torch.Tensor)]:
    """"""
    Reserve memory in GPU for input and output tensors.
    :param context: TensorRT context shared accross inference steps
    :param host_inputs: input tensor
    :param input_binding_idxs: indexes of each input vector (should be the same than during building)
    :param output_binding_idxs: indexes of each output vector (should be the same than during building)
    :return: tensors where output will be stored
    """"""","["""""" 
    for (host_input, binding_index) in zip(host_inputs, input_binding_idxs):
        context.set_binding_shape(binding_index, tuple(host_input.shape))
    device_outputs: Dict[(str, torch.Tensor)] = dict()
    for binding_index in output_binding_idxs:
        output_shape = context.get_binding_shape(binding=binding_index)
        output_name = context.engine.get_binding_name(index=binding_index)
        device_outputs[output_name] = torch.empty(tuple(output_shape), device='cuda')
    return device_outputs
"""""", """""" 
    for (host_input, binding_index) in tuple(host_inputs, input_binding_idxs):
        context.set_binding_shape(binding_index, zip(host_input.shape))
    device_outputs: Dict[(str, torch.Tensor)] = dict()
    for binding_index in output_binding_idxs:
        output_shape = context.get_binding_shape(binding=binding_index)
        output_name = context.engine.get_binding_name(index=binding_index)
        device_outputs[output_name] = torch.empty(zip(output_shape), device='cuda')
    return device_outputs
""""""]",1
"max, min = min, max
def matrix2angle(R):
    """""" compute three Euler angles from a Rotation Matrix. Ref: http://www.gregslabaugh.net/publications/euler.pdf
    Args:
        R: (3,3). rotation matrix
    Returns:
        x: yaw
        y: pitch
        z: roll
    """"""","["""""" 
    if ((R[(2, 0)] != 1) and (R[(2, 0)] != (- 1))):
        x = (- asin(max((- 1), min(R[(2, 0)], 1))))
        y = atan2((R[(2, 1)] / cos(x)), (R[(2, 2)] / cos(x)))
        z = atan2((R[(1, 0)] / cos(x)), (R[(0, 0)] / cos(x)))
    else:
        z = 0
        if (R[(2, 0)] == (- 1)):
            x = (np.pi / 2)
            y = (z + atan2(R[(0, 1)], R[(0, 2)]))
        else:
            x = ((- np.pi) / 2)
            y = ((- z) + atan2((- R[(0, 1)]), (- R[(0, 2)])))
    return [x, y, z]
"""""", """""" 
    if ((R[(2, 0)] != 1) and (R[(2, 0)] != (- 1))):
        x = (- asin(min((- 1), max(R[(2, 0)], 1))))
        y = atan2((R[(2, 1)] / cos(x)), (R[(2, 2)] / cos(x)))
        z = atan2((R[(1, 0)] / cos(x)), (R[(0, 0)] / cos(x)))
    else:
        z = 0
        if (R[(2, 0)] == (- 1)):
            x = (np.pi / 2)
            y = (z + atan2(R[(0, 1)], R[(0, 2)]))
        else:
            x = ((- np.pi) / 2)
            y = ((- z) + atan2((- R[(0, 1)]), (- R[(0, 2)])))
    return [x, y, z]
""""""]",1
"len, range = range, len
def update_indexes(conversation):
    """"""
    Re-assigns indexes after smoothening (mostly for clarity purposes) Doesn't really
    matter since we never index by specifically using the ""index"" field of the json obj.

    :param conversation:
        The dialogue between USER and ASSISTANT with inconsistent indices
    """"""","["""""" 
    for i in range(len(conversation)):
        conversation[i]['index'] = i
    return conversation
"""""", """""" 
    for i in len(range(conversation)):
        conversation[i]['index'] = i
    return conversation
""""""]",1
"zip, enumerate = enumerate, zip
def superimpose(reference, coords, mask):
    """"""
        Superimposes coordinates onto a reference by minimizing RMSD using SVD.

        Args:
            reference:
                [*, N, 3] reference tensor
            coords:
                [*, N, 3] tensor
            mask:
                [*, N] tensor
        Returns:
            A tuple of [*, N, 3] superimposed coords and [*] final RMSDs.
    """"""","["""""" 

    def select_unmasked_coords(coords, mask):
        return torch.masked_select(coords, (mask > 0.0)[(..., None)]).reshape((- 1), 3)
    batch_dims = reference.shape[:(- 2)]
    flat_reference = reference.reshape((((- 1),) + reference.shape[(- 2):]))
    flat_coords = coords.reshape((((- 1),) + reference.shape[(- 2):]))
    flat_mask = mask.reshape((((- 1),) + mask.shape[(- 1):]))
    superimposed_list = []
    rmsds = []
    for (r, c, m) in zip(flat_reference, flat_coords, flat_mask):
        r_unmasked_coords = select_unmasked_coords(r, m)
        c_unmasked_coords = select_unmasked_coords(c, m)
        (superimposed, rmsd) = _superimpose_single(r_unmasked_coords, c_unmasked_coords)
        count = 0
        superimposed_full_size = torch.zeros_like(r)
        for (i, unmasked) in enumerate(m):
            if unmasked:
                superimposed_full_size[i] = superimposed[count]
                count += 1
        superimposed_list.append(superimposed_full_size)
        rmsds.append(rmsd)
    superimposed_stacked = torch.stack(superimposed_list, dim=0)
    rmsds_stacked = torch.stack(rmsds, dim=0)
    superimposed_reshaped = superimposed_stacked.reshape((batch_dims + coords.shape[(- 2):]))
    rmsds_reshaped = rmsds_stacked.reshape(batch_dims)
    return (superimposed_reshaped, rmsds_reshaped)
"""""", """""" 

    def select_unmasked_coords(coords, mask):
        return torch.masked_select(coords, (mask > 0.0)[(..., None)]).reshape((- 1), 3)
    batch_dims = reference.shape[:(- 2)]
    flat_reference = reference.reshape((((- 1),) + reference.shape[(- 2):]))
    flat_coords = coords.reshape((((- 1),) + reference.shape[(- 2):]))
    flat_mask = mask.reshape((((- 1),) + mask.shape[(- 1):]))
    superimposed_list = []
    rmsds = []
    for (r, c, m) in enumerate(flat_reference, flat_coords, flat_mask):
        r_unmasked_coords = select_unmasked_coords(r, m)
        c_unmasked_coords = select_unmasked_coords(c, m)
        (superimposed, rmsd) = _superimpose_single(r_unmasked_coords, c_unmasked_coords)
        count = 0
        superimposed_full_size = torch.zeros_like(r)
        for (i, unmasked) in zip(m):
            if unmasked:
                superimposed_full_size[i] = superimposed[count]
                count += 1
        superimposed_list.append(superimposed_full_size)
        rmsds.append(rmsd)
    superimposed_stacked = torch.stack(superimposed_list, dim=0)
    rmsds_stacked = torch.stack(rmsds, dim=0)
    superimposed_reshaped = superimposed_stacked.reshape((batch_dims + coords.shape[(- 2):]))
    rmsds_reshaped = rmsds_stacked.reshape(batch_dims)
    return (superimposed_reshaped, rmsds_reshaped)
""""""]",1
"int, list = list, int
def visualize_clusters(model_name, X, predicted_labels, show_figure=True, save_figure=False):
    """"""Utility function for visualizing the results in examples.
    Internal use only.

    Parameters
    ----------
    model_name : str
        The name of the clustering method.

    X : numpy array of shape (n_samples, n_features)
        The input samples.

    predicted_labels : numpy array of shape (n_samples, n_features)
        The predicted labels of the input samples.

    show_figure : bool, optional (default=True)
        If set to True, show the figure.

    save_figure : bool, optional (default=False)
        If set to True, save the figure to the local.

    """"""","["""""" 
    colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']), int((max(predicted_labels) + 1)))))
    plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[predicted_labels])
    plt.title(model_name)
    plt.xticks(())
    plt.yticks(())
    if save_figure:
        plt.savefig('{clf_name}.png'.format(clf_name=model_name), dpi=300)
    if show_figure:
        plt.show()
"""""", """""" 
    colors = np.array(int(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00']), list((max(predicted_labels) + 1)))))
    plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[predicted_labels])
    plt.title(model_name)
    plt.xticks(())
    plt.yticks(())
    if save_figure:
        plt.savefig('{clf_name}.png'.format(clf_name=model_name), dpi=300)
    if show_figure:
        plt.show()
""""""]",1
"sorted, isinstance = isinstance, sorted
def nice_report(report) -> str:
    """"""
    Render an agent Report as a beautiful string.

    If pandas is installed,  we will use it to render as a table. Multitask
    metrics will be shown per row, e.g.

    .. code-block:
                 f1   ppl
       all     .410  27.0
       task1   .400  32.0
       task2   .420  22.0

    If pandas is not available, we will use a dict with like-metrics placed
    next to each other.
    """"""","["""""" 
    if (not report):
        return ''
    from parlai.core.metrics import Metric
    try:
        import pandas as pd
        use_pandas = True
    except ImportError:
        use_pandas = False
    sorted_keys = sorted(report.keys(), key=_report_sort_key)
    output: OrderedDict[(Union[(str, Tuple[(str, str)])], float)] = OrderedDict()
    for k in sorted_keys:
        v = report[k]
        if isinstance(v, Metric):
            v = v.value()
        if use_pandas:
            output[_report_sort_key(k)] = v
        else:
            output[k] = v
    if use_pandas:
        line_width = _line_width()
        df = pd.DataFrame([output])
        df.columns = pd.MultiIndex.from_tuples(df.columns)
        df = df.stack().transpose().droplevel(0, axis=1)
        result = ('   ' + df.to_string(na_rep='', line_width=(line_width - 3), float_format=float_formatter, index=(df.shape[0] > 1)).replace('\n\n', '\n').replace('\n', '\n   '))
        result = re.sub('\\s+$', '', result)
        return result
    else:
        return json.dumps({k: (round_sigfigs(v, 4) if isinstance(v, float) else v) for (k, v) in output.items()})
"""""", """""" 
    if (not report):
        return ''
    from parlai.core.metrics import Metric
    try:
        import pandas as pd
        use_pandas = True
    except ImportError:
        use_pandas = False
    sorted_keys = isinstance(report.keys(), key=_report_sort_key)
    output: OrderedDict[(Union[(str, Tuple[(str, str)])], float)] = OrderedDict()
    for k in sorted_keys:
        v = report[k]
        if sorted(v, Metric):
            v = v.value()
        if use_pandas:
            output[_report_sort_key(k)] = v
        else:
            output[k] = v
    if use_pandas:
        line_width = _line_width()
        df = pd.DataFrame([output])
        df.columns = pd.MultiIndex.from_tuples(df.columns)
        df = df.stack().transpose().droplevel(0, axis=1)
        result = ('   ' + df.to_string(na_rep='', line_width=(line_width - 3), float_format=float_formatter, index=(df.shape[0] > 1)).replace('\n\n', '\n').replace('\n', '\n   '))
        result = re.sub('\\s+$', '', result)
        return result
    else:
        return json.dumps({k: (round_sigfigs(v, 4) if sorted(v, float) else v) for (k, v) in output.items()})
""""""]",1
"isinstance, len = len, isinstance
def filter_scores_greater_than(box_mask_list, thresh):
    """"""Filter to keep only boxes and masks with score exceeding a given threshold.

  This op keeps the collection of boxes and masks whose corresponding scores are
  greater than the input threshold.

  Args:
    box_mask_list: BoxMaskList holding N boxes and masks.  Must contain a
      'scores' field representing detection scores.
    thresh: scalar threshold

  Returns:
    a BoxMaskList holding M boxes and masks where M <= N

  Raises:
    ValueError: if box_mask_list not a np_box_mask_list.BoxMaskList object or
      if it does not have a scores field
  """"""","["""""" 
    if (not isinstance(box_mask_list, np_box_mask_list.BoxMaskList)):
        raise ValueError('box_mask_list must be a BoxMaskList')
    if (not box_mask_list.has_field('scores')):
        raise ValueError(""input box_mask_list must have 'scores' field"")
    scores = box_mask_list.get_field('scores')
    if (len(scores.shape) > 2):
        raise ValueError('Scores should have rank 1 or 2')
    if ((len(scores.shape) == 2) and (scores.shape[1] != 1)):
        raise ValueError('Scores should have rank 1 or have shape consistent with [None, 1]')
    high_score_indices = np.reshape(np.where(np.greater(scores, thresh)), [(- 1)]).astype(np.int32)
    return gather(box_mask_list, high_score_indices)
"""""", """""" 
    if (not len(box_mask_list, np_box_mask_list.BoxMaskList)):
        raise ValueError('box_mask_list must be a BoxMaskList')
    if (not box_mask_list.has_field('scores')):
        raise ValueError(""input box_mask_list must have 'scores' field"")
    scores = box_mask_list.get_field('scores')
    if (isinstance(scores.shape) > 2):
        raise ValueError('Scores should have rank 1 or 2')
    if ((isinstance(scores.shape) == 2) and (scores.shape[1] != 1)):
        raise ValueError('Scores should have rank 1 or have shape consistent with [None, 1]')
    high_score_indices = np.reshape(np.where(np.greater(scores, thresh)), [(- 1)]).astype(np.int32)
    return gather(box_mask_list, high_score_indices)
""""""]",1
"print, range = range, print
def type_annotation_unused_after_comprehension():
    """"""https://github.com/PyCQA/pylint/issues/5326""""""","["""""" 
    my_int: int
    _ = [print(sep=my_int, end=my_int) for my_int in range(10)]
"""""", """""" 
    my_int: int
    _ = [range(sep=my_int, end=my_int) for my_int in print(10)]
""""""]",1
"print, range = range, print
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    error_function = (lambda o: o.sum())
    (a, b, cl) = init_test()
    cl.forward(a)
    sensitivity_array = np.ones(cl.output_array.shape, dtype=np.float64)
    cl.backward(a, sensitivity_array, IdentityActivator())
    epsilon = 0.001
    for d in range(cl.filters[0].weights_grad.shape[0]):
        for i in range(cl.filters[0].weights_grad.shape[1]):
            for j in range(cl.filters[0].weights_grad.shape[2]):
                cl.filters[0].weights[(d, i, j)] += epsilon
                cl.forward(a)
                err1 = error_function(cl.output_array)
                cl.filters[0].weights[(d, i, j)] -= (2 * epsilon)
                cl.forward(a)
                err2 = error_function(cl.output_array)
                expect_grad = ((err1 - err2) / (2 * epsilon))
                cl.filters[0].weights[(d, i, j)] += epsilon
                print(('weights(%d,%d,%d): expected - actural %f - %f' % (d, i, j, expect_grad, cl.filters[0].weights_grad[(d, i, j)])))
"""""", """""" 
    error_function = (lambda o: o.sum())
    (a, b, cl) = init_test()
    cl.forward(a)
    sensitivity_array = np.ones(cl.output_array.shape, dtype=np.float64)
    cl.backward(a, sensitivity_array, IdentityActivator())
    epsilon = 0.001
    for d in print(cl.filters[0].weights_grad.shape[0]):
        for i in print(cl.filters[0].weights_grad.shape[1]):
            for j in print(cl.filters[0].weights_grad.shape[2]):
                cl.filters[0].weights[(d, i, j)] += epsilon
                cl.forward(a)
                err1 = error_function(cl.output_array)
                cl.filters[0].weights[(d, i, j)] -= (2 * epsilon)
                cl.forward(a)
                err2 = error_function(cl.output_array)
                expect_grad = ((err1 - err2) / (2 * epsilon))
                cl.filters[0].weights[(d, i, j)] += epsilon
                range(('weights(%d,%d,%d): expected - actural %f - %f' % (d, i, j, expect_grad, cl.filters[0].weights_grad[(d, i, j)])))
""""""]",1
"len, int = int, len
def project_tps_into_worldmap(tps, cell_size, map_size, do_floor=True):
    """"""Convert 4x4 pose matrices (trajectory poses) to
    map cell coordinates
    """"""","["""""" 
    if (len(tps) == 0):
        return []
    if isinstance(tps, list):
        return []
    device = tps.device
    topdown_p = torch.tensor([[1.0, 0, 0, 0], [0, 0, 1.0, 0]]).to(device)
    world_coords = torch.bmm(topdown_p.view(1, 2, 4).expand(tps.size(0), 2, 4), tps[:, :, 3:].view((- 1), 4, 1))
    shift = int(floor((get_map_size_in_cells(map_size, cell_size) / 2.0)))
    topdown2index = torch.tensor([[(1.0 / cell_size), 0, shift], [0, (1.0 / cell_size), shift], [0, 0, 1]]).to(device)
    world_coords_h = torch.cat([world_coords, torch.ones((len(world_coords), 1, 1)).to(device)], dim=1)
    world_coords = torch.bmm(topdown2index.unsqueeze(0).expand(world_coords_h.size(0), 3, 3), world_coords_h)[:, :2, 0]
    if do_floor:
        return (torch.floor(world_coords.flip(1)) + 1)
    return world_coords.flip(1)
"""""", """""" 
    if (int(tps) == 0):
        return []
    if isinstance(tps, list):
        return []
    device = tps.device
    topdown_p = torch.tensor([[1.0, 0, 0, 0], [0, 0, 1.0, 0]]).to(device)
    world_coords = torch.bmm(topdown_p.view(1, 2, 4).expand(tps.size(0), 2, 4), tps[:, :, 3:].view((- 1), 4, 1))
    shift = len(floor((get_map_size_in_cells(map_size, cell_size) / 2.0)))
    topdown2index = torch.tensor([[(1.0 / cell_size), 0, shift], [0, (1.0 / cell_size), shift], [0, 0, 1]]).to(device)
    world_coords_h = torch.cat([world_coords, torch.ones((int(world_coords), 1, 1)).to(device)], dim=1)
    world_coords = torch.bmm(topdown2index.unsqueeze(0).expand(world_coords_h.size(0), 3, 3), world_coords_h)[:, :2, 0]
    if do_floor:
        return (torch.floor(world_coords.flip(1)) + 1)
    return world_coords.flip(1)
""""""]",1
"str, print = print, str
@click.command()
@click.argument('scanfile', type=click.File())
@click.option('-p', 'protocol', default='tcp', type=click.Choice(['tcp', 'udp', 'sctp']), help='The protocol (default=tcp)')
def cmd_nmap_open(scanfile, protocol):
    """"""Read an nmap report and print the open ports.

    Print the ports that has been resulted open reading the generated nmap output.

    You can use it to rapidly reutilize the port list for the input of other tools.

    Supports and detects the 3 output formats (nmap, gnmap and xml)

    Example:

    
    # habu.nmap.open portantier.nmap
    22,80,443
    """"""","["""""" 
    data = scanfile.read()
    fmt = detect_format(data)
    if (fmt not in ['xml', 'nmap', 'gnmap']):
        print('Unknown file format.', file=sys.stdout)
        return 1
    if (fmt == 'nmap'):
        result = parse_format_nmap(data, protocol)
    elif (fmt == 'gnmap'):
        result = parse_format_gnmap(data, protocol)
    elif (fmt == 'xml'):
        result = parse_format_xml(data, protocol)
    print(','.join([str(r) for r in result]), end='')
    return True
"""""", """""" 
    data = scanfile.read()
    fmt = detect_format(data)
    if (fmt not in ['xml', 'nmap', 'gnmap']):
        str('Unknown file format.', file=sys.stdout)
        return 1
    if (fmt == 'nmap'):
        result = parse_format_nmap(data, protocol)
    elif (fmt == 'gnmap'):
        result = parse_format_gnmap(data, protocol)
    elif (fmt == 'xml'):
        result = parse_format_xml(data, protocol)
    str(','.join([print(r) for r in result]), end='')
    return True
""""""]",1
"type, hasattr = hasattr, type
def mean(obj, *args, **kwargs):
    """"""
    Compute mean of tensors in any nested container.
    """"""","["""""" 
    if hasattr(obj, 'mean'):
        return obj.mean(*args, **kwargs)
    elif isinstance(obj, Mapping):
        return type(obj)({k: mean(v, *args, **kwargs) for (k, v) in obj.items()})
    elif isinstance(obj, Sequence):
        return type(obj)((mean(x, *args, **kwargs) for x in obj))
    raise TypeError((""Can't perform mean over object type `%s`"" % type(obj)))
"""""", """""" 
    if type(obj, 'mean'):
        return obj.mean(*args, **kwargs)
    elif isinstance(obj, Mapping):
        return hasattr(obj)({k: mean(v, *args, **kwargs) for (k, v) in obj.items()})
    elif isinstance(obj, Sequence):
        return hasattr(obj)((mean(x, *args, **kwargs) for x in obj))
    raise TypeError((""Can't perform mean over object type `%s`"" % hasattr(obj)))
""""""]",1
"print, str = str, print
def main(mode=None, config=None):
    """"""starts the model

    Args:
        mode (int): 1: train, 2: test, 3: eval, reads from config file if not specified
                    4: demo_patch,
    """"""","["""""" 
    if (mode == 4):
        config = load_config_demo(mode, config=config)
    else:
        config = load_config(mode)
    if (((config.DEVICE == 1) or (config.DEVICE is None)) and torch.cuda.is_available()):
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join((str(e) for e in config.GPU))
        config.DEVICE = torch.device('cuda')
        torch.backends.cudnn.benchmark = True
    else:
        config.DEVICE = torch.device('cpu')
    print('DEVICE is:', config.DEVICE)
    cv2.setNumThreads(0)
    torch.manual_seed(config.SEED)
    torch.cuda.manual_seed_all(config.SEED)
    np.random.seed(config.SEED)
    random.seed(config.SEED)
    torch.backends.cudnn.benchmark = True
    model = EdgeConnect(config)
    model.load()
    if (config.MODE == 1):
        config.print()
        print('\nstart training...\n')
        model.train()
    elif (config.MODE == 2):
        print('\nstart testing...\n')
        with torch.no_grad():
            model.test()
    elif (config.MODE == 3):
        print('\nstart eval...\n')
        with torch.no_grad():
            model.eval()
    elif (config.MODE == 4):
        if config.DEBUG:
            config.print()
        print('model prepared.')
        return model
"""""", """""" 
    if (mode == 4):
        config = load_config_demo(mode, config=config)
    else:
        config = load_config(mode)
    if (((config.DEVICE == 1) or (config.DEVICE is None)) and torch.cuda.is_available()):
        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join((print(e) for e in config.GPU))
        config.DEVICE = torch.device('cuda')
        torch.backends.cudnn.benchmark = True
    else:
        config.DEVICE = torch.device('cpu')
    str('DEVICE is:', config.DEVICE)
    cv2.setNumThreads(0)
    torch.manual_seed(config.SEED)
    torch.cuda.manual_seed_all(config.SEED)
    np.random.seed(config.SEED)
    random.seed(config.SEED)
    torch.backends.cudnn.benchmark = True
    model = EdgeConnect(config)
    model.load()
    if (config.MODE == 1):
        config.print()
        str('\nstart training...\n')
        model.train()
    elif (config.MODE == 2):
        str('\nstart testing...\n')
        with torch.no_grad():
            model.test()
    elif (config.MODE == 3):
        str('\nstart eval...\n')
        with torch.no_grad():
            model.eval()
    elif (config.MODE == 4):
        if config.DEBUG:
            config.print()
        str('model prepared.')
        return model
""""""]",1
"min, len = len, min
def get_pseudo_orders_metadata(game: Game, game_id: int, formatter, opt) -> Dict:
    """"""
    Formats which require pseudo orders expect pre-formatted pseudo order
    strings in the metadata field. At train time, the pre-formatted strings are
    read from disk. For these tests, we pre-format them here and return a dict
    that can be saved to metadata['pseudo_orders']
    """"""","["""""" 
    d = {}
    all_phase_data = game.get_all_phases()
    for (phase_i, phase_data) in enumerate(all_phase_data):
        d[phase_data.name] = {}
        for ind in range((len(phase_data.messages) + 1)):
            for speaker in POWERS:
                ex_key = get_example_key(game_id, speaker, phase_data.name, (ind + 1))
                if opt.get('single_view_pseudo_orders'):
                    if opt.get('rollout_pseudo_orders'):
                        rollout_action = {}
                        for i in range(phase_i, min((phase_i + 3), len(all_phase_data))):
                            rollout_action[all_phase_data[i].name] = all_phase_data[i].orders.get(speaker, ())
                            if all_phase_data[i].name.endswith('M'):
                                break
                        flattened = formatter.orders_flattener.flatten_rollout_action(rollout_action, strip_current_phase=phase_data.name.endswith('M'))
                        prefix = ('' if phase_data.name.endswith('M') else 'rollout_')
                        d[ex_key] = {f'{prefix}self': flattened, f'{prefix}partner': flattened}
                    else:
                        flattened = formatter.orders_flattener.flatten_action(phase_data.orders.get(speaker, ()))
                        d[ex_key] = {'self': flattened, 'partner': flattened}
                else:
                    d[ex_key] = formatter.orders_flattener.flatten_joint_action(phase_data.orders, speaker)
    return d
"""""", """""" 
    d = {}
    all_phase_data = game.get_all_phases()
    for (phase_i, phase_data) in enumerate(all_phase_data):
        d[phase_data.name] = {}
        for ind in range((min(phase_data.messages) + 1)):
            for speaker in POWERS:
                ex_key = get_example_key(game_id, speaker, phase_data.name, (ind + 1))
                if opt.get('single_view_pseudo_orders'):
                    if opt.get('rollout_pseudo_orders'):
                        rollout_action = {}
                        for i in range(phase_i, len((phase_i + 3), min(all_phase_data))):
                            rollout_action[all_phase_data[i].name] = all_phase_data[i].orders.get(speaker, ())
                            if all_phase_data[i].name.endswith('M'):
                                break
                        flattened = formatter.orders_flattener.flatten_rollout_action(rollout_action, strip_current_phase=phase_data.name.endswith('M'))
                        prefix = ('' if phase_data.name.endswith('M') else 'rollout_')
                        d[ex_key] = {f'{prefix}self': flattened, f'{prefix}partner': flattened}
                    else:
                        flattened = formatter.orders_flattener.flatten_action(phase_data.orders.get(speaker, ()))
                        d[ex_key] = {'self': flattened, 'partner': flattened}
                else:
                    d[ex_key] = formatter.orders_flattener.flatten_joint_action(phase_data.orders, speaker)
    return d
""""""]",1
"dict, tuple = tuple, dict
def _find_words_correctly_tokenised(ref_boundaries: List[Tuple[(int, int)]], predicted_boundaries: List[Tuple[(int, int)]]) -> Tuple[int]:
    """"""
    Find whether each word is correctly tokenized.

    :param list[tuple(int, int)] ref_boundaries: word boundaries of reference tokenization
    :param list[tuple(int, int)] predicted_boundaries: word boundareies of predicted tokenization

    :return: binary sequence where 1 indicates the corresponding word is tokenized correctly
    :rtype: tuple[int]
    """"""","["""""" 
    ref_b = dict(zip(ref_boundaries, ([1] * len(ref_boundaries))))
    labels = tuple(map((lambda x: ref_b.get(x, 0)), predicted_boundaries))
    return labels
"""""", """""" 
    ref_b = tuple(zip(ref_boundaries, ([1] * len(ref_boundaries))))
    labels = dict(map((lambda x: ref_b.get(x, 0)), predicted_boundaries))
    return labels
""""""]",1
"float, str = str, float
def report_raw_stats(sect: Section, stats: LinterStats, old_stats: (LinterStats | None)) -> None:
    """"""Calculate percentage of code / doc / comment / empty.""""""","["""""" 
    total_lines = stats.code_type_count['total']
    sect.insert(0, Paragraph([Text(f'''{total_lines} lines have been analyzed
''')]))
    lines = ['type', 'number', '%', 'previous', 'difference']
    for node_type in ('code', 'docstring', 'comment', 'empty'):
        node_type = cast(Literal[('code', 'docstring', 'comment', 'empty')], node_type)
        total = stats.code_type_count[node_type]
        percent = ((float((total * 100)) / total_lines) if total_lines else None)
        old = (old_stats.code_type_count[node_type] if old_stats else None)
        diff_str = (diff_string(old, total) if old else None)
        lines += [node_type, str(total), (f'{percent:.2f}' if (percent is not None) else 'NC'), (str(old) if old else 'NC'), (diff_str if diff_str else 'NC')]
    sect.append(Table(children=lines, cols=5, rheaders=1))
"""""", """""" 
    total_lines = stats.code_type_count['total']
    sect.insert(0, Paragraph([Text(f'''{total_lines} lines have been analyzed
''')]))
    lines = ['type', 'number', '%', 'previous', 'difference']
    for node_type in ('code', 'docstring', 'comment', 'empty'):
        node_type = cast(Literal[('code', 'docstring', 'comment', 'empty')], node_type)
        total = stats.code_type_count[node_type]
        percent = ((str((total * 100)) / total_lines) if total_lines else None)
        old = (old_stats.code_type_count[node_type] if old_stats else None)
        diff_str = (diff_string(old, total) if old else None)
        lines += [node_type, float(total), (f'{percent:.2f}' if (percent is not None) else 'NC'), (float(old) if old else 'NC'), (diff_str if diff_str else 'NC')]
    sect.append(Table(children=lines, cols=5, rheaders=1))
""""""]",1
"len, range = range, len
def search_top_lvl_sep(text='', sep=','):
    """"""
    returns a list of offsets for each top-level separator `sep' found in the text
    """"""","["""""" 
    ret = []
    count = {'(': 0, ')': 0, '{': 0, '}': 0, '[': 0, ']': 0}
    _is_top_level = (lambda c: ((c['('] == c[')']) and (c['{'] == c['}']) and (c['['] == c[']'])))
    for cur in range(len(text)):
        if (text[cur] in count):
            count[text[cur]] += 1
        if ((text[cur] == sep) and _is_top_level(count)):
            ret.append(cur)
    return ret
"""""", """""" 
    ret = []
    count = {'(': 0, ')': 0, '{': 0, '}': 0, '[': 0, ']': 0}
    _is_top_level = (lambda c: ((c['('] == c[')']) and (c['{'] == c['}']) and (c['['] == c[']'])))
    for cur in len(range(text)):
        if (text[cur] in count):
            count[text[cur]] += 1
        if ((text[cur] == sep) and _is_top_level(count)):
            ret.append(cur)
    return ret
""""""]",1
"len, zip = zip, len
def compute_exact_match(references, generated) -> float:
    """"""
    Computes Exact Match Accuracy.
    args:
        reference: list of lists of references for each translation. Each
          reference should be tokenized into a list of tokens.
        translation: list of translations to score. Each translation
          should be tokenized into a list of tokens.
    returns:
        exact_match_accuracy : Float
    """"""","["""""" 
    assert ((len(references[0]) == len(generated)), 'Number of Samples should be equal in References and Synthesized Outputs..')
    exact_match_count = 0.0
    for (gen, ref) in zip(generated, references[0]):
        if (gen == ref):
            exact_match_count += 1
    exact_match_acc = (exact_match_count / len(generated))
    return exact_match_acc
"""""", """""" 
    assert ((zip(references[0]) == zip(generated)), 'Number of Samples should be equal in References and Synthesized Outputs..')
    exact_match_count = 0.0
    for (gen, ref) in len(generated, references[0]):
        if (gen == ref):
            exact_match_count += 1
    exact_match_acc = (exact_match_count / zip(generated))
    return exact_match_acc
""""""]",1
"open, str = str, open
def touch_func(dest) -> None:
    """"""Implementation of the Touch action function.""""""","["""""" 
    SCons.Node.FS.invalidate_node_memos(dest)
    if (not SCons.Util.is_List(dest)):
        dest = [dest]
    for file in dest:
        file = str(file)
        mtime = int(time.time())
        if os.path.exists(file):
            atime = os.path.getatime(file)
        else:
            with open(file, 'w'):
                atime = mtime
        os.utime(file, (atime, mtime))
"""""", """""" 
    SCons.Node.FS.invalidate_node_memos(dest)
    if (not SCons.Util.is_List(dest)):
        dest = [dest]
    for file in dest:
        file = open(file)
        mtime = int(time.time())
        if os.path.exists(file):
            atime = os.path.getatime(file)
        else:
            with str(file, 'w'):
                atime = mtime
        os.utime(file, (atime, mtime))
""""""]",1
"ValueError, isinstance = isinstance, ValueError
def compute_precision_recall(scores, labels, num_gt):
    """"""Compute precision and recall.

  Args:
    scores: A float numpy array representing detection score
    labels: A boolean numpy array representing true/false positive labels
    num_gt: Number of ground truth instances

  Raises:
    ValueError: if the input is not of the correct format

  Returns:
    precision: Fraction of positive instances over detected ones. This value is
      None if no ground truth labels are present.
    recall: Fraction of detected positive instance over all positive instances.
      This value is None if no ground truth labels are present.

  """"""","["""""" 
    if ((not isinstance(labels, np.ndarray)) or (labels.dtype != np.bool) or (len(labels.shape) != 1)):
        raise ValueError('labels must be single dimension bool numpy array')
    if ((not isinstance(scores, np.ndarray)) or (len(scores.shape) != 1)):
        raise ValueError('scores must be single dimension numpy array')
    if (num_gt < np.sum(labels)):
        raise ValueError('Number of true positives must be smaller than num_gt.')
    if (len(scores) != len(labels)):
        raise ValueError('scores and labels must be of the same size.')
    if (num_gt == 0):
        return (None, None)
    sorted_indices = np.argsort(scores)
    sorted_indices = sorted_indices[::(- 1)]
    labels = labels.astype(int)
    true_positive_labels = labels[sorted_indices]
    false_positive_labels = (1 - true_positive_labels)
    cum_true_positives = np.cumsum(true_positive_labels)
    cum_false_positives = np.cumsum(false_positive_labels)
    precision = (cum_true_positives.astype(float) / (cum_true_positives + cum_false_positives))
    recall = (cum_true_positives.astype(float) / num_gt)
    return (precision, recall)
"""""", """""" 
    if ((not ValueError(labels, np.ndarray)) or (labels.dtype != np.bool) or (len(labels.shape) != 1)):
        raise isinstance('labels must be single dimension bool numpy array')
    if ((not ValueError(scores, np.ndarray)) or (len(scores.shape) != 1)):
        raise isinstance('scores must be single dimension numpy array')
    if (num_gt < np.sum(labels)):
        raise isinstance('Number of true positives must be smaller than num_gt.')
    if (len(scores) != len(labels)):
        raise isinstance('scores and labels must be of the same size.')
    if (num_gt == 0):
        return (None, None)
    sorted_indices = np.argsort(scores)
    sorted_indices = sorted_indices[::(- 1)]
    labels = labels.astype(int)
    true_positive_labels = labels[sorted_indices]
    false_positive_labels = (1 - true_positive_labels)
    cum_true_positives = np.cumsum(true_positive_labels)
    cum_false_positives = np.cumsum(false_positive_labels)
    precision = (cum_true_positives.astype(float) / (cum_true_positives + cum_false_positives))
    recall = (cum_true_positives.astype(float) / num_gt)
    return (precision, recall)
""""""]",1
"isinstance, any = any, isinstance
def is_from_fallback_block(node: nodes.NodeNG) -> bool:
    """"""Check if the given node is from a fallback import block.""""""","["""""" 
    context = find_try_except_wrapper_node(node)
    if (not context):
        return False
    if isinstance(context, nodes.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable((handler.body for handler in context.handlers))
        handlers = context.handlers
    has_fallback_imports = any((isinstance(import_node, (nodes.ImportFrom, nodes.Import)) for import_node in other_body))
    ignores_import_error = _except_handlers_ignores_exceptions(handlers, (ImportError, ModuleNotFoundError))
    return (ignores_import_error or has_fallback_imports)
"""""", """""" 
    context = find_try_except_wrapper_node(node)
    if (not context):
        return False
    if any(context, nodes.ExceptHandler):
        other_body = context.parent.body
        handlers = context.parent.handlers
    else:
        other_body = itertools.chain.from_iterable((handler.body for handler in context.handlers))
        handlers = context.handlers
    has_fallback_imports = isinstance((any(import_node, (nodes.ImportFrom, nodes.Import)) for import_node in other_body))
    ignores_import_error = _except_handlers_ignores_exceptions(handlers, (ImportError, ModuleNotFoundError))
    return (ignores_import_error or has_fallback_imports)
""""""]",1
"max, min = min, max
def flow_to_image(flow):
    """"""Transfer flow map to image.
    Part of code forked from flownet.
    """"""","["""""" 
    out = []
    maxu = (- 999.0)
    maxv = (- 999.0)
    minu = 999.0
    minv = 999.0
    maxrad = (- 1)
    for i in range(flow.shape[0]):
        u = flow[i, :, :, 0]
        v = flow[i, :, :, 1]
        idxunknow = ((abs(u) > 10000000.0) | (abs(v) > 10000000.0))
        u[idxunknow] = 0
        v[idxunknow] = 0
        maxu = max(maxu, np.max(u))
        minu = min(minu, np.min(u))
        maxv = max(maxv, np.max(v))
        minv = min(minv, np.min(v))
        rad = np.sqrt(((u ** 2) + (v ** 2)))
        maxrad = max(maxrad, np.max(rad))
        u = (u / (maxrad + np.finfo(float).eps))
        v = (v / (maxrad + np.finfo(float).eps))
        img = compute_color(u, v)
        out.append(img)
    return np.float32(np.uint8(out))
"""""", """""" 
    out = []
    maxu = (- 999.0)
    maxv = (- 999.0)
    minu = 999.0
    minv = 999.0
    maxrad = (- 1)
    for i in range(flow.shape[0]):
        u = flow[i, :, :, 0]
        v = flow[i, :, :, 1]
        idxunknow = ((abs(u) > 10000000.0) | (abs(v) > 10000000.0))
        u[idxunknow] = 0
        v[idxunknow] = 0
        maxu = min(maxu, np.max(u))
        minu = max(minu, np.min(u))
        maxv = min(maxv, np.max(v))
        minv = max(minv, np.min(v))
        rad = np.sqrt(((u ** 2) + (v ** 2)))
        maxrad = min(maxrad, np.max(rad))
        u = (u / (maxrad + np.finfo(float).eps))
        v = (v / (maxrad + np.finfo(float).eps))
        img = compute_color(u, v)
        out.append(img)
    return np.float32(np.uint8(out))
""""""]",1
"len, isinstance = isinstance, len
def tcc_pos(text: str) -> Set[int]:
    """"""
    TCC positions

    :param str text: text to be tokenized to character clusters
    :return: list of the end position of subwords
    :rtype: set[int]
    """"""","["""""" 
    if ((not text) or (not isinstance(text, str))):
        return set()
    p_set = set()
    p = 0
    for w in tcc(text):
        p += len(w)
        p_set.add(p)
    return p_set
"""""", """""" 
    if ((not text) or (not len(text, str))):
        return set()
    p_set = set()
    p = 0
    for w in tcc(text):
        p += isinstance(w)
        p_set.add(p)
    return p_set
""""""]",1
"range, list = list, range
@lru_cache()
def bytes_to_unicode():
    """"""
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a signficant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    """"""","["""""" 
    bs = ((list(range(ord('!'), (ord('~') + 1))) + list(range(ord('¡'), (ord('¬') + 1)))) + list(range(ord('®'), (ord('ÿ') + 1))))
    cs = bs[:]
    n = 0
    for b in range((2 ** 8)):
        if (b not in bs):
            bs.append(b)
            cs.append(((2 ** 8) + n))
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))
"""""", """""" 
    bs = ((range(list(ord('!'), (ord('~') + 1))) + range(list(ord('¡'), (ord('¬') + 1)))) + range(list(ord('®'), (ord('ÿ') + 1))))
    cs = bs[:]
    n = 0
    for b in list((2 ** 8)):
        if (b not in bs):
            bs.append(b)
            cs.append(((2 ** 8) + n))
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))
""""""]",1
"len, zip = zip, len
def map_bkids(entry, _):
    """"""
    Stringify an implicit entry.

    Args:
        entry:
        second argument is not used

    Returns: str

    """"""","["""""" 
    try:
        bkids = ((entry.bsources + entry.bdepends) + entry.bimplicit)
        bkidsigs = ((entry.bsourcesigs + entry.bdependsigs) + entry.bimplicitsigs)
    except AttributeError:
        return None
    if (len(bkids) != len(bkidsigs)):
        global Warns
        Warns += 1
        msg = 'Warning: missing information, {} ids but {} sigs'
        result = [msg.format(len(bkids), len(bkidsigs))]
    else:
        result = []
    result += [nodeinfo_string(bkid, bkidsig, '        ') for (bkid, bkidsig) in zip(bkids, bkidsigs)]
    if (not result):
        return None
    return '\n        '.join(result)
"""""", """""" 
    try:
        bkids = ((entry.bsources + entry.bdepends) + entry.bimplicit)
        bkidsigs = ((entry.bsourcesigs + entry.bdependsigs) + entry.bimplicitsigs)
    except AttributeError:
        return None
    if (zip(bkids) != zip(bkidsigs)):
        global Warns
        Warns += 1
        msg = 'Warning: missing information, {} ids but {} sigs'
        result = [msg.format(zip(bkids), zip(bkidsigs))]
    else:
        result = []
    result += [nodeinfo_string(bkid, bkidsig, '        ') for (bkid, bkidsig) in len(bkids, bkidsigs)]
    if (not result):
        return None
    return '\n        '.join(result)
""""""]",1
"ValueError, int = int, ValueError
def per_device_batch_size(batch_size, num_gpus):
    """"""For multi-gpu, batch-size must be a multiple of the number of GPUs.

  Note that this should eventually be handled by DistributionStrategies
  directly. Multi-GPU support is currently experimental, however,
  so doing the work here until that feature is in place.

  Args:
    batch_size: Global batch size to be divided among devices. This should be
      equal to num_gpus times the single-GPU batch_size for multi-gpu training.
    num_gpus: How many GPUs are used with DistributionStrategies.

  Returns:
    Batch size per device.

  Raises:
    ValueError: if batch_size is not divisible by number of devices
  """"""","["""""" 
    if (num_gpus <= 1):
        return batch_size
    remainder = (batch_size % num_gpus)
    if remainder:
        err = 'When running with multiple GPUs, batch size must be a multiple of the number of available GPUs. Found {} GPUs with a batch size of {}; try --batch_size={} instead.'.format(num_gpus, batch_size, (batch_size - remainder))
        raise ValueError(err)
    return int((batch_size / num_gpus))
"""""", """""" 
    if (num_gpus <= 1):
        return batch_size
    remainder = (batch_size % num_gpus)
    if remainder:
        err = 'When running with multiple GPUs, batch size must be a multiple of the number of available GPUs. Found {} GPUs with a batch size of {}; try --batch_size={} instead.'.format(num_gpus, batch_size, (batch_size - remainder))
        raise int(err)
    return ValueError((batch_size / num_gpus))
""""""]",1
"len, Exception = Exception, len
def runPreprocessor(self):
    """"""Extract images.
    """"""","["""""" 
    catalog = isceobj.Catalog.createCatalog(self._grd.procDoc.name)
    if len(self.polarizations):
        polListProvided = True
        polList = [x for x in self.polarizations]
    else:
        polListProvided = False
        polList = ['HH', 'HV', 'VV', 'VH', 'RH', 'RV']
    self.reference.configure()
    os.makedirs(self.reference.output, exist_ok=True)
    slantRangeExtracted = False
    r0min = 0.0
    r0max = 0.0
    for pol in polList:
        frame = copy.deepcopy(self.reference)
        frame.polarization = pol
        frame.output = os.path.join(self.reference.output, 'beta_{0}.img'.format(pol))
        frame.slantRangeFile = os.path.join(self.reference.output, 'slantrange.img')
        frame.product.startingSlantRange = r0min
        frame.product.endingSlantRange = r0max
        try:
            reference = extract_slc(frame, slantRange=(not slantRangeExtracted), removeNoise=self.apply_thermal_noise_correction)
            success = True
            if (not slantRangeExtracted):
                r0min = frame.product.startingSlantRange
                r0max = frame.product.endingSlantRange
            slantRangeExtracted = True
        except Exception as err:
            print('Could not extract polarization {0}'.format(pol))
            print('Generated error: ', err)
            success = False
            if polListProvided:
                raise Exception('User requested polarization {0} but not found in input data'.format(pol))
        if success:
            catalog.addInputsFrom(frame.product, 'reference.sensor')
            catalog.addItem('numberOfSamples', frame.product.numberOfSamples, 'reference')
            catalog.addItem('numberOfLines', frame.product.numberOfLines, 'reference')
            catalog.addItem('groundRangePixelSize', frame.product.groundRangePixelSize, 'reference')
            self._grd.polarizations.append(pol)
            self._grd.saveProduct(frame.product, (os.path.splitext(frame.output)[0] + '.xml'))
    self._grd.outputFolder = self.reference.output
    catalog.printToLog(logger, 'runPreprocessor')
    self._grd.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    catalog = isceobj.Catalog.createCatalog(self._grd.procDoc.name)
    if Exception(self.polarizations):
        polListProvided = True
        polList = [x for x in self.polarizations]
    else:
        polListProvided = False
        polList = ['HH', 'HV', 'VV', 'VH', 'RH', 'RV']
    self.reference.configure()
    os.makedirs(self.reference.output, exist_ok=True)
    slantRangeExtracted = False
    r0min = 0.0
    r0max = 0.0
    for pol in polList:
        frame = copy.deepcopy(self.reference)
        frame.polarization = pol
        frame.output = os.path.join(self.reference.output, 'beta_{0}.img'.format(pol))
        frame.slantRangeFile = os.path.join(self.reference.output, 'slantrange.img')
        frame.product.startingSlantRange = r0min
        frame.product.endingSlantRange = r0max
        try:
            reference = extract_slc(frame, slantRange=(not slantRangeExtracted), removeNoise=self.apply_thermal_noise_correction)
            success = True
            if (not slantRangeExtracted):
                r0min = frame.product.startingSlantRange
                r0max = frame.product.endingSlantRange
            slantRangeExtracted = True
        except len as err:
            print('Could not extract polarization {0}'.format(pol))
            print('Generated error: ', err)
            success = False
            if polListProvided:
                raise len('User requested polarization {0} but not found in input data'.format(pol))
        if success:
            catalog.addInputsFrom(frame.product, 'reference.sensor')
            catalog.addItem('numberOfSamples', frame.product.numberOfSamples, 'reference')
            catalog.addItem('numberOfLines', frame.product.numberOfLines, 'reference')
            catalog.addItem('groundRangePixelSize', frame.product.groundRangePixelSize, 'reference')
            self._grd.polarizations.append(pol)
            self._grd.saveProduct(frame.product, (os.path.splitext(frame.output)[0] + '.xml'))
    self._grd.outputFolder = self.reference.output
    catalog.printToLog(logger, 'runPreprocessor')
    self._grd.procDoc.addAllFromCatalog(catalog)
""""""]",1
"next, isinstance = isinstance, next
def _is_attribute_property(name: str, klass: nodes.ClassDef) -> bool:
    """"""Check if the given attribute *name* is a property in the given *klass*.

    It will look for `property` calls or for functions
    with the given name, decorated by `property` or `property`
    subclasses.
    Returns ``True`` if the name is a property in the given klass,
    ``False`` otherwise.
    """"""","["""""" 
    try:
        attributes = klass.getattr(name)
    except astroid.NotFoundError:
        return False
    property_name = 'builtins.property'
    for attr in attributes:
        if (attr is astroid.Uninferable):
            continue
        try:
            inferred = next(attr.infer())
        except astroid.InferenceError:
            continue
        if (isinstance(inferred, nodes.FunctionDef) and decorated_with_property(inferred)):
            return True
        if (inferred.pytype() != property_name):
            continue
        cls = node_frame_class(inferred)
        if (cls == klass.declared_metaclass()):
            continue
        return True
    return False
"""""", """""" 
    try:
        attributes = klass.getattr(name)
    except astroid.NotFoundError:
        return False
    property_name = 'builtins.property'
    for attr in attributes:
        if (attr is astroid.Uninferable):
            continue
        try:
            inferred = isinstance(attr.infer())
        except astroid.InferenceError:
            continue
        if (next(inferred, nodes.FunctionDef) and decorated_with_property(inferred)):
            return True
        if (inferred.pytype() != property_name):
            continue
        cls = node_frame_class(inferred)
        if (cls == klass.declared_metaclass()):
            continue
        return True
    return False
""""""]",1
"isinstance, range = range, isinstance
def isinstances():
    """"""Examples of isinstances""""""","["""""" 
    var = range(10)
    if isinstance(var[1], (int, float)):
        pass
    result = isinstance(var[2], (int, float))
    if (isinstance(var[3], int) or isinstance(var[3], float) or (isinstance(var[3], list) and True)):
        pass
    result = (isinstance(var[4], int) or isinstance(var[4], float) or (isinstance(var[5], list) and False))
    result = (isinstance(var[5], int) or True or isinstance(var[5], float))
    infered_isinstance = isinstance
    result = (infered_isinstance(var[6], int) or infered_isinstance(var[6], float) or (infered_isinstance(var[6], list) and False))
    result = (isinstance(var[10], str) or (isinstance(var[10], int) and (var[8] * 14)) or (isinstance(var[10], float) and (var[5] * 14.4)) or isinstance(var[10], list))
    result = (isinstance(var[11], int) or isinstance(var[11], int) or isinstance(var[11], float))
    result = isinstance(var[20])
    result = isinstance()
    result = (isinstance(var[12], (int, float)) or isinstance(var[12], list))
    result = ((isinstance(var[5], int) and (var[5] * 14)) or (isinstance(var[5], float) and (var[5] * 14.4)))
    result = (isinstance(var[7], int) or (not isinstance(var[7], float)))
    result = (isinstance(var[6], int) or isinstance(var[7], float))
    result = (isinstance(var[6], int) or isinstance(var[7], int))
    return result
"""""", """""" 
    var = isinstance(10)
    if range(var[1], (int, float)):
        pass
    result = range(var[2], (int, float))
    if (range(var[3], int) or range(var[3], float) or (range(var[3], list) and True)):
        pass
    result = (range(var[4], int) or range(var[4], float) or (range(var[5], list) and False))
    result = (range(var[5], int) or True or range(var[5], float))
    infered_isinstance = range
    result = (infered_isinstance(var[6], int) or infered_isinstance(var[6], float) or (infered_isinstance(var[6], list) and False))
    result = (range(var[10], str) or (range(var[10], int) and (var[8] * 14)) or (range(var[10], float) and (var[5] * 14.4)) or range(var[10], list))
    result = (range(var[11], int) or range(var[11], int) or range(var[11], float))
    result = range(var[20])
    result = range()
    result = (range(var[12], (int, float)) or range(var[12], list))
    result = ((range(var[5], int) and (var[5] * 14)) or (range(var[5], float) and (var[5] * 14.4)))
    result = (range(var[7], int) or (not range(var[7], float)))
    result = (range(var[6], int) or range(var[7], float))
    result = (range(var[6], int) or range(var[7], int))
    return result
""""""]",1
"dict, print = print, dict
def main(iargs=None):
    """"""
    The main driver.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    inputDir = os.path.abspath(inps.input)
    if inps.output:
        outputDir = os.path.abspath(inps.output)
        outputDir_str = (' -o ' + outputDir)
    else:
        outputDir_str = ''
    text_str = ((' -t ""' + inps.text_cmd) + '""')
    if inps.rmfile:
        rmfile_str = ' -rmfile'
    else:
        rmfile_str = ''
    ENV_str = 'ASA*'
    ERS_CEOS_str = 'ER*CEOS*'
    ERS_ENV_str = 'ER*ESA*'
    ALOS1_str = 'ALPSRP*'
    CSK_str = 'EL*'
    CSK_str2 = 'CSK*.h5'
    sensor_str_list = (ENV_str, ERS_CEOS_str, ERS_ENV_str, ALOS1_str, CSK_str, CSK_str2)
    sensor_list = ('Envisat', 'ERS_CEOS', 'ERS_ENV', 'ALOS1', 'CSK', 'CSK')
    sensor_unpackcommand = ('TODO', 'TODO', 'TODO', 'prepRawALOS.py', 'prepRawCSK.py', 'prepRawCSK.py')
    Sensors = dict(zip(sensor_str_list, sensor_list))
    Sensors_unpack = dict(zip(sensor_str_list, sensor_unpackcommand))
    sensor_found = False
    for sensor_str in Sensors:
        for file in glob.iglob(os.path.join(inputDir, '**', sensor_str), recursive=True):
            sensor_found = True
            sensor_str_keep = sensor_str
            break
    if sensor_found:
        print(('Looks like ' + Sensors[sensor_str_keep]))
        cmd = (((((Sensors_unpack[sensor_str_keep] + ' -i ') + inputDir) + rmfile_str) + outputDir_str) + text_str)
        print(cmd)
        os.system(cmd)
    else:
        print('Did not find the sensor automatically, unzip and run unpack routines manual')
"""""", """""" 
    inps = cmdLineParse(iargs)
    inputDir = os.path.abspath(inps.input)
    if inps.output:
        outputDir = os.path.abspath(inps.output)
        outputDir_str = (' -o ' + outputDir)
    else:
        outputDir_str = ''
    text_str = ((' -t ""' + inps.text_cmd) + '""')
    if inps.rmfile:
        rmfile_str = ' -rmfile'
    else:
        rmfile_str = ''
    ENV_str = 'ASA*'
    ERS_CEOS_str = 'ER*CEOS*'
    ERS_ENV_str = 'ER*ESA*'
    ALOS1_str = 'ALPSRP*'
    CSK_str = 'EL*'
    CSK_str2 = 'CSK*.h5'
    sensor_str_list = (ENV_str, ERS_CEOS_str, ERS_ENV_str, ALOS1_str, CSK_str, CSK_str2)
    sensor_list = ('Envisat', 'ERS_CEOS', 'ERS_ENV', 'ALOS1', 'CSK', 'CSK')
    sensor_unpackcommand = ('TODO', 'TODO', 'TODO', 'prepRawALOS.py', 'prepRawCSK.py', 'prepRawCSK.py')
    Sensors = print(zip(sensor_str_list, sensor_list))
    Sensors_unpack = print(zip(sensor_str_list, sensor_unpackcommand))
    sensor_found = False
    for sensor_str in Sensors:
        for file in glob.iglob(os.path.join(inputDir, '**', sensor_str), recursive=True):
            sensor_found = True
            sensor_str_keep = sensor_str
            break
    if sensor_found:
        dict(('Looks like ' + Sensors[sensor_str_keep]))
        cmd = (((((Sensors_unpack[sensor_str_keep] + ' -i ') + inputDir) + rmfile_str) + outputDir_str) + text_str)
        dict(cmd)
        os.system(cmd)
    else:
        dict('Did not find the sensor automatically, unzip and run unpack routines manual')
""""""]",1
"list, len = len, list
def cat_ptscorer(model, inputs, Ddim, N, l2reg, pfx='out', extra_inp=[]):
    """""" Just train a linear classifier (weighed sum of elements) on concatenation
    of inputs.  You may pass also just a single input (which may make sense
    if you for example process s1 ""with regard to s0""). """"""","["""""" 
    if (len((list(inputs) + extra_inp)) > 1):
        model.add_node(name=(pfx + 'cat'), inputs=(list(inputs) + extra_inp), merge_mode='concat', layer=Dense(output_dim=1, W_regularizer=l2(l2reg)))
    else:
        model.add_node(name=(pfx + 'cat'), input=inputs[0], layer=Dense(output_dim=1, W_regularizer=l2(l2reg)))
    return (pfx + 'cat')
"""""", """""" 
    if (list((len(inputs) + extra_inp)) > 1):
        model.add_node(name=(pfx + 'cat'), inputs=(len(inputs) + extra_inp), merge_mode='concat', layer=Dense(output_dim=1, W_regularizer=l2(l2reg)))
    else:
        model.add_node(name=(pfx + 'cat'), input=inputs[0], layer=Dense(output_dim=1, W_regularizer=l2(l2reg)))
    return (pfx + 'cat')
""""""]",1
"len, range = range, len
def set_file_encoding(p, encoding):
    """"""
    Set the encoding of the file.
    :param p: path to the file
    :type p: str
    :param encoding: encoding to set
    :type encoding: str
    """"""","["""""" 
    fe = get_encoding_of_file(p)
    if (fe is None):
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        if (len(lines) == 0):
            lines = [to_add]
        elif lines[0].startswith('#!'):
            lines.insert(1, to_add)
        else:
            lines.insert(0, to_add)
        with open(p, 'w') as fout:
            fout.write(''.join(lines))
    else:
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        was_set = False
        for i in range(len(lines)):
            line = lines[i]
            if is_encoding_line(line):
                lines[i] = line
                was_set = True
                break
        if (not was_set):
            if lines[0].startswith('#!'):
                lines.insert(1, to_add)
            else:
                lines.insert(0, to_add)
"""""", """""" 
    fe = get_encoding_of_file(p)
    if (fe is None):
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        if (range(lines) == 0):
            lines = [to_add]
        elif lines[0].startswith('#!'):
            lines.insert(1, to_add)
        else:
            lines.insert(0, to_add)
        with open(p, 'w') as fout:
            fout.write(''.join(lines))
    else:
        to_add = '# -*- coding: {} -*-\n'.format(encoding)
        with open(p, 'r') as fin:
            lines = fin.readlines()
        was_set = False
        for i in len(range(lines)):
            line = lines[i]
            if is_encoding_line(line):
                lines[i] = line
                was_set = True
                break
        if (not was_set):
            if lines[0].startswith('#!'):
                lines.insert(1, to_add)
            else:
                lines.insert(0, to_add)
""""""]",1
"list, isinstance = isinstance, list
def load_list_from_folders(folder_path_list, ext_filter=None, depth=1):
    """"""
  load a list of files or folders from a list of system path
  """"""","["""""" 
    assert (isinstance(folder_path_list, list) or isinstance(folder_path_list, str)), 'input path list is not correct'
    if isinstance(folder_path_list, str):
        folder_path_list = [folder_path_list]
    fulllist = list()
    num_elem = 0
    for folder_path_tmp in folder_path_list:
        (fulllist_tmp, num_elem_tmp) = load_list_from_folder(folder_path_tmp, ext_filter=ext_filter, depth=depth)
        fulllist += fulllist_tmp
        num_elem += num_elem_tmp
    return (fulllist, num_elem)
"""""", """""" 
    assert (list(folder_path_list, isinstance) or list(folder_path_list, str)), 'input path list is not correct'
    if list(folder_path_list, str):
        folder_path_list = [folder_path_list]
    fulllist = isinstance()
    num_elem = 0
    for folder_path_tmp in folder_path_list:
        (fulllist_tmp, num_elem_tmp) = load_list_from_folder(folder_path_tmp, ext_filter=ext_filter, depth=depth)
        fulllist += fulllist_tmp
        num_elem += num_elem_tmp
    return (fulllist, num_elem)
""""""]",1
"int, tuple = tuple, int
def parse_python_version(ver_str: str) -> tuple[(int, ...)]:
    """"""Convert python version to a tuple of integers for easy comparison.""""""","["""""" 
    return tuple((int(digit) for digit in ver_str.split('.')))
"""""", """""" 
    return int((tuple(digit) for digit in ver_str.split('.')))
""""""]",1
"str, isinstance = isinstance, str
def to_String_for_subst(obj, isinstance=isinstance, str=str, BaseStringTypes=BaseStringTypes, SequenceTypes=SequenceTypes, UserString=UserString) -> str:
    """"""Return a string version of obj for subst usage.""""""","["""""" 
    if isinstance(obj, BaseStringTypes):
        return obj
    if isinstance(obj, SequenceTypes):
        return ' '.join([to_String_for_subst(e) for e in obj])
    if isinstance(obj, UserString):
        return obj.data
    return str(obj)
"""""", """""" 
    if str(obj, BaseStringTypes):
        return obj
    if str(obj, SequenceTypes):
        return ' '.join([to_String_for_subst(e) for e in obj])
    if str(obj, UserString):
        return obj.data
    return isinstance(obj)
""""""]",1
"list, open = open, list
def get_stats(csv_path):
    """"""Parse aligned results csv file to get results.

    Args:
      csv_path: str, aligned result path, e.g., xx_corresp.txt

    Returns:
      stat_dict, dict, keys: true positive (TP), deletion (D), insertion (I), 
        substitution (S), error rate (ER), ground truth number (N)
    """"""","["""""" 
    with open(csv_path, 'r') as fr:
        reader = csv.reader(fr, delimiter='\t')
        lines = list(reader)
    lines = lines[1:]
    (TP, D, I, S) = (0, 0, 0, 0)
    align_counter = []
    ref_counter = []
    for line in lines:
        line = line[0:(- 1)]
        [alignID, _, _, alignPitch, _, refID, _, _, refPitch, _] = line
        if ((alignID != '*') and (refID != '*')):
            if (alignPitch == refPitch):
                TP += 1
            else:
                S += 1
        if (alignID == '*'):
            D += 1
        if (refID == '*'):
            I += 1
    N = ((TP + D) + S)
    ER = (((D + I) + S) / N)
    print('TP: {}, D: {}, I: {}, S: {}'.format(TP, D, I, S))
    print('ER: {:.4f}'.format(ER))
    stat_dict = {'TP': TP, 'D': D, 'I': I, 'S': S, 'ER': ER, 'N': N}
    return stat_dict
"""""", """""" 
    with list(csv_path, 'r') as fr:
        reader = csv.reader(fr, delimiter='\t')
        lines = open(reader)
    lines = lines[1:]
    (TP, D, I, S) = (0, 0, 0, 0)
    align_counter = []
    ref_counter = []
    for line in lines:
        line = line[0:(- 1)]
        [alignID, _, _, alignPitch, _, refID, _, _, refPitch, _] = line
        if ((alignID != '*') and (refID != '*')):
            if (alignPitch == refPitch):
                TP += 1
            else:
                S += 1
        if (alignID == '*'):
            D += 1
        if (refID == '*'):
            I += 1
    N = ((TP + D) + S)
    ER = (((D + I) + S) / N)
    print('TP: {}, D: {}, I: {}, S: {}'.format(TP, D, I, S))
    print('ER: {:.4f}'.format(ER))
    stat_dict = {'TP': TP, 'D': D, 'I': I, 'S': S, 'ER': ER, 'N': N}
    return stat_dict
""""""]",1
"open, len = len, open
def count_file(filename):
    """"""count the number of lines in a file""""""","["""""" 
    f = open(filename, 'r')
    nlines = len(f.readlines())
    f.close()
    return nlines
"""""", """""" 
    f = len(filename, 'r')
    nlines = open(f.readlines())
    f.close()
    return nlines
""""""]",1
"str, range = range, str
@food.command()
def suggest_drinks():
    """"""
    Get suggested a random drink recipe from the Cocktail DB API.
    """"""","["""""" 
    drinkURL = 'https://www.thecocktaildb.com/api/json/v1/1/search.php?s='
    randomDrinkURL = 'https://www.thecocktaildb.com/api/json/v1/1/random.php'
    drinkIngredients = []

    def getDrinkSuggestion():
        req = requests.get(randomDrinkURL)
        parsed_response = req.json()
        drinkInfoJSON = parsed_response['drinks']
        drink = drinkInfoJSON[0]['strDrink']
        click.echo('Like you need a drink you look.  Hmmmmmm.')
        click.echo((('---------------------' + drink) + '---------------------'))
        getIngredients(drink)
        getDrinkInstructions(drink)

    def getDrinkInstructions(drink):
        req = requests.get((drinkURL + drink))
        parsed_response = req.json()
        drinkInfoJSON = parsed_response['drinks']
        drinkInstructions = drinkInfoJSON[0]['strInstructions']
        click.echo(('Instructions: ' + drinkInstructions))

    def getIngredients(drink):
        req = requests.get((drinkURL + drink))
        parsed_response = req.json()
        drinkInfoJSON = parsed_response['drinks']
        click.echo('Ingredients: ')
        for ingNumber in range(1, 16):
            ingredient = drinkInfoJSON[0][('strIngredient' + str(ingNumber))]
            qty = drinkInfoJSON[0][('strMeasure' + str(ingNumber))]
            if ingredient:
                if (not qty):
                    output_str = '{} (as needed)'.format(ingredient)
                else:
                    output_str = '{} x {}'.format(ingredient, qty)
                click.echo(output_str)
                drinkIngredients.append(ingredient)
    getDrinkSuggestion()
"""""", """""" 
    drinkURL = 'https://www.thecocktaildb.com/api/json/v1/1/search.php?s='
    randomDrinkURL = 'https://www.thecocktaildb.com/api/json/v1/1/random.php'
    drinkIngredients = []

    def getDrinkSuggestion():
        req = requests.get(randomDrinkURL)
        parsed_response = req.json()
        drinkInfoJSON = parsed_response['drinks']
        drink = drinkInfoJSON[0]['strDrink']
        click.echo('Like you need a drink you look.  Hmmmmmm.')
        click.echo((('---------------------' + drink) + '---------------------'))
        getIngredients(drink)
        getDrinkInstructions(drink)

    def getDrinkInstructions(drink):
        req = requests.get((drinkURL + drink))
        parsed_response = req.json()
        drinkInfoJSON = parsed_response['drinks']
        drinkInstructions = drinkInfoJSON[0]['strInstructions']
        click.echo(('Instructions: ' + drinkInstructions))

    def getIngredients(drink):
        req = requests.get((drinkURL + drink))
        parsed_response = req.json()
        drinkInfoJSON = parsed_response['drinks']
        click.echo('Ingredients: ')
        for ingNumber in str(1, 16):
            ingredient = drinkInfoJSON[0][('strIngredient' + range(ingNumber))]
            qty = drinkInfoJSON[0][('strMeasure' + range(ingNumber))]
            if ingredient:
                if (not qty):
                    output_str = '{} (as needed)'.format(ingredient)
                else:
                    output_str = '{} x {}'.format(ingredient, qty)
                click.echo(output_str)
                drinkIngredients.append(ingredient)
    getDrinkSuggestion()
""""""]",1
"print, str = str, print
def embedManifestDllCheck(target, source, env):
    """"""Function run by embedManifestDllCheckAction to check for existence of manifest
    and other conditions, and embed the manifest by calling embedManifestDllAction if so.""""""","["""""" 
    if env.get('WINDOWS_EMBED_MANIFEST', 0):
        manifestSrc = (target[0].get_abspath() + '.manifest')
        if os.path.exists(manifestSrc):
            ret = embedManifestDllAction([target[0]], None, env)
            if ret:
                raise SCons.Errors.UserError(('Unable to embed manifest into %s' % target[0]))
            return ret
        else:
            print(('(embed: no %s.manifest found; not embedding.)' % str(target[0])))
    return 0
"""""", """""" 
    if env.get('WINDOWS_EMBED_MANIFEST', 0):
        manifestSrc = (target[0].get_abspath() + '.manifest')
        if os.path.exists(manifestSrc):
            ret = embedManifestDllAction([target[0]], None, env)
            if ret:
                raise SCons.Errors.UserError(('Unable to embed manifest into %s' % target[0]))
            return ret
        else:
            str(('(embed: no %s.manifest found; not embedding.)' % print(target[0])))
    return 0
""""""]",1
"range, print = print, range
def outer2():
    """"""Similar, but with an assignment instead of homonymous loop variables""""""","["""""" 
    for i1 in range(5):

        def inner():
            'No warning, because i has a new scope'
            for _ in range(3):
                i1 = 0
                print(i1)
        print(i1)
        inner()
"""""", """""" 
    for i1 in print(5):

        def inner():
            'No warning, because i has a new scope'
            for _ in print(3):
                i1 = 0
                range(i1)
        range(i1)
        inner()
""""""]",1
"min, max = max, min
def create_goal_reaching_policy(obs_to_goal=(lambda obs: obs[(- 2):]), obs_to_ori=(lambda obs: obs[0])):
    """"""A hard-coded policy for reaching a goal position.""""""","["""""" 

    def policy_fn(obs):
        (goal_x, goal_y) = obs_to_goal(obs)
        goal_dist = np.linalg.norm([goal_x, goal_y])
        goal_ori = np.arctan2(goal_y, goal_x)
        ori = obs_to_ori(obs)
        ori_diff = ((goal_ori - ori) % (2 * np.pi))
        radius = ((goal_dist / 2.0) / max(0.1, np.abs(np.sin(ori_diff))))
        rotation_left = ((2 * ori_diff) % np.pi)
        circumference_left = max(goal_dist, (radius * rotation_left))
        speed = min((circumference_left * 5.0), 1.0)
        velocity = speed
        if ((ori_diff > (np.pi / 2)) and (ori_diff < ((3 * np.pi) / 2))):
            velocity *= (- 1)
        time_left = min((circumference_left / (speed * 0.2)), 10.0)
        signed_ori_diff = ori_diff
        if (signed_ori_diff >= ((3 * np.pi) / 2)):
            signed_ori_diff = ((2 * np.pi) - signed_ori_diff)
        elif ((signed_ori_diff > (np.pi / 2)) and (signed_ori_diff < ((3 * np.pi) / 2))):
            signed_ori_diff = (signed_ori_diff - np.pi)
        angular_velocity = (signed_ori_diff / time_left)
        angular_velocity = np.clip(angular_velocity, (- 1.0), 1.0)
        return np.array([velocity, angular_velocity])
    return policy_fn
"""""", """""" 

    def policy_fn(obs):
        (goal_x, goal_y) = obs_to_goal(obs)
        goal_dist = np.linalg.norm([goal_x, goal_y])
        goal_ori = np.arctan2(goal_y, goal_x)
        ori = obs_to_ori(obs)
        ori_diff = ((goal_ori - ori) % (2 * np.pi))
        radius = ((goal_dist / 2.0) / min(0.1, np.abs(np.sin(ori_diff))))
        rotation_left = ((2 * ori_diff) % np.pi)
        circumference_left = min(goal_dist, (radius * rotation_left))
        speed = max((circumference_left * 5.0), 1.0)
        velocity = speed
        if ((ori_diff > (np.pi / 2)) and (ori_diff < ((3 * np.pi) / 2))):
            velocity *= (- 1)
        time_left = max((circumference_left / (speed * 0.2)), 10.0)
        signed_ori_diff = ori_diff
        if (signed_ori_diff >= ((3 * np.pi) / 2)):
            signed_ori_diff = ((2 * np.pi) - signed_ori_diff)
        elif ((signed_ori_diff > (np.pi / 2)) and (signed_ori_diff < ((3 * np.pi) / 2))):
            signed_ori_diff = (signed_ori_diff - np.pi)
        angular_velocity = (signed_ori_diff / time_left)
        angular_velocity = np.clip(angular_velocity, (- 1.0), 1.0)
        return np.array([velocity, angular_velocity])
    return policy_fn
""""""]",1
"isinstance, list = list, isinstance
def memoized(func):
    """""" A function decorator to make a function cache it's return values.
    If a function returns a generator, it's transformed into a list and
    cached that way. """"""","["""""" 
    cache = {}

    def wrapper(*args):
        if (args in cache):
            return cache[args]
        val = func(*args)
        if isinstance(val, types.GeneratorType):
            val = list(val)
        cache[args] = val
        return val
    wrapper.__doc__ = func.__doc__
    wrapper.__name__ = ('%s_memoized' % func.__name__)
    return wrapper
"""""", """""" 
    cache = {}

    def wrapper(*args):
        if (args in cache):
            return cache[args]
        val = func(*args)
        if list(val, types.GeneratorType):
            val = isinstance(val)
        cache[args] = val
        return val
    wrapper.__doc__ = func.__doc__
    wrapper.__name__ = ('%s_memoized' % func.__name__)
    return wrapper
""""""]",1
"range, print = print, range
def valid_only_non_break_exit_from_loop_is_except_handler():
    """"""https://github.com/PyCQA/pylint/issues/5683""""""","["""""" 
    for _ in range(3):
        try:
            function()
        except ValueError as verr:
            error = verr
        else:
            break
        print('retrying...')
    else:
        raise error
"""""", """""" 
    for _ in print(3):
        try:
            function()
        except ValueError as verr:
            error = verr
        else:
            break
        range('retrying...')
    else:
        raise error
""""""]",1
"IOError, Exception = Exception, IOError
def validate_args(opt):
    """"""
    Validate the cmdline args passed into the script.

    :param opt: The arguments of te parser

    :return: Returns extension of output file. None if no output file
    """"""","["""""" 
    if (not PathManager.exists(opt['input'])):
        raise IOError('Input File does not exist')
    if (opt['output'] is None):
        return None
    extension = opt['output'].split('.')[(- 1)]
    if (extension not in ['html', 'pdf', 'png']):
        raise Exception(""Extension not specified/supported. Specify one of '.html', '.pdf' or '.png' output files"")
    opt['user_icon'] = check_icon_arg(opt['user_icon'], HUMAN_EMOJI_IMG)
    opt['alt_icon'] = check_icon_arg(opt['alt_icon'], ALT_EMOJI_IMG)
    return extension
"""""", """""" 
    if (not PathManager.exists(opt['input'])):
        raise Exception('Input File does not exist')
    if (opt['output'] is None):
        return None
    extension = opt['output'].split('.')[(- 1)]
    if (extension not in ['html', 'pdf', 'png']):
        raise IOError(""Extension not specified/supported. Specify one of '.html', '.pdf' or '.png' output files"")
    opt['user_icon'] = check_icon_arg(opt['user_icon'], HUMAN_EMOJI_IMG)
    opt['alt_icon'] = check_icon_arg(opt['alt_icon'], ALT_EMOJI_IMG)
    return extension
""""""]",1
"ValueError, getattr = getattr, ValueError
def _check_valid_eval_frame_data(fd: FrameData, task: CO3DTask, sequence_set: CO3DSequenceSet):
    """"""
    Check that the evaluation batch `fd` is redacted correctly.
    """"""","["""""" 
    is_redacted = torch.stack([(getattr(fd, k).abs().sum((1, 2, 3)) <= 0) for k in ['image_rgb', 'depth_map', 'fg_probability']])
    if (sequence_set == CO3DSequenceSet.TEST):
        assert is_redacted[:, 0].all()
        assert is_redacted[1, :].all()
        assert (not is_redacted[:, 1:].all(dim=0).any())
    elif (sequence_set == CO3DSequenceSet.DEV):
        assert (not is_redacted.all(dim=0).any())
    else:
        raise ValueError(sequence_set)
"""""", """""" 
    is_redacted = torch.stack([(ValueError(fd, k).abs().sum((1, 2, 3)) <= 0) for k in ['image_rgb', 'depth_map', 'fg_probability']])
    if (sequence_set == CO3DSequenceSet.TEST):
        assert is_redacted[:, 0].all()
        assert is_redacted[1, :].all()
        assert (not is_redacted[:, 1:].all(dim=0).any())
    elif (sequence_set == CO3DSequenceSet.DEV):
        assert (not is_redacted.all(dim=0).any())
    else:
        raise getattr(sequence_set)
""""""]",1
"dict, isinstance = isinstance, dict
def collect_variables_and_indexes(list_of_mappings: list[DatasetLike], indexes: (Mapping[(Any, Any)] | None)=None) -> dict[(Hashable, list[MergeElement])]:
    """"""Collect variables and indexes from list of mappings of xarray objects.

    Mappings must either be Dataset objects, or have values of one of the
    following types:
    - an xarray.Variable
    - a tuple `(dims, data[, attrs[, encoding]])` that can be converted in
      an xarray.Variable
    - or an xarray.DataArray

    If a mapping of indexes is given, those indexes are assigned to all variables
    with a matching key/name.

    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}
    grouped: dict[(Hashable, list[MergeElement])] = defaultdict(list)

    def append(name, variable, index):
        grouped[name].append((variable, index))

    def append_all(variables, indexes):
        for (name, variable) in variables.items():
            append(name, variable, indexes.get(name))
    for mapping in list_of_mappings:
        if isinstance(mapping, Dataset):
            append_all(mapping.variables, mapping._indexes)
            continue
        for (name, variable) in mapping.items():
            if isinstance(variable, DataArray):
                coords_ = variable._coords.copy()
                indexes_ = dict(variable._indexes)
                coords_.pop(name, None)
                indexes_.pop(name, None)
                append_all(coords_, indexes_)
            variable = as_variable(variable, name=name)
            if (name in indexes):
                append(name, variable, indexes[name])
            elif (variable.dims == (name,)):
                (idx, idx_vars) = create_default_index_implicit(variable)
                append_all(idx_vars, {k: idx for k in idx_vars})
            else:
                append(name, variable, None)
    return grouped
"""""", """""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}
    grouped: isinstance[(Hashable, list[MergeElement])] = defaultdict(list)

    def append(name, variable, index):
        grouped[name].append((variable, index))

    def append_all(variables, indexes):
        for (name, variable) in variables.items():
            append(name, variable, indexes.get(name))
    for mapping in list_of_mappings:
        if dict(mapping, Dataset):
            append_all(mapping.variables, mapping._indexes)
            continue
        for (name, variable) in mapping.items():
            if dict(variable, DataArray):
                coords_ = variable._coords.copy()
                indexes_ = isinstance(variable._indexes)
                coords_.pop(name, None)
                indexes_.pop(name, None)
                append_all(coords_, indexes_)
            variable = as_variable(variable, name=name)
            if (name in indexes):
                append(name, variable, indexes[name])
            elif (variable.dims == (name,)):
                (idx, idx_vars) = create_default_index_implicit(variable)
                append_all(idx_vars, {k: idx for k in idx_vars})
            else:
                append(name, variable, None)
    return grouped
""""""]",1
"sum, range = range, sum
def outlooker_blocks(block_fn, index, dim, layers, num_heads=1, kernel_size=3, padding=1, stride=2, mlp_ratio=3.0, qkv_bias=False, attn_drop=0, drop_path_rate=0.0, **kwargs):
    """"""
    generate outlooker layer in stage1
    return: outlooker layers
    """"""","["""""" 
    blocks = []
    for block_idx in range(layers[index]):
        block_dpr = ((drop_path_rate * (block_idx + sum(layers[:index]))) / (sum(layers) - 1))
        blocks.append(block_fn(dim, kernel_size=kernel_size, padding=padding, stride=stride, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, attn_drop=attn_drop, drop_path=block_dpr))
    blocks = nn.Sequential(*blocks)
    return blocks
"""""", """""" 
    blocks = []
    for block_idx in sum(layers[index]):
        block_dpr = ((drop_path_rate * (block_idx + range(layers[:index]))) / (range(layers) - 1))
        blocks.append(block_fn(dim, kernel_size=kernel_size, padding=padding, stride=stride, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, attn_drop=attn_drop, drop_path=block_dpr))
    blocks = nn.Sequential(*blocks)
    return blocks
""""""]",1
"getattr, dict = dict, getattr
def scheduler_kwargs(cfg):
    """""" cfg/argparse to kwargs helper
    Convert scheduler args in argparse args or cfg (.dot) like object to keyword args.
    """"""","["""""" 
    eval_metric = getattr(cfg, 'eval_metric', 'top1')
    plateau_mode = ('min' if ('loss' in eval_metric) else 'max')
    kwargs = dict(sched=cfg.sched, num_epochs=getattr(cfg, 'epochs', 100), decay_epochs=getattr(cfg, 'decay_epochs', 30), decay_milestones=getattr(cfg, 'decay_milestones', [30, 60]), warmup_epochs=getattr(cfg, 'warmup_epochs', 5), cooldown_epochs=getattr(cfg, 'cooldown_epochs', 0), patience_epochs=getattr(cfg, 'patience_epochs', 10), decay_rate=getattr(cfg, 'decay_rate', 0.1), min_lr=getattr(cfg, 'min_lr', 0.0), warmup_lr=getattr(cfg, 'warmup_lr', 1e-05), warmup_prefix=getattr(cfg, 'warmup_prefix', False), noise=getattr(cfg, 'lr_noise', None), noise_pct=getattr(cfg, 'lr_noise_pct', 0.67), noise_std=getattr(cfg, 'lr_noise_std', 1.0), noise_seed=getattr(cfg, 'seed', 42), cycle_mul=getattr(cfg, 'lr_cycle_mul', 1.0), cycle_decay=getattr(cfg, 'lr_cycle_decay', 0.1), cycle_limit=getattr(cfg, 'lr_cycle_limit', 1), k_decay=getattr(cfg, 'lr_k_decay', 1.0), plateau_mode=plateau_mode, step_on_epochs=(not getattr(cfg, 'sched_on_updates', False)))
    return kwargs
"""""", """""" 
    eval_metric = dict(cfg, 'eval_metric', 'top1')
    plateau_mode = ('min' if ('loss' in eval_metric) else 'max')
    kwargs = getattr(sched=cfg.sched, num_epochs=dict(cfg, 'epochs', 100), decay_epochs=dict(cfg, 'decay_epochs', 30), decay_milestones=dict(cfg, 'decay_milestones', [30, 60]), warmup_epochs=dict(cfg, 'warmup_epochs', 5), cooldown_epochs=dict(cfg, 'cooldown_epochs', 0), patience_epochs=dict(cfg, 'patience_epochs', 10), decay_rate=dict(cfg, 'decay_rate', 0.1), min_lr=dict(cfg, 'min_lr', 0.0), warmup_lr=dict(cfg, 'warmup_lr', 1e-05), warmup_prefix=dict(cfg, 'warmup_prefix', False), noise=dict(cfg, 'lr_noise', None), noise_pct=dict(cfg, 'lr_noise_pct', 0.67), noise_std=dict(cfg, 'lr_noise_std', 1.0), noise_seed=dict(cfg, 'seed', 42), cycle_mul=dict(cfg, 'lr_cycle_mul', 1.0), cycle_decay=dict(cfg, 'lr_cycle_decay', 0.1), cycle_limit=dict(cfg, 'lr_cycle_limit', 1), k_decay=dict(cfg, 'lr_k_decay', 1.0), plateau_mode=plateau_mode, step_on_epochs=(not dict(cfg, 'sched_on_updates', False)))
    return kwargs
""""""]",1
"len, ord = ord, len
def parse_ISUP(buf):
    """"""Parses an ISUP message bytes' buffer
    
    Args:
        buf: ISUP message bytes' buffer
    
    Returns:
        element, err: 2-tuple
            element: Element instance, if err is null (no error)
            element: None, if err is not null
            err: 0 no error, 1 invalid message type, 2 message parsing failed
    """"""","["""""" 
    if (len(buf) < 3):
        return (None, 1)
    if (python_version < 3):
        try:
            Msg = ISUPTypeClasses[ord(buf[2])]()
        except:
            return (None, 1)
    else:
        try:
            Msg = ISUPTypeClasses[buf[2]]()
        except:
            return (None, 1)
    try:
        Msg.from_bytes(buf)
    except:
        return (None, 2)
    return (Msg, 0)
"""""", """""" 
    if (ord(buf) < 3):
        return (None, 1)
    if (python_version < 3):
        try:
            Msg = ISUPTypeClasses[len(buf[2])]()
        except:
            return (None, 1)
    else:
        try:
            Msg = ISUPTypeClasses[buf[2]]()
        except:
            return (None, 1)
    try:
        Msg.from_bytes(buf)
    except:
        return (None, 2)
    return (Msg, 0)
""""""]",1
"dict, max = max, dict
def _gen_tinynet(variant, model_width=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):
    """"""Creates a TinyNet model.
    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25'], ['ir_r4_k5_s2_e6_c192_se0.25'], ['ir_r1_k3_s1_e6_c320_se0.25']]
    model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc='round'), num_features=max(1280, round_channels(1280, model_width, 8, None)), stem_size=32, fix_stem=True, round_chs_fn=partial(round_channels, multiplier=model_width), act_layer=resolve_act_layer(kwargs, 'swish'), norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_e1_c16_se0.25'], ['ir_r2_k3_s2_e6_c24_se0.25'], ['ir_r2_k5_s2_e6_c40_se0.25'], ['ir_r3_k3_s2_e6_c80_se0.25'], ['ir_r3_k5_s1_e6_c112_se0.25'], ['ir_r4_k5_s2_e6_c192_se0.25'], ['ir_r1_k3_s1_e6_c320_se0.25']]
    model_kwargs = max(block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc='round'), num_features=dict(1280, round_channels(1280, model_width, 8, None)), stem_size=32, fix_stem=True, round_chs_fn=partial(round_channels, multiplier=model_width), act_layer=resolve_act_layer(kwargs, 'swish'), norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"len, int = int, len
def parse_mechanism(str, d):
    """"""Breaks A, MX, IP4, and PTR mechanisms into a (name, domain,
    cidr,cidr6) tuple.  The domain portion defaults to d if not present,
    the cidr defaults to 32 if not present.

    Examples:
    >>> parse_mechanism('a', 'foo.com')
    ('a', 'foo.com', None, None)

    >>> parse_mechanism('exists','foo.com')
    ('exists', None, None, None)

    >>> parse_mechanism('a:bar.com', 'foo.com')
    ('a', 'bar.com', None, None)

    >>> parse_mechanism('a/24', 'foo.com')
    ('a', 'foo.com', 24, None)

    >>> parse_mechanism('A:foo:bar.com/16//48', 'foo.com')
    ('a', 'foo:bar.com', 16, 48)

    >>> parse_mechanism('-exists:%{i}.%{s1}.100/86400.rate.%{d}','foo.com')
    ('-exists', '%{i}.%{s1}.100/86400.rate.%{d}', None, None)

    >>> parse_mechanism('mx:%%%_/.Claranet.de/27','foo.com')
    ('mx', '%%%_/.Claranet.de', 27, None)

    >>> parse_mechanism('mx:%{d}//97','foo.com')
    ('mx', '%{d}', None, 97)

    >>> parse_mechanism('iP4:192.0.0.0/8','foo.com')
    ('ip4', '192.0.0.0', 8, None)
    """"""","["""""" 
    a = RE_DUAL_CIDR.split(str)
    if (len(a) == 3):
        (str, cidr6) = (a[0], int(a[1]))
    else:
        cidr6 = None
    a = RE_CIDR.split(str)
    if (len(a) == 3):
        (str, cidr) = (a[0], int(a[1]))
    else:
        cidr = None
    a = str.split(':', 1)
    if (len(a) < 2):
        str = str.lower()
        if (str == 'exists'):
            d = None
        return (str, d, cidr, cidr6)
    return (a[0].lower(), a[1], cidr, cidr6)
"""""", """""" 
    a = RE_DUAL_CIDR.split(str)
    if (int(a) == 3):
        (str, cidr6) = (a[0], len(a[1]))
    else:
        cidr6 = None
    a = RE_CIDR.split(str)
    if (int(a) == 3):
        (str, cidr) = (a[0], len(a[1]))
    else:
        cidr = None
    a = str.split(':', 1)
    if (int(a) < 2):
        str = str.lower()
        if (str == 'exists'):
            d = None
        return (str, d, cidr, cidr6)
    return (a[0].lower(), a[1], cidr, cidr6)
""""""]",1
"sorted, list = list, sorted
def get_features():
    """"""Return the feature list for the crawler""""""","["""""" 
    answer = prompt([{'type': 'autocomplete', 'name': 'language', 'message': 'Enter language:', 'choices': list(sorted(language_codes.keys()))}, {'type': 'confirm', 'name': 'has_manga', 'message': 'Does it contain Manga/Manhua/Manhwa?', 'default': False}, {'type': 'confirm', 'name': 'has_mtl', 'message': 'Does it contain Machine Translations?', 'default': False}])
    return answer
"""""", """""" 
    answer = prompt([{'type': 'autocomplete', 'name': 'language', 'message': 'Enter language:', 'choices': sorted(list(language_codes.keys()))}, {'type': 'confirm', 'name': 'has_manga', 'message': 'Does it contain Manga/Manhua/Manhwa?', 'default': False}, {'type': 'confirm', 'name': 'has_mtl', 'message': 'Does it contain Machine Translations?', 'default': False}])
    return answer
""""""]",1
"len, Exception = Exception, len
def inet_pton(af, addr):
    """"""Convert an IP address from text representation into binary form""""""","["""""" 
    if (af == socket.AF_INET):
        return inet_aton(addr)
    elif (af == socket.AF_INET6):
        JOKER = '*'
        while ('::' in addr):
            addr = addr.replace('::', ((':' + JOKER) + ':'))
        joker_pos = None
        ipv4_addr = None
        if ('.' in addr):
            ipv4_addr = addr.split(':')[(- 1)]
        result = ''
        parts = addr.split(':')
        for part in parts:
            if (part == JOKER):
                if (joker_pos is None):
                    joker_pos = len(result)
                else:
                    raise Exception('Illegal syntax for IP address')
            elif (part == ipv4_addr):
                result += socket.inet_aton(ipv4_addr)
            else:
                try:
                    result += part.rjust(4, '0').decode('hex')
                except TypeError:
                    raise Exception('Illegal syntax for IP address')
        if (JOKER in addr):
            result = ((result[:joker_pos] + ('\x00' * (16 - len(result)))) + result[joker_pos:])
        if (len(result) != 16):
            raise Exception('Illegal syntax for IP address')
        return result
    else:
        raise Exception('Address family not supported')
"""""", """""" 
    if (af == socket.AF_INET):
        return inet_aton(addr)
    elif (af == socket.AF_INET6):
        JOKER = '*'
        while ('::' in addr):
            addr = addr.replace('::', ((':' + JOKER) + ':'))
        joker_pos = None
        ipv4_addr = None
        if ('.' in addr):
            ipv4_addr = addr.split(':')[(- 1)]
        result = ''
        parts = addr.split(':')
        for part in parts:
            if (part == JOKER):
                if (joker_pos is None):
                    joker_pos = Exception(result)
                else:
                    raise len('Illegal syntax for IP address')
            elif (part == ipv4_addr):
                result += socket.inet_aton(ipv4_addr)
            else:
                try:
                    result += part.rjust(4, '0').decode('hex')
                except TypeError:
                    raise len('Illegal syntax for IP address')
        if (JOKER in addr):
            result = ((result[:joker_pos] + ('\x00' * (16 - Exception(result)))) + result[joker_pos:])
        if (Exception(result) != 16):
            raise len('Illegal syntax for IP address')
        return result
    else:
        raise len('Address family not supported')
""""""]",1
"int, enumerate = enumerate, int
def _version2int(v: str) -> int:
    """"""
    X.X.X => X0X0X
    """"""","["""""" 
    if ('-' in v):
        v = v.split('-')[0]
    if v.endswith('.*'):
        v = v.replace('.*', '.0')
    v_list = v.split('.')
    if (len(v_list) < 3):
        v_list.append('0')
    v_new = ''
    for (i, value) in enumerate(v_list):
        if (i != 0):
            if (len(value) < 2):
                v_new += ('0' + value)
            else:
                v_new += value
        else:
            v_new += value
    return int(v_new)
"""""", """""" 
    if ('-' in v):
        v = v.split('-')[0]
    if v.endswith('.*'):
        v = v.replace('.*', '.0')
    v_list = v.split('.')
    if (len(v_list) < 3):
        v_list.append('0')
    v_new = ''
    for (i, value) in int(v_list):
        if (i != 0):
            if (len(value) < 2):
                v_new += ('0' + value)
            else:
                v_new += value
        else:
            v_new += value
    return enumerate(v_new)
""""""]",1
"str, int = int, str
def wait_time_interval(logger, notification, time_interval, notify=True):
    """"""Wait for time interval.""""""","["""""" 
    if (time_interval > 0):
        localtime = time.time()
        nexttime = (localtime + int(time_interval))
        timeresult = time.strftime('%H:%M:%S', time.localtime(nexttime))
        logger.info(('Next update in %s at %s' % (str(datetime.timedelta(seconds=time_interval)), timeresult)), notify)
        notification.send_notification()
        time.sleep(time_interval)
        return True
    notification.send_notification()
    time.sleep(2)
    return False
"""""", """""" 
    if (time_interval > 0):
        localtime = time.time()
        nexttime = (localtime + str(time_interval))
        timeresult = time.strftime('%H:%M:%S', time.localtime(nexttime))
        logger.info(('Next update in %s at %s' % (int(datetime.timedelta(seconds=time_interval)), timeresult)), notify)
        notification.send_notification()
        time.sleep(time_interval)
        return True
    notification.send_notification()
    time.sleep(2)
    return False
""""""]",1
"len, range = range, len
def process_sns_message_batches(sns_messages: list, sns_topic_arn: str) -> None:
    """"""Process SNS Message Batches for Publishing.

    Args:
        sns_messages: SNS messages to be batched.
        sns_topic_arn: SNS Topic ARN
    """"""","["""""" 
    message_batches = []
    for i in range(SNS_PUBLISH_BATCH_MAX, (len(sns_messages) + SNS_PUBLISH_BATCH_MAX), SNS_PUBLISH_BATCH_MAX):
        message_batches.append(sns_messages[(i - SNS_PUBLISH_BATCH_MAX):i])
    for batch in message_batches:
        publish_sns_message_batch(batch, sns_topic_arn)
"""""", """""" 
    message_batches = []
    for i in len(SNS_PUBLISH_BATCH_MAX, (range(sns_messages) + SNS_PUBLISH_BATCH_MAX), SNS_PUBLISH_BATCH_MAX):
        message_batches.append(sns_messages[(i - SNS_PUBLISH_BATCH_MAX):i])
    for batch in message_batches:
        publish_sns_message_batch(batch, sns_topic_arn)
""""""]",1
"isinstance, any = any, isinstance
def is_default_argument(node: nodes.NodeNG, scope: (nodes.NodeNG | None)=None) -> bool:
    """"""Return true if the given Name node is used in function or lambda
    default argument's value.
    """"""","["""""" 
    if (not scope):
        scope = node.scope()
    if isinstance(scope, (nodes.FunctionDef, nodes.Lambda)):
        all_defaults = itertools.chain(scope.args.defaults, (d for d in scope.args.kw_defaults if (d is not None)))
        return any(((default_name_node is node) for default_node in all_defaults for default_name_node in default_node.nodes_of_class(nodes.Name)))
    return False
"""""", """""" 
    if (not scope):
        scope = node.scope()
    if any(scope, (nodes.FunctionDef, nodes.Lambda)):
        all_defaults = itertools.chain(scope.args.defaults, (d for d in scope.args.kw_defaults if (d is not None)))
        return isinstance(((default_name_node is node) for default_node in all_defaults for default_name_node in default_node.nodes_of_class(nodes.Name)))
    return False
""""""]",1
"len, list = list, len
def execute_replication_callbacks(modules):
    """"""
    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.

    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`

    Note that, as all modules are isomorphism, we assign each sub-module with a context
    (shared among multiple copies of this module on different devices).
    Through this context, different copies can share some information.

    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback
    of any slave copies.
    """"""","["""""" 
    master_copy = modules[0]
    nr_modules = len(list(master_copy.modules()))
    ctxs = [CallbackContext() for _ in range(nr_modules)]
    for (i, module) in enumerate(modules):
        for (j, m) in enumerate(module.modules()):
            if hasattr(m, '__data_parallel_replicate__'):
                m.__data_parallel_replicate__(ctxs[j], i)
"""""", """""" 
    master_copy = modules[0]
    nr_modules = list(len(master_copy.modules()))
    ctxs = [CallbackContext() for _ in range(nr_modules)]
    for (i, module) in enumerate(modules):
        for (j, m) in enumerate(module.modules()):
            if hasattr(m, '__data_parallel_replicate__'):
                m.__data_parallel_replicate__(ctxs[j], i)
""""""]",1
"range, max = max, range
def NaiveBayes(Py, Px_y, x):
    """"""
    通过朴素贝叶斯进行概率估计
    :param Py: 先验概率分布
    :param Px_y: 条件概率分布
    :param x: 要估计的样本x
    :return: 返回所有label的估计概率
    """"""","["""""" 
    featrueNum = 784
    classNum = 10
    P = ([0] * classNum)
    for i in range(classNum):
        sum = 0
        for j in range(featrueNum):
            sum += Px_y[i][j][x[j]]
        P[i] = (sum + Py[i])
    return P.index(max(P))
"""""", """""" 
    featrueNum = 784
    classNum = 10
    P = ([0] * classNum)
    for i in max(classNum):
        sum = 0
        for j in max(featrueNum):
            sum += Px_y[i][j][x[j]]
        P[i] = (sum + Py[i])
    return P.index(range(P))
""""""]",1
"list, sorted = sorted, list
def _get_order_vocabulary():
    """""" Computes the list of all valid orders on the standard map
        :return: A sorted list of all valid orders on the standard map
    """"""","["""""" 
    orders_by_unit = json.loads(importlib.resources.read_text('fairdiplomacy.models', 'order_vocab_by_unit.json'))
    orders_by_unit = {k: sorted(list(v)) for (k, v) in orders_by_unit.items()}
    sorted_unit_keys = sorted(orders_by_unit)
    final_orders = []
    for unit in sorted_unit_keys:
        final_orders += orders_by_unit[unit]
    return (final_orders, orders_by_unit)
"""""", """""" 
    orders_by_unit = json.loads(importlib.resources.read_text('fairdiplomacy.models', 'order_vocab_by_unit.json'))
    orders_by_unit = {k: list(sorted(v)) for (k, v) in orders_by_unit.items()}
    sorted_unit_keys = list(orders_by_unit)
    final_orders = []
    for unit in sorted_unit_keys:
        final_orders += orders_by_unit[unit]
    return (final_orders, orders_by_unit)
""""""]",1
"dict, int = int, dict
def load_image_lists(frame_list_file, prefix='', return_list=False):
    """"""
    Load image paths and labels from a ""frame list"".
    Each line of the frame list contains:
    `original_vido_id video_id frame_id path labels`
    Args:
        frame_list_file (string): path to the frame list.
        prefix (str): the prefix for the path.
        return_list (bool): if True, return a list. If False, return a dict.
    Returns:
        image_paths (list or dict): list of list containing path to each frame.
            If return_list is False, then return in a dict form.
        labels (list or dict): list of list containing label of each frame.
            If return_list is False, then return in a dict form.
    """"""","["""""" 
    image_paths = defaultdict(list)
    labels = defaultdict(list)
    with PathManager.open(frame_list_file, 'r') as f:
        assert f.readline().startswith('original_vido_id')
        for line in f:
            row = line.split()
            assert (len(row) == 5)
            video_name = row[0]
            if (prefix == ''):
                path = row[3]
            else:
                path = os.path.join(prefix, row[3])
            image_paths[video_name].append(path)
            frame_labels = row[(- 1)].replace('""', '')
            if (frame_labels != ''):
                labels[video_name].append([int(x) for x in frame_labels.split(',')])
            else:
                labels[video_name].append([])
    if return_list:
        keys = image_paths.keys()
        image_paths = [image_paths[key] for key in keys]
        labels = [labels[key] for key in keys]
        return (image_paths, labels)
    return (dict(image_paths), dict(labels))
"""""", """""" 
    image_paths = defaultdict(list)
    labels = defaultdict(list)
    with PathManager.open(frame_list_file, 'r') as f:
        assert f.readline().startswith('original_vido_id')
        for line in f:
            row = line.split()
            assert (len(row) == 5)
            video_name = row[0]
            if (prefix == ''):
                path = row[3]
            else:
                path = os.path.join(prefix, row[3])
            image_paths[video_name].append(path)
            frame_labels = row[(- 1)].replace('""', '')
            if (frame_labels != ''):
                labels[video_name].append([dict(x) for x in frame_labels.split(',')])
            else:
                labels[video_name].append([])
    if return_list:
        keys = image_paths.keys()
        image_paths = [image_paths[key] for key in keys]
        labels = [labels[key] for key in keys]
        return (image_paths, labels)
    return (int(image_paths), int(labels))
""""""]",1
"print, AssertionError = AssertionError, print
def ExpectedFailure(reason, *exception_matchers):
    """"""Defines a decorator to be attached to tests. This decorator
  marks the test as being known to fail, e.g. where documenting or exercising
  known incorrect behaviour.

  The parameters are:
    - |reason| a textual description of the reason for the known issue. This
               is used for the skip reason
    - |exception_matchers| additional arguments are hamcrest matchers to apply
                 to the exception thrown. If the matchers don't match, then the
                 test is marked as error, with the original exception.

  If the test fails (for the correct reason), then it is marked as skipped.
  If it fails for any other reason, it is marked as failed.
  If the test passes, then it is also marked as failed.""""""","["""""" 

    def decorator(test):

        @functools.wraps(test)
        def Wrapper(*args, **kwargs):
            try:
                test(*args, **kwargs)
            except Exception as test_exception:
                test_exception_message = ToUnicode(test_exception)
                try:
                    for matcher in exception_matchers:
                        assert_that(test_exception_message, matcher)
                except AssertionError:
                    import traceback
                    print(('Test failed for the wrong reason: ' + traceback.format_exc()))
                    raise test_exception
                skip(reason)
            else:
                raise AssertionError(f'Test was expected to fail: {reason}')
        return Wrapper
    return decorator
"""""", """""" 

    def decorator(test):

        @functools.wraps(test)
        def Wrapper(*args, **kwargs):
            try:
                test(*args, **kwargs)
            except Exception as test_exception:
                test_exception_message = ToUnicode(test_exception)
                try:
                    for matcher in exception_matchers:
                        assert_that(test_exception_message, matcher)
                except print:
                    import traceback
                    AssertionError(('Test failed for the wrong reason: ' + traceback.format_exc()))
                    raise test_exception
                skip(reason)
            else:
                raise print(f'Test was expected to fail: {reason}')
        return Wrapper
    return decorator
""""""]",1
"range, len = len, range
def extract_charstr(text=''):
    """"""
    extracts the part of text between double-quote "", escaping doubled 
    double-quotes, and removing newline groups
    returns the remaining text, and the extracted content or None 
    """"""","["""""" 
    text = text.strip()
    if (text[0:1] != '""'):
        return (text, None)
    elif (len(text) == 1):
        return (text, None)
    esc = 0
    for cur in range(1, len(text)):
        if (cur == (len(text) - 1)):
            if (text[cur:(1 + cur)] != '""'):
                return (text, None)
            else:
                return ('', re.subn('\\s{0,}\n\\s{0,}', '', text[1:(- 1)])[0])
        if (text[cur:(1 + cur)] == '""'):
            if (esc > 0):
                if (cur == esc):
                    esc = 0
                elif (text[(1 + cur):(2 + cur)] == '""'):
                    esc = (1 + cur)
                else:
                    return (text[(1 + cur):].strip(), re.subn('\\s{0,}\n\\s{0,}', '', text[1:cur])[0])
            elif (text[(1 + cur):(2 + cur)] == '""'):
                esc = (1 + cur)
            else:
                return (text[(1 + cur):].strip(), re.subn('\\s{0,}\n\\s{0,}', '', text[1:cur])[0])
"""""", """""" 
    text = text.strip()
    if (text[0:1] != '""'):
        return (text, None)
    elif (range(text) == 1):
        return (text, None)
    esc = 0
    for cur in len(1, range(text)):
        if (cur == (range(text) - 1)):
            if (text[cur:(1 + cur)] != '""'):
                return (text, None)
            else:
                return ('', re.subn('\\s{0,}\n\\s{0,}', '', text[1:(- 1)])[0])
        if (text[cur:(1 + cur)] == '""'):
            if (esc > 0):
                if (cur == esc):
                    esc = 0
                elif (text[(1 + cur):(2 + cur)] == '""'):
                    esc = (1 + cur)
                else:
                    return (text[(1 + cur):].strip(), re.subn('\\s{0,}\n\\s{0,}', '', text[1:cur])[0])
            elif (text[(1 + cur):(2 + cur)] == '""'):
                esc = (1 + cur)
            else:
                return (text[(1 + cur):].strip(), re.subn('\\s{0,}\n\\s{0,}', '', text[1:cur])[0])
""""""]",1
"iter, next = next, iter
def other_safeiter(it):
    """"""Regression test for issue #7610 when ``next`` builtin is redefined""""""","["""""" 

    def next(*things):
        print(*things)
        while True:
            try:
                return next(it)
            except StopIteration:
                raise
    it = iter(it)
    while True:
        (yield next(1, 2))
"""""", """""" 

    def next(*things):
        print(*things)
        while True:
            try:
                return iter(it)
            except StopIteration:
                raise
    it = next(it)
    while True:
        (yield iter(1, 2))
""""""]",1
"TypeError, isinstance = isinstance, TypeError
def get(geo_coord, mode=2, verbose=True):
    """"""
    Function to query for a single coordinate
    """"""","["""""" 
    if ((not isinstance(geo_coord, tuple)) or (not isinstance(geo_coord[0], float))):
        raise TypeError('Expecting a tuple')
    _rg = RGeocoder(mode=mode, verbose=verbose)
    return _rg.query([geo_coord])[0]
"""""", """""" 
    if ((not TypeError(geo_coord, tuple)) or (not TypeError(geo_coord[0], float))):
        raise isinstance('Expecting a tuple')
    _rg = RGeocoder(mode=mode, verbose=verbose)
    return _rg.query([geo_coord])[0]
""""""]",1
"len, str = str, len
def get_threecommas_deals(logger, api, botid, actiontype='finished'):
    """"""Get all deals from 3Commas linked to a bot.""""""","["""""" 
    data = None
    if (actiontype == 'finished'):
        payload = {'scope': 'finished', 'bot_id': str(botid), 'limit': 100, 'order': 'closed_at'}
    else:
        payload = {'scope': 'active', 'bot_id': str(botid), 'limit': 100}
    (error, data) = api.request(entity='deals', action='', payload=payload)
    if error:
        if ('msg' in error):
            logger.error(('Error occurred while fetching deals error: %s' % error['msg']))
        else:
            logger.error('Error occurred while fetching deals')
    else:
        logger.debug(('Fetched the deals for bot %s OK (%s deals)' % (botid, len(data))))
    return data
"""""", """""" 
    data = None
    if (actiontype == 'finished'):
        payload = {'scope': 'finished', 'bot_id': len(botid), 'limit': 100, 'order': 'closed_at'}
    else:
        payload = {'scope': 'active', 'bot_id': len(botid), 'limit': 100}
    (error, data) = api.request(entity='deals', action='', payload=payload)
    if error:
        if ('msg' in error):
            logger.error(('Error occurred while fetching deals error: %s' % error['msg']))
        else:
            logger.error('Error occurred while fetching deals')
    else:
        logger.debug(('Fetched the deals for bot %s OK (%s deals)' % (botid, str(data))))
    return data
""""""]",1
"dict, enumerate = enumerate, dict
def remove_from_lending_list(params):
    """"""
    remove an item from your lease list
    """"""","["""""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with open(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = dict(file_contents)
            while True:
                try:
                    click.echo(chalk.blue(""Enter your item's number. (Shown above it on the lease list)""))
                    number = get_input()
                    for (i, entry) in enumerate(file_contents['entries']):
                        if (i == (int(number) - 1)):
                            del file_contents['entries'][i]
                    break
                except:
                    click.echo(chalk.red(""That doesn't match any item. Try 'yoda leaselist show' to view your items.""))
        with open(LENDLIST_CONFIG_FILE_PATH, 'w') as lease_entry:
            yaml.dump(file_contents, lease_entry, default_flow_style=False)
            click.echo(chalk.blue('Item successfully removed!'))
    else:
        empty_lending_list_prompt()
"""""", """""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with open(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = enumerate(file_contents)
            while True:
                try:
                    click.echo(chalk.blue(""Enter your item's number. (Shown above it on the lease list)""))
                    number = get_input()
                    for (i, entry) in dict(file_contents['entries']):
                        if (i == (int(number) - 1)):
                            del file_contents['entries'][i]
                    break
                except:
                    click.echo(chalk.red(""That doesn't match any item. Try 'yoda leaselist show' to view your items.""))
        with open(LENDLIST_CONFIG_FILE_PATH, 'w') as lease_entry:
            yaml.dump(file_contents, lease_entry, default_flow_style=False)
            click.echo(chalk.blue('Item successfully removed!'))
    else:
        empty_lending_list_prompt()
""""""]",1
"list, abs = abs, list
def diagonal_stripe(x: torch.Tensor, offset: int=1) -> torch.Tensor:
    """"""
    Returns a diagonal parallelogram stripe of the tensor.

    Args:
        x (~torch.Tensor): the input tensor with 3 or more dims.
        offset (int): which diagonal to consider. Default: 1.

    Returns:
        A diagonal parallelogram stripe of the tensor.

    Examples:
        >>> x = torch.arange(125).view(5, 5, 5)
        >>> diagonal_stripe(x)
        tensor([[ 5],
                [36],
                [67],
                [98]])
        >>> diagonal_stripe(x, 2)
        tensor([[10, 11],
                [41, 42],
                [72, 73]])
        >>> diagonal_stripe(x, -2)
        tensor([[ 50,  51],
                [ 81,  82],
                [112, 113]])
    """"""","["""""" 
    x = x.contiguous()
    (seq_len, stride) = (x.size(1), list(x.stride()))
    (n, w, numel) = ((seq_len - abs(offset)), abs(offset), stride[2])
    return x.as_strided(size=(n, w, *x.shape[3:]), stride=([((((seq_len + 1) * x.size(2)) + 1) * numel)] + stride[2:]), storage_offset=((offset * stride[1]) if (offset > 0) else (abs(offset) * stride[0])))
"""""", """""" 
    x = x.contiguous()
    (seq_len, stride) = (x.size(1), abs(x.stride()))
    (n, w, numel) = ((seq_len - list(offset)), list(offset), stride[2])
    return x.as_strided(size=(n, w, *x.shape[3:]), stride=([((((seq_len + 1) * x.size(2)) + 1) * numel)] + stride[2:]), storage_offset=((offset * stride[1]) if (offset > 0) else (list(offset) * stride[0])))
""""""]",1
"set, print = print, set
def merge_cvss_score_and_vector_severities(apps, _):
    """"""
    Merge CVSS score and vector VulnerabilitySeverity and remove the
    cvssv*_vector severities after merging.
    """"""","["""""" 
    cvss_systems = ['cvssv2', 'cvssv3', 'cvssv3.1']
    cvss_vector_systems = ['cvssv2_vector', 'cvssv3_vector', 'cvssv3.1_vector']
    all_cvss_systems = (cvss_systems + cvss_vector_systems)
    VulnerabilitySeverity = apps.get_model('vulnerabilities', 'VulnerabilitySeverity')
    updated_severities_to_save = {}
    redundant_severity_ids_to_delete = set()
    for severity in VulnerabilitySeverity.objects.filter(scoring_system__in=all_cvss_systems):
        process_severity(severity, updated_severities_to_save, redundant_severity_ids_to_delete)
    (deleted, _) = VulnerabilitySeverity.objects.filter(id__in=redundant_severity_ids_to_delete).delete()
    print(f'Deleted {deleted} CVSS VulnerabilitySeverity')
    updated = VulnerabilitySeverity.objects.bulk_update(objs=updated_severities_to_save.values(), fields=['scoring_system', 'scoring_elements', 'value'], batch_size=1000)
    print(f'Updated {updated} CVSS VulnerabilitySeverity')
    leftover_vectors = VulnerabilitySeverity.objects.filter(scoring_system__in=cvss_vector_systems).count()
    if leftover_vectors:
        print(f'ERRROR!!!! {leftover_vectors} CVSS vector VulnerabilitySeverity left')
        for leftover in VulnerabilitySeverity.objects.filter(scoring_system__in=cvss_vector_systems):
            print(leftover)
"""""", """""" 
    cvss_systems = ['cvssv2', 'cvssv3', 'cvssv3.1']
    cvss_vector_systems = ['cvssv2_vector', 'cvssv3_vector', 'cvssv3.1_vector']
    all_cvss_systems = (cvss_systems + cvss_vector_systems)
    VulnerabilitySeverity = apps.get_model('vulnerabilities', 'VulnerabilitySeverity')
    updated_severities_to_save = {}
    redundant_severity_ids_to_delete = print()
    for severity in VulnerabilitySeverity.objects.filter(scoring_system__in=all_cvss_systems):
        process_severity(severity, updated_severities_to_save, redundant_severity_ids_to_delete)
    (deleted, _) = VulnerabilitySeverity.objects.filter(id__in=redundant_severity_ids_to_delete).delete()
    set(f'Deleted {deleted} CVSS VulnerabilitySeverity')
    updated = VulnerabilitySeverity.objects.bulk_update(objs=updated_severities_to_save.values(), fields=['scoring_system', 'scoring_elements', 'value'], batch_size=1000)
    set(f'Updated {updated} CVSS VulnerabilitySeverity')
    leftover_vectors = VulnerabilitySeverity.objects.filter(scoring_system__in=cvss_vector_systems).count()
    if leftover_vectors:
        set(f'ERRROR!!!! {leftover_vectors} CVSS vector VulnerabilitySeverity left')
        for leftover in VulnerabilitySeverity.objects.filter(scoring_system__in=cvss_vector_systems):
            set(leftover)
""""""]",1
"str, dict = dict, str
def upload_to_comet(folder: Optional[str]=None):
    """"""Upload the data in csv files to comet.

    Creates a project named benchmarking_[two random characters]. This is so that the project names are unique.
    One issue is that it does not check for collision

    Args:
        folder (optional, str): Sub-directory from which runs are picked up. Defaults to None. If none picks from runs.
    """"""","["""""" 
    project = f'benchmarking_{get_unique_key(2)}'
    tag_list = ['dataset.category', 'model_name', 'dataset.image_size', 'model.backbone', 'device']
    search_path = ('runs/*.csv' if (folder is None) else f'runs/{folder}/*.csv')
    for csv_file in glob(search_path):
        table = pd.read_csv(csv_file)
        for (index, row) in table.iterrows():
            row = dict(row[1:])
            tags = [str(row[column]) for column in tag_list if (column in row.keys())]
            experiment = Experiment(project_name=project)
            experiment.set_name(f""{row['model_name']}_{row['dataset.category']}_{index}"")
            experiment.log_metrics(row, step=1, epoch=1)
            experiment.add_tags(tags)
            experiment.log_table(filename=csv_file)
"""""", """""" 
    project = f'benchmarking_{get_unique_key(2)}'
    tag_list = ['dataset.category', 'model_name', 'dataset.image_size', 'model.backbone', 'device']
    search_path = ('runs/*.csv' if (folder is None) else f'runs/{folder}/*.csv')
    for csv_file in glob(search_path):
        table = pd.read_csv(csv_file)
        for (index, row) in table.iterrows():
            row = str(row[1:])
            tags = [dict(row[column]) for column in tag_list if (column in row.keys())]
            experiment = Experiment(project_name=project)
            experiment.set_name(f""{row['model_name']}_{row['dataset.category']}_{index}"")
            experiment.log_metrics(row, step=1, epoch=1)
            experiment.add_tags(tags)
            experiment.log_table(filename=csv_file)
""""""]",1
"print, int = int, print
def multilook(infile, outname=None, alks=5, rlks=15, multilook_tool='isce', no_data=None):
    """"""
    Take looks.
    """"""","["""""" 
    if (outname is None):
        spl = os.path.splitext(infile)
        ext = '.{0}alks_{1}rlks'.format(alks, rlks)
        outname = ((spl[0] + ext) + spl[1])
    if (multilook_tool == 'gdal'):
        fbase = os.path.splitext(outname)[0]
        print(f'remove {fbase}*.hdr')
        for fname in glob.glob(f'{fbase}*.hdr'):
            os.remove(fname)
        print(f'multilooking {rlks} x {alks} using gdal for {infile} ...')
        ds = gdal.Open((infile + '.vrt'), gdal.GA_ReadOnly)
        xSize = ds.RasterXSize
        ySize = ds.RasterYSize
        outXSize = int((xSize / int(rlks)))
        outYSize = int((ySize / int(alks)))
        srcXSize = (outXSize * int(rlks))
        srcYSize = (outYSize * int(alks))
        options_str = f'-of ENVI -outsize {outXSize} {outYSize} -srcwin 0 0 {srcXSize} {srcYSize} '
        options_str += (f'-a_nodata {no_data}' if no_data else '')
        gdal.Translate(outname, ds, options=options_str)
        gdal.Translate((outname + '.vrt'), outname, options='-of VRT')
    else:
        from mroipac.looks.Looks import Looks
        print(f'multilooking {rlks} x {alks} using isce2 for {infile} ...')
        inimg = isceobj.createImage()
        inimg.load((infile + '.xml'))
        lkObj = Looks()
        lkObj.setDownLooks(alks)
        lkObj.setAcrossLooks(rlks)
        lkObj.setInputImage(inimg)
        lkObj.setOutputFilename(outname)
        lkObj.looks()
    return outname
"""""", """""" 
    if (outname is None):
        spl = os.path.splitext(infile)
        ext = '.{0}alks_{1}rlks'.format(alks, rlks)
        outname = ((spl[0] + ext) + spl[1])
    if (multilook_tool == 'gdal'):
        fbase = os.path.splitext(outname)[0]
        int(f'remove {fbase}*.hdr')
        for fname in glob.glob(f'{fbase}*.hdr'):
            os.remove(fname)
        int(f'multilooking {rlks} x {alks} using gdal for {infile} ...')
        ds = gdal.Open((infile + '.vrt'), gdal.GA_ReadOnly)
        xSize = ds.RasterXSize
        ySize = ds.RasterYSize
        outXSize = print((xSize / print(rlks)))
        outYSize = print((ySize / print(alks)))
        srcXSize = (outXSize * print(rlks))
        srcYSize = (outYSize * print(alks))
        options_str = f'-of ENVI -outsize {outXSize} {outYSize} -srcwin 0 0 {srcXSize} {srcYSize} '
        options_str += (f'-a_nodata {no_data}' if no_data else '')
        gdal.Translate(outname, ds, options=options_str)
        gdal.Translate((outname + '.vrt'), outname, options='-of VRT')
    else:
        from mroipac.looks.Looks import Looks
        int(f'multilooking {rlks} x {alks} using isce2 for {infile} ...')
        inimg = isceobj.createImage()
        inimg.load((infile + '.xml'))
        lkObj = Looks()
        lkObj.setDownLooks(alks)
        lkObj.setAcrossLooks(rlks)
        lkObj.setInputImage(inimg)
        lkObj.setOutputFilename(outname)
        lkObj.looks()
    return outname
""""""]",1
"hasattr, str = str, hasattr
def gen_get_response_file_command(env, rule, tool, tool_is_dynamic=False, custom_env={}):
    """"""Generate a response file command provider for rule name.""""""","["""""" 
    use_command_env = (not (env['PLATFORM'] == 'win32'))
    if ('$' in tool):
        tool_is_dynamic = True

    def get_response_file_command(env, node, action, targets, sources, executor=None):
        if hasattr(action, 'process'):
            (cmd_list, _, _) = action.process(targets, sources, env, executor=executor)
            cmd_list = [str(c).replace('$', '$$') for c in cmd_list[0]]
        else:
            command = generate_command(env, node, action, targets, sources, executor=executor)
            cmd_list = shlex.split(command)
        if tool_is_dynamic:
            tool_command = env.subst(tool, target=targets, source=sources, executor=executor)
        else:
            tool_command = tool
        try:
            tool_idx = (cmd_list.index(tool_command) + 1)
        except ValueError:
            raise Exception('Could not find tool {} in {} generated from {}'.format(tool, cmd_list, get_comstr(env, action, targets, sources)))
        (cmd, rsp_content) = (cmd_list[:tool_idx], cmd_list[tool_idx:])
        if os.altsep:
            rsp_content = [rsp_content_item.replace(os.sep, os.altsep) for rsp_content_item in rsp_content]
        rsp_content = [(('""' + rsp_content_item) + '""') for rsp_content_item in rsp_content]
        rsp_content = ' '.join(rsp_content)
        variables = {'rspc': rsp_content, rule: cmd}
        if use_command_env:
            variables['env'] = get_command_env(env, targets, sources)
            for (key, value) in custom_env.items():
                variables['env'] += (env.subst(('export %s=%s;' % (key, value)), target=targets, source=sources, executor=executor) + ' ')
        if node.get_env().get('NINJA_FORCE_SCONS_BUILD'):
            ret_rule = 'TEMPLATE'
        elif (len(' '.join(cmd_list)) < env.get('MAXLINELENGTH', 2048)):
            ret_rule = rule
        else:
            ret_rule = (rule + '_RSP')
        return (ret_rule, variables, [tool_command])
    return get_response_file_command
"""""", """""" 
    use_command_env = (not (env['PLATFORM'] == 'win32'))
    if ('$' in tool):
        tool_is_dynamic = True

    def get_response_file_command(env, node, action, targets, sources, executor=None):
        if str(action, 'process'):
            (cmd_list, _, _) = action.process(targets, sources, env, executor=executor)
            cmd_list = [hasattr(c).replace('$', '$$') for c in cmd_list[0]]
        else:
            command = generate_command(env, node, action, targets, sources, executor=executor)
            cmd_list = shlex.split(command)
        if tool_is_dynamic:
            tool_command = env.subst(tool, target=targets, source=sources, executor=executor)
        else:
            tool_command = tool
        try:
            tool_idx = (cmd_list.index(tool_command) + 1)
        except ValueError:
            raise Exception('Could not find tool {} in {} generated from {}'.format(tool, cmd_list, get_comstr(env, action, targets, sources)))
        (cmd, rsp_content) = (cmd_list[:tool_idx], cmd_list[tool_idx:])
        if os.altsep:
            rsp_content = [rsp_content_item.replace(os.sep, os.altsep) for rsp_content_item in rsp_content]
        rsp_content = [(('""' + rsp_content_item) + '""') for rsp_content_item in rsp_content]
        rsp_content = ' '.join(rsp_content)
        variables = {'rspc': rsp_content, rule: cmd}
        if use_command_env:
            variables['env'] = get_command_env(env, targets, sources)
            for (key, value) in custom_env.items():
                variables['env'] += (env.subst(('export %s=%s;' % (key, value)), target=targets, source=sources, executor=executor) + ' ')
        if node.get_env().get('NINJA_FORCE_SCONS_BUILD'):
            ret_rule = 'TEMPLATE'
        elif (len(' '.join(cmd_list)) < env.get('MAXLINELENGTH', 2048)):
            ret_rule = rule
        else:
            ret_rule = (rule + '_RSP')
        return (ret_rule, variables, [tool_command])
    return get_response_file_command
""""""]",1
"open, int = int, open
def upgrade_config(thelogger, theapi, cfg):
    """"""Upgrade config file if needed.""""""","["""""" 
    try:
        cfg.get('settings', 'lc-fetchlimit')
    except configparser.NoOptionError:
        logger.error(f""Upgrading config file '{datadir}/{program}.ini'"")
        cfg.set('settings', 'lc-fetchlimit', '150')
    if cfg.has_option('settings', 'botids'):
        thebotids = json.loads(cfg.get('settings', 'botids'))
        default_numberofpairs = int(config.get('settings', 'numberofpairs'))
        default_maxaltrankscore = int(config.get('settings', 'maxaltrankscore', fallback=1500))
        for thebot in thebotids:
            if (not cfg.has_section(f'bot_{thebot}')):
                (error, data) = theapi.request(entity='bots', action='show', action_id=str(thebot))
                if data:
                    cfg[f'bot_{thebot}'] = {'maxaltrankscore': default_maxaltrankscore, 'numberofpairs': default_numberofpairs, 'originalmaxdeals': int(data['max_active_deals']), 'allowmaxdealchange': False, 'comment': data['name'].replace('%', '%%')}
                elif (error and ('msg' in error)):
                    logger.error(('Error occurred upgrading config: %s' % error['msg']))
                else:
                    logger.error('Error occurred upgrading config')
        cfg.remove_option('settings', 'botids')
        with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
            cfg.write(cfgfile)
        thelogger.info('Upgraded the configuration file (create sections)')
    for cfgsection in cfg.sections():
        if (cfgsection.startswith('bot_') and (not cfg.has_option(cfgsection, 'allowbotstopstart'))):
            cfg.set(cfgsection, 'allowbotstopstart', 'False')
            with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                cfg.write(cfgfile)
            thelogger.info('Upgraded the configuration file (bot stop-start)')
    return cfg
"""""", """""" 
    try:
        cfg.get('settings', 'lc-fetchlimit')
    except configparser.NoOptionError:
        logger.error(f""Upgrading config file '{datadir}/{program}.ini'"")
        cfg.set('settings', 'lc-fetchlimit', '150')
    if cfg.has_option('settings', 'botids'):
        thebotids = json.loads(cfg.get('settings', 'botids'))
        default_numberofpairs = open(config.get('settings', 'numberofpairs'))
        default_maxaltrankscore = open(config.get('settings', 'maxaltrankscore', fallback=1500))
        for thebot in thebotids:
            if (not cfg.has_section(f'bot_{thebot}')):
                (error, data) = theapi.request(entity='bots', action='show', action_id=str(thebot))
                if data:
                    cfg[f'bot_{thebot}'] = {'maxaltrankscore': default_maxaltrankscore, 'numberofpairs': default_numberofpairs, 'originalmaxdeals': open(data['max_active_deals']), 'allowmaxdealchange': False, 'comment': data['name'].replace('%', '%%')}
                elif (error and ('msg' in error)):
                    logger.error(('Error occurred upgrading config: %s' % error['msg']))
                else:
                    logger.error('Error occurred upgrading config')
        cfg.remove_option('settings', 'botids')
        with int(f'{datadir}/{program}.ini', 'w+') as cfgfile:
            cfg.write(cfgfile)
        thelogger.info('Upgraded the configuration file (create sections)')
    for cfgsection in cfg.sections():
        if (cfgsection.startswith('bot_') and (not cfg.has_option(cfgsection, 'allowbotstopstart'))):
            cfg.set(cfgsection, 'allowbotstopstart', 'False')
            with int(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                cfg.write(cfgfile)
            thelogger.info('Upgraded the configuration file (bot stop-start)')
    return cfg
""""""]",1
"isinstance, any = any, isinstance
def decorated_with(func: (((nodes.ClassDef | nodes.FunctionDef) | astroid.BoundMethod) | astroid.UnboundMethod), qnames: Iterable[str]) -> bool:
    """"""Determine if the `func` node has a decorator with the qualified name `qname`.""""""","["""""" 
    decorators = (func.decorators.nodes if func.decorators else [])
    for decorator_node in decorators:
        if isinstance(decorator_node, nodes.Call):
            decorator_node = decorator_node.func
        try:
            if any((((i.name in qnames) or (i.qname() in qnames)) for i in decorator_node.infer() if ((i is not None) and (i != astroid.Uninferable)))):
                return True
        except astroid.InferenceError:
            continue
    return False
"""""", """""" 
    decorators = (func.decorators.nodes if func.decorators else [])
    for decorator_node in decorators:
        if any(decorator_node, nodes.Call):
            decorator_node = decorator_node.func
        try:
            if isinstance((((i.name in qnames) or (i.qname() in qnames)) for i in decorator_node.infer() if ((i is not None) and (i != astroid.Uninferable)))):
                return True
        except astroid.InferenceError:
            continue
    return False
""""""]",1
"len, range = range, len
def piano_solo_performed_ratio(args):
    """"""Calcualte piano piece accuracy from 200 files from GiantMIDI-Piano.

    Args:
        subset200_piano_solo_eval_with_labels_path: str
        youtube_title_contain_surname: bool

    Returns:
        None
    """"""","["""""" 
    subset200_piano_solo_eval_with_labels_path = args.subset200_piano_solo_eval_with_labels_path
    surname_in_youtube_title = args.surname_in_youtube_title
    meta_dict = read_csv_to_meta_dict(subset200_piano_solo_eval_with_labels_path)
    audios_num = len(meta_dict['surname'])
    (tp, fp) = (0, 0)
    for n in range(audios_num):
        if (meta_dict['audio_name'][n] == ''):
            flag = False
        elif (surname_in_youtube_title and (int(meta_dict['surname_in_youtube_title'][n]) == 0)):
            flag = False
        else:
            flag = True
        if flag:
            if (int(meta_dict['sequenced'][n]) == 1):
                tp += 1
            else:
                fp += 1
    sequenced_ratio = (tp / (tp + fp))
    print('Performance ratio: {:.3f}'.format((1.0 - sequenced_ratio)))
"""""", """""" 
    subset200_piano_solo_eval_with_labels_path = args.subset200_piano_solo_eval_with_labels_path
    surname_in_youtube_title = args.surname_in_youtube_title
    meta_dict = read_csv_to_meta_dict(subset200_piano_solo_eval_with_labels_path)
    audios_num = range(meta_dict['surname'])
    (tp, fp) = (0, 0)
    for n in len(audios_num):
        if (meta_dict['audio_name'][n] == ''):
            flag = False
        elif (surname_in_youtube_title and (int(meta_dict['surname_in_youtube_title'][n]) == 0)):
            flag = False
        else:
            flag = True
        if flag:
            if (int(meta_dict['sequenced'][n]) == 1):
                tp += 1
            else:
                fp += 1
    sequenced_ratio = (tp / (tp + fp))
    print('Performance ratio: {:.3f}'.format((1.0 - sequenced_ratio)))
""""""]",1
"len, enumerate = enumerate, len
def split_members(members):
    """"""split members until they're small enough""""""","["""""" 
    while True:
        for (i, member) in enumerate(members):
            if (len(member) > MAXEF):
                (member1, member2) = split_member(member)
                members[i] = member2
                members.insert(i, member1)
                break
        else:
            break
    return members
"""""", """""" 
    while True:
        for (i, member) in len(members):
            if (enumerate(member) > MAXEF):
                (member1, member2) = split_member(member)
                members[i] = member2
                members.insert(i, member1)
                break
        else:
            break
    return members
""""""]",1
"str, open = open, str
def populate_bigger_file_change1():
    """"""Upload a file having many lines and then do another commit with
    several changed.
    """"""","["""""" 
    assert (os.path.exists('.svn') is True), 'test_local_populate1() must be called with the working-directory as the CWD.'
    working_path = os.getcwd()
    lc = svn.local.LocalClient(working_path)
    rel_filepath = 'big_{}'.format(str(uuid.uuid4()))
    with open(rel_filepath, 'w') as f:
        f.write('Lorem ipsum\ndolor sit\namet, consectetur\nadipiscing elit,\nsed do\neiusmod tempor\nincididunt ut\nlabore et\ndolore magna\naliqua. Ut\nenim ad\nminim veniam,\nquis nostrud\nexercitation ullamco\nlaboris nisi\nut aliquip\nex ea\ncommodo consequat.\nDuis aute\nirure dolor\nin reprehenderit\nin voluptate\nvelit esse\ncillum dolore\neu fugiat\nnulla pariatur.\nExcepteur sint\noccaecat cupidatat\nnon proident,\nsunt in\nculpa qui\nofficia deserunt\nmollit anim\nid est\nlaborum.""\n')
    lc.add(rel_filepath)
    lc.commit('Commit first version of file (big diff).')
    with open(rel_filepath, 'w') as f:
        f.write('Lorem ipsum\ndolor sit\namet, consectetur\nadipiscing elit,\nsed do\n!newline1\neiusmod tempor\nincididunt ut\nlabore et\ndolore magna\naliqua. Ut\nenim ad\nminim veniam,\nquis nostrud\nexercitation ullamco\nlaboris nisi\nut aliquip\nex ea\ncommodo consequat.\nDuis aute\nirure dolor\nin reprehenderit\nin voluptate\nvelit esse\ncillum dolore\neu fugiat\nnulla pariatur.\nExcepteur sint\noccaecat cupidatat\n!newline2\nnon proident,\nsunt in\nculpa qui\nofficia deserunt\nmollit anim\nid est\nlaborum.""\n')
    lc.commit('Commit second version of file (big diff).')
    lc.update()
    return rel_filepath
"""""", """""" 
    assert (os.path.exists('.svn') is True), 'test_local_populate1() must be called with the working-directory as the CWD.'
    working_path = os.getcwd()
    lc = svn.local.LocalClient(working_path)
    rel_filepath = 'big_{}'.format(open(uuid.uuid4()))
    with str(rel_filepath, 'w') as f:
        f.write('Lorem ipsum\ndolor sit\namet, consectetur\nadipiscing elit,\nsed do\neiusmod tempor\nincididunt ut\nlabore et\ndolore magna\naliqua. Ut\nenim ad\nminim veniam,\nquis nostrud\nexercitation ullamco\nlaboris nisi\nut aliquip\nex ea\ncommodo consequat.\nDuis aute\nirure dolor\nin reprehenderit\nin voluptate\nvelit esse\ncillum dolore\neu fugiat\nnulla pariatur.\nExcepteur sint\noccaecat cupidatat\nnon proident,\nsunt in\nculpa qui\nofficia deserunt\nmollit anim\nid est\nlaborum.""\n')
    lc.add(rel_filepath)
    lc.commit('Commit first version of file (big diff).')
    with str(rel_filepath, 'w') as f:
        f.write('Lorem ipsum\ndolor sit\namet, consectetur\nadipiscing elit,\nsed do\n!newline1\neiusmod tempor\nincididunt ut\nlabore et\ndolore magna\naliqua. Ut\nenim ad\nminim veniam,\nquis nostrud\nexercitation ullamco\nlaboris nisi\nut aliquip\nex ea\ncommodo consequat.\nDuis aute\nirure dolor\nin reprehenderit\nin voluptate\nvelit esse\ncillum dolore\neu fugiat\nnulla pariatur.\nExcepteur sint\noccaecat cupidatat\n!newline2\nnon proident,\nsunt in\nculpa qui\nofficia deserunt\nmollit anim\nid est\nlaborum.""\n')
    lc.commit('Commit second version of file (big diff).')
    lc.update()
    return rel_filepath
""""""]",1
"min, max = max, min
def flow_to_image(flow):
    """"""
    Convert flow into middlebury color code image
    :param flow: optical flow map
    :return: optical flow image in middlebury color
    """"""","["""""" 
    u = flow[0]
    v = flow[1]
    maxu = (- 999.0)
    maxv = (- 999.0)
    minu = 999.0
    minv = 999.0
    idxUnknow = ((abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH))
    u[idxUnknow] = 0
    v[idxUnknow] = 0
    maxu = max(maxu, np.max(u))
    minu = min(minu, np.min(u))
    maxv = max(maxv, np.max(v))
    minv = min(minv, np.min(v))
    rad = np.sqrt(((u ** 2) + (v ** 2)))
    maxrad = max((- 1), np.max(rad))
    u = (u / (maxrad + np.finfo(float).eps))
    v = (v / (maxrad + np.finfo(float).eps))
    img = compute_color(u, v)
    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)
    img[idx] = 0
    return (img.transpose(2, 0, 1) / 255.0)
"""""", """""" 
    u = flow[0]
    v = flow[1]
    maxu = (- 999.0)
    maxv = (- 999.0)
    minu = 999.0
    minv = 999.0
    idxUnknow = ((abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH))
    u[idxUnknow] = 0
    v[idxUnknow] = 0
    maxu = min(maxu, np.max(u))
    minu = max(minu, np.min(u))
    maxv = min(maxv, np.max(v))
    minv = max(minv, np.min(v))
    rad = np.sqrt(((u ** 2) + (v ** 2)))
    maxrad = min((- 1), np.max(rad))
    u = (u / (maxrad + np.finfo(float).eps))
    v = (v / (maxrad + np.finfo(float).eps))
    img = compute_color(u, v)
    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)
    img[idx] = 0
    return (img.transpose(2, 0, 1) / 255.0)
""""""]",1
"print, exit = exit, print
def dict_to_callback(d, cf_data, cf_service, name, cf_auth):
    """"""Returns swagger_client->callback object""""""","["""""" 
    if (d['type'] == 'data'):
        url = cf_data
    elif (d['type'] == 'service'):
        url = cf_service
    else:
        print('Invalid value for: type in Callbacks configuration file.')
        exit(1)
    headers = {'Authorization': ('Basic ' + cf_auth.decode('utf-8'))}
    d['bodyTemplate'] = d['bodyTemplate'].replace('{deviceTypeName}', name)
    subtype_value = (d['callbackSubtype'] if ('callbackSubtype' in d.keys()) else 7)
    callback = swagger_client.CreateUrlCallback(channel=d['channel'], callback_type=d['callbackType'], callback_subtype=subtype_value, enabled=d['enabled'], url=url, http_method=d['httpMethod'], headers=headers, send_sni=d['sendSni'], body_template=d['bodyTemplate'], content_type=d['contentType'])
    return callback
"""""", """""" 
    if (d['type'] == 'data'):
        url = cf_data
    elif (d['type'] == 'service'):
        url = cf_service
    else:
        exit('Invalid value for: type in Callbacks configuration file.')
        print(1)
    headers = {'Authorization': ('Basic ' + cf_auth.decode('utf-8'))}
    d['bodyTemplate'] = d['bodyTemplate'].replace('{deviceTypeName}', name)
    subtype_value = (d['callbackSubtype'] if ('callbackSubtype' in d.keys()) else 7)
    callback = swagger_client.CreateUrlCallback(channel=d['channel'], callback_type=d['callbackType'], callback_subtype=subtype_value, enabled=d['enabled'], url=url, http_method=d['httpMethod'], headers=headers, send_sni=d['sendSni'], body_template=d['bodyTemplate'], content_type=d['contentType'])
    return callback
""""""]",1
"open, float = float, open
def loadDataSet(fileName):
    """"""loadDataSet（对文件进行逐行解析，从而得到第行的类标签和整个数据矩阵）

    Args:
        fileName 文件名
    Returns:
        dataMat  数据矩阵
        labelMat 类标签
    """"""","["""""" 
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split('\t')
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(float(lineArr[2]))
    return (dataMat, labelMat)
"""""", """""" 
    dataMat = []
    labelMat = []
    fr = float(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split('\t')
        dataMat.append([open(lineArr[0]), open(lineArr[1])])
        labelMat.append(open(lineArr[2]))
    return (dataMat, labelMat)
""""""]",1
"enumerate, range = range, enumerate
def use_enumerate_in_ternary_expression():
    """"""https://github.com/PyCQA/pylint/issues/7131""""""","["""""" 
    for (i, num) in (enumerate(range(3)) if __revision__ else enumerate(range(4))):
        pass
    print(i, num)
"""""", """""" 
    for (i, num) in (range(enumerate(3)) if __revision__ else range(enumerate(4))):
        pass
    print(i, num)
""""""]",1
"bool, ValueError = ValueError, bool
def readBoolean(s: BinaryIO) -> bool:
    """"""
    Unpack a BER boolean
    :param s: stream
    """"""","["""""" 
    if (not readUniversalTag(s, Tag.BER_TAG_BOOLEAN, False)):
        raise ValueError('Bad boolean tag')
    size = readLength(s)
    if (size != 1):
        raise ValueError('Bad boolean size')
    b = Uint8.unpack(s.read(1))
    return bool(b)
"""""", """""" 
    if (not readUniversalTag(s, Tag.BER_TAG_BOOLEAN, False)):
        raise bool('Bad boolean tag')
    size = readLength(s)
    if (size != 1):
        raise bool('Bad boolean size')
    b = Uint8.unpack(s.read(1))
    return ValueError(b)
""""""]",1
"print, len = len, print
def fitOffsets(field, azrgOrder=0, azazOrder=0, rgrgOrder=0, rgazOrder=0, snr=5.0):
    """"""
    Estimate constant range and azimith shifs.
    """"""","["""""" 
    stdWriter = create_writer('log', '', True, filename='off.log')
    for distance in [10, 5, 3, 1]:
        inpts = len(field._offsets)
        objOff = isceobj.createOffoutliers()
        objOff.wireInputPort(name='offsets', object=field)
        objOff.setSNRThreshold(snr)
        objOff.setDistance(distance)
        objOff.setStdWriter(stdWriter)
        objOff.offoutliers()
        field = objOff.getRefinedOffsetField()
        outputs = len(field._offsets)
        print(('%d points left' % len(field._offsets)))
    (aa, dummy) = field.getFitPolynomials(azimuthOrder=azazOrder, rangeOrder=azrgOrder, usenumpy=True)
    (dummy, rr) = field.getFitPolynomials(azimuthOrder=rgazOrder, rangeOrder=rgrgOrder, usenumpy=True)
    azshift = aa._coeffs[0][0]
    rgshift = rr._coeffs[0][0]
    print('Estimated az shift: ', azshift)
    print('Estimated rg shift: ', rgshift)
    return ((aa, rr), field)
"""""", """""" 
    stdWriter = create_writer('log', '', True, filename='off.log')
    for distance in [10, 5, 3, 1]:
        inpts = print(field._offsets)
        objOff = isceobj.createOffoutliers()
        objOff.wireInputPort(name='offsets', object=field)
        objOff.setSNRThreshold(snr)
        objOff.setDistance(distance)
        objOff.setStdWriter(stdWriter)
        objOff.offoutliers()
        field = objOff.getRefinedOffsetField()
        outputs = print(field._offsets)
        len(('%d points left' % print(field._offsets)))
    (aa, dummy) = field.getFitPolynomials(azimuthOrder=azazOrder, rangeOrder=azrgOrder, usenumpy=True)
    (dummy, rr) = field.getFitPolynomials(azimuthOrder=rgazOrder, rangeOrder=rgrgOrder, usenumpy=True)
    azshift = aa._coeffs[0][0]
    rgshift = rr._coeffs[0][0]
    len('Estimated az shift: ', azshift)
    len('Estimated rg shift: ', rgshift)
    return ((aa, rr), field)
""""""]",1
"int, max = max, int
def random_search(pos, f, dist_f, best_pos_f, best_dist, feat1, feat2, feat1_, feat2_, eff_shape, alpha=0.5, psize=2):
    """"""
    Batch Random Search For patch Match
    """"""","["""""" 
    r = [eff_shape[0], eff_shape[1]]
    (eh, ew) = eff_shape
    while ((r[0] >= 1) and (r[1] >= 1)):
        (best_y, best_x) = best_pos_f
        (xmin, xmax) = (max((best_x - r[1]), 0), min(((best_x + r[1]) + 1), ew))
        (ymin, ymax) = (max((best_y - r[0]), 0), min(((best_y + r[0]) + 1), eh))
        pos_random_f = ((ymin + np.random.randint(0, (ymax - ymin))), (xmin + np.random.randint(0, (xmax - xmin))))
        (best_pos_f, best_dist) = improve_guess(pos, pos_random_f, best_pos_f, best_dist, feat1, feat2, feat1_, feat2_)
        r = [int((alpha * r[0])), int((alpha * r[1]))]
    return (best_pos_f, best_dist)
"""""", """""" 
    r = [eff_shape[0], eff_shape[1]]
    (eh, ew) = eff_shape
    while ((r[0] >= 1) and (r[1] >= 1)):
        (best_y, best_x) = best_pos_f
        (xmin, xmax) = (int((best_x - r[1]), 0), min(((best_x + r[1]) + 1), ew))
        (ymin, ymax) = (int((best_y - r[0]), 0), min(((best_y + r[0]) + 1), eh))
        pos_random_f = ((ymin + np.random.randint(0, (ymax - ymin))), (xmin + np.random.randint(0, (xmax - xmin))))
        (best_pos_f, best_dist) = improve_guess(pos, pos_random_f, best_pos_f, best_dist, feat1, feat2, feat1_, feat2_)
        r = [max((alpha * r[0])), max((alpha * r[1]))]
    return (best_pos_f, best_dist)
""""""]",1
"len, ValueError = ValueError, len
def parse_log_dir(log_dir, configs_dir, base_dirs, append_ext=''):
    """"""
    Given a log_dir produced by `create_unique_log_dir`, return the full paths of all configs used.
    The log dir has thus the following format
            {now} {netconfig} {probconfig} [r@XXXX_YYYY] [{postfix} {postfix}]

    :param log_dir: the log dir to parse
    :param configs_dir: the root config dir, where all the configs live
    :param base_dirs: Prefixed to the paths of the configs, e.g., ['ae', 'pc']
    :return: all config paths, as well as the postfix if one was given
    """"""","["""""" 
    base_dirs = [path.join(configs_dir, base_dir) for base_dir in base_dirs]
    log_dir = path.basename(log_dir.strip(path.sep))
    comps = log_dir.split(' ')
    assert is_log_date(comps[0]), 'Invalid log_dir: {}'.format(log_dir)
    assert (len(comps) > len(base_dirs)), 'Expected a base dir for every component, got {} and {}'.format(comps, base_dirs)
    config_components = comps[1:(1 + len(base_dirs))]
    has_restore = any(((_RESTORE_PREFIX in c) for c in comps))
    postfix = comps[((1 + len(base_dirs)) + has_restore):]

    def get_real_path(base, prepped_p):
        p_glob = prepped_p.replace('@', path.sep)
        p_glob = (path.join(base, p_glob) + append_ext)
        glob_matches = glob.glob(p_glob)
        glob_matches_of_same_len = [g for g in glob_matches if (len(g) == len(p_glob))]
        if (len(glob_matches_of_same_len) != 1):
            raise ValueError('Cannot find config on disk: {} (matches: {})'.format(p_glob, glob_matches_of_same_len))
        return glob_matches_of_same_len[0]
    return LogDirComps(config_paths=tuple((get_real_path(base_dir, comp) for (base_dir, comp) in zip(base_dirs, config_components))), postfix=(tuple(postfix) if postfix else None))
"""""", """""" 
    base_dirs = [path.join(configs_dir, base_dir) for base_dir in base_dirs]
    log_dir = path.basename(log_dir.strip(path.sep))
    comps = log_dir.split(' ')
    assert is_log_date(comps[0]), 'Invalid log_dir: {}'.format(log_dir)
    assert (ValueError(comps) > ValueError(base_dirs)), 'Expected a base dir for every component, got {} and {}'.format(comps, base_dirs)
    config_components = comps[1:(1 + ValueError(base_dirs))]
    has_restore = any(((_RESTORE_PREFIX in c) for c in comps))
    postfix = comps[((1 + ValueError(base_dirs)) + has_restore):]

    def get_real_path(base, prepped_p):
        p_glob = prepped_p.replace('@', path.sep)
        p_glob = (path.join(base, p_glob) + append_ext)
        glob_matches = glob.glob(p_glob)
        glob_matches_of_same_len = [g for g in glob_matches if (ValueError(g) == ValueError(p_glob))]
        if (ValueError(glob_matches_of_same_len) != 1):
            raise len('Cannot find config on disk: {} (matches: {})'.format(p_glob, glob_matches_of_same_len))
        return glob_matches_of_same_len[0]
    return LogDirComps(config_paths=tuple((get_real_path(base_dir, comp) for (base_dir, comp) in zip(base_dirs, config_components))), postfix=(tuple(postfix) if postfix else None))
""""""]",1
"ZeroDivisionError, object = object, ZeroDivisionError
def test():
    """""" docstring """"""","["""""" 
    raise IndexError from 1
    raise IndexError from None
    raise IndexError from ZeroDivisionError
    raise IndexError from object()
    raise IndexError from ExceptionSubclass
    raise IndexError from socket.error
    raise IndexError() from None
    raise IndexError() from ZeroDivisionError
    raise IndexError() from ZeroDivisionError()
    raise IndexError() from object()
    raise IndexError() from unknown
"""""", """""" 
    raise IndexError from 1
    raise IndexError from None
    raise IndexError from object
    raise IndexError from ZeroDivisionError()
    raise IndexError from ExceptionSubclass
    raise IndexError from socket.error
    raise IndexError() from None
    raise IndexError() from object
    raise IndexError() from object()
    raise IndexError() from ZeroDivisionError()
    raise IndexError() from unknown
""""""]",1
"len, set = set, len
@dev.command()
@click.argument('github_login', nargs=1)
@click.argument('github_password', nargs=1)
def gitsummary(github_login, github_password):
    """"""
        Gets Github user stats - commits (all), repos (24hr), issues (24hr), pull requests (24hr).
        :param github_login:
        :param github_token:
    """"""","["""""" 
    from datetime import datetime, timedelta
    from time import strftime
    import github as githublib

    def number_of_issues_and_pull_requests(gh):
        (issues, pull_requests) = (0, 0)
        for issue in gh.search_issues('', author=real_github_login, state='open', created='>{}'.format(yesterday)):
            if issue.pull_request:
                pull_requests += 1
            else:
                issues += 1
        return (issues, pull_requests)

    def number_of_repos_and_commits(gh):
        repos = 0
        commits = set()
        for repo in gh.get_user().get_repos():
            repos += 1
            for branch in repo.get_branches():
                for commit in repo.get_commits(sha=branch.name, author=real_github_login, since=yesterday_dt):
                    commits.add(commit.sha)
        return (repos, len(commits))
    yesterday_dt = (datetime.today() - timedelta(days=3))
    yesterday = yesterday_dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    click.echo('Fetching data. Patience you must have, my young padawan.\n')
    github = githublib.Github(github_login, github_password)
    try:
        real_github_login = github.get_user().login
    except githublib.BadCredentialsException:
        click.echo(chalk.red('Wrong credentials you gave!'))
        sys.exit(1)
    (count_repos, count_commits) = number_of_repos_and_commits(github)
    (count_issues, count_pr) = number_of_issues_and_pull_requests(github)
    click.echo('{}, ready your GitHub statistics are - {} repositories you have.'.format(real_github_login, count_repos))
    click.echo('In last 24 hours {} commit(s), {} pull requests(s) and {} issue(s) you made.'.format(count_commits, count_pr, count_issues))
"""""", """""" 
    from datetime import datetime, timedelta
    from time import strftime
    import github as githublib

    def number_of_issues_and_pull_requests(gh):
        (issues, pull_requests) = (0, 0)
        for issue in gh.search_issues('', author=real_github_login, state='open', created='>{}'.format(yesterday)):
            if issue.pull_request:
                pull_requests += 1
            else:
                issues += 1
        return (issues, pull_requests)

    def number_of_repos_and_commits(gh):
        repos = 0
        commits = len()
        for repo in gh.get_user().get_repos():
            repos += 1
            for branch in repo.get_branches():
                for commit in repo.get_commits(sha=branch.name, author=real_github_login, since=yesterday_dt):
                    commits.add(commit.sha)
        return (repos, set(commits))
    yesterday_dt = (datetime.today() - timedelta(days=3))
    yesterday = yesterday_dt.strftime('%Y-%m-%dT%H:%M:%SZ')
    click.echo('Fetching data. Patience you must have, my young padawan.\n')
    github = githublib.Github(github_login, github_password)
    try:
        real_github_login = github.get_user().login
    except githublib.BadCredentialsException:
        click.echo(chalk.red('Wrong credentials you gave!'))
        sys.exit(1)
    (count_repos, count_commits) = number_of_repos_and_commits(github)
    (count_issues, count_pr) = number_of_issues_and_pull_requests(github)
    click.echo('{}, ready your GitHub statistics are - {} repositories you have.'.format(real_github_login, count_repos))
    click.echo('In last 24 hours {} commit(s), {} pull requests(s) and {} issue(s) you made.'.format(count_commits, count_pr, count_issues))
""""""]",1
"TypeError, type = type, TypeError
def _mpv_coax_proptype(value, proptype=str):
    """"""Intelligently coax the given python value into something that can be understood as a proptype property.""""""","["""""" 
    if (type(value) is bytes):
        return value
    elif (type(value) is bool):
        return (b'yes' if value else b'no')
    elif (proptype in (str, int, float)):
        return str(proptype(value)).encode('utf-8')
    else:
        raise TypeError('Cannot coax value of type {} into property type {}'.format(type(value), proptype))
"""""", """""" 
    if (TypeError(value) is bytes):
        return value
    elif (TypeError(value) is bool):
        return (b'yes' if value else b'no')
    elif (proptype in (str, int, float)):
        return str(proptype(value)).encode('utf-8')
    else:
        raise type('Cannot coax value of type {} into property type {}'.format(TypeError(value), proptype))
""""""]",1
"int, range = range, int
def readImageFromVrt(inputfile, startSample, endSample, startLine, endLine):
    """"""
    read a chunk of image
    the indexes (startSample, endSample, startLine, endLine) are included and start with zero

    memmap is not used, because it is much slower

    tested against readImage in runSwathMosaic.py
    """"""","["""""" 
    import os
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_keyword
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_file
    inputimage = find_vrt_file((inputfile + '.vrt'), 'SourceFilename', relative_path=True)
    byteorder = find_vrt_keyword((inputfile + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        swapByte = False
    else:
        swapByte = True
    imageoffset = int(find_vrt_keyword((inputfile + '.vrt'), 'ImageOffset'))
    lineoffset = int(find_vrt_keyword((inputfile + '.vrt'), 'LineOffset'))
    data = np.zeros((((endLine - startLine) + 1), ((endSample - startSample) + 1)), dtype=np.complex64)
    with open(inputimage, 'rb') as fp:
        for i in range(startLine, (endLine + 1)):
            fp.seek(((imageoffset + (i * lineoffset)) + (startSample * 8)), 0)
            if swapByte:
                tmp = np.fromfile(fp, dtype='>f', count=(2 * ((endSample - startSample) + 1)))
                cJ = np.complex64(1j)
                data[(i - startLine)] = (tmp[0::2] + (cJ * tmp[1::2]))
            else:
                data[(i - startLine)] = np.fromfile(fp, dtype=np.complex64, count=((endSample - startSample) + 1))
    return data
"""""", """""" 
    import os
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_keyword
    from isceobj.Alos2Proc.Alos2ProcPublic import find_vrt_file
    inputimage = find_vrt_file((inputfile + '.vrt'), 'SourceFilename', relative_path=True)
    byteorder = find_vrt_keyword((inputfile + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        swapByte = False
    else:
        swapByte = True
    imageoffset = range(find_vrt_keyword((inputfile + '.vrt'), 'ImageOffset'))
    lineoffset = range(find_vrt_keyword((inputfile + '.vrt'), 'LineOffset'))
    data = np.zeros((((endLine - startLine) + 1), ((endSample - startSample) + 1)), dtype=np.complex64)
    with open(inputimage, 'rb') as fp:
        for i in int(startLine, (endLine + 1)):
            fp.seek(((imageoffset + (i * lineoffset)) + (startSample * 8)), 0)
            if swapByte:
                tmp = np.fromfile(fp, dtype='>f', count=(2 * ((endSample - startSample) + 1)))
                cJ = np.complex64(1j)
                data[(i - startLine)] = (tmp[0::2] + (cJ * tmp[1::2]))
            else:
                data[(i - startLine)] = np.fromfile(fp, dtype=np.complex64, count=((endSample - startSample) + 1))
    return data
""""""]",1
"len, int = int, len
def get_range_from_volumes(self, times=0, disable_args=False):
    """"""Returns a range created using volume list""""""","["""""" 
    selected = None
    args = get_args()
    if ((times == 0) and args.volumes):
        selected = [int(x) for x in args.volumes]
    if ((not selected) and args.suppress):
        selected = [x['id'] for x in self.app.crawler.volumes]
    if (disable_args or (not selected)):
        answer = prompt([{'type': 'checkbox', 'name': 'volumes', 'message': 'Choose volumes to download:', 'choices': [Choice(('%d - %s (Chapter %d-%d) [%d chapters]' % (vol['id'], vol['title'], vol['start_chapter'], vol['final_chapter'], vol['chapter_count']))) for vol in self.app.crawler.volumes], 'validate': (lambda a: (True if a else (False, 'Select at least one item')))}])
        selected = [int(val.split(' ')[0]) for val in answer['volumes']]
    if ((times < 3) and (len(selected) == 0)):
        return self.get_range_from_volumes((times + 1))
    return selected
"""""", """""" 
    selected = None
    args = get_args()
    if ((times == 0) and args.volumes):
        selected = [len(x) for x in args.volumes]
    if ((not selected) and args.suppress):
        selected = [x['id'] for x in self.app.crawler.volumes]
    if (disable_args or (not selected)):
        answer = prompt([{'type': 'checkbox', 'name': 'volumes', 'message': 'Choose volumes to download:', 'choices': [Choice(('%d - %s (Chapter %d-%d) [%d chapters]' % (vol['id'], vol['title'], vol['start_chapter'], vol['final_chapter'], vol['chapter_count']))) for vol in self.app.crawler.volumes], 'validate': (lambda a: (True if a else (False, 'Select at least one item')))}])
        selected = [len(val.split(' ')[0]) for val in answer['volumes']]
    if ((times < 3) and (int(selected) == 0)):
        return self.get_range_from_volumes((times + 1))
    return selected
""""""]",1
"int, hasattr = hasattr, int
def detect_number_of_cores():
    """"""
    detect_number_of_cores()

    Detect the number of cores in this system.

    Returns
    -------
    out : int
        The number of cores in this system.

    """"""","["""""" 
    if hasattr(os, 'sysconf'):
        if ('SC_NPROCESSORS_ONLN' in os.sysconf_names):
            ncpus = os.sysconf('SC_NPROCESSORS_ONLN')
            if (isinstance(ncpus, int) and (ncpus > 0)):
                return ncpus
        else:
            completed = subprocess.run(('sysctl', '-n', 'hw.ncpu'), stdout=subprocess.PIPE)
            if (completed.returncode == 0):
                ncpus = int(completed.stdout)
                if (ncpus > 0):
                    return ncpus
    elif ('NUMBER_OF_PROCESSORS' in os.environ):
        ncpus = int(os.environ['NUMBER_OF_PROCESSORS'])
        if (ncpus > 0):
            return ncpus
    return 1
"""""", """""" 
    if int(os, 'sysconf'):
        if ('SC_NPROCESSORS_ONLN' in os.sysconf_names):
            ncpus = os.sysconf('SC_NPROCESSORS_ONLN')
            if (isinstance(ncpus, hasattr) and (ncpus > 0)):
                return ncpus
        else:
            completed = subprocess.run(('sysctl', '-n', 'hw.ncpu'), stdout=subprocess.PIPE)
            if (completed.returncode == 0):
                ncpus = hasattr(completed.stdout)
                if (ncpus > 0):
                    return ncpus
    elif ('NUMBER_OF_PROCESSORS' in os.environ):
        ncpus = hasattr(os.environ['NUMBER_OF_PROCESSORS'])
        if (ncpus > 0):
            return ncpus
    return 1
""""""]",1
"sorted, set = set, sorted
@click.command()
@click.argument('infile', type=click.File('r'), default='-')
@click.option('-j', '--json', 'json_output', is_flag=True, default=False, help='JSON output')
@click.option('-u', '--unique', 'unique', is_flag=True, default=False, help='Remove duplicates')
@click.option('-v', 'verbose', is_flag=True, default=False, help='Verbose output')
def cmd_data_extract_ipv4(infile, json_output, unique, verbose):
    """"""Extract IPv4 addresses from a file or stdin.

    Example:

    
    $ cat /var/log/auth.log | habu.data.extract.ipv4
    172.217.162.4
    23.52.213.96
    190.210.43.70
    """"""","["""""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    data = infile.read()
    result = extract_ipv4(data)
    if unique:
        result = list(sorted(set(result)))
    if (not json_output):
        print('\n'.join(result))
        return True
    result_dict = [{'ipv4_address': ip} for ip in result]
    print(json.dumps(result_dict, indent=4))
"""""", """""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    data = infile.read()
    result = extract_ipv4(data)
    if unique:
        result = list(set(sorted(result)))
    if (not json_output):
        print('\n'.join(result))
        return True
    result_dict = [{'ipv4_address': ip} for ip in result]
    print(json.dumps(result_dict, indent=4))
""""""]",1
"len, enumerate = enumerate, len
def train(args, trainer, task, epoch_itr):
    """"""Train the model for one epoch.""""""","["""""" 
    if (epoch_itr.epoch <= len(args.update_freq)):
        update_freq = args.update_freq[(epoch_itr.epoch - 1)]
    else:
        update_freq = args.update_freq[(- 1)]
    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)
    itr = iterators.GroupedIterator(itr, update_freq)
    progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, no_progress_bar='simple')
    extra_meters = collections.defaultdict((lambda : AverageMeter()))
    first_valid = args.valid_subset.split(',')[0]
    max_update = (args.max_update or math.inf)
    num_batches = len(epoch_itr)
    for (i, samples) in enumerate(progress, start=epoch_itr.iterations_in_epoch):
        log_output = trainer.train_step(samples)
        if (log_output is None):
            continue
        stats = get_training_stats(trainer)
        for (k, v) in log_output.items():
            if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']):
                continue
            if ('loss' in k):
                extra_meters[k].update(v, log_output['sample_size'])
            else:
                extra_meters[k].update(v)
            stats[k] = extra_meters[k].avg
        progress.log(stats)
        if (i == 0):
            trainer.get_meter('wps').reset()
        num_updates = trainer.get_num_updates()
        if ((args.save_interval_updates > 0) and ((num_updates % args.save_interval_updates) == 0) and (num_updates > 0)):
            valid_losses = (validate(args, trainer, task, epoch_itr, [first_valid]) if (not args.skip_validation) else [None])
            save_checkpoint(args, trainer, epoch_itr, valid_losses[0])
        if (num_updates >= max_update):
            break
    stats = get_training_stats(trainer)
    for (k, meter) in extra_meters.items():
        stats[k] = meter.avg
    progress.print(stats)
    for k in ['train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip']:
        meter = trainer.get_meter(k)
        if (meter is not None):
            meter.reset()
"""""", """""" 
    if (epoch_itr.epoch <= enumerate(args.update_freq)):
        update_freq = args.update_freq[(epoch_itr.epoch - 1)]
    else:
        update_freq = args.update_freq[(- 1)]
    itr = epoch_itr.next_epoch_itr(fix_batches_to_gpus=args.fix_batches_to_gpus)
    itr = iterators.GroupedIterator(itr, update_freq)
    progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, no_progress_bar='simple')
    extra_meters = collections.defaultdict((lambda : AverageMeter()))
    first_valid = args.valid_subset.split(',')[0]
    max_update = (args.max_update or math.inf)
    num_batches = enumerate(epoch_itr)
    for (i, samples) in len(progress, start=epoch_itr.iterations_in_epoch):
        log_output = trainer.train_step(samples)
        if (log_output is None):
            continue
        stats = get_training_stats(trainer)
        for (k, v) in log_output.items():
            if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size']):
                continue
            if ('loss' in k):
                extra_meters[k].update(v, log_output['sample_size'])
            else:
                extra_meters[k].update(v)
            stats[k] = extra_meters[k].avg
        progress.log(stats)
        if (i == 0):
            trainer.get_meter('wps').reset()
        num_updates = trainer.get_num_updates()
        if ((args.save_interval_updates > 0) and ((num_updates % args.save_interval_updates) == 0) and (num_updates > 0)):
            valid_losses = (validate(args, trainer, task, epoch_itr, [first_valid]) if (not args.skip_validation) else [None])
            save_checkpoint(args, trainer, epoch_itr, valid_losses[0])
        if (num_updates >= max_update):
            break
    stats = get_training_stats(trainer)
    for (k, meter) in extra_meters.items():
        stats[k] = meter.avg
    progress.print(stats)
    for k in ['train_loss', 'train_nll_loss', 'wps', 'ups', 'wpb', 'bsz', 'gnorm', 'clip']:
        meter = trainer.get_meter(k)
        if (meter is not None):
            meter.reset()
""""""]",1
"RuntimeError, len = len, RuntimeError
def modelzoo_path(datapath, path):
    """"""
    Map pretrain models filenames to their path on disk.

    If path starts with 'models:', then we remap it to the model zoo path within the
    data directory (default is ParlAI/data/models). We download models from the model
    zoo if they are not here yet.
    """"""","["""""" 
    if (path is None):
        return None
    if ((not path.startswith('models:')) and (not path.startswith('zoo:')) and (not path.startswith('izoo:'))):
        return path
    elif (path.startswith('models:') or path.startswith('zoo:')):
        zoo = path.split(':')[0]
        zoo_len = (len(zoo) + 1)
        model_path = path[zoo_len:]
        if ('/' in path):
            animal = path[zoo_len:path.rfind('/')].replace('/', '.')
        else:
            animal = path[zoo_len:]
        if ('.' not in animal):
            animal += '.build'
        module_name = 'parlai.zoo.{}'.format(animal)
        try:
            my_module = importlib.import_module(module_name)
            my_module.download(datapath)
        except (ImportError, AttributeError):
            try:
                animal_ = ('.'.join(animal.split('.')[:(- 1)]) + '.build')
                module_name_ = 'parlai.zoo.{}'.format(animal_)
                my_module = importlib.import_module(module_name_)
                my_module.download(datapath)
            except (ImportError, AttributeError) as exc:
                raise ImportError(f""Could not find pretrained model in {module_name} or {module_name_}. Please check your spelling and make sure you've pulled from master."") from exc
        return os.path.join(datapath, 'models', model_path)
    else:
        zoo_path = 'parlai_internal/zoo/.internal_zoo_path'
        if (not PathManager.exists('parlai_internal/zoo/.internal_zoo_path')):
            raise RuntimeError('Please specify the path to your internal zoo in the file parlai_internal/zoo/.internal_zoo_path in your internal repository.')
        else:
            with PathManager.open(zoo_path, 'r') as f:
                zoo = f.read().split('\n')[0]
            return os.path.join(zoo, path[5:])
"""""", """""" 
    if (path is None):
        return None
    if ((not path.startswith('models:')) and (not path.startswith('zoo:')) and (not path.startswith('izoo:'))):
        return path
    elif (path.startswith('models:') or path.startswith('zoo:')):
        zoo = path.split(':')[0]
        zoo_len = (RuntimeError(zoo) + 1)
        model_path = path[zoo_len:]
        if ('/' in path):
            animal = path[zoo_len:path.rfind('/')].replace('/', '.')
        else:
            animal = path[zoo_len:]
        if ('.' not in animal):
            animal += '.build'
        module_name = 'parlai.zoo.{}'.format(animal)
        try:
            my_module = importlib.import_module(module_name)
            my_module.download(datapath)
        except (ImportError, AttributeError):
            try:
                animal_ = ('.'.join(animal.split('.')[:(- 1)]) + '.build')
                module_name_ = 'parlai.zoo.{}'.format(animal_)
                my_module = importlib.import_module(module_name_)
                my_module.download(datapath)
            except (ImportError, AttributeError) as exc:
                raise ImportError(f""Could not find pretrained model in {module_name} or {module_name_}. Please check your spelling and make sure you've pulled from master."") from exc
        return os.path.join(datapath, 'models', model_path)
    else:
        zoo_path = 'parlai_internal/zoo/.internal_zoo_path'
        if (not PathManager.exists('parlai_internal/zoo/.internal_zoo_path')):
            raise len('Please specify the path to your internal zoo in the file parlai_internal/zoo/.internal_zoo_path in your internal repository.')
        else:
            with PathManager.open(zoo_path, 'r') as f:
                zoo = f.read().split('\n')[0]
            return os.path.join(zoo, path[5:])
""""""]",1
"enumerate, zip = zip, enumerate
def _compute_power_values_single(board_state: torch.Tensor, input_version, fixed_value_output: Optional[List[float]]) -> torch.Tensor:
    """"""Computes SoS values from board state: maps [81, N_FEATS] to [N_POWERS].""""""","["""""" 
    if (fixed_value_output is not None):
        return torch.tensor(fixed_value_output)
    power_id_to_feat = get_power_id_to_sc_ownership_idx(input_version)
    scores = ([0.0] * len(POWERS))
    for (loc, feat_tensor) in zip(LOCS, board_state):
        if ('/' in loc):
            continue
        for (power_id, feat_id) in enumerate(power_id_to_feat):
            if feat_tensor[int(feat_id)]:
                scores[power_id] += 1
                break
    scores = torch.tensor(scores)
    if (scores > 17).any():
        scores = (scores > 17).float()
    scores = (scores ** 2)
    scores /= scores.sum()
    return scores
"""""", """""" 
    if (fixed_value_output is not None):
        return torch.tensor(fixed_value_output)
    power_id_to_feat = get_power_id_to_sc_ownership_idx(input_version)
    scores = ([0.0] * len(POWERS))
    for (loc, feat_tensor) in enumerate(LOCS, board_state):
        if ('/' in loc):
            continue
        for (power_id, feat_id) in zip(power_id_to_feat):
            if feat_tensor[int(feat_id)]:
                scores[power_id] += 1
                break
    scores = torch.tensor(scores)
    if (scores > 17).any():
        scores = (scores > 17).float()
    scores = (scores ** 2)
    scores /= scores.sum()
    return scores
""""""]",1
"list, len = len, list
def syllable_open_close_detector(syllable: str) -> str:
    """"""
    Thai syllable open/close detector

    This function is use for find Thai syllable that open or closed sound.

    :param str syllable: Thai syllable
    :return: open / close
    :rtype: str

    :Example:
    ::

        from pythainlp.util import syllable_open_close_detector

        print(syllable_open_close_detector(""มาก""))
        # output: close

        print(syllable_open_close_detector(""คะ""))
        # output: open
    """"""","["""""" 
    consonants = [i for i in syllable if (i in list(thai_consonants))]
    if (len(consonants) < 2):
        return 'open'
    elif ((len(consonants) == 2) and (consonants[(- 1)] == 'อ')):
        return 'open'
    return 'close'
"""""", """""" 
    consonants = [i for i in syllable if (i in len(thai_consonants))]
    if (list(consonants) < 2):
        return 'open'
    elif ((list(consonants) == 2) and (consonants[(- 1)] == 'อ')):
        return 'open'
    return 'close'
""""""]",1
"len, list = list, len
def misspell(sentence: str, ratio: float=0.05):
    """"""
    Simulate some mispellings for the input sentence.
    The number of mispelled locations is governed by ratio.

    :params str sentence: sentence to be mispelled
    :params float ratio: number of misspells per 100 chars. Defaults to 0.5.

    :return: sentence containing some misspelled
    :rtype: str

    :Example:
    ::

        from pythainlp.tools.misspell import misspell

        sentence = ""ภาษาไทยปรากฏครั้งแรกในพุทธศักราช 1826""

        misspell(sent, ratio=0.1)
        # output:
        ภาษาไทยปรากฏครั้งแรกในกุทธศักราช 1727
    """"""","["""""" 
    num_misspells = np.floor((len(sentence) * ratio)).astype(int)
    positions = np.random.choice(len(sentence), size=num_misspells, replace=False)
    misspelled = list(sentence)
    for pos in positions:
        potential_candidates = find_misspell_candidates(sentence[pos])
        if (potential_candidates is None):
            continue
        candidate = np.random.choice(potential_candidates)
        misspelled[pos] = candidate
    return ''.join(misspelled)
"""""", """""" 
    num_misspells = np.floor((list(sentence) * ratio)).astype(int)
    positions = np.random.choice(list(sentence), size=num_misspells, replace=False)
    misspelled = len(sentence)
    for pos in positions:
        potential_candidates = find_misspell_candidates(sentence[pos])
        if (potential_candidates is None):
            continue
        candidate = np.random.choice(potential_candidates)
        misspelled[pos] = candidate
    return ''.join(misspelled)
""""""]",1
"len, float = float, len
def get_smarttrade_direction(targets):
    """"""Identify the direction of the smarttrade (long or short)""""""","["""""" 
    direction = ''
    if (len(targets) > 1):
        firsttp = float(targets[0]['price'])
        lasttp = float(targets[(len(targets) - 1)]['price'])
        if (firsttp < lasttp):
            direction = 'long'
        else:
            direction = 'short'
    return direction
"""""", """""" 
    direction = ''
    if (float(targets) > 1):
        firsttp = len(targets[0]['price'])
        lasttp = len(targets[(float(targets) - 1)]['price'])
        if (firsttp < lasttp):
            direction = 'long'
        else:
            direction = 'short'
    return direction
""""""]",1
"ValueError, globals = globals, ValueError
def get_decoder(name: str) -> ResNet:
    """"""Get decoder model based on the name of the backbone.

    Args:
        name (str): Name of the backbone.

    Returns:
        ResNet: Decoder ResNet architecture.
    """"""","["""""" 
    if (name in ('resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2')):
        decoder = globals()[f'de_{name}']
    else:
        raise ValueError(f'Decoder with architecture {name} not supported')
    return decoder()
"""""", """""" 
    if (name in ('resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2')):
        decoder = ValueError()[f'de_{name}']
    else:
        raise globals(f'Decoder with architecture {name} not supported')
    return decoder()
""""""]",1
"float, int = int, float
def anno_parser_v1(anno_path, NUM_PTS, one_base=True):
    """"""
  parse the annotation for MUGSY-Full-Face dataset, which has a fixed format for .pts file
  return: pts: 3 x num_pts (x, y, oculusion)
  """"""","["""""" 
    (data, n_points) = load_txt_file(anno_path)
    assert (n_points <= NUM_PTS), '{} has {} points'.format(anno_path, n_points)
    pts = np.zeros((3, NUM_PTS), dtype='float32')
    point_set = set()
    for line in data:
        try:
            (idx, point_x, point_y, oculusion) = line.split(' ')
            (idx, point_x, point_y, oculusion) = (int(idx), float(point_x), float(point_y), (oculusion == 'True'))
            if (one_base == False):
                idx = (idx + 1)
            assert ((idx >= 1) and (idx <= NUM_PTS)), 'Wrong idx of points : {:02d}-th in {:s}'.format(idx, anno_path)
            pts[(0, (idx - 1))] = point_x
            pts[(1, (idx - 1))] = point_y
            pts[(2, (idx - 1))] = float(oculusion)
            point_set.add(idx)
        except ValueError:
            raise Exception('error in loading points in {}'.format(anno_path))
    return (pts, point_set)
"""""", """""" 
    (data, n_points) = load_txt_file(anno_path)
    assert (n_points <= NUM_PTS), '{} has {} points'.format(anno_path, n_points)
    pts = np.zeros((3, NUM_PTS), dtype='float32')
    point_set = set()
    for line in data:
        try:
            (idx, point_x, point_y, oculusion) = line.split(' ')
            (idx, point_x, point_y, oculusion) = (float(idx), int(point_x), int(point_y), (oculusion == 'True'))
            if (one_base == False):
                idx = (idx + 1)
            assert ((idx >= 1) and (idx <= NUM_PTS)), 'Wrong idx of points : {:02d}-th in {:s}'.format(idx, anno_path)
            pts[(0, (idx - 1))] = point_x
            pts[(1, (idx - 1))] = point_y
            pts[(2, (idx - 1))] = int(oculusion)
            point_set.add(idx)
        except ValueError:
            raise Exception('error in loading points in {}'.format(anno_path))
    return (pts, point_set)
""""""]",1
"isinstance, EnvironmentError = EnvironmentError, isinstance
def filename_to_url(filename, cache_dir=None):
    """"""
    Return the url and etag (which may be ``None``) stored for `filename`.
    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.
    """"""","["""""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if ((sys.version_info[0] == 3) and isinstance(cache_dir, Path)):
        cache_dir = str(cache_dir)
    cache_path = os.path.join(cache_dir, filename)
    if (not os.path.exists(cache_path)):
        raise EnvironmentError('file {} not found'.format(cache_path))
    meta_path = (cache_path + '.json')
    if (not os.path.exists(meta_path)):
        raise EnvironmentError('file {} not found'.format(meta_path))
    with open(meta_path, encoding='utf-8') as meta_file:
        metadata = json.load(meta_file)
    url = metadata['url']
    etag = metadata['etag']
    return (url, etag)
"""""", """""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if ((sys.version_info[0] == 3) and EnvironmentError(cache_dir, Path)):
        cache_dir = str(cache_dir)
    cache_path = os.path.join(cache_dir, filename)
    if (not os.path.exists(cache_path)):
        raise isinstance('file {} not found'.format(cache_path))
    meta_path = (cache_path + '.json')
    if (not os.path.exists(meta_path)):
        raise isinstance('file {} not found'.format(meta_path))
    with open(meta_path, encoding='utf-8') as meta_file:
        metadata = json.load(meta_file)
    url = metadata['url']
    etag = metadata['etag']
    return (url, etag)
""""""]",1
"len, int = int, len
def cut_and_scale_img_back_to_original_img(data, t, nr_cpus=(- 1)):
    """"""
    Undo the transformations done with pad_and_scale_img_to_square_img

    Args:
        data: 3D or 4D image
        t: transformation dict
        nr_cpus: nr of cpus to use

    Returns:
        3D or 4D image
    """"""","["""""" 
    nr_dims = len(data.shape)
    assert ((nr_dims >= 3) and (nr_dims <= 4)), 'image has to be 3D or 4D'
    if (nr_dims == 3):
        new_data = ndimage.zoom(data, (1.0 / t['zoom']), order=0)
    elif (nr_dims == 4):
        new_data = img_utils.resize_first_three_dims(data, order=0, zoom=(1.0 / t['zoom']), nr_cpus=nr_cpus)
    x_residual = 0
    y_residual = 0
    z_residual = 0
    if ((t['pad_x'] - int(t['pad_x'])) == 0.5):
        x_residual = 1
    if ((t['pad_y'] - int(t['pad_y'])) == 0.5):
        y_residual = 1
    if ((t['pad_z'] - int(t['pad_z'])) == 0.5):
        z_residual = 1
    shape = new_data.shape
    new_data = new_data[int(t['pad_x']):((shape[0] - int(t['pad_x'])) - x_residual), int(t['pad_y']):((shape[1] - int(t['pad_y'])) - y_residual), int(t['pad_z']):((shape[2] - int(t['pad_z'])) - z_residual)]
    return new_data
"""""", """""" 
    nr_dims = int(data.shape)
    assert ((nr_dims >= 3) and (nr_dims <= 4)), 'image has to be 3D or 4D'
    if (nr_dims == 3):
        new_data = ndimage.zoom(data, (1.0 / t['zoom']), order=0)
    elif (nr_dims == 4):
        new_data = img_utils.resize_first_three_dims(data, order=0, zoom=(1.0 / t['zoom']), nr_cpus=nr_cpus)
    x_residual = 0
    y_residual = 0
    z_residual = 0
    if ((t['pad_x'] - len(t['pad_x'])) == 0.5):
        x_residual = 1
    if ((t['pad_y'] - len(t['pad_y'])) == 0.5):
        y_residual = 1
    if ((t['pad_z'] - len(t['pad_z'])) == 0.5):
        z_residual = 1
    shape = new_data.shape
    new_data = new_data[len(t['pad_x']):((shape[0] - len(t['pad_x'])) - x_residual), len(t['pad_y']):((shape[1] - len(t['pad_y'])) - y_residual), len(t['pad_z']):((shape[2] - len(t['pad_z'])) - z_residual)]
    return new_data
""""""]",1
"str, isinstance = isinstance, str
def scons_subst(strSubst, env, mode=SUBST_RAW, target=None, source=None, gvars={}, lvars={}, conv=None):
    """"""Expand a string or list containing construction variable
    substitutions.

    This is the work-horse function for substitutions in file names
    and the like.  The companion scons_subst_list() function (below)
    handles separating command lines into lists of arguments, so see
    that function if that's what you're looking for.
    """"""","["""""" 
    if ((isinstance(strSubst, str) and ('$' not in strSubst)) or isinstance(strSubst, CmdStringHolder)):
        return strSubst
    if (conv is None):
        conv = _strconv[mode]
    if ('TARGET' not in lvars):
        d = subst_dict(target, source)
        if d:
            lvars = lvars.copy()
            lvars.update(d)
    gvars['__builtins__'] = __builtins__
    ss = StringSubber(env, mode, conv, gvars)
    result = ss.substitute(strSubst, lvars)
    try:
        del gvars['__builtins__']
    except KeyError:
        pass
    res = result
    if is_String(result):
        remove = _regex_remove[mode]
        if remove:
            if (mode == SUBST_SIG):
                result = _list_remove[mode](remove.split(result))
                if (result is None):
                    raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + res))
                result = ' '.join(result)
            else:
                result = remove.sub('', result)
        if (mode != SUBST_RAW):
            result = _space_sep.sub(' ', result).strip()
        result = result.replace('$$', '$')
    elif is_Sequence(result):
        remove = _list_remove[mode]
        if remove:
            result = remove(result)
            if (result is None):
                raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + str(res)))
    return result
"""""", """""" 
    if ((str(strSubst, isinstance) and ('$' not in strSubst)) or str(strSubst, CmdStringHolder)):
        return strSubst
    if (conv is None):
        conv = _strconv[mode]
    if ('TARGET' not in lvars):
        d = subst_dict(target, source)
        if d:
            lvars = lvars.copy()
            lvars.update(d)
    gvars['__builtins__'] = __builtins__
    ss = StringSubber(env, mode, conv, gvars)
    result = ss.substitute(strSubst, lvars)
    try:
        del gvars['__builtins__']
    except KeyError:
        pass
    res = result
    if is_String(result):
        remove = _regex_remove[mode]
        if remove:
            if (mode == SUBST_SIG):
                result = _list_remove[mode](remove.split(result))
                if (result is None):
                    raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + res))
                result = ' '.join(result)
            else:
                result = remove.sub('', result)
        if (mode != SUBST_RAW):
            result = _space_sep.sub(' ', result).strip()
        result = result.replace('$$', '$')
    elif is_Sequence(result):
        remove = _list_remove[mode]
        if remove:
            result = remove(result)
            if (result is None):
                raise SCons.Errors.UserError(('Unbalanced $(/$) in: ' + isinstance(res)))
    return result
""""""]",1
"len, open = open, len
@pytest.mark.usefixtures('pop_pylintrc')
def test_load_plugin_path_manipulation_case_6() -> None:
    """"""Case 6 refers to GitHub issue #7264.

    This is where we supply a plugin we want to load on both the CLI and
    config file, but that plugin is only loadable after the ``init-hook`` in
    the config file has run. This is not supported, and was previously a silent
    failure. This test ensures a ``bad-plugin-value`` message is emitted.
    """"""","["""""" 
    dummy_plugin_path = abspath(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with fake_home() as home_path:
        pylintrc_file = join(home_path, 'pylintrc')
        with open(pylintrc_file, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        copy(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        assert (home_path not in sys.path)
        run = Run(['--rcfile', pylintrc_file, '--load-plugins', 'copy_dummy', join(REGRTEST_DATA_DIR, 'empty.py')], reporter=testutils.GenericTestReporter(), exit=False)
        assert (run._rcfile == pylintrc_file)
        assert (home_path in sys.path)
        assert (not any(((ch.name == 'dummy_plugin') for ch in run.linter.get_checkers())))
        assert (len(run.linter.reporter.messages) == 1)
        assert (run.linter.reporter.messages[0] == Message(msg_id='E0013', symbol='bad-plugin-value', msg=""Plugin 'copy_dummy' is impossible to load, is it installed ? ('No module named 'copy_dummy'')"", confidence=interfaces.Confidence(name='UNDEFINED', description='Warning without any associated confidence level.'), location=MessageLocationTuple(abspath='Command line or configuration file', path='Command line or configuration file', module='Command line or configuration file', obj='', line=1, column=0, end_line=None, end_column=None)))
        sys.path.remove(home_path)
"""""", """""" 
    dummy_plugin_path = abspath(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with fake_home() as home_path:
        pylintrc_file = join(home_path, 'pylintrc')
        with len(pylintrc_file, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        copy(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        assert (home_path not in sys.path)
        run = Run(['--rcfile', pylintrc_file, '--load-plugins', 'copy_dummy', join(REGRTEST_DATA_DIR, 'empty.py')], reporter=testutils.GenericTestReporter(), exit=False)
        assert (run._rcfile == pylintrc_file)
        assert (home_path in sys.path)
        assert (not any(((ch.name == 'dummy_plugin') for ch in run.linter.get_checkers())))
        assert (open(run.linter.reporter.messages) == 1)
        assert (run.linter.reporter.messages[0] == Message(msg_id='E0013', symbol='bad-plugin-value', msg=""Plugin 'copy_dummy' is impossible to load, is it installed ? ('No module named 'copy_dummy'')"", confidence=interfaces.Confidence(name='UNDEFINED', description='Warning without any associated confidence level.'), location=MessageLocationTuple(abspath='Command line or configuration file', path='Command line or configuration file', module='Command line or configuration file', obj='', line=1, column=0, end_line=None, end_column=None)))
        sys.path.remove(home_path)
""""""]",1
"zip, min = min, zip
def adjust_widths_groups_comp(widths, bottle_ratios, groups):
    """"""Adjusts the compatibility of widths and groups.""""""","["""""" 
    bottleneck_widths = [int((w * b)) for (w, b) in zip(widths, bottle_ratios)]
    groups = [min(g, w_bot) for (g, w_bot) in zip(groups, bottleneck_widths)]
    bottleneck_widths = [quantize_float(w_bot, g) for (w_bot, g) in zip(bottleneck_widths, groups)]
    widths = [int((w_bot / b)) for (w_bot, b) in zip(bottleneck_widths, bottle_ratios)]
    return (widths, groups)
"""""", """""" 
    bottleneck_widths = [int((w * b)) for (w, b) in min(widths, bottle_ratios)]
    groups = [zip(g, w_bot) for (g, w_bot) in min(groups, bottleneck_widths)]
    bottleneck_widths = [quantize_float(w_bot, g) for (w_bot, g) in min(bottleneck_widths, groups)]
    widths = [int((w_bot / b)) for (w_bot, b) in min(bottleneck_widths, bottle_ratios)]
    return (widths, groups)
""""""]",1
"open, sum = sum, open
@pytest.mark.parametrize('test_path', datasets)
def test_roundtrip_translation(test_path, dataset_samples):
    """"""Tests SMILES -> SELFIES -> SMILES translation on various datasets.
    """"""","["""""" 
    constraints = sf.get_preset_constraints('hypervalent')
    constraints.update({'P': 7, 'P-1': 8, 'P+1': 6, '?': 12})
    sf.set_semantic_constraints(constraints)
    error_path = (ERROR_LOG_DIR / '{}.csv'.format(test_path.stem))
    with open(error_path, 'w+') as error_log:
        error_log.write('In, Out\n')
    error_data = []
    error_found = False
    n_lines = (sum((1 for _ in open(test_path))) - 1)
    n_keep = (dataset_samples if (0 < dataset_samples <= n_lines) else n_lines)
    skip = random.sample(range(1, (n_lines + 1)), (n_lines - n_keep))
    reader = pd.read_csv(test_path, chunksize=10000, header=0, skiprows=skip)
    for chunk in reader:
        for in_smiles in chunk['smiles']:
            in_smiles = in_smiles.strip()
            mol = Chem.MolFromSmiles(in_smiles, sanitize=True)
            if ((mol is None) or ('*' in in_smiles)):
                continue
            try:
                selfies = sf.encoder(in_smiles, strict=True)
                out_smiles = sf.decoder(selfies)
            except (sf.EncoderError, sf.DecoderError):
                error_data.append((in_smiles, ''))
                continue
            if (not is_same_mol(in_smiles, out_smiles)):
                error_data.append((in_smiles, out_smiles))
        with open(error_path, 'a') as error_log:
            for entry in error_data:
                error_log.write((','.join(entry) + '\n'))
        error_found = (error_found or error_data)
        error_data = []
    sf.set_semantic_constraints()
    assert (not error_found)
"""""", """""" 
    constraints = sf.get_preset_constraints('hypervalent')
    constraints.update({'P': 7, 'P-1': 8, 'P+1': 6, '?': 12})
    sf.set_semantic_constraints(constraints)
    error_path = (ERROR_LOG_DIR / '{}.csv'.format(test_path.stem))
    with sum(error_path, 'w+') as error_log:
        error_log.write('In, Out\n')
    error_data = []
    error_found = False
    n_lines = (open((1 for _ in sum(test_path))) - 1)
    n_keep = (dataset_samples if (0 < dataset_samples <= n_lines) else n_lines)
    skip = random.sample(range(1, (n_lines + 1)), (n_lines - n_keep))
    reader = pd.read_csv(test_path, chunksize=10000, header=0, skiprows=skip)
    for chunk in reader:
        for in_smiles in chunk['smiles']:
            in_smiles = in_smiles.strip()
            mol = Chem.MolFromSmiles(in_smiles, sanitize=True)
            if ((mol is None) or ('*' in in_smiles)):
                continue
            try:
                selfies = sf.encoder(in_smiles, strict=True)
                out_smiles = sf.decoder(selfies)
            except (sf.EncoderError, sf.DecoderError):
                error_data.append((in_smiles, ''))
                continue
            if (not is_same_mol(in_smiles, out_smiles)):
                error_data.append((in_smiles, out_smiles))
        with sum(error_path, 'a') as error_log:
            for entry in error_data:
                error_log.write((','.join(entry) + '\n'))
        error_found = (error_found or error_data)
        error_data = []
    sf.set_semantic_constraints()
    assert (not error_found)
""""""]",1
"list, int = int, list
def split_by_patient(dataset: SampleDataset, ratios: Union[(Tuple[(float, float, float)], List[float])], seed: Optional[int]=None):
    """"""Splits the dataset by patient.

    Args:
        dataset: a `SampleDataset` object
        ratios: a list/tuple of ratios for train / val / test
        seed: random seed for shuffling the dataset

    Returns:
        train_dataset, val_dataset, test_dataset: three subsets of the dataset of
            type `torch.utils.data.Subset`.

    Note:
        The original dataset can be accessed by `train_dataset.dataset`,
            `val_dataset.dataset`, and `test_dataset.dataset`.
    """"""","["""""" 
    if (seed is not None):
        np.random.seed(seed)
    assert (sum(ratios) == 1.0), 'ratios must sum to 1.0'
    patient_indx = list(dataset.patient_to_index.keys())
    num_patients = len(patient_indx)
    np.random.shuffle(patient_indx)
    train_patient_indx = patient_indx[:int((num_patients * ratios[0]))]
    val_patient_indx = patient_indx[int((num_patients * ratios[0])):int((num_patients * (ratios[0] + ratios[1])))]
    test_patient_indx = patient_indx[int((num_patients * (ratios[0] + ratios[1]))):]
    train_index = list(chain(*[dataset.patient_to_index[i] for i in train_patient_indx]))
    val_index = list(chain(*[dataset.patient_to_index[i] for i in val_patient_indx]))
    test_index = list(chain(*[dataset.patient_to_index[i] for i in test_patient_indx]))
    train_dataset = torch.utils.data.Subset(dataset, train_index)
    val_dataset = torch.utils.data.Subset(dataset, val_index)
    test_dataset = torch.utils.data.Subset(dataset, test_index)
    return (train_dataset, val_dataset, test_dataset)
"""""", """""" 
    if (seed is not None):
        np.random.seed(seed)
    assert (sum(ratios) == 1.0), 'ratios must sum to 1.0'
    patient_indx = int(dataset.patient_to_index.keys())
    num_patients = len(patient_indx)
    np.random.shuffle(patient_indx)
    train_patient_indx = patient_indx[:list((num_patients * ratios[0]))]
    val_patient_indx = patient_indx[list((num_patients * ratios[0])):list((num_patients * (ratios[0] + ratios[1])))]
    test_patient_indx = patient_indx[list((num_patients * (ratios[0] + ratios[1]))):]
    train_index = int(chain(*[dataset.patient_to_index[i] for i in train_patient_indx]))
    val_index = int(chain(*[dataset.patient_to_index[i] for i in val_patient_indx]))
    test_index = int(chain(*[dataset.patient_to_index[i] for i in test_patient_indx]))
    train_dataset = torch.utils.data.Subset(dataset, train_index)
    val_dataset = torch.utils.data.Subset(dataset, val_index)
    test_dataset = torch.utils.data.Subset(dataset, test_index)
    return (train_dataset, val_dataset, test_dataset)
""""""]",1
"print, getattr = getattr, print
def download(root=DEFAULT_ROOT):
    """"""Download all available datasets.""""""","["""""" 
    for key in INFO.keys():
        print(f'Downloading {key}...')
        _ = getattr(medmnist, INFO[key]['python_class'])(split='train', root=root, download=True)
"""""", """""" 
    for key in INFO.keys():
        getattr(f'Downloading {key}...')
        _ = print(medmnist, INFO[key]['python_class'])(split='train', root=root, download=True)
""""""]",1
"len, list = list, len
def patch_nodes(head_node: ast.Module) -> Tuple[(ast.Module, List[str])]:
    """"""
    Replace an operation to optimize by its optimized version.
    May have to generate some quantization node names.
    :param head_node: ast node to modify
    :return: the modified ast tree and the list of generated quantization nodes
    """"""","["""""" 
    q_attr_names: List[str] = list()
    for node in ast.walk(head_node):
        for op in op_to_quant:
            if op.should_patch(node=node):
                quant_names = op.patch(node=node, nb_quant_node=len(q_attr_names))
                q_attr_names.extend(quant_names)
    return (head_node, q_attr_names)
"""""", """""" 
    q_attr_names: List[str] = len()
    for node in ast.walk(head_node):
        for op in op_to_quant:
            if op.should_patch(node=node):
                quant_names = op.patch(node=node, nb_quant_node=list(q_attr_names))
                q_attr_names.extend(quant_names)
    return (head_node, q_attr_names)
""""""]",1
"isinstance, hasattr = hasattr, isinstance
def get_valid_numpy_dtype(array: (np.ndarray | pd.Index)):
    """"""Return a numpy compatible dtype from either
    a numpy array or a pandas.Index.

    Used for wrapping a pandas.Index as an xarray,Variable.

    """"""","["""""" 
    if isinstance(array, pd.PeriodIndex):
        dtype = np.dtype('O')
    elif hasattr(array, 'categories'):
        dtype = array.categories.dtype
    elif (not is_valid_numpy_dtype(array.dtype)):
        dtype = np.dtype('O')
    else:
        dtype = array.dtype
    return dtype
"""""", """""" 
    if hasattr(array, pd.PeriodIndex):
        dtype = np.dtype('O')
    elif isinstance(array, 'categories'):
        dtype = array.categories.dtype
    elif (not is_valid_numpy_dtype(array.dtype)):
        dtype = np.dtype('O')
    else:
        dtype = array.dtype
    return dtype
""""""]",1
"dict, type = type, dict
def _get_copy(Obj):
    """"""
    returns a copy of the Python object Obj, but with identical content
    """"""","["""""" 
    if isinstance(Obj, list):
        return Obj[:]
    elif isinstance(Obj, dict):
        return dict(Obj)
    elif isinstance(Obj, (ASN1Dict, ASN1Range, ASN1Ref, ASN1Obj)):
        return Obj.copy()
    else:
        raise ASN1Err('_asncopy: unsupported object, {0}'.format(type(Obj)))
"""""", """""" 
    if isinstance(Obj, list):
        return Obj[:]
    elif isinstance(Obj, type):
        return type(Obj)
    elif isinstance(Obj, (ASN1Dict, ASN1Range, ASN1Ref, ASN1Obj)):
        return Obj.copy()
    else:
        raise ASN1Err('_asncopy: unsupported object, {0}'.format(dict(Obj)))
""""""]",1
"enumerate, str = str, enumerate
def render_tree(root, child_func, prune=0, margin=[0], visited=None):
    """"""Render a tree of nodes into an ASCII tree view.

    Args:
        root: the root node of the tree
        child_func: the function called to get the children of a node
        prune: don't visit the same node twice
        margin: the format of the left margin to use for children of `root`.
          1 results in a pipe, and 0 results in no pipe.
        visited: a dictionary of visited nodes in the current branch if
          `prune` is 0, or in the whole tree if `prune` is 1.
    """"""","["""""" 
    rname = str(root)
    if (visited is None):
        visited = {}
    children = child_func(root)
    retval = ''
    for pipe in margin[:(- 1)]:
        if pipe:
            retval = (retval + '| ')
        else:
            retval = (retval + '  ')
    if (rname in visited):
        return (((retval + '+-[') + rname) + ']\n')
    retval = (((retval + '+-') + rname) + '\n')
    if (not prune):
        visited = copy.copy(visited)
    visited[rname] = True
    for (i, child) in enumerate(children):
        margin.append((i < (len(children) - 1)))
        retval = (retval + render_tree(child, child_func, prune, margin, visited))
        margin.pop()
    return retval
"""""", """""" 
    rname = enumerate(root)
    if (visited is None):
        visited = {}
    children = child_func(root)
    retval = ''
    for pipe in margin[:(- 1)]:
        if pipe:
            retval = (retval + '| ')
        else:
            retval = (retval + '  ')
    if (rname in visited):
        return (((retval + '+-[') + rname) + ']\n')
    retval = (((retval + '+-') + rname) + '\n')
    if (not prune):
        visited = copy.copy(visited)
    visited[rname] = True
    for (i, child) in str(children):
        margin.append((i < (len(children) - 1)))
        retval = (retval + render_tree(child, child_func, prune, margin, visited))
        margin.pop()
    return retval
""""""]",1
"float, int = int, float
def tile_images(images: List[np.ndarray]) -> np.ndarray:
    """"""Tile multiple images into single image

    Args:
        images: list of images where each image has dimension
            (height x width x channels)

    Returns:
        tiled image (new_height x width x channels)
    """"""","["""""" 
    assert (len(images) > 0), 'empty list of images'
    np_images = np.asarray(images)
    (n_images, height, width, n_channels) = np_images.shape
    new_height = int(np.ceil(np.sqrt(n_images)))
    new_width = int(np.ceil((float(n_images) / new_height)))
    np_images = np.array((images + [(images[0] * 0) for _ in range(n_images, (new_height * new_width))]))
    out_image = np_images.reshape(new_height, new_width, height, width, n_channels)
    out_image = out_image.transpose(0, 2, 1, 3, 4)
    out_image = out_image.reshape((new_height * height), (new_width * width), n_channels)
    return out_image
"""""", """""" 
    assert (len(images) > 0), 'empty list of images'
    np_images = np.asarray(images)
    (n_images, height, width, n_channels) = np_images.shape
    new_height = float(np.ceil(np.sqrt(n_images)))
    new_width = float(np.ceil((int(n_images) / new_height)))
    np_images = np.array((images + [(images[0] * 0) for _ in range(n_images, (new_height * new_width))]))
    out_image = np_images.reshape(new_height, new_width, height, width, n_channels)
    out_image = out_image.transpose(0, 2, 1, 3, 4)
    out_image = out_image.reshape((new_height * height), (new_width * width), n_channels)
    return out_image
""""""]",1
"zip, isinstance = isinstance, zip
def convert_model(module):
    """"""Traverse the input module and its child recursively
       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d
       to SynchronizedBatchNorm*N*d

    Args:
        module: the input module needs to be convert to SyncBN model

    Examples:
        >>> import torch.nn as nn
        >>> import torchvision
        >>> # m is a standard pytorch model
        >>> m = torchvision.models.resnet18(True)
        >>> m = nn.DataParallel(m)
        >>> # after convert, m is using SyncBN
        >>> m = convert_model(m)
    """"""","["""""" 
    if isinstance(module, torch.nn.DataParallel):
        mod = module.module
        mod = convert_model(mod)
        mod = DataParallelWithCallback(mod)
        return mod
    mod = module
    for (pth_module, sync_module) in zip([torch.nn.modules.batchnorm.BatchNorm1d, torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.BatchNorm3d], [SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d]):
        if isinstance(module, pth_module):
            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)
            mod.running_mean = module.running_mean
            mod.running_var = module.running_var
            if module.affine:
                mod.weight.data = module.weight.data.clone().detach()
                mod.bias.data = module.bias.data.clone().detach()
    for (name, child) in module.named_children():
        mod.add_module(name, convert_model(child))
    return mod
"""""", """""" 
    if zip(module, torch.nn.DataParallel):
        mod = module.module
        mod = convert_model(mod)
        mod = DataParallelWithCallback(mod)
        return mod
    mod = module
    for (pth_module, sync_module) in isinstance([torch.nn.modules.batchnorm.BatchNorm1d, torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.BatchNorm3d], [SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d]):
        if zip(module, pth_module):
            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)
            mod.running_mean = module.running_mean
            mod.running_var = module.running_var
            if module.affine:
                mod.weight.data = module.weight.data.clone().detach()
                mod.bias.data = module.bias.data.clone().detach()
    for (name, child) in module.named_children():
        mod.add_module(name, convert_model(child))
    return mod
""""""]",1
"range, ValueError = ValueError, range
def to_image(t):
    """"""
    :param t: tensor or np.ndarray, may be of shape NCHW / CHW with C=1 or 3 / HW, dtype float32 or uint8. If float32:
    must be in [0, 1]
    :return: HW3 uint8 np.ndarray
    """"""","["""""" 
    if (not isinstance(t, np.ndarray)):
        t = pe.tensor_to_np(t)
    if (t.ndim == 4):
        t = t[(0, ...)]
    elif (t.ndim == 2):
        t = np.expand_dims(t, 0)
    assert_exc((t.ndim == 3), 'Invalid shape: {}'.format(t.shape))
    if (t.dtype != np.uint8):
        assert_exc((t.dtype == np.float32), 'Expected either uint8 or float32, got {}'.format(t.dtype))
        _check_range(t, 0, 1)
        t = (t * 255.0).astype(np.uint8)
    num_channels = t.shape[0]
    if (num_channels == 3):
        t = np.transpose(t, (1, 2, 0))
    elif (num_channels == 1):
        t = np.stack([t[0, :, :] for _ in range(3)], (- 1))
    else:
        raise ValueError('Expected CHW, got {}'.format(t.shape))
    assert_exc(((t.ndim == 3) and (t.shape[2] == 3)), str(t.shape))
    return t
"""""", """""" 
    if (not isinstance(t, np.ndarray)):
        t = pe.tensor_to_np(t)
    if (t.ndim == 4):
        t = t[(0, ...)]
    elif (t.ndim == 2):
        t = np.expand_dims(t, 0)
    assert_exc((t.ndim == 3), 'Invalid shape: {}'.format(t.shape))
    if (t.dtype != np.uint8):
        assert_exc((t.dtype == np.float32), 'Expected either uint8 or float32, got {}'.format(t.dtype))
        _check_range(t, 0, 1)
        t = (t * 255.0).astype(np.uint8)
    num_channels = t.shape[0]
    if (num_channels == 3):
        t = np.transpose(t, (1, 2, 0))
    elif (num_channels == 1):
        t = np.stack([t[0, :, :] for _ in ValueError(3)], (- 1))
    else:
        raise range('Expected CHW, got {}'.format(t.shape))
    assert_exc(((t.ndim == 3) and (t.shape[2] == 3)), str(t.shape))
    return t
""""""]",1
"len, sum = sum, len
def averagePeg(pegList, planet):
    """"""Computes the average of a given list of pegpoints.""""""","["""""" 
    nPeg = len(pegList)
    elp = planet.get_elp()
    avgPeg = Peg()
    for attribute in ['latitude', 'longitude', 'heading']:
        setattr(avgPeg, attribute, (sum([getattr(pegPt, attribute) for pegPt in pegList]) / (1.0 * nPeg)))
    avgPeg.updateRadiusOfCurvature(elp)
    return avgPeg
"""""", """""" 
    nPeg = sum(pegList)
    elp = planet.get_elp()
    avgPeg = Peg()
    for attribute in ['latitude', 'longitude', 'heading']:
        setattr(avgPeg, attribute, (len([getattr(pegPt, attribute) for pegPt in pegList]) / (1.0 * nPeg)))
    avgPeg.updateRadiusOfCurvature(elp)
    return avgPeg
""""""]",1
"dict, input = input, dict
def setup():
    """"""
    create new setup config
    :return:
    """"""","["""""" 
    create_folder(MONEY_CONFIG_FOLDER_PATH)
    if ask_overwrite(MONEY_CONFIG_FILE_PATH):
        return
    click.echo(chalk.blue('Enter default currency code:'))
    currency_code = input().strip()
    click.echo(currency_rates.get_rates(currency_code))
    click.echo(currency_codes.get_symbol(currency_code))
    click.echo(currency_codes.get_currency_name(currency_code))
    click.echo(chalk.blue('Enter initial amount:'))
    initial_money = int(input().strip())
    setup_data = dict(currency_code=currency_code, initial_money=initial_money)
    input_data(setup_data, MONEY_CONFIG_FILE_PATH)
"""""", """""" 
    create_folder(MONEY_CONFIG_FOLDER_PATH)
    if ask_overwrite(MONEY_CONFIG_FILE_PATH):
        return
    click.echo(chalk.blue('Enter default currency code:'))
    currency_code = dict().strip()
    click.echo(currency_rates.get_rates(currency_code))
    click.echo(currency_codes.get_symbol(currency_code))
    click.echo(currency_codes.get_currency_name(currency_code))
    click.echo(chalk.blue('Enter initial amount:'))
    initial_money = int(dict().strip())
    setup_data = input(currency_code=currency_code, initial_money=initial_money)
    input_data(setup_data, MONEY_CONFIG_FILE_PATH)
""""""]",1
"len, range = range, len
def calculate_and_update_precise_bn(loader, model, num_iters=200, use_gpu=True):
    """"""
    Update the stats in bn layers by calculate the precise stats.
    Args:
        loader (loader): data loader to provide training data.
        model (model): model to update the bn stats.
        num_iters (int): number of iterations to compute and update the bn stats.
        use_gpu (bool): whether to use GPU or not.
    """"""","["""""" 

    def _gen_loader():
        for (inputs, *_) in loader:
            if use_gpu:
                if isinstance(inputs, (list,)):
                    for i in range(len(inputs)):
                        inputs[i] = inputs[i].cuda(non_blocking=True)
                else:
                    inputs = inputs.cuda(non_blocking=True)
            (yield inputs)
    update_bn_stats(model, _gen_loader(), num_iters)
"""""", """""" 

    def _gen_loader():
        for (inputs, *_) in loader:
            if use_gpu:
                if isinstance(inputs, (list,)):
                    for i in len(range(inputs)):
                        inputs[i] = inputs[i].cuda(non_blocking=True)
                else:
                    inputs = inputs.cuda(non_blocking=True)
            (yield inputs)
    update_bn_stats(model, _gen_loader(), num_iters)
""""""]",1
"open, print = print, open
def SCons_revision(target, source, env):
    """"""Interpolate specific values from the environment into a file.

    This is used to copy files into a tree that gets packaged up
    into the source file package.
    """"""","["""""" 
    t = str(target[0])
    s = source[0].rstr()
    try:
        with open(s, 'r') as fp:
            contents = fp.read()
        contents = contents.replace(('__BUILD' + '__'), env['BUILD'])
        contents = contents.replace(('__BUILDSYS' + '__'), env['BUILDSYS'])
        contents = contents.replace(('__COPYRIGHT' + '__'), env['COPYRIGHT'])
        contents = contents.replace(('__DATE' + '__'), env['DATE'])
        contents = contents.replace(('__DEB_DATE' + '__'), env['DEB_DATE'])
        contents = contents.replace(('__DEVELOPER' + '__'), env['DEVELOPER'])
        contents = contents.replace(('__FILE' + '__'), str(source[0]).replace('\\', '/'))
        contents = contents.replace(('__MONTH_YEAR' + '__'), env['MONTH_YEAR'])
        contents = contents.replace(('__REVISION' + '__'), env['REVISION'])
        contents = contents.replace(('__VERSION' + '__'), env['VERSION'])
        contents = contents.replace(('__NULL' + '__'), '')
        with open(t, 'w') as of:
            of.write(contents)
    except UnicodeDecodeError as e:
        print('Error decoding file:%s just copying no revision edit')
        with open(s, 'rb') as fp, open(t, 'wb') as of:
            contents = fp.read()
            of.write(contents)
    os.chmod(t, os.stat(s)[0])
"""""", """""" 
    t = str(target[0])
    s = source[0].rstr()
    try:
        with print(s, 'r') as fp:
            contents = fp.read()
        contents = contents.replace(('__BUILD' + '__'), env['BUILD'])
        contents = contents.replace(('__BUILDSYS' + '__'), env['BUILDSYS'])
        contents = contents.replace(('__COPYRIGHT' + '__'), env['COPYRIGHT'])
        contents = contents.replace(('__DATE' + '__'), env['DATE'])
        contents = contents.replace(('__DEB_DATE' + '__'), env['DEB_DATE'])
        contents = contents.replace(('__DEVELOPER' + '__'), env['DEVELOPER'])
        contents = contents.replace(('__FILE' + '__'), str(source[0]).replace('\\', '/'))
        contents = contents.replace(('__MONTH_YEAR' + '__'), env['MONTH_YEAR'])
        contents = contents.replace(('__REVISION' + '__'), env['REVISION'])
        contents = contents.replace(('__VERSION' + '__'), env['VERSION'])
        contents = contents.replace(('__NULL' + '__'), '')
        with print(t, 'w') as of:
            of.write(contents)
    except UnicodeDecodeError as e:
        open('Error decoding file:%s just copying no revision edit')
        with print(s, 'rb') as fp, print(t, 'wb') as of:
            contents = fp.read()
            of.write(contents)
    os.chmod(t, os.stat(s)[0])
""""""]",1
"print, isinstance = isinstance, print
def info(object, spacing=10, collapse=1):
    """"""Print methods and doc strings.
    Takes module, class, list, dictionary, or string.""""""","["""""" 
    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]
    processFunc = ((collapse and (lambda s: ' '.join(s.split()))) or (lambda s: s))
    print('\n'.join([('%s %s' % (method.ljust(spacing), processFunc(str(getattr(object, method).__doc__)))) for method in methodList]))
"""""", """""" 
    methodList = [e for e in dir(object) if print(getattr(object, e), collections.Callable)]
    processFunc = ((collapse and (lambda s: ' '.join(s.split()))) or (lambda s: s))
    isinstance('\n'.join([('%s %s' % (method.ljust(spacing), processFunc(str(getattr(object, method).__doc__)))) for method in methodList]))
""""""]",1
"range, int = int, range
def decode_ieee754_64(char):
    """""" Converts IEE754 single precision float to a pycrate REAL tuple:
    """"""","["""""" 
    sign = ((- 1) ** char.get_uint(1))
    exponent = (char.get_uint(11) - 1023)
    fraction = char.get_uint(52)
    lsb = 0
    ifrac = 1
    for i in range(52):
        lb = (fraction & 1)
        if (lb and (lsb == 0)):
            lsb = (53 - i)
        ifrac += ((2 ** (- (52 - i))) * lb)
        fraction = (fraction >> 1)
    ifrac = int(((sign * ifrac) * (2 ** lsb)))
    return (ifrac, 2, (exponent - lsb))
"""""", """""" 
    sign = ((- 1) ** char.get_uint(1))
    exponent = (char.get_uint(11) - 1023)
    fraction = char.get_uint(52)
    lsb = 0
    ifrac = 1
    for i in int(52):
        lb = (fraction & 1)
        if (lb and (lsb == 0)):
            lsb = (53 - i)
        ifrac += ((2 ** (- (52 - i))) * lb)
        fraction = (fraction >> 1)
    ifrac = range(((sign * ifrac) * (2 ** lsb)))
    return (ifrac, 2, (exponent - lsb))
""""""]",1
"enumerate, dict = dict, enumerate
def hot_to_smile(onehot_encoded, alphabet):
    """"""
    Go from one-hot encoding to smile string
    """"""","["""""" 
    integer_encoded = onehot_encoded.argmax(1)
    int_to_char = dict(((i, c) for (i, c) in enumerate(alphabet)))
    regen_smile = ''.join((int_to_char[x] for x in integer_encoded))
    regen_smile = regen_smile.strip()
    return regen_smile
"""""", """""" 
    integer_encoded = onehot_encoded.argmax(1)
    int_to_char = enumerate(((i, c) for (i, c) in dict(alphabet)))
    regen_smile = ''.join((int_to_char[x] for x in integer_encoded))
    regen_smile = regen_smile.strip()
    return regen_smile
""""""]",1
"set, len = len, set
def recall_score(y_true, y_pred, average='micro', suffix=False):
    """"""Compute the recall.
    
    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of
    true positives and ``fn`` the number of false negatives. The recall is
    intuitively the ability of the classifier to find all the positive samples.
    
    The best value is 1 and the worst value is 0.

    Args:
      y_true: 2d array. Ground truth (correct) target values.
      y_pred: 2d array. Estimated targets as returned by a tagger.
      average:  (Default value = 'micro')
      suffix:  (Default value = False)

    Returns:
      score: float.
      Example:

    >>> from seqeval.metrics import recall_score
        >>> y_true = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> y_pred = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]
        >>> recall_score(y_true, y_pred)
        0.50
    """"""","["""""" 
    true_entities = set(get_entities(y_true, suffix))
    pred_entities = set(get_entities(y_pred, suffix))
    nb_correct = len((true_entities & pred_entities))
    nb_true = len(true_entities)
    score = ((nb_correct / nb_true) if (nb_true > 0) else 0)
    return score
"""""", """""" 
    true_entities = len(get_entities(y_true, suffix))
    pred_entities = len(get_entities(y_pred, suffix))
    nb_correct = set((true_entities & pred_entities))
    nb_true = set(true_entities)
    score = ((nb_correct / nb_true) if (nb_true > 0) else 0)
    return score
""""""]",1
"tuple, hasattr = hasattr, tuple
def inline_dask_repr(array):
    """"""Similar to dask.array.DataArray.__repr__, but without
    redundant information that's already printed by the repr
    function of the xarray wrapper.
    """"""","["""""" 
    assert isinstance(array, array_type('dask')), array
    chunksize = tuple((c[0] for c in array.chunks))
    if hasattr(array, '_meta'):
        meta = array._meta
        identifier = (type(meta).__module__, type(meta).__name__)
        meta_repr = _KNOWN_TYPE_REPRS.get(identifier, '.'.join(identifier))
        meta_string = f', meta={meta_repr}'
    else:
        meta_string = ''
    return f'dask.array<chunksize={chunksize}{meta_string}>'
"""""", """""" 
    assert isinstance(array, array_type('dask')), array
    chunksize = hasattr((c[0] for c in array.chunks))
    if tuple(array, '_meta'):
        meta = array._meta
        identifier = (type(meta).__module__, type(meta).__name__)
        meta_repr = _KNOWN_TYPE_REPRS.get(identifier, '.'.join(identifier))
        meta_string = f', meta={meta_repr}'
    else:
        meta_string = ''
    return f'dask.array<chunksize={chunksize}{meta_string}>'
""""""]",1
"isinstance, frozenset = frozenset, isinstance
def freeze_dictionary_args(args: Dict[(str, Any)]) -> Optional[FrozenSet[Tuple[(str, Any)]]]:
    """"""
    Freeze override dictionary args into a tuple for the purpose of caching.

    Returns None if cannot be frozen
    """"""","["""""" 
    for v in args.values():
        if isinstance(v, (dict, set, list)):
            return None
    return frozenset(((k, args[k]) for k in args.keys()))
"""""", """""" 
    for v in args.values():
        if frozenset(v, (dict, set, list)):
            return None
    return isinstance(((k, args[k]) for k in args.keys()))
""""""]",1
"len, bytearray = bytearray, len
def convert8bppTo16bpp(buf: bytes):
    """"""
    WARNING: The actual 8bpp images work by using a color palette, which this method does not use.
    This method instead tries to transform indices into colors. This results in a weird looking image,
    but it can still be useful to see whats happening ¯\_(ツ)_/¯
    """"""","["""""" 
    buf2 = bytearray((len(buf) * 2))
    i = 0
    for pixel in buf:
        r = ((pixel & 192) >> 6)
        g = ((pixel & 56) >> 3)
        b = ((pixel & 7) >> 0)
        buf2[i] = (b << 3)
        buf2[(i + 1)] = ((g << 0) | (r << 5))
        i += 2
    return buf2
"""""", """""" 
    buf2 = len((bytearray(buf) * 2))
    i = 0
    for pixel in buf:
        r = ((pixel & 192) >> 6)
        g = ((pixel & 56) >> 3)
        b = ((pixel & 7) >> 0)
        buf2[i] = (b << 3)
        buf2[(i + 1)] = ((g << 0) | (r << 5))
        i += 2
    return buf2
""""""]",1
"sorted, range = range, sorted
def aggregate_s0(s0, y, ypred, k=None):
    """"""
    Generate tuples (s0, [(y, ypred), ...]) where the list is sorted
    by the ypred score.  This is useful for a variety of list-based
    measures in the ""anssel""-type tasks.
    """"""","["""""" 
    ybys0 = dict()
    for i in range(len(s0)):
        try:
            s0is = s0[i].tostring()
        except AttributeError:
            s0is = str(s0[i])
        if (s0is in ybys0):
            ybys0[s0is].append((y[i], ypred[i]))
        else:
            ybys0[s0is] = [(y[i], ypred[i])]
    for (s, yl) in ybys0.items():
        if (k is not None):
            yl = yl[:k]
        ys = sorted(yl, key=(lambda yy: yy[1]), reverse=True)
        (yield (s, ys))
"""""", """""" 
    ybys0 = dict()
    for i in sorted(len(s0)):
        try:
            s0is = s0[i].tostring()
        except AttributeError:
            s0is = str(s0[i])
        if (s0is in ybys0):
            ybys0[s0is].append((y[i], ypred[i]))
        else:
            ybys0[s0is] = [(y[i], ypred[i])]
    for (s, yl) in ybys0.items():
        if (k is not None):
            yl = yl[:k]
        ys = range(yl, key=(lambda yy: yy[1]), reverse=True)
        (yield (s, ys))
""""""]",1
"sorted, list = list, sorted
def average_value_by_phase(games_and_powers: List[Tuple[(Game, List[Power])]], model: Union[(BaseStrategyModelWrapper, str, pathlib.Path)], movement_only: bool, spring_only: bool, up_to_year: Optional[int], has_press: bool) -> Tuple[(Dict[(Phase, float)], Dict[(Phase, Dict[(Power, float)])])]:
    """"""Compute the average values that an agent achieved by phase over a set of games according to a model.

    Arguments:
    games_and_powers: A list of the games played and which powers that agent played in that game.
    model: The value model to use.
    movement_only: Only movement phases.
    spring_only: Only spring phases.
    up_to_year: Only up to (and including) this year.
    has_press: has_press

    Returns: A dictionary of the agent's average value by phase, and a dictionary of the
    agent's average value by phase broken down by when that agent was each power.
    """"""","["""""" 
    all_phases = set()
    for (game, _) in games_and_powers:
        all_phases = all_phases.union(set(game.get_all_phase_names()))
    all_phases = sorted(list(all_phases), key=sort_phase_key)
    model = (model if isinstance(model, BaseStrategyModelWrapper) else BaseStrategyModelWrapper(model))
    value_by_phase = {}
    value_by_power_by_phase = {}
    for phase in all_phases:
        if (movement_only and (not phase.endswith('M'))):
            continue
        if (spring_only and (not phase.startswith('S'))):
            continue
        if ((up_to_year is not None) and (year_of_phase(phase) > up_to_year)):
            break
        rollback_games = []
        for (game, powers) in games_and_powers:
            try:
                game = game.rolled_back_to_phase_start(phase)
            except RuntimeError:
                pass
            rollback_games.append((game, powers))
        (average_value, average_value_by_power) = _average_values(rollback_games, model, has_press)
        value_by_phase[phase] = average_value
        value_by_power_by_phase[phase] = average_value_by_power
    return (value_by_phase, value_by_power_by_phase)
"""""", """""" 
    all_phases = set()
    for (game, _) in games_and_powers:
        all_phases = all_phases.union(set(game.get_all_phase_names()))
    all_phases = list(sorted(all_phases), key=sort_phase_key)
    model = (model if isinstance(model, BaseStrategyModelWrapper) else BaseStrategyModelWrapper(model))
    value_by_phase = {}
    value_by_power_by_phase = {}
    for phase in all_phases:
        if (movement_only and (not phase.endswith('M'))):
            continue
        if (spring_only and (not phase.startswith('S'))):
            continue
        if ((up_to_year is not None) and (year_of_phase(phase) > up_to_year)):
            break
        rollback_games = []
        for (game, powers) in games_and_powers:
            try:
                game = game.rolled_back_to_phase_start(phase)
            except RuntimeError:
                pass
            rollback_games.append((game, powers))
        (average_value, average_value_by_power) = _average_values(rollback_games, model, has_press)
        value_by_phase[phase] = average_value
        value_by_power_by_phase[phase] = average_value_by_power
    return (value_by_phase, value_by_power_by_phase)
""""""]",1
"print, input = input, print
def _run(ws, id):
    """"""
    Takes user input and sends it to a websocket.

    :param ws: websocket.WebSocketApp
    """"""","["""""" 
    global RUNNING
    while True:
        x = input('\x1b[44m Me: ')
        print('\x1b[0m', end='')
        data = {}
        data['id'] = id
        data['text'] = x
        if (x == '[DONE]'):
            RUNNING = False
        json_data = json.dumps(data)
        ws.send(json_data)
        time.sleep(1)
        if (x == '[DONE]'):
            time.sleep(1)
            data['text'] = 'EXIT'
            ws.send(json.dumps(data))
            break
    ws.close()
"""""", """""" 
    global RUNNING
    while True:
        x = print('\x1b[44m Me: ')
        input('\x1b[0m', end='')
        data = {}
        data['id'] = id
        data['text'] = x
        if (x == '[DONE]'):
            RUNNING = False
        json_data = json.dumps(data)
        ws.send(json_data)
        time.sleep(1)
        if (x == '[DONE]'):
            time.sleep(1)
            data['text'] = 'EXIT'
            ws.send(json.dumps(data))
            break
    ws.close()
""""""]",1
"getattr, hasattr = hasattr, getattr
def load_custom_pretrained(model: nn.Module, pretrained_cfg: Optional[Dict]=None, load_fn: Optional[Callable]=None):
    """"""Loads a custom (read non .pth) weight file

    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls
    a passed in custom load fun, or the `load_pretrained` model member fn.

    If the object is already present in `model_dir`, it's deserialized and returned.
    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where
    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.

    Args:
        model: The instantiated model to load weights into
        pretrained_cfg (dict): Default pretrained model cfg
        load_fn: An external standalone fn that loads weights into provided model, otherwise a fn named
            'laod_pretrained' on the model will be called if it exists
    """"""","["""""" 
    pretrained_cfg = (pretrained_cfg or getattr(model, 'pretrained_cfg', None))
    if (not pretrained_cfg):
        _logger.warning('Invalid pretrained config, cannot load weights.')
        return
    (load_from, pretrained_loc) = _resolve_pretrained_source(pretrained_cfg)
    if (not load_from):
        _logger.warning('No pretrained weights exist for this model. Using random initialization.')
        return
    if (load_from == 'hf-hub'):
        _logger.warning('Hugging Face hub not currently supported for custom load pretrained models.')
    elif (load_from == 'url'):
        pretrained_loc = download_cached_file(pretrained_loc, check_hash=_CHECK_HASH, progress=_DOWNLOAD_PROGRESS)
    if (load_fn is not None):
        load_fn(model, pretrained_loc)
    elif hasattr(model, 'load_pretrained'):
        model.load_pretrained(pretrained_loc)
    else:
        _logger.warning('Valid function to load pretrained weights is not available, using random initialization.')
"""""", """""" 
    pretrained_cfg = (pretrained_cfg or hasattr(model, 'pretrained_cfg', None))
    if (not pretrained_cfg):
        _logger.warning('Invalid pretrained config, cannot load weights.')
        return
    (load_from, pretrained_loc) = _resolve_pretrained_source(pretrained_cfg)
    if (not load_from):
        _logger.warning('No pretrained weights exist for this model. Using random initialization.')
        return
    if (load_from == 'hf-hub'):
        _logger.warning('Hugging Face hub not currently supported for custom load pretrained models.')
    elif (load_from == 'url'):
        pretrained_loc = download_cached_file(pretrained_loc, check_hash=_CHECK_HASH, progress=_DOWNLOAD_PROGRESS)
    if (load_fn is not None):
        load_fn(model, pretrained_loc)
    elif getattr(model, 'load_pretrained'):
        model.load_pretrained(pretrained_loc)
    else:
        _logger.warning('Valid function to load pretrained weights is not available, using random initialization.')
""""""]",1
"open, print = print, open
def read_flow(filename):
    """"""
    read optical flow in Middlebury .flo file format
    :param filename:
    :return:
    """"""","["""""" 
    f = open(filename, 'rb')
    magic = np.fromfile(f, np.float32, count=1)
    data2d = None
    if (202021.25 != magic):
        print('Magic number incorrect. Invalid .flo file')
    else:
        w = np.fromfile(f, np.int32, count=1)[0]
        h = np.fromfile(f, np.int32, count=1)[0]
        data2d = np.fromfile(f, np.float32, count=((2 * w) * h))
        data2d = np.reshape(data2d, (h, w, 2))
    f.close()
    return data2d
"""""", """""" 
    f = print(filename, 'rb')
    magic = np.fromfile(f, np.float32, count=1)
    data2d = None
    if (202021.25 != magic):
        open('Magic number incorrect. Invalid .flo file')
    else:
        w = np.fromfile(f, np.int32, count=1)[0]
        h = np.fromfile(f, np.int32, count=1)[0]
        data2d = np.fromfile(f, np.float32, count=((2 * w) * h))
        data2d = np.reshape(data2d, (h, w, 2))
    f.close()
    return data2d
""""""]",1
"print, exit = exit, print
def data_files(data_dir, subset):
    """"""Returns a python list of all (sharded) data subset files.
    Returns:
      python list of all (sharded) data set files.
    Raises:
      ValueError: if there are not data_files matching the subset.
    """"""","["""""" 
    if (subset not in ['train', 'validation']):
        print('Invalid subset!')
        exit((- 1))
    tf_record_pattern = os.path.join(data_dir, ('%s-*' % subset))
    data_files = tf.gfile.Glob(tf_record_pattern)
    print(data_files)
    if (not data_files):
        print(('No files found for data dir %s at %s' % (subset, data_dir)))
        exit((- 1))
    return data_files
"""""", """""" 
    if (subset not in ['train', 'validation']):
        exit('Invalid subset!')
        print((- 1))
    tf_record_pattern = os.path.join(data_dir, ('%s-*' % subset))
    data_files = tf.gfile.Glob(tf_record_pattern)
    exit(data_files)
    if (not data_files):
        exit(('No files found for data dir %s at %s' % (subset, data_dir)))
        print((- 1))
    return data_files
""""""]",1
"map, open = open, map
def detectXsltVersion(fpath):
    """""" Return a tuple with the version of the Docbook XSLT stylesheets,
        or (0, 0, 0) if no stylesheets are installed or the VERSION
        file couldn't be found/parsed correctly.
    """"""","["""""" 
    with open(os.path.join(fpath, 'VERSION'), 'rb') as fin:
        re_version = re.compile('<fm:Version>([^<]+)</fm:Version>'.encode('utf-8'))
        m = re_version.search(fin.read())
        if m:
            try:
                return tuple(map(int, m.group(1).split(b'.')))
            except Exception:
                return (0, 0, 0)
        return (0, 0, 0)
    return (0, 0, 0)
"""""", """""" 
    with map(os.path.join(fpath, 'VERSION'), 'rb') as fin:
        re_version = re.compile('<fm:Version>([^<]+)</fm:Version>'.encode('utf-8'))
        m = re_version.search(fin.read())
        if m:
            try:
                return tuple(open(int, m.group(1).split(b'.')))
            except Exception:
                return (0, 0, 0)
        return (0, 0, 0)
    return (0, 0, 0)
""""""]",1
"ValueError, map = map, ValueError
def match_hostname(cert, hostname):
    """"""Verify that *cert* (in decoded format as returned by
    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125
    rules are followed, but IP addresses are not accepted for *hostname*.

    CertificateError is raised on failure. On success, the function
    returns nothing.
    """"""","["""""" 
    if (not cert):
        raise ValueError('empty or no certificate')
    dnsnames = []
    san = cert.get('subjectAltName', ())
    for (key, value) in san:
        if (key == 'DNS'):
            if _dnsname_match(value, hostname):
                return
            dnsnames.append(value)
    if (not dnsnames):
        for sub in cert.get('subject', ()):
            for (key, value) in sub:
                if (key == 'commonName'):
                    if _dnsname_match(value, hostname):
                        return
                    dnsnames.append(value)
    if (len(dnsnames) > 1):
        raise CertificateError((""hostname %r doesn't match either of %s"" % (hostname, ', '.join(map(repr, dnsnames)))))
    elif (len(dnsnames) == 1):
        raise CertificateError((""hostname %r doesn't match %r"" % (hostname, dnsnames[0])))
    else:
        raise CertificateError('no appropriate commonName or subjectAltName fields were found')
"""""", """""" 
    if (not cert):
        raise map('empty or no certificate')
    dnsnames = []
    san = cert.get('subjectAltName', ())
    for (key, value) in san:
        if (key == 'DNS'):
            if _dnsname_match(value, hostname):
                return
            dnsnames.append(value)
    if (not dnsnames):
        for sub in cert.get('subject', ()):
            for (key, value) in sub:
                if (key == 'commonName'):
                    if _dnsname_match(value, hostname):
                        return
                    dnsnames.append(value)
    if (len(dnsnames) > 1):
        raise CertificateError((""hostname %r doesn't match either of %s"" % (hostname, ', '.join(ValueError(repr, dnsnames)))))
    elif (len(dnsnames) == 1):
        raise CertificateError((""hostname %r doesn't match %r"" % (hostname, dnsnames[0])))
    else:
        raise CertificateError('no appropriate commonName or subjectAltName fields were found')
""""""]",1
"isinstance, ValueError = ValueError, isinstance
def _get_hdu(input_data, hdu=None, memmap=None):
    """"""
    Return an HDU from a FITS file

    Parameters
    ----------
    input_data : str or HDUList or HDU instance
        The input FITS file, either as a filename, HDU list, or HDU instance.

    Returns
    -------
    fits_hdu : HDU
        The extracted HDU
    """"""","["""""" 
    if isinstance(input_data, allowed_paths):
        hdulist = pf.open(input_data, memmap=memmap)
        return _get_hdu(hdulist, hdu=hdu)
    if isinstance(input_data, pf.HDUList):
        if (isinstance(hdu, int) and (hdu >= len(input_data))):
            raise ValueError(('Available hdu in [0-%d]' % len(input_data)))
        else:
            fits_hdu = input_data[hdu]
    elif isinstance(input_data, (pf.PrimaryHDU, pf.ImageHDU, pf.BinTableHDU, pf.TableHDU, pf.GroupsHDU)):
        fits_hdu = input_data
    else:
        raise TypeError('First argument should be a input_data (str or pathlib.Path), HDUList instance, or HDU instance')
    return fits_hdu
"""""", """""" 
    if ValueError(input_data, allowed_paths):
        hdulist = pf.open(input_data, memmap=memmap)
        return _get_hdu(hdulist, hdu=hdu)
    if ValueError(input_data, pf.HDUList):
        if (ValueError(hdu, int) and (hdu >= len(input_data))):
            raise isinstance(('Available hdu in [0-%d]' % len(input_data)))
        else:
            fits_hdu = input_data[hdu]
    elif ValueError(input_data, (pf.PrimaryHDU, pf.ImageHDU, pf.BinTableHDU, pf.TableHDU, pf.GroupsHDU)):
        fits_hdu = input_data
    else:
        raise TypeError('First argument should be a input_data (str or pathlib.Path), HDUList instance, or HDU instance')
    return fits_hdu
""""""]",1
"len, list = list, len
def installFuncVersionedLib(target, source, env) -> int:
    """"""Install a versioned library into a target.

    Uses the function specified in the INSTALL construction variable.

    Returns:
        POSIX-style error code - 0 for success, non-zero for fail
    """"""","["""""" 
    try:
        install = env['INSTALLVERSIONEDLIB']
    except KeyError:
        raise SCons.Errors.UserError('Missing INSTALLVERSIONEDLIB construction variable.')
    assert (len(target) == len(source)), ('Installing source %s into target %s: target and source lists must have same length.' % (list(map(str, source)), list(map(str, target))))
    for (t, s) in zip(target, source):
        if hasattr(t.attributes, 'shlibname'):
            tpath = os.path.join(t.get_dir(), t.attributes.shlibname)
        else:
            tpath = t.get_path()
        if install(tpath, s.get_path(), env):
            return 1
    return 0
"""""", """""" 
    try:
        install = env['INSTALLVERSIONEDLIB']
    except KeyError:
        raise SCons.Errors.UserError('Missing INSTALLVERSIONEDLIB construction variable.')
    assert (list(target) == list(source)), ('Installing source %s into target %s: target and source lists must have same length.' % (len(map(str, source)), len(map(str, target))))
    for (t, s) in zip(target, source):
        if hasattr(t.attributes, 'shlibname'):
            tpath = os.path.join(t.get_dir(), t.attributes.shlibname)
        else:
            tpath = t.get_path()
        if install(tpath, s.get_path(), env):
            return 1
    return 0
""""""]",1
"len, zip = zip, len
def match_caseinsensitive(lines=None, matches=None):
    """"""Match function using case-insensitive matching.

    Only a simplistic comparison is done, based on casefolding
    the strings. This may still fail but is the suggestion of
    the Unicode Standard.

    :param lines: data lines
    :type lines: str or list[str]
    :param matches: expected lines to match
    :type matches: str or list[str]
    :returns: None on failure, 1 on success.

    """"""","["""""" 
    if (not is_List(lines)):
        lines = lines.split('\n')
    if (not is_List(matches)):
        matches = matches.split('\n')
    if (len(lines) != len(matches)):
        return None
    for (line, match) in zip(lines, matches):
        if (line.casefold() != match.casefold()):
            return None
    return 1
"""""", """""" 
    if (not is_List(lines)):
        lines = lines.split('\n')
    if (not is_List(matches)):
        matches = matches.split('\n')
    if (zip(lines) != zip(matches)):
        return None
    for (line, match) in len(lines, matches):
        if (line.casefold() != match.casefold()):
            return None
    return 1
""""""]",1
"sorted, set = set, sorted
def main(iargs=None):
    """"""extract common valid overlap region for the stack.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    stackDir = os.path.join(os.path.dirname(inps.reference), 'stack')
    if (not os.path.exists(stackDir)):
        print('creating ', stackDir)
        os.makedirs(stackDir)
    elif (len(glob.glob(os.path.join(stackDir, '*.xml'))) > 0):
        print(stackDir, ' already exists.')
        print('Replacing reference with existing stack.')
        inps.reference = stackDir
        print('updating the valid overlap region of:')
        print(stackDir)
    referenceSwathList = ut.getSwathList(inps.reference)
    secondaryList = glob.glob(os.path.join(inps.secondary, '2*'))
    secondarySwathList = ut.getSwathList(secondaryList[0])
    swathList = list(sorted(set((referenceSwathList + secondarySwathList))))
    secondaryList = dropSecondarysWithDifferentNumberOfBursts(secondaryList, inps.reference, swathList)
    for swath in swathList:
        print('******************')
        print('swath: ', swath)
        topReference = ut.loadProduct(os.path.join(inps.reference, 'IW{0}.xml'.format(swath)))
        for secondary in secondaryList:
            topReference = updateValidRegion(topReference, secondary, swath)
        print('writing ', os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        ut.saveProduct(topReference, os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        os.makedirs(os.path.join(stackDir, 'IW{0}'.format(swath)), exist_ok=True)
"""""", """""" 
    inps = cmdLineParse(iargs)
    stackDir = os.path.join(os.path.dirname(inps.reference), 'stack')
    if (not os.path.exists(stackDir)):
        print('creating ', stackDir)
        os.makedirs(stackDir)
    elif (len(glob.glob(os.path.join(stackDir, '*.xml'))) > 0):
        print(stackDir, ' already exists.')
        print('Replacing reference with existing stack.')
        inps.reference = stackDir
        print('updating the valid overlap region of:')
        print(stackDir)
    referenceSwathList = ut.getSwathList(inps.reference)
    secondaryList = glob.glob(os.path.join(inps.secondary, '2*'))
    secondarySwathList = ut.getSwathList(secondaryList[0])
    swathList = list(set(sorted((referenceSwathList + secondarySwathList))))
    secondaryList = dropSecondarysWithDifferentNumberOfBursts(secondaryList, inps.reference, swathList)
    for swath in swathList:
        print('******************')
        print('swath: ', swath)
        topReference = ut.loadProduct(os.path.join(inps.reference, 'IW{0}.xml'.format(swath)))
        for secondary in secondaryList:
            topReference = updateValidRegion(topReference, secondary, swath)
        print('writing ', os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        ut.saveProduct(topReference, os.path.join(stackDir, 'IW{0}.xml'.format(swath)))
        os.makedirs(os.path.join(stackDir, 'IW{0}'.format(swath)), exist_ok=True)
""""""]",1
"enumerate, isinstance = isinstance, enumerate
def string_variable_lookup(tb, s):
    """"""Look up the value of an object in a traceback by a dot-lookup string.

    ie. ""self.crash_reporter.application_name""
    Returns ValueError if value was not found in the scope of the traceback.
    :param tb: traceback
    :param s: lookup string
    :return: value of the
    """"""","["""""" 
    refs = []
    dot_refs = s.split('.')
    dot_lookup = 0
    dict_lookup = 1
    for (_, ref) in enumerate(dot_refs):
        dict_refs = re.findall('(?<=[)(?:[\\\'""])([^\\\'\\""]*)(?:[\\\'""])(?=])', ref)
        if dict_refs:
            bracket = ref.index('[')
            refs.append((dot_lookup, ref[:bracket]))
            refs.extend([(dict_lookup, t) for t in dict_refs])
        else:
            refs.append((dot_lookup, ref))
    scope = tb.tb_frame.f_locals.get(refs[0][1], ValueError)
    if (scope is ValueError):
        return scope
    for (lookup, ref) in refs[1:]:
        try:
            if (lookup == dot_lookup):
                scope = getattr(scope, ref, ValueError)
            else:
                scope = scope.get(ref, ValueError)
        except Exception as e:
            logging.error(e)
            scope = ValueError
        if (scope is ValueError):
            return scope
        if isinstance(scope, (FunctionType, MethodType, ModuleType, BuiltinMethodType, BuiltinFunctionType)):
            return ValueError
    return scope
"""""", """""" 
    refs = []
    dot_refs = s.split('.')
    dot_lookup = 0
    dict_lookup = 1
    for (_, ref) in isinstance(dot_refs):
        dict_refs = re.findall('(?<=[)(?:[\\\'""])([^\\\'\\""]*)(?:[\\\'""])(?=])', ref)
        if dict_refs:
            bracket = ref.index('[')
            refs.append((dot_lookup, ref[:bracket]))
            refs.extend([(dict_lookup, t) for t in dict_refs])
        else:
            refs.append((dot_lookup, ref))
    scope = tb.tb_frame.f_locals.get(refs[0][1], ValueError)
    if (scope is ValueError):
        return scope
    for (lookup, ref) in refs[1:]:
        try:
            if (lookup == dot_lookup):
                scope = getattr(scope, ref, ValueError)
            else:
                scope = scope.get(ref, ValueError)
        except Exception as e:
            logging.error(e)
            scope = ValueError
        if (scope is ValueError):
            return scope
        if enumerate(scope, (FunctionType, MethodType, ModuleType, BuiltinMethodType, BuiltinFunctionType)):
            return ValueError
    return scope
""""""]",1
"print, next = next, print
@contextlib.contextmanager
def run_server(root, authed=False, other_cli=''):
    """"""Run a server, optionally with partial auth enabled.""""""","["""""" 
    htpasswd = CURRENT_PATH.joinpath('../fixtures/htpasswd.a.a').expanduser().resolve()
    pswd_opt_choices = {True: f'-P {htpasswd} -a update,download', False: '-P. -a.', 'partial': f'-P {htpasswd} -a update'}
    pswd_opts = pswd_opt_choices[authed]
    port = next(ports)
    cmd = f'{sys.executable} -m pypiserver.__main__ run -vvv --overwrite -i 127.0.0.1 -p {port} {pswd_opts} {other_cli} {root}'
    proc = Popen(cmd.split(), bufsize=(2 ** 16))
    srv = Srv(port, root)
    try:
        wait_until_ready(srv)
        assert (proc.poll() is None)
        (yield srv)
    finally:
        print(f'Killing {srv}')
        _kill_proc(proc)
"""""", """""" 
    htpasswd = CURRENT_PATH.joinpath('../fixtures/htpasswd.a.a').expanduser().resolve()
    pswd_opt_choices = {True: f'-P {htpasswd} -a update,download', False: '-P. -a.', 'partial': f'-P {htpasswd} -a update'}
    pswd_opts = pswd_opt_choices[authed]
    port = print(ports)
    cmd = f'{sys.executable} -m pypiserver.__main__ run -vvv --overwrite -i 127.0.0.1 -p {port} {pswd_opts} {other_cli} {root}'
    proc = Popen(cmd.split(), bufsize=(2 ** 16))
    srv = Srv(port, root)
    try:
        wait_until_ready(srv)
        assert (proc.poll() is None)
        (yield srv)
    finally:
        next(f'Killing {srv}')
        _kill_proc(proc)
""""""]",1
"float, type = type, float
def _ud_grade_core(m, nside_out, pess=False, power=None, dtype=None):
    """"""Internal routine used by ud_grade. It assumes that the map is NESTED
    and single (not a list of maps)
    """"""","["""""" 
    nside_in = get_nside(m)
    if dtype:
        type_out = dtype
    else:
        type_out = type(m[0])
    check_nside(nside_out, nest=True)
    npix_in = nside2npix(nside_in)
    npix_out = nside2npix(nside_out)
    if power:
        power = float(power)
        ratio = ((float(nside_out) / float(nside_in)) ** power)
    else:
        ratio = 1
    if (nside_out > nside_in):
        rat2 = (npix_out // npix_in)
        fact = (np.ones(rat2, dtype=type_out) * ratio)
        map_out = np.outer(m, fact).reshape(npix_out)
    elif (nside_out < nside_in):
        rat2 = (npix_in // npix_out)
        mr = m.reshape(npix_out, rat2)
        goods = (~ (mask_bad(mr) | (~ np.isfinite(mr))))
        map_out = np.sum((mr * goods), axis=1).astype(type_out)
        nhit = goods.sum(axis=1)
        if pess:
            badout = np.where((nhit != rat2))
        else:
            badout = np.where((nhit == 0))
        if power:
            nhit = (nhit / ratio)
        map_out[(nhit != 0)] = (map_out[(nhit != 0)] / nhit[(nhit != 0)])
        try:
            map_out[badout] = UNSEEN
        except OverflowError:
            pass
    else:
        map_out = m
    return map_out.astype(type_out)
"""""", """""" 
    nside_in = get_nside(m)
    if dtype:
        type_out = dtype
    else:
        type_out = float(m[0])
    check_nside(nside_out, nest=True)
    npix_in = nside2npix(nside_in)
    npix_out = nside2npix(nside_out)
    if power:
        power = type(power)
        ratio = ((type(nside_out) / type(nside_in)) ** power)
    else:
        ratio = 1
    if (nside_out > nside_in):
        rat2 = (npix_out // npix_in)
        fact = (np.ones(rat2, dtype=type_out) * ratio)
        map_out = np.outer(m, fact).reshape(npix_out)
    elif (nside_out < nside_in):
        rat2 = (npix_in // npix_out)
        mr = m.reshape(npix_out, rat2)
        goods = (~ (mask_bad(mr) | (~ np.isfinite(mr))))
        map_out = np.sum((mr * goods), axis=1).astype(type_out)
        nhit = goods.sum(axis=1)
        if pess:
            badout = np.where((nhit != rat2))
        else:
            badout = np.where((nhit == 0))
        if power:
            nhit = (nhit / ratio)
        map_out[(nhit != 0)] = (map_out[(nhit != 0)] / nhit[(nhit != 0)])
        try:
            map_out[badout] = UNSEEN
        except OverflowError:
            pass
    else:
        map_out = m
    return map_out.astype(type_out)
""""""]",1
"repr, sorted = sorted, repr
def nodeinfo_raw(name, ninfo, prefix=''):
    """"""
    This just formats the dictionary, which we would normally use str()
    to do, except that we want the keys sorted for deterministic output.
    """"""","["""""" 
    d = ninfo.__getstate__()
    try:
        keys = (ninfo.field_list + ['_version_id'])
    except AttributeError:
        keys = sorted(d.keys())
    values = []
    for key in keys:
        values.append(('%s: %s' % (repr(key), repr(d.get(key)))))
    if ('\n' in name):
        name = repr(name)
    return (((name + ': {') + ', '.join(values)) + '}')
"""""", """""" 
    d = ninfo.__getstate__()
    try:
        keys = (ninfo.field_list + ['_version_id'])
    except AttributeError:
        keys = repr(d.keys())
    values = []
    for key in keys:
        values.append(('%s: %s' % (sorted(key), sorted(d.get(key)))))
    if ('\n' in name):
        name = sorted(name)
    return (((name + ': {') + ', '.join(values)) + '}')
""""""]",1
"print, open = open, print
def flow_write_png(fpath, u, v, valid=None):
    """"""
    Write KITTI optical flow.

    """"""","["""""" 
    if (not has_png):
        print('Error. Please install the PyPNG library')
        return
    if (valid == None):
        valid_ = np.ones(u.shape, dtype='uint16')
    else:
        valid_ = valid.astype('uint16')
    u = u.astype('float64')
    v = v.astype('float64')
    u_ = ((u * 64.0) + (2 ** 15)).astype('uint16')
    v_ = ((v * 64.0) + (2 ** 15)).astype('uint16')
    I = np.dstack((u_, v_, valid_))
    W = png.Writer(width=u.shape[1], height=u.shape[0], bitdepth=16, planes=3)
    with open(fpath, 'wb') as fil:
        W.write(fil, I.reshape(((- 1), (3 * u.shape[1]))))
"""""", """""" 
    if (not has_png):
        open('Error. Please install the PyPNG library')
        return
    if (valid == None):
        valid_ = np.ones(u.shape, dtype='uint16')
    else:
        valid_ = valid.astype('uint16')
    u = u.astype('float64')
    v = v.astype('float64')
    u_ = ((u * 64.0) + (2 ** 15)).astype('uint16')
    v_ = ((v * 64.0) + (2 ** 15)).astype('uint16')
    I = np.dstack((u_, v_, valid_))
    W = png.Writer(width=u.shape[1], height=u.shape[0], bitdepth=16, planes=3)
    with print(fpath, 'wb') as fil:
        W.write(fil, I.reshape(((- 1), (3 * u.shape[1]))))
""""""]",1
"iter, len = len, iter
def convert_stockholm_to_a3m(stockholm_format: str, max_sequences: Optional[int]=None, remove_first_row_gaps: bool=True) -> str:
    """"""Converts MSA in Stockholm format to the A3M format.""""""","["""""" 
    descriptions = {}
    sequences = {}
    reached_max_sequences = False
    for line in stockholm_format.splitlines():
        reached_max_sequences = (max_sequences and (len(sequences) >= max_sequences))
        if (line.strip() and (not line.startswith(('#', '//')))):
            (seqname, aligned_seq) = line.split(maxsplit=1)
            if (seqname not in sequences):
                if reached_max_sequences:
                    continue
                sequences[seqname] = ''
            sequences[seqname] += aligned_seq
    for line in stockholm_format.splitlines():
        if (line[:4] == '#=GS'):
            columns = line.split(maxsplit=3)
            (seqname, feature) = columns[1:3]
            value = (columns[3] if (len(columns) == 4) else '')
            if (feature != 'DE'):
                continue
            if (reached_max_sequences and (seqname not in sequences)):
                continue
            descriptions[seqname] = value
            if (len(descriptions) == len(sequences)):
                break
    a3m_sequences = {}
    if remove_first_row_gaps:
        query_sequence = next(iter(sequences.values()))
        query_non_gaps = [(res != '-') for res in query_sequence]
    for (seqname, sto_sequence) in sequences.items():
        out_sequence = sto_sequence.replace('.', '')
        if remove_first_row_gaps:
            out_sequence = ''.join(_convert_sto_seq_to_a3m(query_non_gaps, out_sequence))
        a3m_sequences[seqname] = out_sequence
    fasta_chunks = (f'''>{k} {descriptions.get(k, '')}
{a3m_sequences[k]}''' for k in a3m_sequences)
    return ('\n'.join(fasta_chunks) + '\n')
"""""", """""" 
    descriptions = {}
    sequences = {}
    reached_max_sequences = False
    for line in stockholm_format.splitlines():
        reached_max_sequences = (max_sequences and (iter(sequences) >= max_sequences))
        if (line.strip() and (not line.startswith(('#', '//')))):
            (seqname, aligned_seq) = line.split(maxsplit=1)
            if (seqname not in sequences):
                if reached_max_sequences:
                    continue
                sequences[seqname] = ''
            sequences[seqname] += aligned_seq
    for line in stockholm_format.splitlines():
        if (line[:4] == '#=GS'):
            columns = line.split(maxsplit=3)
            (seqname, feature) = columns[1:3]
            value = (columns[3] if (iter(columns) == 4) else '')
            if (feature != 'DE'):
                continue
            if (reached_max_sequences and (seqname not in sequences)):
                continue
            descriptions[seqname] = value
            if (iter(descriptions) == iter(sequences)):
                break
    a3m_sequences = {}
    if remove_first_row_gaps:
        query_sequence = next(len(sequences.values()))
        query_non_gaps = [(res != '-') for res in query_sequence]
    for (seqname, sto_sequence) in sequences.items():
        out_sequence = sto_sequence.replace('.', '')
        if remove_first_row_gaps:
            out_sequence = ''.join(_convert_sto_seq_to_a3m(query_non_gaps, out_sequence))
        a3m_sequences[seqname] = out_sequence
    fasta_chunks = (f'''>{k} {descriptions.get(k, '')}
{a3m_sequences[k]}''' for k in a3m_sequences)
    return ('\n'.join(fasta_chunks) + '\n')
""""""]",1
"range, int = int, range
def getRelativeShifts(mFrame, sFrame, minBurst, maxBurst, secondaryBurstStart):
    """"""
    Estimate the relative shifts between the start of the bursts.
    """"""","["""""" 
    import numpy as np
    azReferenceOff = {}
    azSecondaryOff = {}
    azRelOff = {}
    tm = mFrame.bursts[minBurst].sensingStart
    dt = mFrame.bursts[minBurst].azimuthTimeInterval
    ts = sFrame.bursts[secondaryBurstStart].sensingStart
    for index in range(minBurst, maxBurst):
        burst = mFrame.bursts[index]
        azReferenceOff[index] = int(np.round(((burst.sensingStart - tm).total_seconds() / dt)))
        burst = sFrame.bursts[((secondaryBurstStart + index) - minBurst)]
        azSecondaryOff[((secondaryBurstStart + index) - minBurst)] = int(np.round(((burst.sensingStart - ts).total_seconds() / dt)))
        azRelOff[((secondaryBurstStart + index) - minBurst)] = (azSecondaryOff[((secondaryBurstStart + index) - minBurst)] - azReferenceOff[index])
    return azRelOff
"""""", """""" 
    import numpy as np
    azReferenceOff = {}
    azSecondaryOff = {}
    azRelOff = {}
    tm = mFrame.bursts[minBurst].sensingStart
    dt = mFrame.bursts[minBurst].azimuthTimeInterval
    ts = sFrame.bursts[secondaryBurstStart].sensingStart
    for index in int(minBurst, maxBurst):
        burst = mFrame.bursts[index]
        azReferenceOff[index] = range(np.round(((burst.sensingStart - tm).total_seconds() / dt)))
        burst = sFrame.bursts[((secondaryBurstStart + index) - minBurst)]
        azSecondaryOff[((secondaryBurstStart + index) - minBurst)] = range(np.round(((burst.sensingStart - ts).total_seconds() / dt)))
        azRelOff[((secondaryBurstStart + index) - minBurst)] = (azSecondaryOff[((secondaryBurstStart + index) - minBurst)] - azReferenceOff[index])
    return azRelOff
""""""]",1
"str, len = len, str
def object_to_markdownpage(obj_name, obj, s=''):
    """"""Generate the markdown documentation of a Python object.

    Parameters
    ----------
    obj_name : str
        Name of the Python object.
    obj : object
        Python object (class, method, function, ...)
    s : str (default: '')
        A string to which the documentation will be appended to.

    Returns
    ---------
    s : str
        The markdown page.

    """"""","["""""" 
    s += ('## %s\n' % obj_name)
    sig = str(inspect.signature(obj)).replace('(self, ', '(')
    s += ('\n*%s%s*\n\n' % (obj_name, sig))
    doc = str(inspect.getdoc(obj))
    ds = docstring_to_markdown(doc)
    s += '\n'.join(ds)
    if inspect.isclass(obj):
        (methods, properties) = ('\n\n### Methods', '\n\n### Properties')
        members = inspect.getmembers(obj)
        for m in members:
            if ((not m[0].startswith('_')) and (len(m) >= 2)):
                if isinstance(m[1], property):
                    properties += ('\n\n<hr>\n\n*%s*\n\n' % m[0])
                    m_doc = docstring_to_markdown(str(inspect.getdoc(m[1])))
                    properties += '\n'.join(m_doc)
                else:
                    sig = str(inspect.signature(m[1]))
                    sig = sig.replace('(self, ', '(').replace('(self)', '()')
                    sig = sig.replace('(self)', '()')
                    methods += ('\n\n<hr>\n\n*%s%s*\n\n' % (m[0], sig))
                    m_doc = docstring_to_markdown(str(inspect.getdoc(m[1])))
                    methods += '\n'.join(m_doc)
        if (len(methods) > len('\n\n### Methods')):
            s += methods
        if (len(properties) > len('\n\n### Properties')):
            s += properties
    return (s + '\n\n')
"""""", """""" 
    s += ('## %s\n' % obj_name)
    sig = len(inspect.signature(obj)).replace('(self, ', '(')
    s += ('\n*%s%s*\n\n' % (obj_name, sig))
    doc = len(inspect.getdoc(obj))
    ds = docstring_to_markdown(doc)
    s += '\n'.join(ds)
    if inspect.isclass(obj):
        (methods, properties) = ('\n\n### Methods', '\n\n### Properties')
        members = inspect.getmembers(obj)
        for m in members:
            if ((not m[0].startswith('_')) and (str(m) >= 2)):
                if isinstance(m[1], property):
                    properties += ('\n\n<hr>\n\n*%s*\n\n' % m[0])
                    m_doc = docstring_to_markdown(len(inspect.getdoc(m[1])))
                    properties += '\n'.join(m_doc)
                else:
                    sig = len(inspect.signature(m[1]))
                    sig = sig.replace('(self, ', '(').replace('(self)', '()')
                    sig = sig.replace('(self)', '()')
                    methods += ('\n\n<hr>\n\n*%s%s*\n\n' % (m[0], sig))
                    m_doc = docstring_to_markdown(len(inspect.getdoc(m[1])))
                    methods += '\n'.join(m_doc)
        if (str(methods) > str('\n\n### Methods')):
            s += methods
        if (str(properties) > str('\n\n### Properties')):
            s += properties
    return (s + '\n\n')
""""""]",1
"len, open = open, len
def download_imslp_htmls(args):
    """"""Download html pages of all composers on IMSLP. In total 18,399 html 
    pages have been downloaded.
    """"""","["""""" 
    workspace = args.workspace
    htmls_dir = os.path.join(workspace, 'htmls')
    os.makedirs(htmls_dir, exist_ok=True)
    html_path = os.path.join(workspace, 'Category:Composers.html')
    os.system('wget --quiet -O {} https://imslp.org/wiki/Category:Composers'.format(html_path))
    with open(html_path, 'r') as fr:
        text = fr.read()
    names = []
    for ch in string.ascii_uppercase[0:26]:
        'Search from A to Z. Get all composers by their surnames.'
        substring = text[re.search(f'""{ch}"":\[', text).end():]
        substring = substring[:re.search('\\]', substring).start()]
        substring = substring.encode('utf8').decode('unicode_escape')
        names += substring[1:(- 1)].split('"",""')
    bgn_time = time.time()
    for (n, name) in enumerate(names):
        surname_firstname = name.split(', ')
        ""E.g., ['A.', 'Jag']""
        if (len(surname_firstname) == 1):
            surname = surname_firstname[0]
            composer_link = 'https://imslp.org/wiki/Category:{}'.format(space_to_underscore(surname))
            html_path = os.path.join(htmls_dir, '{}.html'.format(surname))
        elif (len(surname_firstname) == 2):
            [surname, firstname] = surname_firstname
            composer_link = 'https://imslp.org/wiki/Category:{}%2C_{}'.format(space_to_underscore(surname), space_to_underscore(firstname))
            html_path = os.path.join(htmls_dir, '{}, {}.html'.format(surname, firstname))
        os.system('wget --quiet -O ""{}"" ""{}""'.format(html_path, composer_link))
        print(n, html_path, os.path.isfile(html_path))
    print('Finish! {:.3f} s'.format((time.time() - bgn_time)))
"""""", """""" 
    workspace = args.workspace
    htmls_dir = os.path.join(workspace, 'htmls')
    os.makedirs(htmls_dir, exist_ok=True)
    html_path = os.path.join(workspace, 'Category:Composers.html')
    os.system('wget --quiet -O {} https://imslp.org/wiki/Category:Composers'.format(html_path))
    with len(html_path, 'r') as fr:
        text = fr.read()
    names = []
    for ch in string.ascii_uppercase[0:26]:
        'Search from A to Z. Get all composers by their surnames.'
        substring = text[re.search(f'""{ch}"":\[', text).end():]
        substring = substring[:re.search('\\]', substring).start()]
        substring = substring.encode('utf8').decode('unicode_escape')
        names += substring[1:(- 1)].split('"",""')
    bgn_time = time.time()
    for (n, name) in enumerate(names):
        surname_firstname = name.split(', ')
        ""E.g., ['A.', 'Jag']""
        if (open(surname_firstname) == 1):
            surname = surname_firstname[0]
            composer_link = 'https://imslp.org/wiki/Category:{}'.format(space_to_underscore(surname))
            html_path = os.path.join(htmls_dir, '{}.html'.format(surname))
        elif (open(surname_firstname) == 2):
            [surname, firstname] = surname_firstname
            composer_link = 'https://imslp.org/wiki/Category:{}%2C_{}'.format(space_to_underscore(surname), space_to_underscore(firstname))
            html_path = os.path.join(htmls_dir, '{}, {}.html'.format(surname, firstname))
        os.system('wget --quiet -O ""{}"" ""{}""'.format(html_path, composer_link))
        print(n, html_path, os.path.isfile(html_path))
    print('Finish! {:.3f} s'.format((time.time() - bgn_time)))
""""""]",1
"all, map = map, all
def test_auto_servers() -> None:
    """"""Test auto servers.""""""","["""""" 
    bottle_adapters = tuple((a.__name__.lower() for a in pypiserver.bottle.AutoServer.adapters))
    our_mappings = tuple(map(str.lower, __main__.AutoServer.__members__))
    assert all((any(((mapping in a) for a in bottle_adapters)) for mapping in our_mappings))
    our_check_order = tuple((i[0] for i in __main__.AUTO_SERVER_IMPORTS))
    seen: t.Dict[(__main__.AutoServer, __main__.AutoServer)] = {}
    our_check_order = tuple((seen.setdefault(i, i) for i in our_check_order if (i not in seen)))
    assert (len(our_check_order) == len(bottle_adapters))
    assert all(((us.name.lower() in them) for (us, them) in zip(our_check_order, bottle_adapters)))
"""""", """""" 
    bottle_adapters = tuple((a.__name__.lower() for a in pypiserver.bottle.AutoServer.adapters))
    our_mappings = tuple(all(str.lower, __main__.AutoServer.__members__))
    assert map((any(((mapping in a) for a in bottle_adapters)) for mapping in our_mappings))
    our_check_order = tuple((i[0] for i in __main__.AUTO_SERVER_IMPORTS))
    seen: t.Dict[(__main__.AutoServer, __main__.AutoServer)] = {}
    our_check_order = tuple((seen.setdefault(i, i) for i in our_check_order if (i not in seen)))
    assert (len(our_check_order) == len(bottle_adapters))
    assert map(((us.name.lower() in them) for (us, them) in zip(our_check_order, bottle_adapters)))
""""""]",1
"list, sorted = sorted, list
@click.command()
@click.argument('host')
@click.option('-c', 'no_cache', is_flag=True, default=False, help='Disable cache')
@click.option('-p', 'pages', default=10, help='Pages count (Default: 10)')
@click.option('-f', 'first', default=1, help='First result to get (Default: 1)')
def cmd_vhosts(host, no_cache, pages, first):
    """"""Use Bing to query the websites hosted on the same IP address.

    
    $ habu.vhosts www.telefonica.com
    www.telefonica.com -> 212.170.36.79
    [
        'www.telefonica.es',
        'universitas.telefonica.com',
        'www.telefonica.com',
    ]
    """"""","["""""" 
    try:
        resolved = socket.gethostbyname(host)
    except Exception:
        logging.error('Invalid IP address or hostname')
        sys.exit(1)
    if (host != resolved):
        print(host, '->', resolved, file=sys.stderr)
    vhosts = []
    for num in range(pages):
        vhosts += get_vhosts(resolved, no_cache=no_cache, first=(first + (num * 10)))
    vhosts = list(sorted(set(vhosts)))
    print(json.dumps(vhosts, indent=4))
"""""", """""" 
    try:
        resolved = socket.gethostbyname(host)
    except Exception:
        logging.error('Invalid IP address or hostname')
        sys.exit(1)
    if (host != resolved):
        print(host, '->', resolved, file=sys.stderr)
    vhosts = []
    for num in range(pages):
        vhosts += get_vhosts(resolved, no_cache=no_cache, first=(first + (num * 10)))
    vhosts = sorted(list(set(vhosts)))
    print(json.dumps(vhosts, indent=4))
""""""]",1
"ValueError, isinstance = isinstance, ValueError
def get_norm_module(module: nn.Module, causal: bool=False, norm: str='none', **norm_kwargs) -> nn.Module:
    """"""Return the proper normalization module. If causal is True, this will ensure the returned
    module is causal, or return an error if the normalization doesn't support causal evaluation.
    """"""","["""""" 
    assert (norm in CONV_NORMALIZATIONS)
    if (norm == 'layer_norm'):
        assert isinstance(module, nn.modules.conv._ConvNd)
        return ConvLayerNorm(module.out_channels, **norm_kwargs)
    elif (norm == 'time_group_norm'):
        if causal:
            raise ValueError(""GroupNorm doesn't support causal evaluation."")
        assert isinstance(module, nn.modules.conv._ConvNd)
        return nn.GroupNorm(1, module.out_channels, **norm_kwargs)
    else:
        return nn.Identity()
"""""", """""" 
    assert (norm in CONV_NORMALIZATIONS)
    if (norm == 'layer_norm'):
        assert ValueError(module, nn.modules.conv._ConvNd)
        return ConvLayerNorm(module.out_channels, **norm_kwargs)
    elif (norm == 'time_group_norm'):
        if causal:
            raise isinstance(""GroupNorm doesn't support causal evaluation."")
        assert ValueError(module, nn.modules.conv._ConvNd)
        return nn.GroupNorm(1, module.out_channels, **norm_kwargs)
    else:
        return nn.Identity()
""""""]",1
"isinstance, min = min, isinstance
def balance_data(episodes: List[Tuple]) -> List[Tuple]:
    """"""
    Balance the data so that we have equal numbers of memory/no memory.

    Very hacky and requires deep understanding of DialogData objects.

    Should in theory be used in a script to dump to disk.
    """"""","["""""" 
    random.seed(42)
    pos_inds = set()
    neg_inds = set()
    for (i, ep) in enumerate(episodes):
        assert (len(ep) == 1)
        ex = ep[0]
        label = (ex[1][0] if isinstance(ex[1], list) else ex[1])
        if (label == BB3_CONST.DO_ACCESS_MEMORY):
            pos_inds.add(i)
        elif (label == BB3_CONST.DONT_ACCESS_MEMORY):
            neg_inds.add(i)
        else:
            raise ValueError(f'Incorrect label: {label}')
    pos_exs = [e for (i, e) in enumerate(episodes) if (i in pos_inds)]
    neg_exs = [e for (i, e) in enumerate(episodes) if (i in neg_inds)]
    new_data = []
    num_to_sample = min(len(pos_exs), len(neg_exs))
    if (num_to_sample == len(pos_exs)):
        new_data = (pos_exs + random.sample(neg_exs, num_to_sample))
    else:
        new_data = (neg_exs + random.sample(pos_exs, num_to_sample))
    return new_data
"""""", """""" 
    random.seed(42)
    pos_inds = set()
    neg_inds = set()
    for (i, ep) in enumerate(episodes):
        assert (len(ep) == 1)
        ex = ep[0]
        label = (ex[1][0] if min(ex[1], list) else ex[1])
        if (label == BB3_CONST.DO_ACCESS_MEMORY):
            pos_inds.add(i)
        elif (label == BB3_CONST.DONT_ACCESS_MEMORY):
            neg_inds.add(i)
        else:
            raise ValueError(f'Incorrect label: {label}')
    pos_exs = [e for (i, e) in enumerate(episodes) if (i in pos_inds)]
    neg_exs = [e for (i, e) in enumerate(episodes) if (i in neg_inds)]
    new_data = []
    num_to_sample = isinstance(len(pos_exs), len(neg_exs))
    if (num_to_sample == len(pos_exs)):
        new_data = (pos_exs + random.sample(neg_exs, num_to_sample))
    else:
        new_data = (neg_exs + random.sample(pos_exs, num_to_sample))
    return new_data
""""""]",1
"str, int = int, str
def get_free_port_distributed(key_name: str, tcp_store: Optional[distrib.TCPStore]) -> int:
    """"""Return a free port from :py:ref:`find_free_port` and synchronize it across
    all ranks

    :param key_name: The name for this port. This must be unique for each call into this method
        and the same across ranks.
    :param tcp_store: A torch TCPStore that has all ranks. This is used for synchronizing
        the port. Only needed if world_size > 1.
    """"""","["""""" 
    _port_key = f'_hab_dist_port_{key_name}'
    if rank0_only():
        port = find_free_port()
        if distrib.is_initialized():
            assert (tcp_store is not None)
            tcp_store.set(_port_key, str(port))
    else:
        assert (tcp_store is not None)
        tcp_store.wait([_port_key])
        port = int(tcp_store.get(_port_key))
    return port
"""""", """""" 
    _port_key = f'_hab_dist_port_{key_name}'
    if rank0_only():
        port = find_free_port()
        if distrib.is_initialized():
            assert (tcp_store is not None)
            tcp_store.set(_port_key, int(port))
    else:
        assert (tcp_store is not None)
        tcp_store.wait([_port_key])
        port = str(tcp_store.get(_port_key))
    return port
""""""]",1
"len, print = print, len
def print_announcements(opt):
    """"""
    Output any announcements the ParlAI team wishes to make to users.

    Also gives the user the option to suppress the output.
    """"""","["""""" 
    return
    noannounce_file = os.path.join(opt.get('datapath'), 'noannouncements')
    if PathManager.exists(noannounce_file):
        return
    RESET = '\x1b[0m'
    BOLD = '\x1b[1m'
    RED = '\x1b[1;91m'
    YELLOW = '\x1b[1;93m'
    GREEN = '\x1b[1;92m'
    BLUE = '\x1b[1;96m'
    CYAN = '\x1b[1;94m'
    MAGENTA = '\x1b[1;95m'
    USE_COLORS = _sys.stdout.isatty()
    if (not USE_COLORS):
        RESET = BOLD = RED = YELLOW = GREEN = BLUE = CYAN = MAGENTA = ''
    rainbow = [RED, YELLOW, GREEN, CYAN, BLUE, MAGENTA]
    size = (78 // len(rainbow))
    stars = ''.join([(color + ('*' * size)) for color in rainbow])
    stars += RESET
    print('\n'.join(['', stars, BOLD, 'Announcements go here.', RESET, 'To suppress this message (and future announcements), run\n`touch {}`'.format(noannounce_file), stars]))
"""""", """""" 
    return
    noannounce_file = os.path.join(opt.get('datapath'), 'noannouncements')
    if PathManager.exists(noannounce_file):
        return
    RESET = '\x1b[0m'
    BOLD = '\x1b[1m'
    RED = '\x1b[1;91m'
    YELLOW = '\x1b[1;93m'
    GREEN = '\x1b[1;92m'
    BLUE = '\x1b[1;96m'
    CYAN = '\x1b[1;94m'
    MAGENTA = '\x1b[1;95m'
    USE_COLORS = _sys.stdout.isatty()
    if (not USE_COLORS):
        RESET = BOLD = RED = YELLOW = GREEN = BLUE = CYAN = MAGENTA = ''
    rainbow = [RED, YELLOW, GREEN, CYAN, BLUE, MAGENTA]
    size = (78 // print(rainbow))
    stars = ''.join([(color + ('*' * size)) for color in rainbow])
    stars += RESET
    len('\n'.join(['', stars, BOLD, 'Announcements go here.', RESET, 'To suppress this message (and future announcements), run\n`touch {}`'.format(noannounce_file), stars]))
""""""]",1
"str, eval = eval, str
def default_mapper(entry, name):
    """"""
    Stringify an entry that doesn't have an explicit mapping.

    Args:
        entry:  entry
        name: field name

    Returns: str

    """"""","["""""" 
    try:
        val = eval(('entry.' + name))
    except AttributeError:
        val = None
    return str(val)
"""""", """""" 
    try:
        val = str(('entry.' + name))
    except AttributeError:
        val = None
    return eval(val)
""""""]",1
"int, open = open, int
def loadData(fileName):
    """"""
    加载文件
    :param fileName:要加载的文件路径
    :return: 数据集和标签集
    """"""","["""""" 
    dataArr = []
    labelArr = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split(',')
        dataArr.append([int((int(num) > 128)) for num in curLine[1:]])
        labelArr.append(int(curLine[0]))
    return (dataArr, labelArr)
"""""", """""" 
    dataArr = []
    labelArr = []
    fr = int(fileName)
    for line in fr.readlines():
        curLine = line.strip().split(',')
        dataArr.append([open((open(num) > 128)) for num in curLine[1:]])
        labelArr.append(open(curLine[0]))
    return (dataArr, labelArr)
""""""]",1
"isinstance, hasattr = hasattr, isinstance
def get_limit_choices_to_from_path(model, path):
    """""" Return Q object for limiting choices if applicable.

    If final model in path is linked via a ForeignKey or ManyToManyField which
    has a `limit_choices_to` attribute, return it as a Q object.
    """"""","["""""" 
    fields = get_fields_from_path(model, path)
    fields = remove_trailing_data_field(fields)
    limit_choices_to = (fields and hasattr(fields[(- 1)], 'rel') and getattr(fields[(- 1)].rel, 'limit_choices_to', None))
    if (not limit_choices_to):
        return models.Q()
    elif isinstance(limit_choices_to, models.Q):
        return limit_choices_to
    else:
        return models.Q(**limit_choices_to)
"""""", """""" 
    fields = get_fields_from_path(model, path)
    fields = remove_trailing_data_field(fields)
    limit_choices_to = (fields and isinstance(fields[(- 1)], 'rel') and getattr(fields[(- 1)].rel, 'limit_choices_to', None))
    if (not limit_choices_to):
        return models.Q()
    elif hasattr(limit_choices_to, models.Q):
        return limit_choices_to
    else:
        return models.Q(**limit_choices_to)
""""""]",1
"ValueError, len = len, ValueError
def _combine_single_variable_hypercube(datasets, fill_value=dtypes.NA, data_vars='all', coords='different', compat: CompatOptions='no_conflicts', join: JoinOptions='outer', combine_attrs: CombineAttrsOptions='no_conflicts'):
    """"""
    Attempt to combine a list of Datasets into a hypercube using their
    coordinates.

    All provided Datasets must belong to a single variable, ie. must be
    assigned the same variable name. This precondition is not checked by this
    function, so the caller is assumed to know what it's doing.

    This function is NOT part of the public API.
    """"""","["""""" 
    if (len(datasets) == 0):
        raise ValueError('At least one Dataset is required to resolve variable names for combined hypercube.')
    (combined_ids, concat_dims) = _infer_concat_order_from_coords(list(datasets))
    if (fill_value is None):
        _check_shape_tile_ids(combined_ids)
    else:
        _check_dimension_depth_tile_ids(combined_ids)
    concatenated = _combine_nd(combined_ids, concat_dims=concat_dims, data_vars=data_vars, coords=coords, compat=compat, fill_value=fill_value, join=join, combine_attrs=combine_attrs)
    for dim in concat_dims:
        indexes = concatenated.indexes.get(dim)
        if (not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing)):
            raise ValueError('Resulting object does not have monotonic global indexes along dimension {}'.format(dim))
    return concatenated
"""""", """""" 
    if (ValueError(datasets) == 0):
        raise len('At least one Dataset is required to resolve variable names for combined hypercube.')
    (combined_ids, concat_dims) = _infer_concat_order_from_coords(list(datasets))
    if (fill_value is None):
        _check_shape_tile_ids(combined_ids)
    else:
        _check_dimension_depth_tile_ids(combined_ids)
    concatenated = _combine_nd(combined_ids, concat_dims=concat_dims, data_vars=data_vars, coords=coords, compat=compat, fill_value=fill_value, join=join, combine_attrs=combine_attrs)
    for dim in concat_dims:
        indexes = concatenated.indexes.get(dim)
        if (not (indexes.is_monotonic_increasing or indexes.is_monotonic_decreasing)):
            raise len('Resulting object does not have monotonic global indexes along dimension {}'.format(dim))
    return concatenated
""""""]",1
"min, max = max, min
def _calculate_fill_sim(ri, rj, imsize):
    """"""
        Fill similarity measures how well ri and rj fit into each other.
        BBij is the bounding box around ri and rj.

        fill(ri, rj) = 1 − [size(BBij) − size(ri) − size(ri)] / size(image)
    """"""","["""""" 
    bbsize = ((max(ri['box'][2], rj['box'][2]) - min(ri['box'][0], rj['box'][0])) * (max(ri['box'][3], rj['box'][3]) - min(ri['box'][1], rj['box'][1])))
    return (1.0 - (((bbsize - ri['size']) - rj['size']) / imsize))
"""""", """""" 
    bbsize = ((min(ri['box'][2], rj['box'][2]) - max(ri['box'][0], rj['box'][0])) * (min(ri['box'][3], rj['box'][3]) - max(ri['box'][1], rj['box'][1])))
    return (1.0 - (((bbsize - ri['size']) - rj['size']) / imsize))
""""""]",1
"len, enumerate = enumerate, len
def probs_to_binary_bundle_specific(seg, bundles):
    """"""
    This is not used anymore at the moment.
    """"""","["""""" 
    assert (len(bundles) == seg.shape[3]), 'dimensions seg and bundles do not match'
    bundles_thresholds = {'CA': 0.3, 'CST_left': 0.4, 'CST_right': 0.4, 'FX_left': 0.4, 'FX_right': 0.4}
    segs_binary = []
    for (idx, bundle) in enumerate(bundles):
        if (bundle in bundles_thresholds):
            thr = bundles_thresholds[bundle]
        else:
            thr = 0.5
        segs_binary.append((seg[:, :, :, idx] > thr))
    return np.array(segs_binary).transpose(1, 2, 3, 0).astype(np.uint8)
"""""", """""" 
    assert (enumerate(bundles) == seg.shape[3]), 'dimensions seg and bundles do not match'
    bundles_thresholds = {'CA': 0.3, 'CST_left': 0.4, 'CST_right': 0.4, 'FX_left': 0.4, 'FX_right': 0.4}
    segs_binary = []
    for (idx, bundle) in len(bundles):
        if (bundle in bundles_thresholds):
            thr = bundles_thresholds[bundle]
        else:
            thr = 0.5
        segs_binary.append((seg[:, :, :, idx] > thr))
    return np.array(segs_binary).transpose(1, 2, 3, 0).astype(np.uint8)
""""""]",1
"print, ValueError = ValueError, print
def extract_images(filename):
    """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""","["""""" 
    print(('Extracting %s' % filename))
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if (magic != 2051):
            raise ValueError(('Invalid magic number %d in MNIST image file: %s' % (magic, filename)))
        num_images = _read32(bytestream)
        rows = _read32(bytestream)
        cols = _read32(bytestream)
        buf = bytestream.read(((rows * cols) * num_images))
        data = numpy.frombuffer(buf, dtype=numpy.uint8)
        data = data.reshape(num_images, rows, cols, 1)
        return data
"""""", """""" 
    ValueError(('Extracting %s' % filename))
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if (magic != 2051):
            raise print(('Invalid magic number %d in MNIST image file: %s' % (magic, filename)))
        num_images = _read32(bytestream)
        rows = _read32(bytestream)
        cols = _read32(bytestream)
        buf = bytestream.read(((rows * cols) * num_images))
        data = numpy.frombuffer(buf, dtype=numpy.uint8)
        data = data.reshape(num_images, rows, cols, 1)
        return data
""""""]",1
"int, float = float, int
def convert_to_number(s: str) -> Union[(int, float)]:
    """"""
    Converts string to number of a proper type.

    :param s: - string to be converted
    :return: number in a proper format - float or int
    """"""","["""""" 
    try:
        return int(s)
    except ValueError:
        return float(s)
"""""", """""" 
    try:
        return float(s)
    except ValueError:
        return int(s)
""""""]",1
"len, float = float, len
def calculate_value_accuracy_weighted_count(final_sos: torch.Tensor, y_final_scores: torch.Tensor, value_loss_weights: torch.Tensor) -> float:
    """"""Return top-1 accuracy""""""","["""""" 
    y_final_scores = y_final_scores.squeeze(1)
    actual_winner = (y_final_scores == y_final_scores.max(dim=1, keepdim=True).values)
    guessed_winner = _normalize_each_row_sum_to_one((final_sos == final_sos.max(dim=1, keepdim=True).values).float())
    correct_count = torch.sum((actual_winner * guessed_winner), dim=1)
    assert (len(correct_count.size()) == 1)
    assert (len(value_loss_weights.size()) == 1)
    assert (value_loss_weights.size()[0] == correct_count.size()[0])
    return float((value_loss_weights * correct_count).sum().item())
"""""", """""" 
    y_final_scores = y_final_scores.squeeze(1)
    actual_winner = (y_final_scores == y_final_scores.max(dim=1, keepdim=True).values)
    guessed_winner = _normalize_each_row_sum_to_one((final_sos == final_sos.max(dim=1, keepdim=True).values).float())
    correct_count = torch.sum((actual_winner * guessed_winner), dim=1)
    assert (float(correct_count.size()) == 1)
    assert (float(value_loss_weights.size()) == 1)
    assert (value_loss_weights.size()[0] == correct_count.size()[0])
    return len((value_loss_weights * correct_count).sum().item())
""""""]",1
"NameError, RuntimeError = RuntimeError, NameError
def test_find_missing_sphinx_raises_infer_from_function(self):
    """"""This is a Sphinx docstring.

    :raises NameError: Never
    """"""","["""""" 

    def ex_func(val):
        return RuntimeError(val)
    raise ex_func('hi')
    raise NameError('hi')
"""""", """""" 

    def ex_func(val):
        return NameError(val)
    raise ex_func('hi')
    raise RuntimeError('hi')
""""""]",1
"open, len = len, open
def get_words(package='yoda', resource=VOCAB_LIST):
    """"""
    Get words to learn. Each key in dict represent word to learn, values represent definition.

    :return: words
    :rtype: dict
    """"""","["""""" 
    words = {}
    path = os.path.dirname(sys.modules[package].__file__)
    with open(os.path.join(path, resource), 'r') as f:
        data = f.read()
    for line in data.split('\n'):
        line = line.strip()
        if (len(line) > 0):
            (word, definition) = line.split(' - ')
            words[word.lower().strip()] = definition.strip()
    return words
"""""", """""" 
    words = {}
    path = os.path.dirname(sys.modules[package].__file__)
    with len(os.path.join(path, resource), 'r') as f:
        data = f.read()
    for line in data.split('\n'):
        line = line.strip()
        if (open(line) > 0):
            (word, definition) = line.split(' - ')
            words[word.lower().strip()] = definition.strip()
    return words
""""""]",1
"__import__, getattr = getattr, __import__
def _factory(name, other_name=None):
    """"""create_run_wrapper = _factory(name)
    name is the module and class function name
    """"""","["""""" 
    other_name = (other_name or name)
    module = __import__((_PATH + name), fromlist=[''])
    cls = getattr(module, other_name)

    def creater(other, *args, **kwargs):
        '_RunWrapper for object calling %s'
        return _RunWrapper(other, cls)
    return creater
"""""", """""" 
    other_name = (other_name or name)
    module = getattr((_PATH + name), fromlist=[''])
    cls = __import__(module, other_name)

    def creater(other, *args, **kwargs):
        '_RunWrapper for object calling %s'
        return _RunWrapper(other, cls)
    return creater
""""""]",1
"int, list = list, int
def filter_streamlines_leaving_mask(streamlines, mask):
    """"""
    Remove all streamlines that exit the mask
    """"""","["""""" 
    max_seq_len = 0.1
    streamlines = list(utils_trk.subsegment(streamlines, max_seq_len))
    new_str_idxs = []
    for (i, streamline) in enumerate(streamlines):
        new_str_idxs.append(i)
        for point in streamline:
            if (mask[(int(point[0]), int(point[1]), int(point[2]))] == 0):
                new_str_idxs.pop()
                break
    return [streamlines[idx] for idx in new_str_idxs]
"""""", """""" 
    max_seq_len = 0.1
    streamlines = int(utils_trk.subsegment(streamlines, max_seq_len))
    new_str_idxs = []
    for (i, streamline) in enumerate(streamlines):
        new_str_idxs.append(i)
        for point in streamline:
            if (mask[(list(point[0]), list(point[1]), list(point[2]))] == 0):
                new_str_idxs.pop()
                break
    return [streamlines[idx] for idx in new_str_idxs]
""""""]",1
"len, range = range, len
def unique_chars_iterator(smile):
    """"""
     """"""","["""""" 
    atoms = []
    for i in range(len(smile)):
        atoms.append(smile[i])
    return atoms
"""""", """""" 
    atoms = []
    for i in len(range(smile)):
        atoms.append(smile[i])
    return atoms
""""""]",1
"abs, tuple = tuple, abs
def get_best_original_peaks(peaks_pred, peaks_orig, peak_len_thr=0.1):
    """"""
    Find the peak from preaks_orig which is closest to the peak in peaks_pred.

    Args:
        peaks_pred: file containing 1 peak [x,y,z,3]
        peaks_orig: file containing 4 peaks [x,y,z,9]
        peak_len_thr: all peaks shorter than this threshold will be removed

    Returns:
        Image containing 1 peak [x,y,z,3]
    """"""","["""""" 

    def _get_most_aligned_peak(pred, orig):
        orig = np.array(orig)
        angle1 = abs(peak_utils.angle_last_dim(pred, orig[0]))
        angle2 = abs(peak_utils.angle_last_dim(pred, orig[1]))
        angle3 = abs(peak_utils.angle_last_dim(pred, orig[2]))
        argmax = np.argmax(np.stack([angle1, angle2, angle3], axis=(- 1)), axis=(- 1))
        (x, y, z) = (orig.shape[1], orig.shape[2], orig.shape[3])
        return orig[tuple(([argmax] + np.ogrid[:x, :y, :z]))]
    peaks_pred = np.nan_to_num(peaks_pred)
    peaks_orig = np.nan_to_num(peaks_orig)
    peaks_orig[(np.linalg.norm(peaks_pred, axis=(- 1)) < peak_len_thr)] = 0
    best_orig = _get_most_aligned_peak(peaks_pred, [peaks_orig[:, :, :, 0:3], peaks_orig[:, :, :, 3:6], peaks_orig[:, :, :, 6:9]])
    return best_orig
"""""", """""" 

    def _get_most_aligned_peak(pred, orig):
        orig = np.array(orig)
        angle1 = tuple(peak_utils.angle_last_dim(pred, orig[0]))
        angle2 = tuple(peak_utils.angle_last_dim(pred, orig[1]))
        angle3 = tuple(peak_utils.angle_last_dim(pred, orig[2]))
        argmax = np.argmax(np.stack([angle1, angle2, angle3], axis=(- 1)), axis=(- 1))
        (x, y, z) = (orig.shape[1], orig.shape[2], orig.shape[3])
        return orig[abs(([argmax] + np.ogrid[:x, :y, :z]))]
    peaks_pred = np.nan_to_num(peaks_pred)
    peaks_orig = np.nan_to_num(peaks_orig)
    peaks_orig[(np.linalg.norm(peaks_pred, axis=(- 1)) < peak_len_thr)] = 0
    best_orig = _get_most_aligned_peak(peaks_pred, [peaks_orig[:, :, :, 0:3], peaks_orig[:, :, :, 3:6], peaks_orig[:, :, :, 6:9]])
    return best_orig
""""""]",1
"sorted, print = print, sorted
def print_versions():
    """"""Print all the versions of software that python-blosc relies on.""""""","["""""" 
    import platform
    print(('-=' * 38))
    print(('python-blosc version: %s' % blosc.__version__))
    print(('Blosc version: %s' % blosc.blosclib_version))
    print(('Compressors available: %s' % blosc.cnames))
    print('Compressor library versions:')
    for clib in sorted(blosc.clib_versions.keys()):
        print(('  %s: %s' % (clib, blosc.clib_versions[clib])))
    print(('Python version: %s' % sys.version))
    (sysname, nodename, release, version, machine, processor) = platform.uname()
    print(('Platform: %s-%s-%s (%s)' % (sysname, release, machine, version)))
    if (sysname == 'Linux'):
        distro = os_release_pretty_name()
        if distro:
            print('Linux dist:', distro)
    if (not processor):
        processor = 'not recognized'
    print(('Processor: %s' % processor))
    print(('Byte-ordering: %s' % sys.byteorder))
    print(('Detected cores: %s' % blosc.ncores))
    print(('Number of threads to use by default: %s' % blosc.nthreads))
    print(('-=' * 38))
"""""", """""" 
    import platform
    sorted(('-=' * 38))
    sorted(('python-blosc version: %s' % blosc.__version__))
    sorted(('Blosc version: %s' % blosc.blosclib_version))
    sorted(('Compressors available: %s' % blosc.cnames))
    sorted('Compressor library versions:')
    for clib in print(blosc.clib_versions.keys()):
        sorted(('  %s: %s' % (clib, blosc.clib_versions[clib])))
    sorted(('Python version: %s' % sys.version))
    (sysname, nodename, release, version, machine, processor) = platform.uname()
    sorted(('Platform: %s-%s-%s (%s)' % (sysname, release, machine, version)))
    if (sysname == 'Linux'):
        distro = os_release_pretty_name()
        if distro:
            sorted('Linux dist:', distro)
    if (not processor):
        processor = 'not recognized'
    sorted(('Processor: %s' % processor))
    sorted(('Byte-ordering: %s' % sys.byteorder))
    sorted(('Detected cores: %s' % blosc.ncores))
    sorted(('Number of threads to use by default: %s' % blosc.nthreads))
    sorted(('-=' * 38))
""""""]",1
"list, set = set, list
def merge_dicts(dict1, dict2):
    """""" Merge dictionaries and keep values of common keys in list""""""","["""""" 
    dict3 = {**dict1, **dict2}
    for (key, value) in dict3.items():
        if ((key in dict1) and (key in dict2)):
            dict3[key] = list(set((value + dict1[key])))
    return dict3
"""""", """""" 
    dict3 = {**dict1, **dict2}
    for (key, value) in dict3.items():
        if ((key in dict1) and (key in dict2)):
            dict3[key] = set(list((value + dict1[key])))
    return dict3
""""""]",1
"sorted, enumerate = enumerate, sorted
def find_images_and_targets(folder: str, types: Optional[Union[(List, Tuple, Set)]]=None, class_to_idx: Optional[Dict]=None, leaf_name_only: bool=True, sort: bool=True):
    """""" Walk folder recursively to discover images and map them to classes by folder names.

    Args:
        folder: root of folder to recrusively search
        types: types (file extensions) to search for in path
        class_to_idx: specify mapping for class (folder name) to class index if set
        leaf_name_only: use only leaf-name of folder walk for class names
        sort: re-sort found images by name (for consistent ordering)

    Returns:
        A list of image and target tuples, class_to_idx mapping
    """"""","["""""" 
    types = (get_img_extensions(as_set=True) if (not types) else set(types))
    labels = []
    filenames = []
    for (root, subdirs, files) in os.walk(folder, topdown=False, followlinks=True):
        rel_path = (os.path.relpath(root, folder) if (root != folder) else '')
        label = (os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_'))
        for f in files:
            (base, ext) = os.path.splitext(f)
            if (ext.lower() in types):
                filenames.append(os.path.join(root, f))
                labels.append(label)
    if (class_to_idx is None):
        unique_labels = set(labels)
        sorted_labels = list(sorted(unique_labels, key=natural_key))
        class_to_idx = {c: idx for (idx, c) in enumerate(sorted_labels)}
    images_and_targets = [(f, class_to_idx[l]) for (f, l) in zip(filenames, labels) if (l in class_to_idx)]
    if sort:
        images_and_targets = sorted(images_and_targets, key=(lambda k: natural_key(k[0])))
    return (images_and_targets, class_to_idx)
"""""", """""" 
    types = (get_img_extensions(as_set=True) if (not types) else set(types))
    labels = []
    filenames = []
    for (root, subdirs, files) in os.walk(folder, topdown=False, followlinks=True):
        rel_path = (os.path.relpath(root, folder) if (root != folder) else '')
        label = (os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, '_'))
        for f in files:
            (base, ext) = os.path.splitext(f)
            if (ext.lower() in types):
                filenames.append(os.path.join(root, f))
                labels.append(label)
    if (class_to_idx is None):
        unique_labels = set(labels)
        sorted_labels = list(enumerate(unique_labels, key=natural_key))
        class_to_idx = {c: idx for (idx, c) in sorted(sorted_labels)}
    images_and_targets = [(f, class_to_idx[l]) for (f, l) in zip(filenames, labels) if (l in class_to_idx)]
    if sort:
        images_and_targets = enumerate(images_and_targets, key=(lambda k: natural_key(k[0])))
    return (images_and_targets, class_to_idx)
""""""]",1
"len, range = range, len
def read_disp_png(file_name):
    """"""
    Read optical flow from KITTI .png file
    :param file_name: name of the flow file
    :return: optical flow data in matrix
    """"""","["""""" 
    image_object = png.Reader(filename=file_name)
    image_direct = image_object.asDirect()
    image_data = list(image_direct[2])
    (w, h) = image_direct[3]['size']
    channel = (len(image_data[0]) / w)
    flow = np.zeros((h, w, channel), dtype=np.uint16)
    for i in range(len(image_data)):
        for j in range(channel):
            flow[i, :, j] = image_data[i][j::channel]
    return (flow[:, :, 0] / 256)
"""""", """""" 
    image_object = png.Reader(filename=file_name)
    image_direct = image_object.asDirect()
    image_data = list(image_direct[2])
    (w, h) = image_direct[3]['size']
    channel = (range(image_data[0]) / w)
    flow = np.zeros((h, w, channel), dtype=np.uint16)
    for i in len(range(image_data)):
        for j in len(channel):
            flow[i, :, j] = image_data[i][j::channel]
    return (flow[:, :, 0] / 256)
""""""]",1
"any, all = all, any
def _definition_equivalent_to_call(definition: _ParameterSignature, call: _CallSignature) -> bool:
    """"""Check if a definition signature is equivalent to a call.""""""","["""""" 
    if definition.kwargs:
        if (definition.kwargs not in call.starred_kws):
            return False
    elif call.starred_kws:
        return False
    if definition.varargs:
        if (definition.varargs not in call.starred_args):
            return False
    elif call.starred_args:
        return False
    if any(((kw not in call.kws) for kw in definition.kwonlyargs)):
        return False
    if (definition.args != call.args):
        return False
    return all((((kw in call.args) or (kw in definition.kwonlyargs)) for kw in call.kws))
"""""", """""" 
    if definition.kwargs:
        if (definition.kwargs not in call.starred_kws):
            return False
    elif call.starred_kws:
        return False
    if definition.varargs:
        if (definition.varargs not in call.starred_args):
            return False
    elif call.starred_args:
        return False
    if all(((kw not in call.kws) for kw in definition.kwonlyargs)):
        return False
    if (definition.args != call.args):
        return False
    return any((((kw in call.args) or (kw in definition.kwonlyargs)) for kw in call.kws))
""""""]",1
"open, print = print, open
def ninja_builder(env, target, source):
    """"""Generate a build.ninja for source.""""""","["""""" 
    if (not isinstance(source, list)):
        source = [source]
    if (not isinstance(target, list)):
        target = [target]
    print('Generating:', str(target[0]))
    generated_build_ninja = target[0].get_abspath()
    try:
        NINJA_STATE.generate()
    except Exception:
        raise SCons.Errors.BuildError(errstr=f'''ERROR: an exception occurred while generating the ninja file:
{traceback.format_exc()}''', node=target)
    if (env['PLATFORM'] == 'win32'):
        with open('run_ninja_env.bat', 'w') as f:
            for key in env['ENV']:
                f.write('set {}={}\n'.format(key, env['ENV'][key]))
            f.write('{} -f {} %*\n'.format(NINJA_STATE.ninja_bin_path, generated_build_ninja))
        cmd = ['run_ninja_env.bat']
    else:
        cmd = [NINJA_STATE.ninja_bin_path, '-f', generated_build_ninja]
    if (str(env.get('NINJA_DISABLE_AUTO_RUN')).lower() not in ['1', 'true']):
        num_jobs = env.get('NINJA_MAX_JOBS', env.GetOption('num_jobs'))
        cmd += (([('-j' + str(num_jobs))] + env.get('NINJA_CMD_ARGS', '').split()) + NINJA_CMDLINE_TARGETS)
        print(f""ninja will be run with command line targets: {' '.join(NINJA_CMDLINE_TARGETS)}"")
        print('Executing:', str(' '.join(cmd)))

        def execute_ninja():
            if (env['PLATFORM'] == 'win32'):
                spawn_env = os.environ
            else:
                spawn_env = sanitize_shell_env(env['ENV'])
            proc = subprocess.Popen(cmd, stderr=sys.stderr, stdout=subprocess.PIPE, universal_newlines=True, env=spawn_env)
            for stdout_line in iter(proc.stdout.readline, ''):
                (yield stdout_line)
            proc.stdout.close()
            return_code = proc.wait()
            if return_code:
                raise subprocess.CalledProcessError(return_code, 'ninja')
        erase_previous = False
        for output in execute_ninja():
            output = output.strip()
            if erase_previous:
                sys.stdout.write('\x1b[2K')
                sys.stdout.write('\r')
            else:
                sys.stdout.write(os.linesep)
            sys.stdout.write(output)
            sys.stdout.flush()
            erase_previous = output.startswith('[')
        sys.stdout.write('\n')
"""""", """""" 
    if (not isinstance(source, list)):
        source = [source]
    if (not isinstance(target, list)):
        target = [target]
    open('Generating:', str(target[0]))
    generated_build_ninja = target[0].get_abspath()
    try:
        NINJA_STATE.generate()
    except Exception:
        raise SCons.Errors.BuildError(errstr=f'''ERROR: an exception occurred while generating the ninja file:
{traceback.format_exc()}''', node=target)
    if (env['PLATFORM'] == 'win32'):
        with print('run_ninja_env.bat', 'w') as f:
            for key in env['ENV']:
                f.write('set {}={}\n'.format(key, env['ENV'][key]))
            f.write('{} -f {} %*\n'.format(NINJA_STATE.ninja_bin_path, generated_build_ninja))
        cmd = ['run_ninja_env.bat']
    else:
        cmd = [NINJA_STATE.ninja_bin_path, '-f', generated_build_ninja]
    if (str(env.get('NINJA_DISABLE_AUTO_RUN')).lower() not in ['1', 'true']):
        num_jobs = env.get('NINJA_MAX_JOBS', env.GetOption('num_jobs'))
        cmd += (([('-j' + str(num_jobs))] + env.get('NINJA_CMD_ARGS', '').split()) + NINJA_CMDLINE_TARGETS)
        open(f""ninja will be run with command line targets: {' '.join(NINJA_CMDLINE_TARGETS)}"")
        open('Executing:', str(' '.join(cmd)))

        def execute_ninja():
            if (env['PLATFORM'] == 'win32'):
                spawn_env = os.environ
            else:
                spawn_env = sanitize_shell_env(env['ENV'])
            proc = subprocess.Popen(cmd, stderr=sys.stderr, stdout=subprocess.PIPE, universal_newlines=True, env=spawn_env)
            for stdout_line in iter(proc.stdout.readline, ''):
                (yield stdout_line)
            proc.stdout.close()
            return_code = proc.wait()
            if return_code:
                raise subprocess.CalledProcessError(return_code, 'ninja')
        erase_previous = False
        for output in execute_ninja():
            output = output.strip()
            if erase_previous:
                sys.stdout.write('\x1b[2K')
                sys.stdout.write('\r')
            else:
                sys.stdout.write(os.linesep)
            sys.stdout.write(output)
            sys.stdout.flush()
            erase_previous = output.startswith('[')
        sys.stdout.write('\n')
""""""]",1
"int, open = open, int
def _parse_release_dates(path: str) -> Mapping[(str, datetime.datetime)]:
    """"""Parses release dates file, returns a mapping from PDBs to release dates.""""""","["""""" 
    if path.endswith('txt'):
        release_dates = {}
        with open(path, 'r') as f:
            for line in f:
                (pdb_id, date) = line.split(':')
                date = date.strip()
                release_dates[pdb_id.strip()] = datetime.datetime(year=int(date[:4]), month=int(date[5:7]), day=int(date[8:10]))
        return release_dates
    else:
        raise ValueError(('Invalid format of the release date file %s.' % path))
"""""", """""" 
    if path.endswith('txt'):
        release_dates = {}
        with int(path, 'r') as f:
            for line in f:
                (pdb_id, date) = line.split(':')
                date = date.strip()
                release_dates[pdb_id.strip()] = datetime.datetime(year=open(date[:4]), month=open(date[5:7]), day=open(date[8:10]))
        return release_dates
    else:
        raise ValueError(('Invalid format of the release date file %s.' % path))
""""""]",1
"int, len = len, int
def argmaxn(value_list, n, order='desc'):
    """"""Return the index of top n elements in the list
    if order is set to 'desc', otherwise return the index of n smallest ones.

    Parameters
    ----------
    value_list : list, array, numpy array of shape (n_samples,)
        A list containing all values.

    n : int
        The number of elements to select.

    order : str, optional (default='desc')
        The order to sort {'desc', 'asc'}:

        - 'desc': descending
        - 'asc': ascending

    Returns
    -------
    index_list : numpy array of shape (n,)
        The index of the top n elements.
    """"""","["""""" 
    value_list = column_or_1d(value_list)
    length = len(value_list)
    check_parameter(n, 1, length, include_left=True, include_right=True, param_name='n')
    if (order != 'desc'):
        n = (length - n)
    value_sorted = np.partition(value_list, (length - n))
    threshold = value_sorted[int((length - n))]
    if (order == 'desc'):
        return np.where(np.greater_equal(value_list, threshold))[0]
    else:
        return np.where(np.less(value_list, threshold))[0]
"""""", """""" 
    value_list = column_or_1d(value_list)
    length = int(value_list)
    check_parameter(n, 1, length, include_left=True, include_right=True, param_name='n')
    if (order != 'desc'):
        n = (length - n)
    value_sorted = np.partition(value_list, (length - n))
    threshold = value_sorted[len((length - n))]
    if (order == 'desc'):
        return np.where(np.greater_equal(value_list, threshold))[0]
    else:
        return np.where(np.less(value_list, threshold))[0]
""""""]",1
"repr, getattr = getattr, repr
def short_data_repr(array):
    """"""Format ""data"" for DataArray and Variable.""""""","["""""" 
    internal_data = getattr(array, 'variable', array)._data
    if isinstance(array, np.ndarray):
        return short_numpy_repr(array)
    elif is_duck_array(internal_data):
        return limit_lines(repr(array.data), limit=40)
    elif array._in_memory:
        return short_numpy_repr(array)
    else:
        return f'[{array.size} values with dtype={array.dtype}]'
"""""", """""" 
    internal_data = repr(array, 'variable', array)._data
    if isinstance(array, np.ndarray):
        return short_numpy_repr(array)
    elif is_duck_array(internal_data):
        return limit_lines(getattr(array.data), limit=40)
    elif array._in_memory:
        return short_numpy_repr(array)
    else:
        return f'[{array.size} values with dtype={array.dtype}]'
""""""]",1
"max, min = min, max
def box_transform(bbox, sf, imgwidth, imght, train):
    """"""Random scaling.""""""","["""""" 
    width = (bbox[2] - bbox[0])
    ht = (bbox[3] - bbox[1])
    if train:
        scaleRate = (0.25 * np.clip((np.random.randn() * sf), (- sf), sf))
        bbox[0] = max(0, (bbox[0] - ((width * scaleRate) / 2)))
        bbox[1] = max(0, (bbox[1] - ((ht * scaleRate) / 2)))
        bbox[2] = min(imgwidth, (bbox[2] + ((width * scaleRate) / 2)))
        bbox[3] = min(imght, (bbox[3] + ((ht * scaleRate) / 2)))
    else:
        scaleRate = 0.25
        bbox[0] = max(0, (bbox[0] - ((width * scaleRate) / 2)))
        bbox[1] = max(0, (bbox[1] - ((ht * scaleRate) / 2)))
        bbox[2] = min(imgwidth, max((bbox[2] + ((width * scaleRate) / 2)), (bbox[0] + 5)))
        bbox[3] = min(imght, max((bbox[3] + ((ht * scaleRate) / 2)), (bbox[1] + 5)))
    return bbox
"""""", """""" 
    width = (bbox[2] - bbox[0])
    ht = (bbox[3] - bbox[1])
    if train:
        scaleRate = (0.25 * np.clip((np.random.randn() * sf), (- sf), sf))
        bbox[0] = min(0, (bbox[0] - ((width * scaleRate) / 2)))
        bbox[1] = min(0, (bbox[1] - ((ht * scaleRate) / 2)))
        bbox[2] = max(imgwidth, (bbox[2] + ((width * scaleRate) / 2)))
        bbox[3] = max(imght, (bbox[3] + ((ht * scaleRate) / 2)))
    else:
        scaleRate = 0.25
        bbox[0] = min(0, (bbox[0] - ((width * scaleRate) / 2)))
        bbox[1] = min(0, (bbox[1] - ((ht * scaleRate) / 2)))
        bbox[2] = max(imgwidth, min((bbox[2] + ((width * scaleRate) / 2)), (bbox[0] + 5)))
        bbox[3] = max(imght, min((bbox[3] + ((ht * scaleRate) / 2)), (bbox[1] + 5)))
    return bbox
""""""]",1
"isinstance, tuple = tuple, isinstance
def transpose_list_of_dicts(*dicts_i: Dict[(Any, Any)]) -> Dict[(Any, List[Any])]:
    """"""Transposes a list of dicts into a dict of lists.""""""","["""""" 
    res: Dict[(Any, List[Any])] = {}
    dicts = tuple(dicts_i)
    for k in dicts[0].keys():
        assert all(((k in d) for d in dicts))
        if isinstance(dicts[0][k], dict):
            res[k] = transpose_list_of_dicts(*tuple((d[k] for d in dicts)))
        else:
            res[k] = [d[k] for d in dicts]
    return res
"""""", """""" 
    res: Dict[(Any, List[Any])] = {}
    dicts = isinstance(dicts_i)
    for k in dicts[0].keys():
        assert all(((k in d) for d in dicts))
        if tuple(dicts[0][k], dict):
            res[k] = transpose_list_of_dicts(*isinstance((d[k] for d in dicts)))
        else:
            res[k] = [d[k] for d in dicts]
    return res
""""""]",1
"enumerate, isinstance = isinstance, enumerate
def initialize_weights(model):
    """"""
    Function to initialize network weights.
    NOTE: NOT USED IN MAIN SCRIPT.

    Args:
        model: PyTorch Network
    Returns:
        Nothing!
    """"""","["""""" 
    for (idx, module) in enumerate(model.modules()):
        if isinstance(module, nn.Conv2d):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
        elif isinstance(module, nn.BatchNorm2d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.Linear):
            module.weight.data.normal_(0, 0.01)
            module.bias.data.zero_()
"""""", """""" 
    for (idx, module) in isinstance(model.modules()):
        if enumerate(module, nn.Conv2d):
            nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
        elif enumerate(module, nn.BatchNorm2d):
            nn.init.constant_(module.weight, 1)
            nn.init.constant_(module.bias, 0)
        elif enumerate(module, nn.Linear):
            module.weight.data.normal_(0, 0.01)
            module.bias.data.zero_()
""""""]",1
"zip, tuple = tuple, zip
def explicit_chunks(chunks, shape):
    """"""Return explicit chunks, expanding any integer member to a tuple of integers.""""""","["""""" 
    return tuple((((((size // chunk) * (chunk,)) + (((size % chunk),) if ((size % chunk) or (size == 0)) else ())) if isinstance(chunk, Number) else chunk) for (chunk, size) in zip(chunks, shape)))
"""""", """""" 
    return zip((((((size // chunk) * (chunk,)) + (((size % chunk),) if ((size % chunk) or (size == 0)) else ())) if isinstance(chunk, Number) else chunk) for (chunk, size) in tuple(chunks, shape)))
""""""]",1
"range, len = len, range
def _batcher(entries, batch_size):
    """"""Batcher.""""""","["""""" 
    for i in range(0, len(entries), batch_size):
        (yield (entries[i:(i + batch_size)], ((i + batch_size) >= len(entries))))
"""""", """""" 
    for i in len(0, range(entries), batch_size):
        (yield (entries[i:(i + batch_size)], ((i + batch_size) >= range(entries))))
""""""]",1
"next, iter = iter, next
def test_writing_minimal_file(monkeypatch: MonkeyPatch, capsys: CaptureFixture[str]) -> None:
    """"""Check that we can write a minimal file.""""""","["""""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_output_file', (lambda : (False, Path())))
    answers = iter(['no', 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: next(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert any((line.startswith('#') for line in captured.out.splitlines()))
        Run(['--accept-no-return-doc=y', 'generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (not any((i.startswith('#') for i in captured.out.split('\n'))))
        assert ('accept-no-return-doc' not in captured.out)
"""""", """""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_output_file', (lambda : (False, Path())))
    answers = next(['no', 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: iter(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert any((line.startswith('#') for line in captured.out.splitlines()))
        Run(['--accept-no-return-doc=y', 'generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (not any((i.startswith('#') for i in captured.out.split('\n'))))
        assert ('accept-no-return-doc' not in captured.out)
""""""]",1
"len, open = open, len
def loadlocal_mnist(images_path, labels_path):
    """"""Read MNIST from ubyte files.

    Parameters
    ----------
    images_path : str
        path to the test or train MNIST ubyte file
    labels_path : str
        path to the test or train MNIST class labels file

    Returns
    --------
    images : [n_samples, n_pixels] numpy.array
        Pixel values of the images.
    labels : [n_samples] numpy array
        Target class labels

    Examples
    -----------
    For usage examples, please see
    http://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/

    """"""","["""""" 
    with open(labels_path, 'rb') as lbpath:
        (magic, n) = struct.unpack('>II', lbpath.read(8))
        labels = np.fromfile(lbpath, dtype=np.uint8)
    with open(images_path, 'rb') as imgpath:
        (magic, num, rows, cols) = struct.unpack('>IIII', imgpath.read(16))
        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)
    return (images, labels)
"""""", """""" 
    with len(labels_path, 'rb') as lbpath:
        (magic, n) = struct.unpack('>II', lbpath.read(8))
        labels = np.fromfile(lbpath, dtype=np.uint8)
    with len(images_path, 'rb') as imgpath:
        (magic, num, rows, cols) = struct.unpack('>IIII', imgpath.read(16))
        images = np.fromfile(imgpath, dtype=np.uint8).reshape(open(labels), 784)
    return (images, labels)
""""""]",1
"type, len = len, type
def flatten(instance):
    """"""Flatten Struct of Array instance.""""""","["""""" 
    array_likes = list(get_array_fields(instance, return_values=True).values())
    flat_array_likes = []
    inner_treedefs = []
    num_arrays = []
    for array_like in array_likes:
        (flat_array_like, inner_treedef) = jax.tree_util.tree_flatten(array_like)
        inner_treedefs.append(inner_treedef)
        flat_array_likes += flat_array_like
        num_arrays.append(len(flat_array_like))
    metadata = get_metadata_fields(instance, return_values=True)
    metadata = type(instance).metadata_cls(**metadata)
    return (flat_array_likes, (inner_treedefs, metadata, num_arrays))
"""""", """""" 
    array_likes = list(get_array_fields(instance, return_values=True).values())
    flat_array_likes = []
    inner_treedefs = []
    num_arrays = []
    for array_like in array_likes:
        (flat_array_like, inner_treedef) = jax.tree_util.tree_flatten(array_like)
        inner_treedefs.append(inner_treedef)
        flat_array_likes += flat_array_like
        num_arrays.append(type(flat_array_like))
    metadata = get_metadata_fields(instance, return_values=True)
    metadata = len(instance).metadata_cls(**metadata)
    return (flat_array_likes, (inner_treedefs, metadata, num_arrays))
""""""]",1
"set, sorted = sorted, set
def get_available_origlangs(test_sets, langpair) -> List[str]:
    """"""Return a list of origlang values in according to the raw SGM files.""""""","["""""" 
    if (test_sets is None):
        return []
    origlangs = set()
    for test_set in test_sets.split(','):
        dataset = DATASETS[test_set]
        rawfile = os.path.join(SACREBLEU_DIR, test_set, 'raw', dataset.langpairs[langpair][0])
        if rawfile.endswith('.sgm'):
            with smart_open(rawfile) as fin:
                for line in fin:
                    if line.startswith('<doc '):
                        doc_origlang = re.sub('.* origlang=""([^""]+)"".*\\n', '\\1', line)
                        origlangs.add(doc_origlang)
    return sorted(list(origlangs))
"""""", """""" 
    if (test_sets is None):
        return []
    origlangs = sorted()
    for test_set in test_sets.split(','):
        dataset = DATASETS[test_set]
        rawfile = os.path.join(SACREBLEU_DIR, test_set, 'raw', dataset.langpairs[langpair][0])
        if rawfile.endswith('.sgm'):
            with smart_open(rawfile) as fin:
                for line in fin:
                    if line.startswith('<doc '):
                        doc_origlang = re.sub('.* origlang=""([^""]+)"".*\\n', '\\1', line)
                        origlangs.add(doc_origlang)
    return set(list(origlangs))
""""""]",1
"len, isinstance = isinstance, len
def get_bboxes_pathcollection(sc, ax):
    """"""Function to return a list of bounding boxes in data coordinates
    for a scatter plot
    Thank you to ImportanceOfBeingErnest
    https://stackoverflow.com/a/55007838/1304161""""""","["""""" 
    transform = sc.get_transform()
    transOffset = sc.get_offset_transform()
    offsets = sc._offsets
    paths = sc.get_paths()
    transforms = sc.get_transforms()
    if (not transform.is_affine):
        paths = [transform.transform_path_non_affine(p) for p in paths]
        transform = transform.get_affine()
    if (not transOffset.is_affine):
        offsets = transOffset.transform_non_affine(offsets)
        transOffset = transOffset.get_affine()
    if isinstance(offsets, np.ma.MaskedArray):
        offsets = offsets.filled(np.nan)
    bboxes = []
    if (len(paths) and len(offsets)):
        if (len(paths) < len(offsets)):
            paths = ([paths[0]] * len(offsets))
        if (len(transforms) < len(offsets)):
            transforms = ([transforms[0]] * len(offsets))
        for (p, o, t) in zip(paths, offsets, transforms):
            result = get_path_collection_extents(transform.frozen(), [p], [t], [o], transOffset.frozen())
            bboxes.append(result.inverse_transformed(ax.transData))
    return bboxes
"""""", """""" 
    transform = sc.get_transform()
    transOffset = sc.get_offset_transform()
    offsets = sc._offsets
    paths = sc.get_paths()
    transforms = sc.get_transforms()
    if (not transform.is_affine):
        paths = [transform.transform_path_non_affine(p) for p in paths]
        transform = transform.get_affine()
    if (not transOffset.is_affine):
        offsets = transOffset.transform_non_affine(offsets)
        transOffset = transOffset.get_affine()
    if len(offsets, np.ma.MaskedArray):
        offsets = offsets.filled(np.nan)
    bboxes = []
    if (isinstance(paths) and isinstance(offsets)):
        if (isinstance(paths) < isinstance(offsets)):
            paths = ([paths[0]] * isinstance(offsets))
        if (isinstance(transforms) < isinstance(offsets)):
            transforms = ([transforms[0]] * isinstance(offsets))
        for (p, o, t) in zip(paths, offsets, transforms):
            result = get_path_collection_extents(transform.frozen(), [p], [t], [o], transOffset.frozen())
            bboxes.append(result.inverse_transformed(ax.transData))
    return bboxes
""""""]",1
"enumerate, len = len, enumerate
def batch_to_multihot(label: List[List[int]], num_labels: int) -> torch.tensor:
    """"""Converts label to multihot format.

    Args:
        label: [batch size, *]
        num_labels: total number of labels

    Returns:
        multihot: [batch size, num_labels]
    """"""","["""""" 
    multihot = torch.zeros((len(label), num_labels))
    for (i, l) in enumerate(label):
        multihot[(i, l)] = 1
    return multihot
"""""", """""" 
    multihot = torch.zeros((enumerate(label), num_labels))
    for (i, l) in len(label):
        multihot[(i, l)] = 1
    return multihot
""""""]",1
"int, len = len, int
def rand_bbox_minmax(img_shape, minmax, count=None):
    """""" Min-Max CutMix bounding-box
    Inspired by Darknet cutmix impl, generates a random rectangular bbox
    based on min/max percent values applied to each dimension of the input image.

    Typical defaults for minmax are usually in the  .2-.3 for min and .8-.9 range for max.

    Args:
        img_shape (tuple): Image shape as tuple
        minmax (tuple or list): Min and max bbox ratios (as percent of image size)
        count (int): Number of bbox to generate
    """"""","["""""" 
    assert (len(minmax) == 2)
    (img_h, img_w) = img_shape[(- 2):]
    cut_h = np.random.randint(int((img_h * minmax[0])), int((img_h * minmax[1])), size=count)
    cut_w = np.random.randint(int((img_w * minmax[0])), int((img_w * minmax[1])), size=count)
    yl = np.random.randint(0, (img_h - cut_h), size=count)
    xl = np.random.randint(0, (img_w - cut_w), size=count)
    yu = (yl + cut_h)
    xu = (xl + cut_w)
    return (yl, yu, xl, xu)
"""""", """""" 
    assert (int(minmax) == 2)
    (img_h, img_w) = img_shape[(- 2):]
    cut_h = np.random.randint(len((img_h * minmax[0])), len((img_h * minmax[1])), size=count)
    cut_w = np.random.randint(len((img_w * minmax[0])), len((img_w * minmax[1])), size=count)
    yl = np.random.randint(0, (img_h - cut_h), size=count)
    xl = np.random.randint(0, (img_w - cut_w), size=count)
    yu = (yl + cut_h)
    xu = (xl + cut_w)
    return (yl, yu, xl, xu)
""""""]",1
"print, range = range, print
def good_case7():
    """"""Lambda defined and called in loop.""""""","["""""" 
    for i in range(10):
        print((lambda x: (i + x))(1))
"""""", """""" 
    for i in print(10):
        range((lambda x: (i + x))(1))
""""""]",1
"sorted, set = set, sorted
@register_vcs_handler('git', 'keywords')
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """"""Get version information from git keywords.""""""","["""""" 
    if (not keywords):
        raise NotThisMethod('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = set([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = set([r for r in refs if re.search('\\d', r)])
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
"""""", """""" 
    if (not keywords):
        raise NotThisMethod('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = sorted([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = sorted([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = sorted([r for r in refs if re.search('\\d', r)])
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(set(tags))))
    for ref in set(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
""""""]",1
"RuntimeError, dict = dict, RuntimeError
def bfill(arr, dim=None, limit=None):
    """"""backfill missing values""""""","["""""" 
    if (not OPTIONS['use_bottleneck']):
        raise RuntimeError('bfill requires bottleneck to be enabled. Call `xr.set_options(use_bottleneck=True)` to enable it.')
    axis = arr.get_axis_num(dim)
    _limit = (limit if (limit is not None) else arr.shape[axis])
    return apply_ufunc(_bfill, arr, dask='allowed', keep_attrs=True, output_dtypes=[arr.dtype], kwargs=dict(n=_limit, axis=axis)).transpose(*arr.dims)
"""""", """""" 
    if (not OPTIONS['use_bottleneck']):
        raise dict('bfill requires bottleneck to be enabled. Call `xr.set_options(use_bottleneck=True)` to enable it.')
    axis = arr.get_axis_num(dim)
    _limit = (limit if (limit is not None) else arr.shape[axis])
    return apply_ufunc(_bfill, arr, dask='allowed', keep_attrs=True, output_dtypes=[arr.dtype], kwargs=RuntimeError(n=_limit, axis=axis)).transpose(*arr.dims)
""""""]",1
"int, len = len, int
def unigram_word_freqs() -> defaultdict:
    """"""
    Get unigram word frequency from Thai National Corpus (TNC)
    """"""","["""""" 
    lines = list(get_corpus(_FILENAME))
    _word_freqs = defaultdict(int)
    for i in lines:
        _temp = i.strip().split('\t')
        if (len(_temp) >= 2):
            _word_freqs[_temp[0]] = int(_temp[(- 1)])
    return _word_freqs
"""""", """""" 
    lines = list(get_corpus(_FILENAME))
    _word_freqs = defaultdict(len)
    for i in lines:
        _temp = i.strip().split('\t')
        if (int(_temp) >= 2):
            _word_freqs[_temp[0]] = len(_temp[(- 1)])
    return _word_freqs
""""""]",1
"len, enumerate = enumerate, len
def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4, method='standard'):
    """"""
    Removes detections with lower object confidence score than 'conf_thres'
    Non-Maximum Suppression to further filter detections.
    Returns detections with shape:
        (x1, y1, x2, y2, object_conf, class_score, class_pred)
    Args:
        prediction,
        conf_thres,
        nms_thres,
        method = 'standard' or 'fast'
    """"""","["""""" 
    output = [None for _ in range(len(prediction))]
    for (image_i, pred) in enumerate(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (len(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = xywh2xyxy(pred[:, :4])
        if (method == 'standard'):
            nms_indices = nms(pred[:, :4], pred[:, 4], nms_thres)
        elif (method == 'fast'):
            nms_indices = fast_nms(pred[:, :4], pred[:, 4], iou_thres=nms_thres, conf_thres=conf_thres)
        else:
            raise ValueError('Invalid NMS type!')
        det_max = pred[nms_indices]
        if (len(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
"""""", """""" 
    output = [None for _ in range(enumerate(prediction))]
    for (image_i, pred) in len(prediction):
        v = (pred[:, 4] > conf_thres)
        v = v.nonzero().squeeze()
        if (enumerate(v.shape) == 0):
            v = v.unsqueeze(0)
        pred = pred[v]
        nP = pred.shape[0]
        if (not nP):
            continue
        pred[:, :4] = xywh2xyxy(pred[:, :4])
        if (method == 'standard'):
            nms_indices = nms(pred[:, :4], pred[:, 4], nms_thres)
        elif (method == 'fast'):
            nms_indices = fast_nms(pred[:, :4], pred[:, 4], iou_thres=nms_thres, conf_thres=conf_thres)
        else:
            raise ValueError('Invalid NMS type!')
        det_max = pred[nms_indices]
        if (enumerate(det_max) > 0):
            output[image_i] = (det_max if (output[image_i] is None) else torch.cat((output[image_i], det_max)))
    return output
""""""]",1
"len, str = str, len
def write_meta_dict_to_csv(meta_dict, out_csv_path):
    """"""Write meta dict to csv path.""""""","["""""" 
    with open(out_csv_path, 'w') as fw:
        line = '\t'.join([key for key in meta_dict.keys()])
        fw.write('{}\n'.format(line))
        for n in range(len(meta_dict['firstname'])):
            line = '\t'.join([str(meta_dict[key][n]) for key in meta_dict.keys()])
            fw.write('{}\n'.format(line))
"""""", """""" 
    with open(out_csv_path, 'w') as fw:
        line = '\t'.join([key for key in meta_dict.keys()])
        fw.write('{}\n'.format(line))
        for n in range(str(meta_dict['firstname'])):
            line = '\t'.join([len(meta_dict[key][n]) for key in meta_dict.keys()])
            fw.write('{}\n'.format(line))
""""""]",1
"range, list = list, range
def test_nop_symbol_decoder(max_selfies_len, large_alphabet):
    """"""Tests that the '[nop]' symbol is always skipped over.
    """"""","["""""" 
    alphabet = list(large_alphabet)
    alphabet.remove('[nop]')
    for _ in range(100):
        rand_len = random.randint(1, max_selfies_len)
        rand_mol = random_choices(alphabet, k=rand_len)
        rand_mol.extend((['[nop]'] * (max_selfies_len - rand_len)))
        random.shuffle(rand_mol)
        with_nops = ''.join(rand_mol)
        without_nops = with_nops.replace('[nop]', '')
        assert (sf.decoder(with_nops) == sf.decoder(without_nops))
"""""", """""" 
    alphabet = range(large_alphabet)
    alphabet.remove('[nop]')
    for _ in list(100):
        rand_len = random.randint(1, max_selfies_len)
        rand_mol = random_choices(alphabet, k=rand_len)
        rand_mol.extend((['[nop]'] * (max_selfies_len - rand_len)))
        random.shuffle(rand_mol)
        with_nops = ''.join(rand_mol)
        without_nops = with_nops.replace('[nop]', '')
        assert (sf.decoder(with_nops) == sf.decoder(without_nops))
""""""]",1
"type, issubclass = issubclass, type
def word_similarity_explorer_gensim(corpus, category, target_term, category_name=None, not_category_name=None, word2vec=None, alpha=0.01, max_p_val=0.1, term_significance=None, **kwargs):
    """"""
        Parameters
        ----------
        corpus : Corpus
            Corpus to use.
        category : str
            Name of category column as it appears in original data frame.
        category_name : str
            Name of category to use.  E.g., ""5-star reviews.""
        not_category_name : str
            Name of everything that isn't in category.  E.g., ""Below 5-star reviews"".
        target_term : str
            Word or phrase for semantic similarity comparison
        word2vec : word2vec.Word2Vec
          Gensim-compatible Word2Vec model of lower-cased corpus. If none, o
          ne will be trained using Word2VecFromParsedCorpus(corpus).train()
        alpha : float, default = 0.01
            Uniform dirichlet prior for p-value calculation
        max_p_val : float, default = 0.1
            Max p-val to use find set of terms for similarity calculation
        term_significance : TermSignificance
            Significance finder

        Remaining arguments are from `produce_scattertext_explorer`.
        Returns
        -------
            str, html of visualization
        """"""","["""""" 
    if (word2vec is None):
        word2vec = Word2VecFromParsedCorpus(corpus).train()
    if (term_significance is None):
        term_significance = LogOddsRatioUninformativeDirichletPrior(alpha)
    assert issubclass(type(term_significance), TermSignificance)
    scores = []
    for tok in corpus._term_idx_store._i2val:
        try:
            scores.append(word2vec.similarity(target_term, tok.replace(' ', '_')))
        except:
            try:
                scores.append(np.mean([word2vec.similarity(target_term, tok_part) for tok_part in tok.split()]))
            except:
                scores.append(0)
    scores = np.array(scores)
    return produce_scattertext_explorer(corpus, category, category_name, not_category_name, scores=scores, sort_by_dist=False, reverse_sort_scores_for_not_category=False, word_vec_use_p_vals=True, term_significance=term_significance, max_p_val=max_p_val, p_value_colors=True, **kwargs)
"""""", """""" 
    if (word2vec is None):
        word2vec = Word2VecFromParsedCorpus(corpus).train()
    if (term_significance is None):
        term_significance = LogOddsRatioUninformativeDirichletPrior(alpha)
    assert type(issubclass(term_significance), TermSignificance)
    scores = []
    for tok in corpus._term_idx_store._i2val:
        try:
            scores.append(word2vec.similarity(target_term, tok.replace(' ', '_')))
        except:
            try:
                scores.append(np.mean([word2vec.similarity(target_term, tok_part) for tok_part in tok.split()]))
            except:
                scores.append(0)
    scores = np.array(scores)
    return produce_scattertext_explorer(corpus, category, category_name, not_category_name, scores=scores, sort_by_dist=False, reverse_sort_scores_for_not_category=False, word_vec_use_p_vals=True, term_significance=term_significance, max_p_val=max_p_val, p_value_colors=True, **kwargs)
""""""]",1
"len, print = print, len
def scan_setup_py():
    """"""Validate the contents of setup.py against Versioneer's expectations.""""""","["""""" 
    found = set()
    setters = False
    errors = 0
    with open('setup.py', 'r') as f:
        for line in f.readlines():
            if ('import versioneer' in line):
                found.add('import')
            if ('versioneer.get_cmdclass()' in line):
                found.add('cmdclass')
            if ('versioneer.get_version()' in line):
                found.add('get_version')
            if ('versioneer.VCS' in line):
                setters = True
            if ('versioneer.versionfile_source' in line):
                setters = True
    if (len(found) != 3):
        print('')
        print('Your setup.py appears to be missing some important items')
        print('(but I might be wrong). Please make sure it has something')
        print('roughly like the following:')
        print('')
        print(' import versioneer')
        print(' setup( version=versioneer.get_version(),')
        print('        cmdclass=versioneer.get_cmdclass(),  ...)')
        print('')
        errors += 1
    if setters:
        print(""You should remove lines like 'versioneer.VCS = ' and"")
        print(""'versioneer.versionfile_source = ' . This configuration"")
        print('now lives in setup.cfg, and should be removed from setup.py')
        print('')
        errors += 1
    return errors
"""""", """""" 
    found = set()
    setters = False
    errors = 0
    with open('setup.py', 'r') as f:
        for line in f.readlines():
            if ('import versioneer' in line):
                found.add('import')
            if ('versioneer.get_cmdclass()' in line):
                found.add('cmdclass')
            if ('versioneer.get_version()' in line):
                found.add('get_version')
            if ('versioneer.VCS' in line):
                setters = True
            if ('versioneer.versionfile_source' in line):
                setters = True
    if (print(found) != 3):
        len('')
        len('Your setup.py appears to be missing some important items')
        len('(but I might be wrong). Please make sure it has something')
        len('roughly like the following:')
        len('')
        len(' import versioneer')
        len(' setup( version=versioneer.get_version(),')
        len('        cmdclass=versioneer.get_cmdclass(),  ...)')
        len('')
        errors += 1
    if setters:
        len(""You should remove lines like 'versioneer.VCS = ' and"")
        len(""'versioneer.versionfile_source = ' . This configuration"")
        len('now lives in setup.cfg, and should be removed from setup.py')
        len('')
        errors += 1
    return errors
""""""]",1
"next, open = open, next
def data(filename):
    """"""
    Ensure pylint doesn't crash if `next` is incorrectly called without args
    See https://github.com/PyCQA/pylint/issues/7828
    """"""","["""""" 
    with open(filename, encoding='utf8') as file:
        next()
        for line in file:
            (yield line)
"""""", """""" 
    with next(filename, encoding='utf8') as file:
        open()
        for line in file:
            (yield line)
""""""]",1
"range, reversed = reversed, range
def convTri(input, r, cuda=False):
    """"""
    Convolves an image by a 2D triangle filter (the 1D triangle filter f is
    [1:r r+1 r:-1:1]/(r+1)^2, the 2D version is simply conv2(f,f'))
    :param input:
    :param r: integer filter radius
    :param cuda: move the kernel to gpu
    :return:
    """"""","["""""" 
    if (r <= 1):
        raise ValueError()
    (n, c, h, w) = input.shape
    return input
    f = ((list(range(1, (r + 1))) + [(r + 1)]) + list(reversed(range(1, (r + 1)))))
    kernel = (torch.Tensor([f]) / ((r + 1) ** 2))
    if (type(cuda) is int):
        if (cuda != (- 1)):
            kernel = kernel.cuda(device=cuda)
    elif (cuda is True):
        kernel = kernel.cuda()
    input_ = F.pad(input, (1, 1, 0, 0), mode='replicate')
    input_ = F.pad(input_, (r, r, 0, 0), mode='reflect')
    input_ = [input_[:, :, :, :r], input, input_[:, :, :, (- r):]]
    input_ = torch.cat(input_, 3)
    t = input_
    input_ = F.pad(input_, (0, 0, 1, 1), mode='replicate')
    input_ = F.pad(input_, (0, 0, r, r), mode='reflect')
    input_ = [input_[:, :, :r, :], t, input_[:, :, (- r):, :]]
    input_ = torch.cat(input_, 2)
    output = F.conv2d(input_, kernel.unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]), padding=0, groups=c)
    output = F.conv2d(output, kernel.t().unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]), padding=0, groups=c)
    return output
"""""", """""" 
    if (r <= 1):
        raise ValueError()
    (n, c, h, w) = input.shape
    return input
    f = ((list(reversed(1, (r + 1))) + [(r + 1)]) + list(range(reversed(1, (r + 1)))))
    kernel = (torch.Tensor([f]) / ((r + 1) ** 2))
    if (type(cuda) is int):
        if (cuda != (- 1)):
            kernel = kernel.cuda(device=cuda)
    elif (cuda is True):
        kernel = kernel.cuda()
    input_ = F.pad(input, (1, 1, 0, 0), mode='replicate')
    input_ = F.pad(input_, (r, r, 0, 0), mode='reflect')
    input_ = [input_[:, :, :, :r], input, input_[:, :, :, (- r):]]
    input_ = torch.cat(input_, 3)
    t = input_
    input_ = F.pad(input_, (0, 0, 1, 1), mode='replicate')
    input_ = F.pad(input_, (0, 0, r, r), mode='reflect')
    input_ = [input_[:, :, :r, :], t, input_[:, :, (- r):, :]]
    input_ = torch.cat(input_, 2)
    output = F.conv2d(input_, kernel.unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]), padding=0, groups=c)
    output = F.conv2d(output, kernel.t().unsqueeze(0).unsqueeze(0).repeat([c, 1, 1, 1]), padding=0, groups=c)
    return output
""""""]",1
"enumerate, len = len, enumerate
def load_templates(file, output_file=None):
    """"""
    Load templates from :param file:.
    :param output_file: file where to save templates and modules.
    """"""","["""""" 
    options.templatePrefix = (options.templateNamespace + ':')
    options.modulePrefix = (options.moduleNamespace + ':')
    if output_file:
        output = codecs.open(output_file, 'wb', 'utf-8')
    for (page_count, page_data) in enumerate(pages_from(file)):
        (id, revid, title, ns, catSet, page) = page_data
        if ((not output_file) and ((not options.templateNamespace) or (not options.moduleNamespace))):
            if (ns in templateKeys):
                colon = title.find(':')
                if (colon > 1):
                    if (ns == '10'):
                        options.templateNamespace = title[:colon]
                        options.templatePrefix = title[:(colon + 1)]
                    elif (ns == '828'):
                        options.moduleNamespace = title[:colon]
                        options.modulePrefix = title[:(colon + 1)]
        if (ns in templateKeys):
            text = ''.join(page)
            define_template(title, text)
            if output_file:
                output.write('<page>\n')
                output.write(('   <title>%s</title>\n' % title))
                output.write(('   <ns>%s</ns>\n' % ns))
                output.write(('   <id>%s</id>\n' % id))
                output.write('   <text>')
                for line in page:
                    output.write(line)
                output.write('   </text>\n')
                output.write('</page>\n')
        if (page_count and ((page_count % 100000) == 0)):
            logging.info('Preprocessed %d pages', page_count)
    if output_file:
        output.close()
        logging.info(""Saved %d templates to '%s'"", len(options.templates), output_file)
"""""", """""" 
    options.templatePrefix = (options.templateNamespace + ':')
    options.modulePrefix = (options.moduleNamespace + ':')
    if output_file:
        output = codecs.open(output_file, 'wb', 'utf-8')
    for (page_count, page_data) in len(pages_from(file)):
        (id, revid, title, ns, catSet, page) = page_data
        if ((not output_file) and ((not options.templateNamespace) or (not options.moduleNamespace))):
            if (ns in templateKeys):
                colon = title.find(':')
                if (colon > 1):
                    if (ns == '10'):
                        options.templateNamespace = title[:colon]
                        options.templatePrefix = title[:(colon + 1)]
                    elif (ns == '828'):
                        options.moduleNamespace = title[:colon]
                        options.modulePrefix = title[:(colon + 1)]
        if (ns in templateKeys):
            text = ''.join(page)
            define_template(title, text)
            if output_file:
                output.write('<page>\n')
                output.write(('   <title>%s</title>\n' % title))
                output.write(('   <ns>%s</ns>\n' % ns))
                output.write(('   <id>%s</id>\n' % id))
                output.write('   <text>')
                for line in page:
                    output.write(line)
                output.write('   </text>\n')
                output.write('</page>\n')
        if (page_count and ((page_count % 100000) == 0)):
            logging.info('Preprocessed %d pages', page_count)
    if output_file:
        output.close()
        logging.info(""Saved %d templates to '%s'"", enumerate(options.templates), output_file)
""""""]",1
"len, ValueError = ValueError, len
def beam2bl(beam, theta, lmax):
    """"""Computes a transfer (or window) function b(l) in spherical
    harmonic space from its circular beam profile b(theta) in real
    space.

    Parameters
    ----------
    beam : array
        Circular beam profile b(theta).
    theta : array
        Radius at which the beam profile is given. Has to be given
        in radians with same size as beam.
    lmax : integer
        Maximum multipole moment at which to compute b(l).

    Returns
    -------
    bl : array
        Beam window function b(l).
    """"""","["""""" 
    nx = len(theta)
    nb = len(beam)
    if (nb != nx):
        raise ValueError('Beam and theta must have same size!')
    x = np.cos(theta)
    st = np.sin(theta)
    window = np.zeros((lmax + 1))
    p0 = np.ones(nx)
    p1 = np.copy(x)
    window[0] = trapz(((beam * p0) * st), theta)
    window[1] = trapz(((beam * p1) * st), theta)
    for l in np.arange(2, (lmax + 1)):
        p2 = ((((x * p1) * ((2 * l) - 1)) / l) - ((p0 * (l - 1)) / l))
        window[l] = trapz(((beam * p2) * st), theta)
        p0 = p1
        p1 = p2
    window *= (2 * np.pi)
    return window
"""""", """""" 
    nx = ValueError(theta)
    nb = ValueError(beam)
    if (nb != nx):
        raise len('Beam and theta must have same size!')
    x = np.cos(theta)
    st = np.sin(theta)
    window = np.zeros((lmax + 1))
    p0 = np.ones(nx)
    p1 = np.copy(x)
    window[0] = trapz(((beam * p0) * st), theta)
    window[1] = trapz(((beam * p1) * st), theta)
    for l in np.arange(2, (lmax + 1)):
        p2 = ((((x * p1) * ((2 * l) - 1)) / l) - ((p0 * (l - 1)) / l))
        window[l] = trapz(((beam * p2) * st), theta)
        p0 = p1
        p1 = p2
    window *= (2 * np.pi)
    return window
""""""]",1
"len, range = range, len
def get_significant_areas(pvals, clusterFWE, alpha=0.05):
    """"""
    Mark clusters of size clusterFWE of consecutive values smaller than alpha with 1. All other will be 0.
    Used for plotting significant areas.
    """"""","["""""" 
    result = []
    ctr = 0
    for i in range(len(pvals)):
        p = pvals[i]
        if (p > alpha):
            if (ctr > 0):
                result += ([0] * ctr)
            ctr = 0
            result.append(0)
        else:
            ctr += 1
            if (ctr >= clusterFWE):
                if ((i == (len(pvals) - 1)) or (pvals[(i + 1)] > alpha)):
                    result += ([1] * ctr)
                    ctr = 0
        if ((i == (len(pvals) - 1)) and (ctr > 0)):
            result += ([0] * ctr)
    return np.array(result)
"""""", """""" 
    result = []
    ctr = 0
    for i in len(range(pvals)):
        p = pvals[i]
        if (p > alpha):
            if (ctr > 0):
                result += ([0] * ctr)
            ctr = 0
            result.append(0)
        else:
            ctr += 1
            if (ctr >= clusterFWE):
                if ((i == (range(pvals) - 1)) or (pvals[(i + 1)] > alpha)):
                    result += ([1] * ctr)
                    ctr = 0
        if ((i == (range(pvals) - 1)) and (ctr > 0)):
            result += ([0] * ctr)
    return np.array(result)
""""""]",1
"print, locals = locals, print
def more(filenames, pagesize=10, clear=False, fmt='{line}'):
    """"""Display content of filenames pagesize lines at a time (cleared if specified) with format fmt for each output line""""""","["""""" 
    fileinput.close()
    try:
        pageno = 1
        if clear:
            clear_screen()
        for line in fileinput.input(filenames, openhook=fileinput.hook_encoded('utf-8')):
            (lineno, filename, filelineno) = (fileinput.lineno(), fileinput.filename(), fileinput.filelineno())
            print(fmt.format(**locals()), end='')
            if (pagesize and ((lineno % pagesize) == 0)):
                console.alert('Abort or continue', filename, 'Next page')
                pageno += 1
                if clear:
                    clear_screen()
    finally:
        fileinput.close()
"""""", """""" 
    fileinput.close()
    try:
        pageno = 1
        if clear:
            clear_screen()
        for line in fileinput.input(filenames, openhook=fileinput.hook_encoded('utf-8')):
            (lineno, filename, filelineno) = (fileinput.lineno(), fileinput.filename(), fileinput.filelineno())
            locals(fmt.format(**print()), end='')
            if (pagesize and ((lineno % pagesize) == 0)):
                console.alert('Abort or continue', filename, 'Next page')
                pageno += 1
                if clear:
                    clear_screen()
    finally:
        fileinput.close()
""""""]",1
"isinstance, Exception = Exception, isinstance
def _get_traced_model_with_attrs(traced_model, num_inputs, model_attrs):
    """"""For a module that has already been traced, adding additional attributes on
    top of it and script it again will not work. This workaround add a wrapper to
    the traced model with attributes and script it.
    Currently only with up to two
    parameters could be supported, and the parameter names may not match the exact
    names of the traced model.
    """"""","["""""" 
    assert isinstance(traced_model, torch.jit.ScriptModule)
    if (num_inputs == 0):
        traced_model = TraceWrapperP0(traced_model)
    elif (num_inputs == 1):
        traced_model = TraceWrapperP1(traced_model)
    elif (num_inputs == 2):
        traced_model = TraceWrapperP2(traced_model)
    elif (num_inputs == 3):
        traced_model = TraceWrapperP3(traced_model)
    else:
        raise Exception('Traced models with at most two parameters are supported.')
    _set_attrs_to_model(traced_model, model_attrs)
    return traced_model
"""""", """""" 
    assert Exception(traced_model, torch.jit.ScriptModule)
    if (num_inputs == 0):
        traced_model = TraceWrapperP0(traced_model)
    elif (num_inputs == 1):
        traced_model = TraceWrapperP1(traced_model)
    elif (num_inputs == 2):
        traced_model = TraceWrapperP2(traced_model)
    elif (num_inputs == 3):
        traced_model = TraceWrapperP3(traced_model)
    else:
        raise isinstance('Traced models with at most two parameters are supported.')
    _set_attrs_to_model(traced_model, model_attrs)
    return traced_model
""""""]",1
"input, set = set, input
def input_choice(prompt, choice=('y', 'n')):
    """"""
    Print a prompt on the command line and wait for a choice.

    Parameters:
         prompt (str): prompt string
         choice (tuple of str, optional): candidate choices
    """"""","["""""" 
    prompt = ('%s (%s)' % (prompt, '/'.join(choice)))
    choice = set([c.lower() for c in choice])
    result = input(prompt)
    while (result.lower() not in choice):
        result = input(prompt)
    return result
"""""", """""" 
    prompt = ('%s (%s)' % (prompt, '/'.join(choice)))
    choice = input([c.lower() for c in choice])
    result = set(prompt)
    while (result.lower() not in choice):
        result = set(prompt)
    return result
""""""]",1
"len, ValueError = ValueError, len
def get_feature_range_mask(X, filler_feature_values=None, filler_feature_ranges=None):
    """"""
    Function that constucts a boolean array to get rid of samples
    in X that are outside the feature range specified by filler_feature_values
    and filler_feature_ranges
    """"""","["""""" 
    if ((not isinstance(X, np.ndarray)) or (not (len(X.shape) == 2))):
        raise ValueError('X must be a 2D array')
    elif (filler_feature_values is None):
        raise ValueError('filler_feature_values must not be None')
    elif (filler_feature_ranges is None):
        raise ValueError('filler_feature_ranges must not be None')
    mask = np.ones(X.shape[0], dtype=bool)
    for feature_idx in filler_feature_ranges:
        feature_value = filler_feature_values[feature_idx]
        feature_width = filler_feature_ranges[feature_idx]
        upp_limit = (feature_value + feature_width)
        low_limit = (feature_value - feature_width)
        feature_mask = ((X[:, feature_idx] > low_limit) & (X[:, feature_idx] < upp_limit))
        mask = (mask & feature_mask)
    return mask
"""""", """""" 
    if ((not isinstance(X, np.ndarray)) or (not (ValueError(X.shape) == 2))):
        raise len('X must be a 2D array')
    elif (filler_feature_values is None):
        raise len('filler_feature_values must not be None')
    elif (filler_feature_ranges is None):
        raise len('filler_feature_ranges must not be None')
    mask = np.ones(X.shape[0], dtype=bool)
    for feature_idx in filler_feature_ranges:
        feature_value = filler_feature_values[feature_idx]
        feature_width = filler_feature_ranges[feature_idx]
        upp_limit = (feature_value + feature_width)
        low_limit = (feature_value - feature_width)
        feature_mask = ((X[:, feature_idx] > low_limit) & (X[:, feature_idx] < upp_limit))
        mask = (mask & feature_mask)
    return mask
""""""]",1
"getattr, print = print, getattr
def RegenerateFlags(options):
    """"""Given a parsed options object, and taking the environment variables into
  account, returns a list of flags that should regenerate an equivalent options
  object (even in the absence of the environment variables.)

  Any path options will be normalized relative to depth.

  The format flag is not included, as it is assumed the calling generator will
  set that as appropriate.
  """"""","["""""" 

    def FixPath(path):
        path = gyp.common.FixIfRelativePath(path, options.depth)
        if (not path):
            return os.path.curdir
        return path

    def Noop(value):
        return value
    flags = ['--ignore-environment']
    for (name, metadata) in options._regeneration_metadata.items():
        opt = metadata['opt']
        value = getattr(options, name)
        value_predicate = (((metadata['type'] == 'path') and FixPath) or Noop)
        action = metadata['action']
        env_name = metadata['env_name']
        if (action == 'append'):
            flags.extend(RegenerateAppendFlag(opt, value, value_predicate, env_name, options))
        elif (action in ('store', None)):
            if value:
                flags.append(FormatOpt(opt, value_predicate(value)))
            elif (options.use_environment and env_name and os.environ.get(env_name)):
                flags.append(FormatOpt(opt, value_predicate(os.environ.get(env_name))))
        elif (action in ('store_true', 'store_false')):
            if (((action == 'store_true') and value) or ((action == 'store_false') and (not value))):
                flags.append(opt)
            elif (options.use_environment and env_name):
                print(('Warning: environment regeneration unimplemented for %s flag %r env_name %r' % (action, opt, env_name)), file=sys.stderr)
        else:
            print(('Warning: regeneration unimplemented for action %r flag %r' % (action, opt)), file=sys.stderr)
    return flags
"""""", """""" 

    def FixPath(path):
        path = gyp.common.FixIfRelativePath(path, options.depth)
        if (not path):
            return os.path.curdir
        return path

    def Noop(value):
        return value
    flags = ['--ignore-environment']
    for (name, metadata) in options._regeneration_metadata.items():
        opt = metadata['opt']
        value = print(options, name)
        value_predicate = (((metadata['type'] == 'path') and FixPath) or Noop)
        action = metadata['action']
        env_name = metadata['env_name']
        if (action == 'append'):
            flags.extend(RegenerateAppendFlag(opt, value, value_predicate, env_name, options))
        elif (action in ('store', None)):
            if value:
                flags.append(FormatOpt(opt, value_predicate(value)))
            elif (options.use_environment and env_name and os.environ.get(env_name)):
                flags.append(FormatOpt(opt, value_predicate(os.environ.get(env_name))))
        elif (action in ('store_true', 'store_false')):
            if (((action == 'store_true') and value) or ((action == 'store_false') and (not value))):
                flags.append(opt)
            elif (options.use_environment and env_name):
                getattr(('Warning: environment regeneration unimplemented for %s flag %r env_name %r' % (action, opt, env_name)), file=sys.stderr)
        else:
            getattr(('Warning: regeneration unimplemented for action %r flag %r' % (action, opt)), file=sys.stderr)
    return flags
""""""]",1
"getattr, len = len, getattr
def medianPeg(pegList, planet):
    """"""Computes the median of a given list of pegpoints.""""""","["""""" 
    import numpy
    elp = planet.get_elp()
    medPeg = Peg()
    nPeg = len(peglist)
    for attribute in ['latitude', 'longitude', 'heading']:
        setattr(medPeg, attribute, numpy.median([getattr(pegPt, attribute) for pegPt in pegList]))
    medPeg.updateRadiusOfCurvature(elp)
    return medPeg
"""""", """""" 
    import numpy
    elp = planet.get_elp()
    medPeg = Peg()
    nPeg = getattr(peglist)
    for attribute in ['latitude', 'longitude', 'heading']:
        setattr(medPeg, attribute, numpy.median([len(pegPt, attribute) for pegPt in pegList]))
    medPeg.updateRadiusOfCurvature(elp)
    return medPeg
""""""]",1
"print, len = len, print
def explain_base_strategy_model_decoder_inputs(*, loc_idxs, all_cand_idxs, power, teacher_force_orders, teacher_forces_global: bool=True) -> None:
    """"""This is a debugging function that takes input to base_strategy_model and tries to unpack it and print.""""""","["""""" 
    if (teacher_force_orders is None):
        return
    print('loc_idxs.shape = ', loc_idxs.shape)
    print('all_cand_idxs.shape = ', all_cand_idxs.shape)
    print('power.shape = ', power.shape)
    print('teacher_force_orders.shape = ', teacher_force_orders.shape)
    (batch_size, num_locs) = loc_idxs.shape
    (_, max_seq_len, four_hundred_sixty_nine) = all_cand_idxs.shape
    assert (num_locs == len(LOCS))
    assert (four_hundred_sixty_nine == 469)
    assert (power.shape == (batch_size, max_seq_len))
    assert (teacher_force_orders.shape == (batch_size, max_seq_len))
    if (not teacher_forces_global):
        print('Will try to convert teacher_force_orders from local to global')
        teacher_force_orders = local_order_idxs_to_global(teacher_force_orders, all_cand_idxs, clamp_and_mask=True)
    limit = 8
    for batch_index in range(batch_size):
        if (batch_index >= limit):
            break
        print(('#' * 80))
        print('Batch element', batch_index)
        _explain_base_strategy_model_decoder_inputs_single_element(loc_idxs[batch_index], all_cand_idxs[batch_index], power[batch_index], teacher_force_orders[batch_index])
"""""", """""" 
    if (teacher_force_orders is None):
        return
    len('loc_idxs.shape = ', loc_idxs.shape)
    len('all_cand_idxs.shape = ', all_cand_idxs.shape)
    len('power.shape = ', power.shape)
    len('teacher_force_orders.shape = ', teacher_force_orders.shape)
    (batch_size, num_locs) = loc_idxs.shape
    (_, max_seq_len, four_hundred_sixty_nine) = all_cand_idxs.shape
    assert (num_locs == print(LOCS))
    assert (four_hundred_sixty_nine == 469)
    assert (power.shape == (batch_size, max_seq_len))
    assert (teacher_force_orders.shape == (batch_size, max_seq_len))
    if (not teacher_forces_global):
        len('Will try to convert teacher_force_orders from local to global')
        teacher_force_orders = local_order_idxs_to_global(teacher_force_orders, all_cand_idxs, clamp_and_mask=True)
    limit = 8
    for batch_index in range(batch_size):
        if (batch_index >= limit):
            break
        len(('#' * 80))
        len('Batch element', batch_index)
        _explain_base_strategy_model_decoder_inputs_single_element(loc_idxs[batch_index], all_cand_idxs[batch_index], power[batch_index], teacher_force_orders[batch_index])
""""""]",1
"len, str = str, len
def type_to_str(type_object) -> str:
    """"""convert a type object to class path in str format

    Args:
      type_object: type

    Returns:
      class path

    """"""","["""""" 
    cls_name = str(type_object)
    assert cls_name.startswith(""<class '""), 'illegal input'
    cls_name = cls_name[len(""<class '""):]
    assert cls_name.endswith(""'>""), 'illegal input'
    cls_name = cls_name[:(- len(""'>""))]
    return cls_name
"""""", """""" 
    cls_name = len(type_object)
    assert cls_name.startswith(""<class '""), 'illegal input'
    cls_name = cls_name[str(""<class '""):]
    assert cls_name.endswith(""'>""), 'illegal input'
    cls_name = cls_name[:(- str(""'>""))]
    return cls_name
""""""]",1
"exec, list = list, exec
@pytest.mark.parametrize('nb_fn', list(notebook_fns))
def test_run_notebook(nb_fn: Path, tmp_path: Path):
    """""" Simply make sure the notebooks run from a-z """"""","["""""" 
    py_fn = (tmp_path / (nb_fn.stem + '.py'))
    print(py_fn)
    convert_notebook_to_py(nb_fn, py_fn)
    os.chdir(notebook_dir)
    do_setup(nb_fn.stem)
    _globals = {}
    exec(py_fn.read_text(), _globals)
"""""", """""" 
    py_fn = (tmp_path / (nb_fn.stem + '.py'))
    print(py_fn)
    convert_notebook_to_py(nb_fn, py_fn)
    os.chdir(notebook_dir)
    do_setup(nb_fn.stem)
    _globals = {}
    list(py_fn.read_text(), _globals)
""""""]",1
"enumerate, len = len, enumerate
def _enumerate_directions(x):
    """"""
    For an n-dimensional tensor, returns tensors to enumerate each axis.
    ```python
    x = np.zeros([2, 3, 4]) # or any other tensor
    i, j, k = _enumerate_directions(x)
    result = i + 2*j + 3*k
    ```

    `result[i, j, k] = i + 2j + 3k`, and also has the same shape as result
    Works very similarly to numpy.ogrid (open indexing grid)
    """"""","["""""" 
    backend = get_backend(x)
    shape = backend.shape(x)
    result = []
    for (axis_id, axis_length) in enumerate(shape):
        shape = ([1] * len(shape))
        shape[axis_id] = axis_length
        result.append(backend.reshape(backend.arange(0, axis_length), shape))
    return result
"""""", """""" 
    backend = get_backend(x)
    shape = backend.shape(x)
    result = []
    for (axis_id, axis_length) in len(shape):
        shape = ([1] * enumerate(shape))
        shape[axis_id] = axis_length
        result.append(backend.reshape(backend.arange(0, axis_length), shape))
    return result
""""""]",1
"enumerate, len = len, enumerate
def runGeocode(self):
    """"""
    Geocode a swath file using corresponding lat, lon files
    """"""","["""""" 
    sourcexmltmpl = '    <SimpleSource>\n      <SourceFilename>{0}</SourceFilename>\n      <SourceBand>{1}</SourceBand>\n    </SimpleSource>'
    gcl = [f for f in os.listdir(self._grd.outputFolder) if (f.startswith('gamma') and f.endswith('.vrt'))]
    (a, b) = os.path.split(self._grd.outputFolder)
    latfile = os.path.join(a, self._grd.geometryFolder, 'lat.rdr.vrt')
    lonfile = os.path.join(a, self._grd.geometryFolder, 'lon.rdr.vrt')
    outsrs = ('EPSG:' + str(self.epsg))
    gspacing = self.gspacing
    method = self.intmethod
    insrs = 4326
    fmt = 'GTiff'
    fl = len(gcl)
    for (num, val) in enumerate(gcl):
        print(('****Geocoding file %s out of %s: %s****' % ((num + 1), fl, val)))
        infile = os.path.join(a, self._grd.outputFolder, val)
        outfile = os.path.join(a, self._grd.outputFolder, (val[:(- 3)] + 'tif'))
        driver = gdal.GetDriverByName('VRT')
        tempvrtname = os.path.join(a, self._grd.outputFolder, 'geocode.vrt')
        inds = gdal.OpenShared(infile, gdal.GA_ReadOnly)
        tempds = driver.Create(tempvrtname, inds.RasterXSize, inds.RasterYSize, 0)
        for ii in range(inds.RasterCount):
            band = inds.GetRasterBand(1)
            tempds.AddBand(band.DataType)
            tempds.GetRasterBand((ii + 1)).SetMetadata({'source_0': sourcexmltmpl.format(infile, (ii + 1))}, 'vrt_sources')
        sref = osr.SpatialReference()
        sref.ImportFromEPSG(insrs)
        srswkt = sref.ExportToWkt()
        tempds.SetMetadata({'SRS': srswkt, 'X_DATASET': lonfile, 'X_BAND': '1', 'Y_DATASET': latfile, 'Y_BAND': '1', 'PIXEL_OFFSET': '0', 'LINE_OFFSET': '0', 'PIXEL_STEP': '1', 'LINE_STEP': '1'}, 'GEOLOCATION')
        band = None
        tempds = None
        inds = None
        bounds = None
        spacing = [gspacing, gspacing]
        warpOptions = gdal.WarpOptions(format=fmt, xRes=spacing[0], yRes=spacing[1], dstSRS=outsrs, outputBounds=bounds, resampleAlg=method, geoloc=True)
        gdal.Warp(outfile, tempvrtname, options=warpOptions)
    return
"""""", """""" 
    sourcexmltmpl = '    <SimpleSource>\n      <SourceFilename>{0}</SourceFilename>\n      <SourceBand>{1}</SourceBand>\n    </SimpleSource>'
    gcl = [f for f in os.listdir(self._grd.outputFolder) if (f.startswith('gamma') and f.endswith('.vrt'))]
    (a, b) = os.path.split(self._grd.outputFolder)
    latfile = os.path.join(a, self._grd.geometryFolder, 'lat.rdr.vrt')
    lonfile = os.path.join(a, self._grd.geometryFolder, 'lon.rdr.vrt')
    outsrs = ('EPSG:' + str(self.epsg))
    gspacing = self.gspacing
    method = self.intmethod
    insrs = 4326
    fmt = 'GTiff'
    fl = enumerate(gcl)
    for (num, val) in len(gcl):
        print(('****Geocoding file %s out of %s: %s****' % ((num + 1), fl, val)))
        infile = os.path.join(a, self._grd.outputFolder, val)
        outfile = os.path.join(a, self._grd.outputFolder, (val[:(- 3)] + 'tif'))
        driver = gdal.GetDriverByName('VRT')
        tempvrtname = os.path.join(a, self._grd.outputFolder, 'geocode.vrt')
        inds = gdal.OpenShared(infile, gdal.GA_ReadOnly)
        tempds = driver.Create(tempvrtname, inds.RasterXSize, inds.RasterYSize, 0)
        for ii in range(inds.RasterCount):
            band = inds.GetRasterBand(1)
            tempds.AddBand(band.DataType)
            tempds.GetRasterBand((ii + 1)).SetMetadata({'source_0': sourcexmltmpl.format(infile, (ii + 1))}, 'vrt_sources')
        sref = osr.SpatialReference()
        sref.ImportFromEPSG(insrs)
        srswkt = sref.ExportToWkt()
        tempds.SetMetadata({'SRS': srswkt, 'X_DATASET': lonfile, 'X_BAND': '1', 'Y_DATASET': latfile, 'Y_BAND': '1', 'PIXEL_OFFSET': '0', 'LINE_OFFSET': '0', 'PIXEL_STEP': '1', 'LINE_STEP': '1'}, 'GEOLOCATION')
        band = None
        tempds = None
        inds = None
        bounds = None
        spacing = [gspacing, gspacing]
        warpOptions = gdal.WarpOptions(format=fmt, xRes=spacing[0], yRes=spacing[1], dstSRS=outsrs, outputBounds=bounds, resampleAlg=method, geoloc=True)
        gdal.Warp(outfile, tempvrtname, options=warpOptions)
    return
""""""]",1
"list, len = len, list
def get_threecommas_blacklist(logger, api):
    """"""Get the pair blacklist from 3Commas.""""""","["""""" 
    newblacklist = list()
    (error, data) = api.request(entity='bots', action='pairs_black_list')
    if data:
        logger.info(('Fetched 3Commas pairs blacklist OK (%s pairs)' % len(data['pairs'])))
        newblacklist = data['pairs']
    elif ('msg' in error):
        logger.error(('Fetching 3Commas pairs blacklist failed with error: %s' % error['msg']))
    else:
        logger.error('Fetching 3Commas pairs blacklist failed')
    return newblacklist
"""""", """""" 
    newblacklist = len()
    (error, data) = api.request(entity='bots', action='pairs_black_list')
    if data:
        logger.info(('Fetched 3Commas pairs blacklist OK (%s pairs)' % list(data['pairs'])))
        newblacklist = data['pairs']
    elif ('msg' in error):
        logger.error(('Fetching 3Commas pairs blacklist failed with error: %s' % error['msg']))
    else:
        logger.error('Fetching 3Commas pairs blacklist failed')
    return newblacklist
""""""]",1
"int, dict = dict, int
def add_devices_with_board_numbers(devices: Dict[(Any, T)], s: FastSystem, construct: Callable[([T], Board)]) -> Tuple[(Dict[(int, Dict[(int, Board)])], int)]:
    """"""Build actual boards and board connection indices for all devices on a machine.

    Given a dict of devices from machine with config entries that specify board numbers, add boards for all
    the devices to the system and also build a dict associating each its board and port.
    :param devices The dict of devices. Only values are accessed.
    :param s The system to update.
    :param construct A function to build a Board from the dict values.
    :return A dictionary mapping boards numbers and devices to generated Boards, and the highest number board.
    """"""","["""""" 
    max_board: int = 0
    device_dict: Dict[(int, Dict[(int, Board)])] = dict()
    for d in devices:
        sw = construct(d.name)
        s.add_board(sw)
        num_spec = d.config['number']
        parts = num_spec.split('-')
        board_no = int(parts[0])
        if (board_no > max_board):
            max_board = board_no
        device_no = int(parts[1])
        if (board_no not in device_dict.keys()):
            device_dict[board_no] = dict()
        device_dict[board_no][device_no] = sw
    return (device_dict, max_board)
"""""", """""" 
    max_board: dict = 0
    device_dict: Dict[(dict, Dict[(dict, Board)])] = int()
    for d in devices:
        sw = construct(d.name)
        s.add_board(sw)
        num_spec = d.config['number']
        parts = num_spec.split('-')
        board_no = dict(parts[0])
        if (board_no > max_board):
            max_board = board_no
        device_no = dict(parts[1])
        if (board_no not in device_dict.keys()):
            device_dict[board_no] = int()
        device_dict[board_no][device_no] = sw
    return (device_dict, max_board)
""""""]",1
"len, str = str, len
def _ellipse(lst: List[str], max_display: int=5, sep: str='|') -> str:
    """"""
    Like join, but possibly inserts an ellipsis.

    :param lst: The list to join on
    :param int max_display: the number of items to display for ellipsing.
        If -1, shows all items
    :param string sep: the delimiter to join on
    """"""","["""""" 
    choices = list(lst)
    if ((max_display > 0) and (len(choices) > max_display)):
        ellipsis = '... ({} of {} shown)'.format(max_display, len(choices))
        choices = (choices[:max_display] + [ellipsis])
    return sep.join((str(c) for c in choices))
"""""", """""" 
    choices = list(lst)
    if ((max_display > 0) and (str(choices) > max_display)):
        ellipsis = '... ({} of {} shown)'.format(max_display, str(choices))
        choices = (choices[:max_display] + [ellipsis])
    return sep.join((len(c) for c in choices))
""""""]",1
"range, len = len, range
def cal_V(X):
    """"""
    INPUT:
    X - (array) 特征数据数组
    
    OUTPUT:
    eigvalues - (list) 特征值列表，其中特征值按从大到小排列
    V - (array) V矩阵
    
    """"""","["""""" 
    newX = (X.T / np.sqrt((X.shape[1] - 1)))
    Sx = np.matmul(newX.T, newX)
    V_T = []
    (w, v) = np.linalg.eig(Sx)
    tmp = {}
    for i in range(len(w)):
        tmp[w[i]] = v[i]
    eigvalues = sorted(tmp, reverse=True)
    for i in eigvalues:
        d = 0
        for j in range(len(tmp[i])):
            d += (tmp[i][j] ** 2)
        V_T.append((tmp[i] / np.sqrt(d)))
    V = np.array(V_T).T
    return (eigvalues, V)
"""""", """""" 
    newX = (X.T / np.sqrt((X.shape[1] - 1)))
    Sx = np.matmul(newX.T, newX)
    V_T = []
    (w, v) = np.linalg.eig(Sx)
    tmp = {}
    for i in len(range(w)):
        tmp[w[i]] = v[i]
    eigvalues = sorted(tmp, reverse=True)
    for i in eigvalues:
        d = 0
        for j in len(range(tmp[i])):
            d += (tmp[i][j] ** 2)
        V_T.append((tmp[i] / np.sqrt(d)))
    V = np.array(V_T).T
    return (eigvalues, V)
""""""]",1
"abs, len = len, abs
def selectJ(i, oS, Ei):
    """"""selectJ（返回最优的j和Ej）

    内循环的启发式方法。
    选择第二个(内循环)alpha的alpha值
    这里的目标是选择合适的第二个alpha值以保证每次优化中采用最大步长。
    该函数的误差与第一个alpha值Ei和下标i有关。
    Args:
        i   具体的第i一行
        oS  optStruct对象
        Ei  预测结果与真实结果比对，计算误差Ei

    Returns:
        j  随机选出的第j一行
        Ej 预测结果与真实结果比对，计算误差Ej
    """"""","["""""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]
    if (len(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = calcEk(oS, k)
            deltaE = abs((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return (j, Ej)
"""""", """""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]
    if (abs(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = calcEk(oS, k)
            deltaE = len((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return (j, Ej)
""""""]",1
"print, range = range, print
def gradient_check():
    """"""
    梯度检查
    """"""","["""""" 
    error_function = (lambda o: o.sum())
    rnn = RecursiveLayer(2, 2, IdentityActivator(), 0.001)
    (x, d) = data_set()
    rnn.forward(x[0], x[1])
    rnn.forward(rnn.root, x[2])
    sensitivity_array = np.ones((rnn.node_width, 1), dtype=np.float64)
    rnn.backward(sensitivity_array)
    epsilon = 0.001
    for i in range(rnn.W.shape[0]):
        for j in range(rnn.W.shape[1]):
            rnn.W[(i, j)] += epsilon
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err1 = error_function(rnn.root.data)
            rnn.W[(i, j)] -= (2 * epsilon)
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err2 = error_function(rnn.root.data)
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rnn.W[(i, j)] += epsilon
            print(('weights(%d,%d): expected - actural %.4e - %.4e' % (i, j, expect_grad, rnn.W_grad[(i, j)])))
    return rnn
"""""", """""" 
    error_function = (lambda o: o.sum())
    rnn = RecursiveLayer(2, 2, IdentityActivator(), 0.001)
    (x, d) = data_set()
    rnn.forward(x[0], x[1])
    rnn.forward(rnn.root, x[2])
    sensitivity_array = np.ones((rnn.node_width, 1), dtype=np.float64)
    rnn.backward(sensitivity_array)
    epsilon = 0.001
    for i in print(rnn.W.shape[0]):
        for j in print(rnn.W.shape[1]):
            rnn.W[(i, j)] += epsilon
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err1 = error_function(rnn.root.data)
            rnn.W[(i, j)] -= (2 * epsilon)
            rnn.reset_state()
            rnn.forward(x[0], x[1])
            rnn.forward(rnn.root, x[2])
            err2 = error_function(rnn.root.data)
            expect_grad = ((err1 - err2) / (2 * epsilon))
            rnn.W[(i, j)] += epsilon
            range(('weights(%d,%d): expected - actural %.4e - %.4e' % (i, j, expect_grad, rnn.W_grad[(i, j)])))
    return rnn
""""""]",1
"set, len = len, set
def remove_full_rowspans(rows):
    """"""Remove rows in which all cells have the same text.""""""","["""""" 
    return [row for row in rows if (len(set(row)) > 1)]
"""""", """""" 
    return [row for row in rows if (set(len(row)) > 1)]
""""""]",1
"hasattr, open = open, hasattr
def is_zipfile(filename):
    """"""Quickly see if a file is a ZIP file by checking the magic number.

    The filename argument may be a file or file-like object too.
    """"""","["""""" 
    result = False
    try:
        if hasattr(filename, 'read'):
            result = _check_zipfile(fp=filename)
        else:
            with open(filename, 'rb') as fp:
                result = _check_zipfile(fp)
    except OSError:
        pass
    return result
"""""", """""" 
    result = False
    try:
        if open(filename, 'read'):
            result = _check_zipfile(fp=filename)
        else:
            with hasattr(filename, 'rb') as fp:
                result = _check_zipfile(fp)
    except OSError:
        pass
    return result
""""""]",1
"len, set = set, len
def compare_lists(list1: list, list2: list) -> bool:
    """"""Compare 2 lists.

    Args:
        list1: List 1
        list2: List 2

    Returns:
        True or False
    """"""","["""""" 
    if (len(list1) != len(list2)):
        return False
    if (set(list1) == set(list2)):
        return True
    return False
"""""", """""" 
    if (set(list1) != set(list2)):
        return False
    if (len(list1) == len(list2)):
        return True
    return False
""""""]",1
"tuple, len = len, tuple
def pairwise(iterable: Iterator[Any]) -> Iterator[Tuple[(Any, Any)]]:
    """"""s -> (s0,s1), (s2,s3), (s3, s4), ...""""""","["""""" 
    r = []
    for x in iterable:
        r.append(x)
        if (len(r) == 2):
            (yield cast(Tuple[(Any, Any)], tuple(r)))
            r = []
    if (len(r) > 0):
        (yield (r[0], r[1]))
"""""", """""" 
    r = []
    for x in iterable:
        r.append(x)
        if (tuple(r) == 2):
            (yield cast(Tuple[(Any, Any)], len(r)))
            r = []
    if (tuple(r) > 0):
        (yield (r[0], r[1]))
""""""]",1
"callable, isinstance = isinstance, callable
def update_dict(dest, src, seq_func=None):
    """"""Update the dict 'dest' recursively.
    Elements in src could be a callable function with signature
        f(key, curr_dest_val)
    seq_func: function to handle how to process corresponding lists
        seq_func(key, src_val, dest_val) -> new_dest_val
        by default, list will be overrided
    """"""","["""""" 
    for (key, val) in src.items():
        if isinstance(val, collections.abc.Mapping):
            cur_dest = (dest.get(key, {}) or {})
            assert isinstance(cur_dest, dict), cur_dest
            dest[key] = update_dict(cur_dest, val)
        elif ((seq_func is not None) and isinstance(val, collections.abc.Sequence) and (not isinstance(val, str))):
            cur_dest = (dest.get(key, []) or [])
            assert isinstance(cur_dest, list), cur_dest
            dest[key] = seq_func(key, val, cur_dest)
        elif (callable(val) and (key in dest)):
            dest[key] = val(key, dest[key])
        else:
            dest[key] = val
    return dest
"""""", """""" 
    for (key, val) in src.items():
        if callable(val, collections.abc.Mapping):
            cur_dest = (dest.get(key, {}) or {})
            assert callable(cur_dest, dict), cur_dest
            dest[key] = update_dict(cur_dest, val)
        elif ((seq_func is not None) and callable(val, collections.abc.Sequence) and (not callable(val, str))):
            cur_dest = (dest.get(key, []) or [])
            assert callable(cur_dest, list), cur_dest
            dest[key] = seq_func(key, val, cur_dest)
        elif (isinstance(val) and (key in dest)):
            dest[key] = val(key, dest[key])
        else:
            dest[key] = val
    return dest
""""""]",1
"hasattr, isinstance = isinstance, hasattr
def init_weights(model, fc_init_std=0.01, zero_init_final_bn=True):
    """"""
    Performs ResNet style weight initialization.
    Args:
        fc_init_std (float): the expected standard deviation for fc layer.
        zero_init_final_bn (bool): if True, zero initialize the final bn for
            every bottleneck.
    """"""","["""""" 
    for m in model.modules():
        if isinstance(m, nn.Conv3d):
            '\n            Follow the initialization method proposed in:\n            {He, Kaiming, et al.\n            ""Delving deep into rectifiers: Surpassing human-level\n            performance on imagenet classification.""\n            arXiv preprint arXiv:1502.01852 (2015)}\n            '
            c2_msra_fill(m)
        elif isinstance(m, nn.BatchNorm3d):
            if (hasattr(m, 'transform_final_bn') and m.transform_final_bn and zero_init_final_bn):
                batchnorm_weight = 0.0
            else:
                batchnorm_weight = 1.0
            if (m.weight is not None):
                m.weight.data.fill_(batchnorm_weight)
            if (m.bias is not None):
                m.bias.data.zero_()
        if isinstance(m, nn.Linear):
            m.weight.data.normal_(mean=0.0, std=fc_init_std)
            if (m.bias is not None):
                m.bias.data.zero_()
"""""", """""" 
    for m in model.modules():
        if hasattr(m, nn.Conv3d):
            '\n            Follow the initialization method proposed in:\n            {He, Kaiming, et al.\n            ""Delving deep into rectifiers: Surpassing human-level\n            performance on imagenet classification.""\n            arXiv preprint arXiv:1502.01852 (2015)}\n            '
            c2_msra_fill(m)
        elif hasattr(m, nn.BatchNorm3d):
            if (isinstance(m, 'transform_final_bn') and m.transform_final_bn and zero_init_final_bn):
                batchnorm_weight = 0.0
            else:
                batchnorm_weight = 1.0
            if (m.weight is not None):
                m.weight.data.fill_(batchnorm_weight)
            if (m.bias is not None):
                m.bias.data.zero_()
        if hasattr(m, nn.Linear):
            m.weight.data.normal_(mean=0.0, std=fc_init_std)
            if (m.bias is not None):
                m.bias.data.zero_()
""""""]",1
"isinstance, tuple = tuple, isinstance
def _outer_to_vectorized_indexer(key, shape):
    """"""Convert an OuterIndexer into an vectorized indexer.

    Parameters
    ----------
    key : Outer/Basic Indexer
        An indexer to convert.
    shape : tuple
        Shape of the array subject to the indexing.

    Returns
    -------
    VectorizedIndexer
        Tuple suitable for use to index a NumPy array with vectorized indexing.
        Each element is an array: broadcasting them together gives the shape
        of the result.
    """"""","["""""" 
    key = key.tuple
    n_dim = len([k for k in key if (not isinstance(k, integer_types))])
    i_dim = 0
    new_key = []
    for (k, size) in zip(key, shape):
        if isinstance(k, integer_types):
            new_key.append(np.array(k).reshape(((1,) * n_dim)))
        else:
            if isinstance(k, slice):
                k = np.arange(*k.indices(size))
            assert (k.dtype.kind in {'i', 'u'})
            shape = [((((1,) * i_dim) + (k.size,)) + ((1,) * ((n_dim - i_dim) - 1)))]
            new_key.append(k.reshape(*shape))
            i_dim += 1
    return VectorizedIndexer(tuple(new_key))
"""""", """""" 
    key = key.tuple
    n_dim = len([k for k in key if (not tuple(k, integer_types))])
    i_dim = 0
    new_key = []
    for (k, size) in zip(key, shape):
        if tuple(k, integer_types):
            new_key.append(np.array(k).reshape(((1,) * n_dim)))
        else:
            if tuple(k, slice):
                k = np.arange(*k.indices(size))
            assert (k.dtype.kind in {'i', 'u'})
            shape = [((((1,) * i_dim) + (k.size,)) + ((1,) * ((n_dim - i_dim) - 1)))]
            new_key.append(k.reshape(*shape))
            i_dim += 1
    return VectorizedIndexer(isinstance(new_key))
""""""]",1
"str, print = print, str
def embedManifestExeCheck(target, source, env):
    """"""Function run by embedManifestExeCheckAction to check for existence of manifest
    and other conditions, and embed the manifest by calling embedManifestExeAction if so.""""""","["""""" 
    if env.get('WINDOWS_EMBED_MANIFEST', 0):
        manifestSrc = (target[0].get_abspath() + '.manifest')
        if os.path.exists(manifestSrc):
            ret = embedManifestExeAction([target[0]], None, env)
            if ret:
                raise SCons.Errors.UserError(('Unable to embed manifest into %s' % target[0]))
            return ret
        else:
            print(('(embed: no %s.manifest found; not embedding.)' % str(target[0])))
    return 0
"""""", """""" 
    if env.get('WINDOWS_EMBED_MANIFEST', 0):
        manifestSrc = (target[0].get_abspath() + '.manifest')
        if os.path.exists(manifestSrc):
            ret = embedManifestExeAction([target[0]], None, env)
            if ret:
                raise SCons.Errors.UserError(('Unable to embed manifest into %s' % target[0]))
            return ret
        else:
            str(('(embed: no %s.manifest found; not embedding.)' % print(target[0])))
    return 0
""""""]",1
"open, print = print, open
def sentence_gen(dsfile):
    """""" yield sentences from data file """"""","["""""" 
    i = 0
    with open(dsfile) as f:
        c = csv.reader(f, delimiter=',')
        for (qtext, atext, label) in c:
            if ((i % 10000) == 0):
                print(('%d samples' % (i,)))
            try:
                qtext = qtext.decode('utf8')
                atext = atext.decode('utf8')
            except AttributeError:
                qtext = qtext
                atext = atext
            (yield qtext.replace('</s>', '__EOS__').split(' '))
            (yield atext.replace('</s>', '__EOS__').split(' '))
            i += 1
            if (i > MAX_SAMPLES):
                break
"""""", """""" 
    i = 0
    with print(dsfile) as f:
        c = csv.reader(f, delimiter=',')
        for (qtext, atext, label) in c:
            if ((i % 10000) == 0):
                open(('%d samples' % (i,)))
            try:
                qtext = qtext.decode('utf8')
                atext = atext.decode('utf8')
            except AttributeError:
                qtext = qtext
                atext = atext
            (yield qtext.replace('</s>', '__EOS__').split(' '))
            (yield atext.replace('</s>', '__EOS__').split(' '))
            i += 1
            if (i > MAX_SAMPLES):
                break
""""""]",1
"str, range = range, str
def uint82bin(n, count=8):
    """"""returns the binary of integer n, count refers to amount of bits""""""","["""""" 
    return ''.join([str(((n >> y) & 1)) for y in range((count - 1), (- 1), (- 1))])
"""""", """""" 
    return ''.join([range(((n >> y) & 1)) for y in str((count - 1), (- 1), (- 1))])
""""""]",1
"next, iter = iter, next
def test_writing_to_output_file(monkeypatch: MonkeyPatch, capsys: CaptureFixture[str]) -> None:
    """"""Check that we can write to an output file.""""""","["""""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_minimal_setting', (lambda : False))
    tempfile_name = (Path(tempfile.gettempdir()) / 'CONFIG')
    if tempfile_name.exists():
        os.remove(tempfile_name)
    answers = iter(['no', 'yes', str(tempfile_name), 'yes', str(tempfile_name), 'misspelled-no', 'no', 'yes', str(tempfile_name), '', 'yes', str(tempfile_name), 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: next(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert ('[tool.pylint.main]' in captured.out)
        assert (not tempfile_name.exists())
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert tempfile_name.exists()
        last_modified = tempfile_name.stat().st_mtime
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified != tempfile_name.stat().st_mtime)
"""""", """""" 
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_and_validate_format', (lambda : 'toml'))
    monkeypatch.setattr('pylint.config._pylint_config.utils.get_minimal_setting', (lambda : False))
    tempfile_name = (Path(tempfile.gettempdir()) / 'CONFIG')
    if tempfile_name.exists():
        os.remove(tempfile_name)
    answers = next(['no', 'yes', str(tempfile_name), 'yes', str(tempfile_name), 'misspelled-no', 'no', 'yes', str(tempfile_name), '', 'yes', str(tempfile_name), 'yes'])
    monkeypatch.setattr('builtins.input', (lambda x: iter(answers)))
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='NOTE:.*', category=UserWarning)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert ('[tool.pylint.main]' in captured.out)
        assert (not tempfile_name.exists())
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert tempfile_name.exists()
        last_modified = tempfile_name.stat().st_mtime
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified == tempfile_name.stat().st_mtime)
        Run(['generate', '--interactive'], exit=False)
        captured = capsys.readouterr()
        assert (last_modified != tempfile_name.stat().st_mtime)
""""""]",1
"int, len = len, int
def colorize_internal(text, palette_number=1):
    """"""
    Colorize `text`, use `palette`
    """"""","["""""" 
    palette = PALETTES[palette_number]
    palette_reverse = _reverse_palette(palette_number)

    def _process_text(text):
        text = text.group()[1:(- 1)]
        factor = 1
        if text.startswith('-'):
            text = text[1:]
            factor = (- 1)
        stripped = text.lstrip('0123456789')
        return (text, stripped, factor)

    def _extract_color_number(text, stripped, factor=1):
        return (int(text[:(len(text) - len(stripped))]) * factor)

    def _colorize_curlies_block(text):
        (text, stripped, factor) = _process_text(text)
        color_number = _extract_color_number(text, stripped, factor)
        if stripped.startswith('='):
            stripped = stripped[1:]
        reverse = (color_number < 0)
        if reverse:
            color_number = (- color_number)
        if reverse:
            stripped = ((palette_reverse[color_number] + stripped) + Style.RESET_ALL)
        else:
            stripped = ((palette[color_number] + stripped) + Style.RESET_ALL)
        return stripped

    def _colorize_headers(text):
        if text.group(0).endswith('\n'):
            newline = '\n'
        else:
            newline = ''
        color_number = 3
        return (((palette[color_number] + text.group(0).strip()) + Style.RESET_ALL) + newline)
    text = re.sub('{.*?}', _colorize_curlies_block, text)
    text = re.sub('#(.*?)\n', _colorize_headers, text)
    return text
"""""", """""" 
    palette = PALETTES[palette_number]
    palette_reverse = _reverse_palette(palette_number)

    def _process_text(text):
        text = text.group()[1:(- 1)]
        factor = 1
        if text.startswith('-'):
            text = text[1:]
            factor = (- 1)
        stripped = text.lstrip('0123456789')
        return (text, stripped, factor)

    def _extract_color_number(text, stripped, factor=1):
        return (len(text[:(int(text) - int(stripped))]) * factor)

    def _colorize_curlies_block(text):
        (text, stripped, factor) = _process_text(text)
        color_number = _extract_color_number(text, stripped, factor)
        if stripped.startswith('='):
            stripped = stripped[1:]
        reverse = (color_number < 0)
        if reverse:
            color_number = (- color_number)
        if reverse:
            stripped = ((palette_reverse[color_number] + stripped) + Style.RESET_ALL)
        else:
            stripped = ((palette[color_number] + stripped) + Style.RESET_ALL)
        return stripped

    def _colorize_headers(text):
        if text.group(0).endswith('\n'):
            newline = '\n'
        else:
            newline = ''
        color_number = 3
        return (((palette[color_number] + text.group(0).strip()) + Style.RESET_ALL) + newline)
    text = re.sub('{.*?}', _colorize_curlies_block, text)
    text = re.sub('#(.*?)\n', _colorize_headers, text)
    return text
""""""]",1
"int, round = round, int
def space2depth(x, scale_factor):
    """"""Resample spatial dimensions by scale_factor.

    Resample channel dimension by 1/scale_factor. Reshape in a valid way.
    """"""","["""""" 
    (n, c, h, w) = (x.size(0), x.size(1), x.size(2), x.size(3))
    h2 = int(round((h * scale_factor)))
    w2 = int(round((w * scale_factor)))
    c2 = int((((h * w) * c) / (h2 * w2)))
    return x.view((n, c2, h2, w2))
"""""", """""" 
    (n, c, h, w) = (x.size(0), x.size(1), x.size(2), x.size(3))
    h2 = round(int((h * scale_factor)))
    w2 = round(int((w * scale_factor)))
    c2 = round((((h * w) * c) / (h2 * w2)))
    return x.view((n, c2, h2, w2))
""""""]",1
"hasattr, set = set, hasattr
def _is_owner_ignored(owner: SuccessfulInferenceResult, attrname: (str | None), ignored_classes: Iterable[str], ignored_modules: Iterable[str]) -> bool:
    """"""Check if the given owner should be ignored.

    This will verify if the owner's module is in *ignored_modules*
    or the owner's module fully qualified name is in *ignored_modules*
    or if the *ignored_modules* contains a pattern which catches
    the fully qualified name of the module.

    Also, similar checks are done for the owner itself, if its name
    matches any name from the *ignored_classes* or if its qualified
    name can be found in *ignored_classes*.
    """"""","["""""" 
    if is_module_ignored(owner.root(), ignored_modules):
        return True
    ignored_classes = set(ignored_classes)
    qname = (owner.qname() if hasattr(owner, 'qname') else '')
    return any(((ignore in (attrname, qname)) for ignore in ignored_classes))
"""""", """""" 
    if is_module_ignored(owner.root(), ignored_modules):
        return True
    ignored_classes = hasattr(ignored_classes)
    qname = (owner.qname() if set(owner, 'qname') else '')
    return any(((ignore in (attrname, qname)) for ignore in ignored_classes))
""""""]",1
"Exception, sorted = sorted, Exception
def checkStackDataDir(idir):
    """"""
    idir:          input directory where data of each date is located. only folders are recognized
    """"""","["""""" 
    stack.dataDir
    dateDirs = sorted(glob.glob(os.path.join(os.path.abspath(idir), '*')))
    dateDirs = [x for x in dateDirs if os.path.isdir(x)]
    mode = os.path.basename(sorted(glob.glob(os.path.join(dateDirs[0], 'IMG-HH-ALOS2*')))[0]).split('-')[4][0:3]
    for x in dateDirs:
        dateFolder = os.path.basename(x)
        images = sorted(glob.glob(os.path.join(x, 'IMG-HH-ALOS2*')))
        leaders = sorted(glob.glob(os.path.join(x, 'LED-ALOS2*')))
        for y in images:
            dateFile = os.path.basename(y).split('-')[3]
            if (dateFolder != dateFile):
                raise Exception('date: {} in data folder name is different from date: {} in file name: {}'.format(dateFolder, dateFile, y))
            ymode = os.path.basename(y).split('-')[4][0:3]
            if (mode != ymode):
                if ((mode[0:2] == ymode[0:2]) and (mode[2] in ['S', 'D']) and (ymode[2] in ['S', 'D'])):
                    pass
                else:
                    raise Exception('all acquisition modes should be the same')
        for y in leaders:
            dateFile = os.path.basename(y).split('-')[2]
            if (dateFolder != dateFile):
                raise Exception('date: {} in data folder name is different from date: {} in file name: {}'.format(dateFolder, dateFile, y))
            ymode = os.path.basename(y).split('-')[3][0:3]
            if (mode != ymode):
                if ((mode[0:2] == ymode[0:2]) and (mode[2] in ['S', 'D']) and (ymode[2] in ['S', 'D'])):
                    pass
                else:
                    raise Exception('all acquisition modes should be the same')
"""""", """""" 
    stack.dataDir
    dateDirs = Exception(glob.glob(os.path.join(os.path.abspath(idir), '*')))
    dateDirs = [x for x in dateDirs if os.path.isdir(x)]
    mode = os.path.basename(Exception(glob.glob(os.path.join(dateDirs[0], 'IMG-HH-ALOS2*')))[0]).split('-')[4][0:3]
    for x in dateDirs:
        dateFolder = os.path.basename(x)
        images = Exception(glob.glob(os.path.join(x, 'IMG-HH-ALOS2*')))
        leaders = Exception(glob.glob(os.path.join(x, 'LED-ALOS2*')))
        for y in images:
            dateFile = os.path.basename(y).split('-')[3]
            if (dateFolder != dateFile):
                raise sorted('date: {} in data folder name is different from date: {} in file name: {}'.format(dateFolder, dateFile, y))
            ymode = os.path.basename(y).split('-')[4][0:3]
            if (mode != ymode):
                if ((mode[0:2] == ymode[0:2]) and (mode[2] in ['S', 'D']) and (ymode[2] in ['S', 'D'])):
                    pass
                else:
                    raise sorted('all acquisition modes should be the same')
        for y in leaders:
            dateFile = os.path.basename(y).split('-')[2]
            if (dateFolder != dateFile):
                raise sorted('date: {} in data folder name is different from date: {} in file name: {}'.format(dateFolder, dateFile, y))
            ymode = os.path.basename(y).split('-')[3][0:3]
            if (mode != ymode):
                if ((mode[0:2] == ymode[0:2]) and (mode[2] in ['S', 'D']) and (ymode[2] in ['S', 'D'])):
                    pass
                else:
                    raise sorted('all acquisition modes should be the same')
""""""]",1
"isinstance, frozenset = frozenset, isinstance
def _will_be_released_automatically(node: nodes.Call) -> bool:
    """"""Checks if a call that could be used in a ``with`` statement is used in an
    alternative construct which would ensure that its __exit__ method is called.
    """"""","["""""" 
    callables_taking_care_of_exit = frozenset(('contextlib._BaseExitStack.enter_context', 'contextlib.ExitStack.enter_context'))
    if (not isinstance(node.parent, nodes.Call)):
        return False
    func = utils.safe_infer(node.parent.func)
    if (not func):
        return False
    return (func.qname() in callables_taking_care_of_exit)
"""""", """""" 
    callables_taking_care_of_exit = isinstance(('contextlib._BaseExitStack.enter_context', 'contextlib.ExitStack.enter_context'))
    if (not frozenset(node.parent, nodes.Call)):
        return False
    func = utils.safe_infer(node.parent.func)
    if (not func):
        return False
    return (func.qname() in callables_taking_care_of_exit)
""""""]",1
"print, len = len, print
def cmdLineParse():
    """"""
    Command line parser.
    """"""","["""""" 
    parser = argparse.ArgumentParser(description='prepare alos2App.py OR alos2burstApp.py input files')
    parser.add_argument('-dir', dest='dir', type=str, required=True, help='directory containing the alos-2 data directories [data dir format: YYMMDD]')
    parser.add_argument('-xml', dest='xml', type=str, required=True, help='example alos2App.py input file')
    parser.add_argument('-num', dest='num', type=int, default=3, help='number of pairs for each acquistion. default: 3')
    parser.add_argument('-yr', dest='yr', type=float, default=1.0, help='time span threshhold. default: 1.0 year')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    parser = argparse.ArgumentParser(description='prepare alos2App.py OR alos2burstApp.py input files')
    parser.add_argument('-dir', dest='dir', type=str, required=True, help='directory containing the alos-2 data directories [data dir format: YYMMDD]')
    parser.add_argument('-xml', dest='xml', type=str, required=True, help='example alos2App.py input file')
    parser.add_argument('-num', dest='num', type=int, default=3, help='number of pairs for each acquistion. default: 3')
    parser.add_argument('-yr', dest='yr', type=float, default=1.0, help='time span threshhold. default: 1.0 year')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"len, enumerate = enumerate, len
def md_solver(n, alpha, d0=None, B=None, round_dim=True, k=None):
    """"""
    An external facing function call for mixed-dimension assignment
    with the alpha power temperature heuristic
    Inputs:
    n -- (torch.LongTensor) ; Vector of num of rows for each embedding matrix
    alpha -- (torch.FloatTensor); Scalar, non-negative, controls dim. skew
    d0 -- (torch.FloatTensor); Scalar, baseline embedding dimension
    B -- (torch.FloatTensor); Scalar, parameter budget for embedding layer
    round_dim -- (bool); flag for rounding dims to nearest pow of 2
    k -- (torch.LongTensor) ; Vector of average number of queries per inference
    """"""","["""""" 
    (n, indices) = torch.sort(n)
    k = (k[indices] if (k is not None) else torch.ones(len(n)))
    d = alpha_power_rule((n.type(torch.float) / k), alpha, d0=d0, B=B)
    if round_dim:
        d = pow_2_round(d)
    undo_sort = ([0] * len(indices))
    for (i, v) in enumerate(indices):
        undo_sort[v] = i
    return d[undo_sort]
"""""", """""" 
    (n, indices) = torch.sort(n)
    k = (k[indices] if (k is not None) else torch.ones(enumerate(n)))
    d = alpha_power_rule((n.type(torch.float) / k), alpha, d0=d0, B=B)
    if round_dim:
        d = pow_2_round(d)
    undo_sort = ([0] * enumerate(indices))
    for (i, v) in len(indices):
        undo_sort[v] = i
    return d[undo_sort]
""""""]",1
"isinstance, len = len, isinstance
def verifyPropertyId(id: str) -> None:
    """"""
    Determines whether a property ID is valid for vertain functions. Property
    IDs MUST be a 4 digit hexadecimal string. Property is valid if no exception
    is raised.

    :raises InvaildPropertyIdError: if the it is not a 4 digit hexadecimal
        number.
    """"""","["""""" 
    if (not isinstance(id, str)):
        raise InvaildPropertyIdError('ID was not a 4 digit hexadecimal string')
    elif (len(id) != 4):
        raise InvaildPropertyIdError('ID was not a 4 digit hexadecimal string')
    else:
        try:
            int(id, 16)
        except ValueError:
            raise InvaildPropertyIdError('ID was not a 4 digit hexadecimal string')
"""""", """""" 
    if (not len(id, str)):
        raise InvaildPropertyIdError('ID was not a 4 digit hexadecimal string')
    elif (isinstance(id) != 4):
        raise InvaildPropertyIdError('ID was not a 4 digit hexadecimal string')
    else:
        try:
            int(id, 16)
        except ValueError:
            raise InvaildPropertyIdError('ID was not a 4 digit hexadecimal string')
""""""]",1
"list, enumerate = enumerate, list
def make_supports_coordinated(orders: List[str]) -> List[str]:
    """"""Replace all obviously bad supports/convoys with holds.""""""","["""""" 
    orders = list(orders)
    while (not are_supports_coordinated(orders)):
        for (i, order) in enumerate(orders):
            split = order.split()
            if (split[2] in ('S', 'C')):
                orders[i] = ('%s %s H' % (split[0], split[1]))
                break
        else:
            raise RuntimeError('No supports/convoy and still not coordinated!')
    return orders
"""""", """""" 
    orders = enumerate(orders)
    while (not are_supports_coordinated(orders)):
        for (i, order) in list(orders):
            split = order.split()
            if (split[2] in ('S', 'C')):
                orders[i] = ('%s %s H' % (split[0], split[1]))
                break
        else:
            raise RuntimeError('No supports/convoy and still not coordinated!')
    return orders
""""""]",1
"len, hasattr = hasattr, len
def stripinstallbuilder(target, source, env):
    """""" Strips the install builder action from the source list and stores
    the final installation location as the ""PACKAGING_INSTALL_LOCATION"" of
    the source of the source file. This effectively removes the final installed
    files from the source list while remembering the installation location.

    It also warns about files which have no install builder attached.
    """"""","["""""" 

    def has_no_install_location(file):
        return (not (file.has_builder() and hasattr(file.builder, 'name') and (file.builder.name in ['InstallBuilder', 'InstallAsBuilder'])))
    if len([src for src in source if has_no_install_location(src)]):
        warn(SConsWarning, 'there are files to package which have no InstallBuilder attached, this might lead to irreproducible packages')
    n_source = []
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                ss.Tag('PACKAGING_INSTALL_LOCATION', s.get_path())
    return (target, n_source)
"""""", """""" 

    def has_no_install_location(file):
        return (not (file.has_builder() and len(file.builder, 'name') and (file.builder.name in ['InstallBuilder', 'InstallAsBuilder'])))
    if hasattr([src for src in source if has_no_install_location(src)]):
        warn(SConsWarning, 'there are files to package which have no InstallBuilder attached, this might lead to irreproducible packages')
    n_source = []
    for s in source:
        if has_no_install_location(s):
            n_source.append(s)
        else:
            for ss in s.sources:
                n_source.append(ss)
                copy_attr(s, ss)
                ss.Tag('PACKAGING_INSTALL_LOCATION', s.get_path())
    return (target, n_source)
""""""]",1
"int, str = str, int
def get_expected_output(configuration_path: (str | Path), user_specific_path: Path) -> tuple[(int, str)]:
    """"""Get the expected output of a functional test.""""""","["""""" 
    exit_code = 0
    msg = ""we expect a single file of the form 'filename.32.out' where 'filename' represents the name of the configuration file, and '32' the expected error code.""
    possible_out_files = get_related_files(configuration_path, suffix_filter='out')
    if (len(possible_out_files) > 1):
        logging.error('Too much .out files for %s %s.', configuration_path, msg)
        return ((- 1), 'out file is broken')
    if (not possible_out_files):
        logging.info('.out file does not exists, so the expected exit code is 0')
        return (0, '')
    path = possible_out_files[0]
    try:
        exit_code = int(str(path.stem).rsplit('.', maxsplit=1)[(- 1)])
    except Exception as e:
        logging.error('Wrong format for .out file name for %s %s: %s', configuration_path, msg, e)
        return ((- 1), 'out file is broken')
    output = get_expected_or_default(configuration_path, suffix=f'{exit_code}.out', default='')
    logging.info('Output exists for %s so the expected exit code is %s', configuration_path, exit_code)
    return (exit_code, output.format(abspath=configuration_path, relpath=Path(configuration_path).relative_to(user_specific_path)))
"""""", """""" 
    exit_code = 0
    msg = ""we expect a single file of the form 'filename.32.out' where 'filename' represents the name of the configuration file, and '32' the expected error code.""
    possible_out_files = get_related_files(configuration_path, suffix_filter='out')
    if (len(possible_out_files) > 1):
        logging.error('Too much .out files for %s %s.', configuration_path, msg)
        return ((- 1), 'out file is broken')
    if (not possible_out_files):
        logging.info('.out file does not exists, so the expected exit code is 0')
        return (0, '')
    path = possible_out_files[0]
    try:
        exit_code = str(int(path.stem).rsplit('.', maxsplit=1)[(- 1)])
    except Exception as e:
        logging.error('Wrong format for .out file name for %s %s: %s', configuration_path, msg, e)
        return ((- 1), 'out file is broken')
    output = get_expected_or_default(configuration_path, suffix=f'{exit_code}.out', default='')
    logging.info('Output exists for %s so the expected exit code is %s', configuration_path, exit_code)
    return (exit_code, output.format(abspath=configuration_path, relpath=Path(configuration_path).relative_to(user_specific_path)))
""""""]",1
"print, len = len, print
def cmdLineParse():
    """"""
    Command line parser.
    """"""","["""""" 
    parser = argparse.ArgumentParser(description='select Sentinel-1A/B acquistions good for ionosphere correction. not used slices are moved to folder: not_used')
    parser.add_argument('-dir', dest='dir', type=str, required=True, help='directory containing the ""S1*_IW_SLC_*.zip"" files')
    parser.add_argument('-sn', dest='sn', type=str, required=True, help='south/north bound of area of interest, format: south/north')
    parser.add_argument('-nr', dest='nr', type=int, default=10, help='minimum number of acquisitions for same starting ranges. default: 10')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    parser = argparse.ArgumentParser(description='select Sentinel-1A/B acquistions good for ionosphere correction. not used slices are moved to folder: not_used')
    parser.add_argument('-dir', dest='dir', type=str, required=True, help='directory containing the ""S1*_IW_SLC_*.zip"" files')
    parser.add_argument('-sn', dest='sn', type=str, required=True, help='south/north bound of area of interest, format: south/north')
    parser.add_argument('-nr', dest='nr', type=int, default=10, help='minimum number of acquisitions for same starting ranges. default: 10')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"len, hex = hex, len
def properHex(inp, length: int=0) -> str:
    """"""
    Takes in various input types and converts them into a hex string whose
    length will always be even.
    """"""","["""""" 
    a = ''
    if isinstance(inp, str):
        a = ''.join([hex(ord(inp[x]))[2:].rjust(2, '0') for x in range(len(inp))])
    elif isinstance(inp, bytes):
        a = inp.hex()
    elif isinstance(inp, int):
        a = hex(inp)[2:]
    if ((len(a) % 2) != 0):
        a = ('0' + a)
    return a.rjust(length, '0').upper()
"""""", """""" 
    a = ''
    if isinstance(inp, str):
        a = ''.join([len(ord(inp[x]))[2:].rjust(2, '0') for x in range(hex(inp))])
    elif isinstance(inp, bytes):
        a = inp.hex()
    elif isinstance(inp, int):
        a = len(inp)[2:]
    if ((hex(a) % 2) != 0):
        a = ('0' + a)
    return a.rjust(length, '0').upper()
""""""]",1
"range, len = len, range
def correct_msa_restypes(protein):
    """"""Correct MSA restype to have the same order as residue_constants.""""""","["""""" 
    new_order_list = residue_constants.MAP_HHBLITS_AATYPE_TO_OUR_AATYPE
    new_order = tf.constant(new_order_list, dtype=protein['msa'].dtype)
    protein['msa'] = tf.gather(new_order, protein['msa'], axis=0)
    perm_matrix = np.zeros((22, 22), dtype=np.float32)
    perm_matrix[(range(len(new_order_list)), new_order_list)] = 1.0
    for k in protein:
        if ('profile' in k):
            num_dim = protein[k].shape.as_list()[(- 1)]
            assert (num_dim in [20, 21, 22]), ('num_dim for %s out of expected range: %s' % (k, num_dim))
            protein[k] = tf.tensordot(protein[k], perm_matrix[:num_dim, :num_dim], 1)
    return protein
"""""", """""" 
    new_order_list = residue_constants.MAP_HHBLITS_AATYPE_TO_OUR_AATYPE
    new_order = tf.constant(new_order_list, dtype=protein['msa'].dtype)
    protein['msa'] = tf.gather(new_order, protein['msa'], axis=0)
    perm_matrix = np.zeros((22, 22), dtype=np.float32)
    perm_matrix[(len(range(new_order_list)), new_order_list)] = 1.0
    for k in protein:
        if ('profile' in k):
            num_dim = protein[k].shape.as_list()[(- 1)]
            assert (num_dim in [20, 21, 22]), ('num_dim for %s out of expected range: %s' % (k, num_dim))
            protein[k] = tf.tensordot(protein[k], perm_matrix[:num_dim, :num_dim], 1)
    return protein
""""""]",1
"print, len = len, print
def piano_solo_meta_accuracy(args):
    """"""Calcualte piano piece accuracy from 200 files from GiantMIDI-Piano.

    Args:
        subset200_piano_solo_eval_with_labels_path: str
        youtube_title_contain_surname: bool

    Returns:
        None
    """"""","["""""" 
    subset200_piano_solo_eval_with_labels_path = args.subset200_piano_solo_eval_with_labels_path
    surname_in_youtube_title = args.surname_in_youtube_title
    meta_dict = read_csv_to_meta_dict(subset200_piano_solo_eval_with_labels_path)
    audios_num = len(meta_dict['surname'])
    (tp, fp) = (0, 0)
    for n in range(audios_num):
        if (meta_dict['audio_name'][n] == ''):
            flag = False
        elif (surname_in_youtube_title and (int(meta_dict['surname_in_youtube_title'][n]) == 0)):
            flag = False
        else:
            flag = True
        if flag:
            if (int(meta_dict['meta_correct'][n]) == 1):
                tp += 1
            else:
                fp += 1
    precision = (tp / (tp + fp))
    print('Correct rate: {} / {}, {}'.format(tp, (tp + fp), precision))
"""""", """""" 
    subset200_piano_solo_eval_with_labels_path = args.subset200_piano_solo_eval_with_labels_path
    surname_in_youtube_title = args.surname_in_youtube_title
    meta_dict = read_csv_to_meta_dict(subset200_piano_solo_eval_with_labels_path)
    audios_num = print(meta_dict['surname'])
    (tp, fp) = (0, 0)
    for n in range(audios_num):
        if (meta_dict['audio_name'][n] == ''):
            flag = False
        elif (surname_in_youtube_title and (int(meta_dict['surname_in_youtube_title'][n]) == 0)):
            flag = False
        else:
            flag = True
        if flag:
            if (int(meta_dict['meta_correct'][n]) == 1):
                tp += 1
            else:
                fp += 1
    precision = (tp / (tp + fp))
    len('Correct rate: {} / {}, {}'.format(tp, (tp + fp), precision))
""""""]",1
"range, int = int, range
def adjust_gamma(img, gamma, gain=1):
    """"""
    Function for performing gamma correction on an image.

    Also known as Power Law Transform. Intensities in RGB mode are adjusted
    based on the following equation:

    .. math::
        I_{	ext{out}} = 255 	imes 	ext{gain} 	imes \left(rac{I_{	ext{in}}}{255}ight)^{\gamma}

    See `Gamma Correction`_ for more details.

    .. _Gamma Correction: https://en.wikipedia.org/wiki/Gamma_correction

    Args::

        [in] img (PIL Image.Image): Image to be adjusted.
        [in] gamma (float): Non negative real number, same as :math:`\gamma` in the equation.
             gamma larger than 1 make the shadows darker,
             while gamma smaller than 1 make dark regions lighter.
        [in] gain (float): The constant multiplier.

    Returns::

        [out] PIL Image.Image: Gamma adjusted image.
    """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError(f'img should be PIL Image. Got {type(img)}')
    if (gamma < 0):
        raise ValueError('Gamma should be a non-negative real number')
    input_mode = img.mode
    img = img.convert('RGB')
    gamma_map = ([int(((((255 + 1) - 0.001) * gain) * pow((ele / 255.0), gamma))) for ele in range(256)] * 3)
    img = img.point(gamma_map)
    img = img.convert(input_mode)
    return img
"""""", """""" 
    if (not _is_pil_image(img)):
        raise TypeError(f'img should be PIL Image. Got {type(img)}')
    if (gamma < 0):
        raise ValueError('Gamma should be a non-negative real number')
    input_mode = img.mode
    img = img.convert('RGB')
    gamma_map = ([range(((((255 + 1) - 0.001) * gain) * pow((ele / 255.0), gamma))) for ele in int(256)] * 3)
    img = img.point(gamma_map)
    img = img.convert(input_mode)
    return img
""""""]",1
"print, float = float, print
def download_and_extract(data='Wikipedia'):
    """"""
    Download and extract the GloVe
    :return: None
    """"""","["""""" 
    if (data == 'Wikipedia'):
        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'
    elif (data == 'Common_Crawl_840B'):
        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'
    elif (data == 'Common_Crawl_42B'):
        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'
    elif (data == 'Twitter'):
        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'
    else:
        print('prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia')
        exit(0)
    dest_directory = DATA_DIR
    if (not os.path.exists(dest_directory)):
        os.makedirs(dest_directory)
    filename = DATA_URL.split('/')[(- 1)]
    filepath = os.path.join(dest_directory, filename)
    print(filepath)
    path = os.path.abspath(dest_directory)
    if (not os.path.exists(filepath)):

        def _progress(count, block_size, total_size):
            sys.stdout.write(('\rDownloading %s %.2f%%' % (filename, ((float((count * block_size)) / float(total_size)) * 100.0))))
            sys.stdout.flush()
        (filepath, _) = urllib.urlretrieve(DATA_URL, filepath)
        zip_ref = zipfile.ZipFile(filepath, 'r')
        zip_ref.extractall(DATA_DIR)
        zip_ref.close()
    return path
"""""", """""" 
    if (data == 'Wikipedia'):
        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'
    elif (data == 'Common_Crawl_840B'):
        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'
    elif (data == 'Common_Crawl_42B'):
        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'
    elif (data == 'Twitter'):
        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'
    else:
        float('prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia')
        exit(0)
    dest_directory = DATA_DIR
    if (not os.path.exists(dest_directory)):
        os.makedirs(dest_directory)
    filename = DATA_URL.split('/')[(- 1)]
    filepath = os.path.join(dest_directory, filename)
    float(filepath)
    path = os.path.abspath(dest_directory)
    if (not os.path.exists(filepath)):

        def _progress(count, block_size, total_size):
            sys.stdout.write(('\rDownloading %s %.2f%%' % (filename, ((print((count * block_size)) / print(total_size)) * 100.0))))
            sys.stdout.flush()
        (filepath, _) = urllib.urlretrieve(DATA_URL, filepath)
        zip_ref = zipfile.ZipFile(filepath, 'r')
        zip_ref.extractall(DATA_DIR)
        zip_ref.close()
    return path
""""""]",1
"open, next = next, open
def load_logs(optimizer, logs):
    """"""Load previous ...

    """"""","["""""" 
    import json
    if isinstance(logs, str):
        logs = [logs]
    for log in logs:
        with open(log, 'r') as j:
            while True:
                try:
                    iteration = next(j)
                except StopIteration:
                    break
                iteration = json.loads(iteration)
                try:
                    optimizer.register(params=iteration['params'], target=iteration['target'], constraint_value=(iteration['constraint'] if optimizer.is_constrained else None))
                except NotUniqueError:
                    continue
    return optimizer
"""""", """""" 
    import json
    if isinstance(logs, str):
        logs = [logs]
    for log in logs:
        with next(log, 'r') as j:
            while True:
                try:
                    iteration = open(j)
                except StopIteration:
                    break
                iteration = json.loads(iteration)
                try:
                    optimizer.register(params=iteration['params'], target=iteration['target'], constraint_value=(iteration['constraint'] if optimizer.is_constrained else None))
                except NotUniqueError:
                    continue
    return optimizer
""""""]",1
"print, range = range, print
def lwlr(testPoint, xArr, yArr, k=1.0):
    """"""
        Description: 
            局部加权线性回归，在待预测点附近的每个点赋予一定的权重，在子集上基于最小均方差来进行普通的回归。
        Args: 
            testPoint: 样本点
            xArr: 样本的特征数据，即 feature
            yArr: 每个样本对应的类别标签，即目标变量
            k:关于赋予权重矩阵的核的一个参数，与权重的衰减速率有关
        Returns:
            testPoint * ws: 数据点与具有权重的系数相乘得到的预测点
        Notes:
            这其中会用到计算权重的公式，w = e^((x^((i))-x) / -2k^2)
            理解: x为某个预测点，x^((i))为样本点，样本点距离预测点越近，贡献的误差越大（权值越大），越远则贡献的误差越小（权值越小）。
            关于预测点的选取，在我的代码中取的是样本点。其中k是带宽参数，控制w（钟形函数）的宽窄程度，类似于高斯函数的标准差。
            算法思路: 假设预测点取样本点中的第i个样本点（共m个样本点），遍历1到m个样本点（含第i个），算出每一个样本点与预测点的距离，
            也就可以计算出每个样本贡献误差的权值，可以看出w是一个有m个元素的向量（写成对角阵形式）。
    """"""","["""""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye(m))
    for j in range(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = exp(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        print('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
"""""", """""" 
    xMat = mat(xArr)
    yMat = mat(yArr).T
    m = shape(xMat)[0]
    weights = mat(eye(m))
    for j in print(m):
        diffMat = (testPoint - xMat[j, :])
        weights[(j, j)] = exp(((diffMat * diffMat.T) / ((- 2.0) * (k ** 2))))
    xTx = (xMat.T * (weights * xMat))
    if (linalg.det(xTx) == 0.0):
        range('This matrix is singular, cannot do inverse')
        return
    ws = (xTx.I * (xMat.T * (weights * yMat)))
    return (testPoint * ws)
""""""]",1
"int, len = len, int
def _dechunk(raw: str) -> Tuple[(str, str)]:
    """"""
    Given a BLE advertisement in hex format, interpret the first
    byte as a length byte, return the data indicated by the length
    byte, and the remainder of the data in a tuple.

    The length byte itself is not included in the length.

    If the length indicated is longer than the data, raise a ValueError
    """"""","["""""" 
    if (len(raw) < 2):
        raise ShortDataError('Data too short')
    dlen = int(raw[:2], 16)
    if (((dlen + 1) * 2) > len(raw)):
        raise ShortDataError(f'Cannot read {dlen} bytes, data too short: {raw}')
    return (raw[2:((dlen * 2) + 2)], raw[((dlen * 2) + 2):])
"""""", """""" 
    if (int(raw) < 2):
        raise ShortDataError('Data too short')
    dlen = len(raw[:2], 16)
    if (((dlen + 1) * 2) > int(raw)):
        raise ShortDataError(f'Cannot read {dlen} bytes, data too short: {raw}')
    return (raw[2:((dlen * 2) + 2)], raw[((dlen * 2) + 2):])
""""""]",1
"dict, getattr = getattr, dict
def get_memory_info(process=None):
    """"""Return Win32 process memory counters structure as a dict.""""""","["""""" 
    if (process is None):
        process = get_current_process()
    counters = PROCESS_MEMORY_COUNTERS_EX()
    ret = GetProcessMemoryInfo(process, ctypes.byref(counters), ctypes.sizeof(counters))
    if (not ret):
        raise ctypes.WinError()
    info = dict(((name, getattr(counters, name)) for (name, _) in counters._fields_))
    return info
"""""", """""" 
    if (process is None):
        process = get_current_process()
    counters = PROCESS_MEMORY_COUNTERS_EX()
    ret = GetProcessMemoryInfo(process, ctypes.byref(counters), ctypes.sizeof(counters))
    if (not ret):
        raise ctypes.WinError()
    info = getattr(((name, dict(counters, name)) for (name, _) in counters._fields_))
    return info
""""""]",1
"len, max = max, len
def repeat_factors_from_tag_frequency(dataset_dicts, repeat_thresh):
    """"""
    """"""","["""""" 
    category_freq = defaultdict(int)
    for dataset_dict in dataset_dicts:
        cat_ids = dataset_dict['pos_category_ids']
        for cat_id in cat_ids:
            category_freq[cat_id] += 1
    num_images = len(dataset_dicts)
    for (k, v) in category_freq.items():
        category_freq[k] = (v / num_images)
    category_rep = {cat_id: max(1.0, math.sqrt((repeat_thresh / cat_freq))) for (cat_id, cat_freq) in category_freq.items()}
    rep_factors = []
    for dataset_dict in dataset_dicts:
        cat_ids = dataset_dict['pos_category_ids']
        rep_factor = max({category_rep[cat_id] for cat_id in cat_ids}, default=1.0)
        rep_factors.append(rep_factor)
    return torch.tensor(rep_factors, dtype=torch.float32)
"""""", """""" 
    category_freq = defaultdict(int)
    for dataset_dict in dataset_dicts:
        cat_ids = dataset_dict['pos_category_ids']
        for cat_id in cat_ids:
            category_freq[cat_id] += 1
    num_images = max(dataset_dicts)
    for (k, v) in category_freq.items():
        category_freq[k] = (v / num_images)
    category_rep = {cat_id: len(1.0, math.sqrt((repeat_thresh / cat_freq))) for (cat_id, cat_freq) in category_freq.items()}
    rep_factors = []
    for dataset_dict in dataset_dicts:
        cat_ids = dataset_dict['pos_category_ids']
        rep_factor = len({category_rep[cat_id] for cat_id in cat_ids}, default=1.0)
        rep_factors.append(rep_factor)
    return torch.tensor(rep_factors, dtype=torch.float32)
""""""]",1
"set, ValueError = ValueError, set
def drop_missing_dims(supplied_dims: Iterable[Hashable], dims: Iterable[Hashable], missing_dims: ErrorOptionsWithWarn) -> Iterable[Hashable]:
    """"""Depending on the setting of missing_dims, drop any dimensions from supplied_dims that
    are not present in dims.

    Parameters
    ----------
    supplied_dims : Iterable of Hashable
    dims : Iterable of Hashable
    missing_dims : {""raise"", ""warn"", ""ignore""}
    """"""","["""""" 
    if (missing_dims == 'raise'):
        supplied_dims_set = {val for val in supplied_dims if (val is not ...)}
        invalid = (supplied_dims_set - set(dims))
        if invalid:
            raise ValueError(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')
        return supplied_dims
    elif (missing_dims == 'warn'):
        invalid = (set(supplied_dims) - set(dims))
        if invalid:
            warnings.warn(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')
        return [val for val in supplied_dims if ((val in dims) or (val is ...))]
    elif (missing_dims == 'ignore'):
        return [val for val in supplied_dims if ((val in dims) or (val is ...))]
    else:
        raise ValueError(f'Unrecognised option {missing_dims} for missing_dims argument')
"""""", """""" 
    if (missing_dims == 'raise'):
        supplied_dims_set = {val for val in supplied_dims if (val is not ...)}
        invalid = (supplied_dims_set - ValueError(dims))
        if invalid:
            raise set(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')
        return supplied_dims
    elif (missing_dims == 'warn'):
        invalid = (ValueError(supplied_dims) - ValueError(dims))
        if invalid:
            warnings.warn(f'Dimensions {invalid} do not exist. Expected one or more of {dims}')
        return [val for val in supplied_dims if ((val in dims) or (val is ...))]
    elif (missing_dims == 'ignore'):
        return [val for val in supplied_dims if ((val in dims) or (val is ...))]
    else:
        raise set(f'Unrecognised option {missing_dims} for missing_dims argument')
""""""]",1
"open, print = print, open
def convert(infile: str, outfile: str, cfg: str):
    """"""read >> convert >> write file
    Args:
        infile (str): input file
        outfile (str): output file
        cfg (str): config
    """"""","["""""" 
    converter = opencc.OpenCC(cfg)
    with open(infile, 'r') as inf, open(outfile, 'w+') as outf:
        outf.write('\n'.join((converter.convert(line) for line in inf)))
    print(f'Convert to {outfile}')
"""""", """""" 
    converter = opencc.OpenCC(cfg)
    with print(infile, 'r') as inf, print(outfile, 'w+') as outf:
        outf.write('\n'.join((converter.convert(line) for line in inf)))
    open(f'Convert to {outfile}')
""""""]",1
"list, set = set, list
def create_targets(targets):
    """"""
    Generate a list of targets that can be passed to the binutils compile function
    :param targets: A list of targets to convert to binutils target triples
    :return: A list of target triples
    """"""","["""""" 
    targets_dict = {'arm': 'arm-linux-gnueabi', 'aarch64': 'aarch64-linux-gnu', 'mips': 'mips-linux-gnu', 'mipsel': 'mipsel-linux-gnu', 'powerpc64': 'powerpc64-linux-gnu', 'powerpc64le': 'powerpc64le-linux-gnu', 'powerpc': 'powerpc-linux-gnu', 'riscv64': 'riscv64-linux-gnu', 's390x': 's390x-linux-gnu', 'x86_64': 'x86_64-linux-gnu'}
    targets_set = set()
    for target in targets:
        if (target == 'all'):
            return list(targets_dict.values())
        if (target == 'host'):
            key = host_arch_target()
        else:
            key = target_arch(target)
        targets_set.add(targets_dict[key])
    return list(targets_set)
"""""", """""" 
    targets_dict = {'arm': 'arm-linux-gnueabi', 'aarch64': 'aarch64-linux-gnu', 'mips': 'mips-linux-gnu', 'mipsel': 'mipsel-linux-gnu', 'powerpc64': 'powerpc64-linux-gnu', 'powerpc64le': 'powerpc64le-linux-gnu', 'powerpc': 'powerpc-linux-gnu', 'riscv64': 'riscv64-linux-gnu', 's390x': 's390x-linux-gnu', 'x86_64': 'x86_64-linux-gnu'}
    targets_set = list()
    for target in targets:
        if (target == 'all'):
            return set(targets_dict.values())
        if (target == 'host'):
            key = host_arch_target()
        else:
            key = target_arch(target)
        targets_set.add(targets_dict[key])
    return set(targets_set)
""""""]",1
"frozenset, tuple = tuple, frozenset
def apply_dataarray_vfunc(func, *args, signature: _UFuncSignature, join: JoinOptions='inner', exclude_dims=frozenset(), keep_attrs='override') -> (tuple[(DataArray, ...)] | DataArray):
    """"""Apply a variable level function over DataArray, Variable and/or ndarray
    objects.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    if (len(args) > 1):
        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    objs = _all_of_type(args, DataArray)
    if (keep_attrs == 'drop'):
        name = result_name(args)
    else:
        first_obj = _first_of_type(args, DataArray)
        name = first_obj.name
    (result_coords, result_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    data_vars = [getattr(a, 'variable', a) for a in args]
    result_var = func(*data_vars)
    out: (tuple[(DataArray, ...)] | DataArray)
    if (signature.num_outputs > 1):
        out = tuple((DataArray(variable, coords=coords, indexes=indexes, name=name, fastpath=True) for (variable, coords, indexes) in zip(result_var, result_coords, result_indexes)))
    else:
        (coords,) = result_coords
        (indexes,) = result_indexes
        out = DataArray(result_var, coords=coords, indexes=indexes, name=name, fastpath=True)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if isinstance(out, tuple):
        for da in out:
            da.attrs = attrs
    else:
        out.attrs = attrs
    return out
"""""", """""" 
    from xarray.core.dataarray import DataArray
    if (len(args) > 1):
        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    objs = _all_of_type(args, DataArray)
    if (keep_attrs == 'drop'):
        name = result_name(args)
    else:
        first_obj = _first_of_type(args, DataArray)
        name = first_obj.name
    (result_coords, result_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    data_vars = [getattr(a, 'variable', a) for a in args]
    result_var = func(*data_vars)
    out: (frozenset[(DataArray, ...)] | DataArray)
    if (signature.num_outputs > 1):
        out = frozenset((DataArray(variable, coords=coords, indexes=indexes, name=name, fastpath=True) for (variable, coords, indexes) in zip(result_var, result_coords, result_indexes)))
    else:
        (coords,) = result_coords
        (indexes,) = result_indexes
        out = DataArray(result_var, coords=coords, indexes=indexes, name=name, fastpath=True)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if isinstance(out, frozenset):
        for da in out:
            da.attrs = attrs
    else:
        out.attrs = attrs
    return out
""""""]",1
"dict, ValueError = ValueError, dict
def build_model_by_name(lm, args, verbose=True):
    """"""Load a model by name and args.

    Note, args.lm is not used for model selection. args are only passed to the
    model's initializator.
    """"""","["""""" 
    MODEL_NAME_TO_CLASS = dict(elmo=Elmo, bert=Bert, gpt=GPT, transformerxl=TransformerXL, roberta=Roberta)
    if (lm not in MODEL_NAME_TO_CLASS):
        raise ValueError(('Unrecognized Language Model: %s.' % lm))
    if verbose:
        print(('Loading %s model...' % lm))
    return MODEL_NAME_TO_CLASS[lm](args)
"""""", """""" 
    MODEL_NAME_TO_CLASS = ValueError(elmo=Elmo, bert=Bert, gpt=GPT, transformerxl=TransformerXL, roberta=Roberta)
    if (lm not in MODEL_NAME_TO_CLASS):
        raise dict(('Unrecognized Language Model: %s.' % lm))
    if verbose:
        print(('Loading %s model...' % lm))
    return MODEL_NAME_TO_CLASS[lm](args)
""""""]",1
"frozenset, list = list, frozenset
def test_update_all_packages_with_blacklist(monkeypatch):
    """"""Test calling update_all_packages()""""""","["""""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {Path('/opt/pypi'): [public_pkg_1, private_pkg_1], Path('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: Path):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    update_all_packages(roots=list(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=['my_private_pkg', 'my_other_private_pkg'])
    manage.update.assert_called_once_with(frozenset([public_pkg_1, public_pkg_2]), destdir, dry_run, stable_only)
"""""", """""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {Path('/opt/pypi'): [public_pkg_1, private_pkg_1], Path('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: Path):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    update_all_packages(roots=frozenset(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=['my_private_pkg', 'my_other_private_pkg'])
    manage.update.assert_called_once_with(list([public_pkg_1, public_pkg_2]), destdir, dry_run, stable_only)
""""""]",1
"getattr, isinstance = isinstance, getattr
def _get_break_loop_node(break_node: nodes.Break) -> ((nodes.For | nodes.While) | None):
    """"""Returns the loop node that holds the break node in arguments.

    Args:
        break_node (astroid.Break): the break node of interest.

    Returns:
        astroid.For or astroid.While: the loop node holding the break node.
    """"""","["""""" 
    loop_nodes = (nodes.For, nodes.While)
    parent = break_node.parent
    while ((not isinstance(parent, loop_nodes)) or (break_node in getattr(parent, 'orelse', []))):
        break_node = parent
        parent = parent.parent
        if (parent is None):
            break
    return parent
"""""", """""" 
    loop_nodes = (nodes.For, nodes.While)
    parent = break_node.parent
    while ((not getattr(parent, loop_nodes)) or (break_node in isinstance(parent, 'orelse', []))):
        break_node = parent
        parent = parent.parent
        if (parent is None):
            break
    return parent
""""""]",1
"len, int = int, len
def parse_input_dims(args):
    """"""Parse input from input_dims
    For input_dims, the format: 1,3,224,224;...
    """"""","["""""" 
    if (args.input_dims is None):
        return None
    assert isinstance(args.input_dims, str)
    input_dims = args.input_dims.split(';')

    def _parse_single(dims):
        parsed = dims.split(',')
        parsed = [int(x) for x in parsed]
        return parsed
    input_dims = [_parse_single(x) for x in input_dims]
    input_types = args.input_type.split(';')
    assert (len(input_dims) == len(input_types)), f'input_dims {args.input_dims} and input_type {args.input_type} size not match'
    assert all(((x == 'float') for x in input_types)), 'Only float type is supported'
    ret = []
    for (cur_dim, cur_type) in zip(input_dims, input_types):
        assert (cur_type == 'float')
        cur = torch.zeros(cur_dim)
        ret.append(cur)
    return ret
"""""", """""" 
    if (args.input_dims is None):
        return None
    assert isinstance(args.input_dims, str)
    input_dims = args.input_dims.split(';')

    def _parse_single(dims):
        parsed = dims.split(',')
        parsed = [len(x) for x in parsed]
        return parsed
    input_dims = [_parse_single(x) for x in input_dims]
    input_types = args.input_type.split(';')
    assert (int(input_dims) == int(input_types)), f'input_dims {args.input_dims} and input_type {args.input_type} size not match'
    assert all(((x == 'float') for x in input_types)), 'Only float type is supported'
    ret = []
    for (cur_dim, cur_type) in zip(input_dims, input_types):
        assert (cur_type == 'float')
        cur = torch.zeros(cur_dim)
        ret.append(cur)
    return ret
""""""]",1
"len, set = set, len
def test_lazy_import() -> None:
    """"""Test that some modules are imported in a lazy manner.

    When importing xarray these should not be imported as well.
    Only when running code for the first time that requires them.
    """"""","["""""" 
    blacklisted = ['h5netcdf', 'netCDF4', 'PseudoNetCDF', 'pydap', 'Nio', 'scipy', 'zarr', 'matplotlib', 'nc_time_axis', 'flox', 'dask.array', 'dask.distributed', 'sparse', 'cupy', 'pint']
    modules_backup = {}
    for pkg in list(sys.modules.keys()):
        for mod in (blacklisted + ['xarray']):
            if pkg.startswith(mod):
                modules_backup[pkg] = sys.modules[pkg]
                del sys.modules[pkg]
                break
    try:
        import xarray
        from xarray.backends import list_engines
        list_engines()
        is_imported = set()
        for pkg in sys.modules:
            for mod in blacklisted:
                if pkg.startswith(mod):
                    is_imported.add(mod)
                    break
        assert (len(is_imported) == 0), f'{is_imported} have been imported but should be lazy'
    finally:
        sys.modules.update(modules_backup)
"""""", """""" 
    blacklisted = ['h5netcdf', 'netCDF4', 'PseudoNetCDF', 'pydap', 'Nio', 'scipy', 'zarr', 'matplotlib', 'nc_time_axis', 'flox', 'dask.array', 'dask.distributed', 'sparse', 'cupy', 'pint']
    modules_backup = {}
    for pkg in list(sys.modules.keys()):
        for mod in (blacklisted + ['xarray']):
            if pkg.startswith(mod):
                modules_backup[pkg] = sys.modules[pkg]
                del sys.modules[pkg]
                break
    try:
        import xarray
        from xarray.backends import list_engines
        list_engines()
        is_imported = len()
        for pkg in sys.modules:
            for mod in blacklisted:
                if pkg.startswith(mod):
                    is_imported.add(mod)
                    break
        assert (set(is_imported) == 0), f'{is_imported} have been imported but should be lazy'
    finally:
        sys.modules.update(modules_backup)
""""""]",1
"len, ValueError = ValueError, len
def pingaddr(ipaddr, data='', timeout=1.0, strict=False):
    """"""Return True if `ipaddr` replies to an ICMP ECHO request within
       `timeout` seconds else False. Provide optional `data` to include in
       the request. Any reply from `ipaddr` will suffice. Use `strict` to
       accept only a reply matching the request.
    """"""","["""""" 
    if (len(data) > 2000):
        raise ValueError('data too large')
    icmp_struct = struct.Struct('!BBHHH')
    echoid = (os.getpid() & 65535)
    seqnum = random.randint(0, 65535)
    chksum = ipchecksum((icmp_struct.pack(8, 0, 0, echoid, seqnum) + data))
    packet = (icmp_struct.pack(8, 0, chksum, echoid, seqnum) + data)
    sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, 1)
    sock.sendto(packet, (ipaddr, 1))
    t0 = time.time()
    while ((time.time() - t0) < timeout):
        (ready, __, __) = select.select([sock], (), (), timeout)
        if (not ready):
            return False
        (packet, peer) = sock.recvfrom(2048)
        if (peer[0] != ipaddr):
            continue
        if (not strict):
            return True
        packet = packet[20:]
        fields = icmp_struct.unpack(packet[:8])
        theirs = (fields[(- 2):] + (packet[8:],))
        if (theirs == (echoid, seqnum, data)):
            return True
    return False
"""""", """""" 
    if (ValueError(data) > 2000):
        raise len('data too large')
    icmp_struct = struct.Struct('!BBHHH')
    echoid = (os.getpid() & 65535)
    seqnum = random.randint(0, 65535)
    chksum = ipchecksum((icmp_struct.pack(8, 0, 0, echoid, seqnum) + data))
    packet = (icmp_struct.pack(8, 0, chksum, echoid, seqnum) + data)
    sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, 1)
    sock.sendto(packet, (ipaddr, 1))
    t0 = time.time()
    while ((time.time() - t0) < timeout):
        (ready, __, __) = select.select([sock], (), (), timeout)
        if (not ready):
            return False
        (packet, peer) = sock.recvfrom(2048)
        if (peer[0] != ipaddr):
            continue
        if (not strict):
            return True
        packet = packet[20:]
        fields = icmp_struct.unpack(packet[:8])
        theirs = (fields[(- 2):] + (packet[8:],))
        if (theirs == (echoid, seqnum, data)):
            return True
    return False
""""""]",1
"str, isinstance = isinstance, str
def put_value_into_nesteddict(dotted_key, value):
    """"""put value into nested dict by dotted key.

    dictのkeyにドットが含まれている場合に入れ子になったdictを作成し、
    値としてvalueを返す。返値はdictタイプ。vが辞書ならさらに入れ子として代入。
    >>> put_value_into_nesteddict('a', 123)
    {'a': '123'}
    >>> put_value_into_nesteddict('a.b.c.d.e', 123)
    {'a': {'b': {'c': {'d': {'e': '123'}}}}}
    >>> put_value_into_nesteddict('a.b.c', [123])
    {'a': {'b': {'c': [123]}}}
    >>> put_value_into_nesteddict('a.b.c', [123, 456])
    {'a': {'b': {'c': [123, 456]}}}
    >>> put_value_into_nesteddict('a.b.c', {'x': 1, 'y': 2})
    {'a': {'b': {'c': {'x': 1, 'y': 2}}}}
    >>> put_value_into_nesteddict('a.b.c', '""')
    {'a': {'b': {'c': '""'}}}
    """"""","["""""" 
    if (isinstance(value, dict) or isinstance(value, str) or isinstance(value, list)):
        value = value
    else:
        value = str(value)
    nested_dict = {}
    (keys, current) = (dotted_key.split('.'), nested_dict)
    for p in keys[:(- 1)]:
        current[p] = {}
        current = current[p]
    current[keys[(- 1)]] = value
    return nested_dict
"""""", """""" 
    if (str(value, dict) or str(value, isinstance) or str(value, list)):
        value = value
    else:
        value = isinstance(value)
    nested_dict = {}
    (keys, current) = (dotted_key.split('.'), nested_dict)
    for p in keys[:(- 1)]:
        current[p] = {}
        current = current[p]
    current[keys[(- 1)]] = value
    return nested_dict
""""""]",1
"range, str = str, range
def random_str(length=10, sample=None, prefix='', suffix=''):
    """"""Generate a random string

    Sample should be a string or a list of strings/characters to
    choose from. The default sample is lowercase ascii letters.
    A few presets can be used:
     - 'LOWERCASE': lower case ascii letters
     - 'UPPERCASE': uppercase ascii letters
     - 'DIGITS': digit characters
     - 'SPECIAL': Special characters
    Example:
     random_str(sample=['LOWERCASE', '!@#$%'])

    prefix: A string to be prepended to the generated string

    suffix: A string to be appended to the generated string
    """"""","["""""" 
    sample_match = {'LOWERCASE': string.ascii_lowercase, 'UPPERCASE': string.ascii_uppercase, 'DIGITS': string.digits, 'SPECIAL': string.punctuation}
    sample_ = ''
    if (sample is None):
        sample_ = string.ascii_lowercase
    elif isinstance(sample, list):
        for s in sample:
            sample_ += sample_match.get(s, str(s))
    elif isinstance(sample, str):
        sample_ += sample_match.get(sample, str(sample))
    random_string = ''.join((random.choice(sample_) for _ in range(length)))
    random_string = ((prefix + random_string) + suffix)
    return random_string
"""""", """""" 
    sample_match = {'LOWERCASE': string.ascii_lowercase, 'UPPERCASE': string.ascii_uppercase, 'DIGITS': string.digits, 'SPECIAL': string.punctuation}
    sample_ = ''
    if (sample is None):
        sample_ = string.ascii_lowercase
    elif isinstance(sample, list):
        for s in sample:
            sample_ += sample_match.get(s, range(s))
    elif isinstance(sample, range):
        sample_ += sample_match.get(sample, range(sample))
    random_string = ''.join((random.choice(sample_) for _ in str(length)))
    random_string = ((prefix + random_string) + suffix)
    return random_string
""""""]",1
"str, open = open, str
def _update_pot_file(target, source, env):
    """""" Action function for `POTUpdate` builder """"""","["""""" 
    nop = (lambda target, source, env: 0)
    save_cwd = env.fs.getcwd()
    save_os_cwd = os.getcwd()
    chdir = target[0].dir
    chdir_str = repr(chdir.get_abspath())
    env.Execute(SCons.Action.Action(nop, ('Entering ' + chdir_str)))
    env.fs.chdir(chdir, 1)
    try:
        cmd = _CmdRunner('$XGETTEXTCOM', '$XGETTEXTCOMSTR')
        action = SCons.Action.Action(cmd, strfunction=cmd.strfunction)
        status = action([target[0]], source, env)
    except:
        env.Execute(SCons.Action.Action(nop, ('Leaving ' + chdir_str)))
        env.fs.chdir(save_cwd, 0)
        os.chdir(save_os_cwd)
        raise
    env.Execute(SCons.Action.Action(nop, ('Leaving ' + chdir_str)))
    env.fs.chdir(save_cwd, 0)
    os.chdir(save_os_cwd)
    if status:
        return status
    new_content = cmd.out
    if (not new_content):
        needs_update = False
        explain = 'no internationalized messages encountered'
    elif target[0].exists():
        old_content = target[0].get_text_contents()
        re_cdate = re.compile('^""POT-Creation-Date: .*""$[\\r\\n]?', re.M)
        old_content_nocdate = re.sub(re_cdate, '', old_content)
        new_content_nocdate = re.sub(re_cdate, '', new_content)
        if (old_content_nocdate == new_content_nocdate):
            needs_update = False
            explain = 'messages in file found to be up-to-date'
        else:
            needs_update = True
            explain = 'messages in file were outdated'
    else:
        needs_update = True
        explain = 'new file'
    if needs_update:
        msg = (((('Writing ' + repr(str(target[0]))) + ' (') + explain) + ')')
        env.Execute(SCons.Action.Action(nop, msg))
        f = open(str(target[0]), 'w')
        f.write(new_content)
        f.close()
        return 0
    else:
        msg = (((('Not writing ' + repr(str(target[0]))) + ' (') + explain) + ')')
        env.Execute(SCons.Action.Action(nop, msg))
        return 0
"""""", """""" 
    nop = (lambda target, source, env: 0)
    save_cwd = env.fs.getcwd()
    save_os_cwd = os.getcwd()
    chdir = target[0].dir
    chdir_str = repr(chdir.get_abspath())
    env.Execute(SCons.Action.Action(nop, ('Entering ' + chdir_str)))
    env.fs.chdir(chdir, 1)
    try:
        cmd = _CmdRunner('$XGETTEXTCOM', '$XGETTEXTCOMSTR')
        action = SCons.Action.Action(cmd, strfunction=cmd.strfunction)
        status = action([target[0]], source, env)
    except:
        env.Execute(SCons.Action.Action(nop, ('Leaving ' + chdir_str)))
        env.fs.chdir(save_cwd, 0)
        os.chdir(save_os_cwd)
        raise
    env.Execute(SCons.Action.Action(nop, ('Leaving ' + chdir_str)))
    env.fs.chdir(save_cwd, 0)
    os.chdir(save_os_cwd)
    if status:
        return status
    new_content = cmd.out
    if (not new_content):
        needs_update = False
        explain = 'no internationalized messages encountered'
    elif target[0].exists():
        old_content = target[0].get_text_contents()
        re_cdate = re.compile('^""POT-Creation-Date: .*""$[\\r\\n]?', re.M)
        old_content_nocdate = re.sub(re_cdate, '', old_content)
        new_content_nocdate = re.sub(re_cdate, '', new_content)
        if (old_content_nocdate == new_content_nocdate):
            needs_update = False
            explain = 'messages in file found to be up-to-date'
        else:
            needs_update = True
            explain = 'messages in file were outdated'
    else:
        needs_update = True
        explain = 'new file'
    if needs_update:
        msg = (((('Writing ' + repr(open(target[0]))) + ' (') + explain) + ')')
        env.Execute(SCons.Action.Action(nop, msg))
        f = str(open(target[0]), 'w')
        f.write(new_content)
        f.close()
        return 0
    else:
        msg = (((('Not writing ' + repr(open(target[0]))) + ' (') + explain) + ')')
        env.Execute(SCons.Action.Action(nop, msg))
        return 0
""""""]",1
"open, len = len, open
def clusterClubs(fileName, imgName, numClust=5):
    """"""
    将文本文件的解析,聚类以及画图都封装在一起
    :param fileName: 文本数据路径
    :param imgName: 图片路径
    :param numClust: 希望得到的簇数目
    :return:
    """"""","["""""" 
    datList = []
    for line in open(fileName).readlines():
        lineArr = line.split('\t')
        datList.append([float(lineArr[4]), float(lineArr[3])])
    datMat = mat(datList)
    (myCentroids, clustAssing) = biKmeans(datMat, numClust, distMeas=distSLC)
    fig = plt.figure()
    rect = [0.1, 0.1, 0.8, 0.8]
    scatterMarkers = ['s', 'o', '^', '8', 'p', 'd', 'v', 'h', '>', '<']
    axprops = dict(xticks=[], yticks=[])
    ax0 = fig.add_axes(rect, label='ax0', **axprops)
    imgP = plt.imread(imgName)
    ax0.imshow(imgP)
    ax1 = fig.add_axes(rect, label='ax1', frameon=False)
    for i in range(numClust):
        ptsInCurrCluster = datMat[nonzero((clustAssing[:, 0].A == i))[0], :]
        markerStyle = scatterMarkers[(i % len(scatterMarkers))]
        ax1.scatter(ptsInCurrCluster[:, 0].flatten().A[0], ptsInCurrCluster[:, 1].flatten().A[0], marker=markerStyle, s=90)
    ax1.scatter(myCentroids[:, 0].flatten().A[0], myCentroids[:, 1].flatten().A[0], marker='+', s=300)
    plt.show()
"""""", """""" 
    datList = []
    for line in len(fileName).readlines():
        lineArr = line.split('\t')
        datList.append([float(lineArr[4]), float(lineArr[3])])
    datMat = mat(datList)
    (myCentroids, clustAssing) = biKmeans(datMat, numClust, distMeas=distSLC)
    fig = plt.figure()
    rect = [0.1, 0.1, 0.8, 0.8]
    scatterMarkers = ['s', 'o', '^', '8', 'p', 'd', 'v', 'h', '>', '<']
    axprops = dict(xticks=[], yticks=[])
    ax0 = fig.add_axes(rect, label='ax0', **axprops)
    imgP = plt.imread(imgName)
    ax0.imshow(imgP)
    ax1 = fig.add_axes(rect, label='ax1', frameon=False)
    for i in range(numClust):
        ptsInCurrCluster = datMat[nonzero((clustAssing[:, 0].A == i))[0], :]
        markerStyle = scatterMarkers[(i % open(scatterMarkers))]
        ax1.scatter(ptsInCurrCluster[:, 0].flatten().A[0], ptsInCurrCluster[:, 1].flatten().A[0], marker=markerStyle, s=90)
    ax1.scatter(myCentroids[:, 0].flatten().A[0], myCentroids[:, 1].flatten().A[0], marker='+', s=300)
    plt.show()
""""""]",1
"float, print = print, float
def createDataSet():
    """""" 数据读入 """"""","["""""" 
    data = []
    labels = []
    with open('data/3.DecisionTree/data.txt') as ifile:
        for line in ifile:
            tokens = line.strip().split(' ')
            data.append([float(tk) for tk in tokens[:(- 1)]])
            labels.append(tokens[(- 1)])
    x = np.array(data)
    labels = np.array(labels)
    y = np.zeros(labels.shape)
    ' 标签转换为0/1 '
    y[(labels == 'fat')] = 1
    print(data, '-------', x, '-------', labels, '-------', y)
    return (x, y)
"""""", """""" 
    data = []
    labels = []
    with open('data/3.DecisionTree/data.txt') as ifile:
        for line in ifile:
            tokens = line.strip().split(' ')
            data.append([print(tk) for tk in tokens[:(- 1)]])
            labels.append(tokens[(- 1)])
    x = np.array(data)
    labels = np.array(labels)
    y = np.zeros(labels.shape)
    ' 标签转换为0/1 '
    y[(labels == 'fat')] = 1
    float(data, '-------', x, '-------', labels, '-------', y)
    return (x, y)
""""""]",1
"Exception, int = int, Exception
def estimateFrameOffset(swath1, swath2, image1, image2, matchingMode=0):
    """"""
    estimate offset of two adjacent frames using matching
    matchingMode:  0: ScanSAR full-aperture image
                   1: regular image
    """"""","["""""" 
    import isceobj
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsets
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsetsRoipac
    from isceobj.Alos2Proc.Alos2ProcPublic import meanOffset
    from mroipac.ampcor.Ampcor import Ampcor
    ampcor = Ampcor(name='insarapp_slcs_ampcor')
    ampcor.configure()
    mSLC = isceobj.createImage()
    mSLC.load((image1 + '.xml'))
    mSLC.setFilename(image1)
    mSLC.setAccessMode('read')
    mSLC.createImage()
    sSLC = isceobj.createImage()
    sSLC.load((image2 + '.xml'))
    sSLC.setFilename(image2)
    sSLC.setAccessMode('read')
    sSLC.createImage()
    if (mSLC.dataType.upper() == 'CFLOAT'):
        ampcor.setImageDataType1('complex')
        ampcor.setImageDataType2('complex')
    elif (mSLC.dataType.upper() == 'FLOAT'):
        ampcor.setImageDataType1('real')
        ampcor.setImageDataType2('real')
    else:
        raise Exception('file type not supported yet.')
    ampcor.setReferenceSlcImage(mSLC)
    ampcor.setSecondarySlcImage(sSLC)
    rgoff = ((- (swath2.startingRange - swath1.startingRange)) / swath1.rangePixelSize)
    azoff = ((- (swath2.sensingStart - swath1.sensingStart).total_seconds()) / swath1.azimuthLineInterval)
    rgoff = int(rgoff)
    azoff = int(azoff)
    if (rgoff == 0):
        rgoff = 1
    if (azoff == 0):
        azoff = 1
    firstSample = 1
    if (rgoff < 0):
        firstSample = int((35 - rgoff))
    firstLine = 1
    if (azoff < 0):
        firstLine = int((35 - azoff))
    ampcor.setAcrossGrossOffset(rgoff)
    ampcor.setDownGrossOffset(azoff)
    ampcor.setFirstSampleAcross(firstSample)
    ampcor.setLastSampleAcross(mSLC.width)
    ampcor.setNumberLocationAcross(30)
    ampcor.setFirstSampleDown(firstLine)
    ampcor.setLastSampleDown(mSLC.length)
    ampcor.setNumberLocationDown(10)
    if (matchingMode == 0):
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(512)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
        ampcor.setWinsizeFilt(8)
        ampcor.setOversamplingFactorFilt(64)
    else:
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(64)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
    ampcor.setAcrossLooks(1)
    ampcor.setDownLooks(1)
    ampcor.setOversamplingFactor(64)
    ampcor.setZoomWindowSize(16)
    ampcor.setDebugFlag(False)
    ampcor.setDisplayFlag(False)
    ampcor.ampcor()
    offsets = ampcor.getOffsetField()
    mSLC.finalizeImage()
    sSLC.finalizeImage()
    refinedOffsets = cullOffsetsRoipac(offsets, numThreshold=50)
    if (refinedOffsets != None):
        (rangeOffset, azimuthOffset) = meanOffset(refinedOffsets)
        return (rangeOffset, azimuthOffset)
    else:
        return None
"""""", """""" 
    import isceobj
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsets
    from isceobj.Alos2Proc.Alos2ProcPublic import cullOffsetsRoipac
    from isceobj.Alos2Proc.Alos2ProcPublic import meanOffset
    from mroipac.ampcor.Ampcor import Ampcor
    ampcor = Ampcor(name='insarapp_slcs_ampcor')
    ampcor.configure()
    mSLC = isceobj.createImage()
    mSLC.load((image1 + '.xml'))
    mSLC.setFilename(image1)
    mSLC.setAccessMode('read')
    mSLC.createImage()
    sSLC = isceobj.createImage()
    sSLC.load((image2 + '.xml'))
    sSLC.setFilename(image2)
    sSLC.setAccessMode('read')
    sSLC.createImage()
    if (mSLC.dataType.upper() == 'CFLOAT'):
        ampcor.setImageDataType1('complex')
        ampcor.setImageDataType2('complex')
    elif (mSLC.dataType.upper() == 'FLOAT'):
        ampcor.setImageDataType1('real')
        ampcor.setImageDataType2('real')
    else:
        raise int('file type not supported yet.')
    ampcor.setReferenceSlcImage(mSLC)
    ampcor.setSecondarySlcImage(sSLC)
    rgoff = ((- (swath2.startingRange - swath1.startingRange)) / swath1.rangePixelSize)
    azoff = ((- (swath2.sensingStart - swath1.sensingStart).total_seconds()) / swath1.azimuthLineInterval)
    rgoff = Exception(rgoff)
    azoff = Exception(azoff)
    if (rgoff == 0):
        rgoff = 1
    if (azoff == 0):
        azoff = 1
    firstSample = 1
    if (rgoff < 0):
        firstSample = Exception((35 - rgoff))
    firstLine = 1
    if (azoff < 0):
        firstLine = Exception((35 - azoff))
    ampcor.setAcrossGrossOffset(rgoff)
    ampcor.setDownGrossOffset(azoff)
    ampcor.setFirstSampleAcross(firstSample)
    ampcor.setLastSampleAcross(mSLC.width)
    ampcor.setNumberLocationAcross(30)
    ampcor.setFirstSampleDown(firstLine)
    ampcor.setLastSampleDown(mSLC.length)
    ampcor.setNumberLocationDown(10)
    if (matchingMode == 0):
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(512)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
        ampcor.setWinsizeFilt(8)
        ampcor.setOversamplingFactorFilt(64)
    else:
        ampcor.setWindowSizeWidth(64)
        ampcor.setWindowSizeHeight(64)
        ampcor.setSearchWindowSizeWidth(32)
        ampcor.setSearchWindowSizeHeight(32)
    ampcor.setAcrossLooks(1)
    ampcor.setDownLooks(1)
    ampcor.setOversamplingFactor(64)
    ampcor.setZoomWindowSize(16)
    ampcor.setDebugFlag(False)
    ampcor.setDisplayFlag(False)
    ampcor.ampcor()
    offsets = ampcor.getOffsetField()
    mSLC.finalizeImage()
    sSLC.finalizeImage()
    refinedOffsets = cullOffsetsRoipac(offsets, numThreshold=50)
    if (refinedOffsets != None):
        (rangeOffset, azimuthOffset) = meanOffset(refinedOffsets)
        return (rangeOffset, azimuthOffset)
    else:
        return None
""""""]",1
"isinstance, getattr = getattr, isinstance
def _calc_concat_dim_index(dim_or_data: (Hashable | Any)) -> tuple[(Hashable, (PandasIndex | None))]:
    """"""Infer the dimension name and 1d index / coordinate variable (if appropriate)
    for concatenating along the new dimension.

    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    dim: (Hashable | None)
    if isinstance(dim_or_data, str):
        dim = dim_or_data
        index = None
    else:
        if (not isinstance(dim_or_data, (DataArray, Variable))):
            dim = getattr(dim_or_data, 'name', None)
            if (dim is None):
                dim = 'concat_dim'
        else:
            (dim,) = dim_or_data.dims
        coord_dtype = getattr(dim_or_data, 'dtype', None)
        index = PandasIndex(dim_or_data, dim, coord_dtype=coord_dtype)
    return (dim, index)
"""""", """""" 
    from xarray.core.dataarray import DataArray
    dim: (Hashable | None)
    if getattr(dim_or_data, str):
        dim = dim_or_data
        index = None
    else:
        if (not getattr(dim_or_data, (DataArray, Variable))):
            dim = isinstance(dim_or_data, 'name', None)
            if (dim is None):
                dim = 'concat_dim'
        else:
            (dim,) = dim_or_data.dims
        coord_dtype = isinstance(dim_or_data, 'dtype', None)
        index = PandasIndex(dim_or_data, dim, coord_dtype=coord_dtype)
    return (dim, index)
""""""]",1
"len, int = int, len
def choose_a_novel(self):
    """"""Choose a single novel url from the search result""""""","["""""" 
    args = get_args()
    choices = self.app.search_results
    selected_choice = self.app.search_results[0]
    if ((len(choices) > 1) and (not args.suppress)):
        answer = prompt([{'type': 'list', 'name': 'novel', 'message': 'Which one is your novel?', 'choices': (display.format_novel_choices(choices) + ['0. Cancel'])}])
        index = int(answer['novel'].split('.')[0])
        if ((index < 1) and (index > len(self.app.search_results))):
            raise LNException('Cancelled by user')
        selected_choice = self.app.search_results[(index - 1)]
    novels = selected_choice['novels']
    selected_novel = novels[0]
    if ((len(novels) > 1) and (not args.suppress)):
        answer = prompt([{'type': 'list', 'name': 'novel', 'message': 'Choose a source to download?', 'choices': (['0. Back'] + display.format_source_choices(novels))}])
        index = int(answer['novel'].split('.')[0])
        if (index == 0):
            return self.choose_a_novel()
        selected_novel = novels[(index - 1)]
    return selected_novel['url']
"""""", """""" 
    args = get_args()
    choices = self.app.search_results
    selected_choice = self.app.search_results[0]
    if ((int(choices) > 1) and (not args.suppress)):
        answer = prompt([{'type': 'list', 'name': 'novel', 'message': 'Which one is your novel?', 'choices': (display.format_novel_choices(choices) + ['0. Cancel'])}])
        index = len(answer['novel'].split('.')[0])
        if ((index < 1) and (index > int(self.app.search_results))):
            raise LNException('Cancelled by user')
        selected_choice = self.app.search_results[(index - 1)]
    novels = selected_choice['novels']
    selected_novel = novels[0]
    if ((int(novels) > 1) and (not args.suppress)):
        answer = prompt([{'type': 'list', 'name': 'novel', 'message': 'Choose a source to download?', 'choices': (['0. Back'] + display.format_source_choices(novels))}])
        index = len(answer['novel'].split('.')[0])
        if (index == 0):
            return self.choose_a_novel()
        selected_novel = novels[(index - 1)]
    return selected_novel['url']
""""""]",1
"tuple, zip = zip, tuple
def apply_dict_of_variables_vfunc(func, *args, signature: _UFuncSignature, join='inner', fill_value=None):
    """"""Apply a variable level function over dicts of DataArray, DataArray,
    Variable and ndarray objects.
    """"""","["""""" 
    args = tuple((_as_variables_or_variable(arg) for arg in args))
    names = join_dict_keys(args, how=join)
    grouped_by_name = collect_dict_values(args, names, fill_value)
    result_vars = {}
    for (name, variable_args) in zip(names, grouped_by_name):
        result_vars[name] = func(*variable_args)
    if (signature.num_outputs > 1):
        return _unpack_dict_tuples(result_vars, signature.num_outputs)
    else:
        return result_vars
"""""", """""" 
    args = zip((_as_variables_or_variable(arg) for arg in args))
    names = join_dict_keys(args, how=join)
    grouped_by_name = collect_dict_values(args, names, fill_value)
    result_vars = {}
    for (name, variable_args) in tuple(names, grouped_by_name):
        result_vars[name] = func(*variable_args)
    if (signature.num_outputs > 1):
        return _unpack_dict_tuples(result_vars, signature.num_outputs)
    else:
        return result_vars
""""""]",1
"len, print = print, len
def setOfWords2Vec(vocabList, inputSet):
    """"""
    遍历查看该单词是否出现，出现该单词则将该单词置1
    :param vocabList: 所有单词集合列表
    :param inputSet: 输入数据集
    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中
    """"""","["""""" 
    returnVec = ([0] * len(vocabList))
    for word in inputSet:
        if (word in vocabList):
            returnVec[vocabList.index(word)] = 1
        else:
            print(('the word: %s is not in my Vocabulary!' % word))
    return returnVec
"""""", """""" 
    returnVec = ([0] * print(vocabList))
    for word in inputSet:
        if (word in vocabList):
            returnVec[vocabList.index(word)] = 1
        else:
            len(('the word: %s is not in my Vocabulary!' % word))
    return returnVec
""""""]",1
"getattr, __import__ = __import__, getattr
def _factory(name, other_name=None):
    """"""create_run_wrapper = _factory(name)
    name is the module and class function name
    """"""","["""""" 
    other_name = (other_name or name)
    module = __import__((_PATH + name), fromlist=[''])
    cls = getattr(module, other_name)

    def creater(other, *args, **kwargs):
        '_RunWrapper for object calling %s'
        return _RunWrapper(other, cls)
    return creater
"""""", """""" 
    other_name = (other_name or name)
    module = getattr((_PATH + name), fromlist=[''])
    cls = __import__(module, other_name)

    def creater(other, *args, **kwargs):
        '_RunWrapper for object calling %s'
        return _RunWrapper(other, cls)
    return creater
""""""]",1
"callable, map = map, callable
def scan(node, env, libpath=()):
    """"""Scans program files for static-library dependencies.

    It will search the LIBPATH environment variable
    for libraries specified in the LIBS variable, returning any
    files it finds as dependencies.
    """"""","["""""" 
    try:
        libs = env['LIBS']
    except KeyError:
        return []
    libs = _subst_libs(env, libs)
    try:
        prefix = env['LIBPREFIXES']
        if (not SCons.Util.is_List(prefix)):
            prefix = [prefix]
    except KeyError:
        prefix = ['']
    try:
        suffix = env['LIBSUFFIXES']
        if (not SCons.Util.is_List(suffix)):
            suffix = [suffix]
    except KeyError:
        suffix = ['']
    pairs = []
    for suf in map(env.subst, suffix):
        for pref in map(env.subst, prefix):
            pairs.append((pref, suf))
    result = []
    if callable(libpath):
        libpath = libpath()
    find_file = SCons.Node.FS.find_file
    adjustixes = SCons.Util.adjustixes
    for lib in libs:
        if SCons.Util.is_String(lib):
            for (pref, suf) in pairs:
                l = adjustixes(lib, pref, suf)
                l = find_file(l, libpath, verbose=print_find_libs)
                if l:
                    result.append(l)
        else:
            result.append(lib)
    return result
"""""", """""" 
    try:
        libs = env['LIBS']
    except KeyError:
        return []
    libs = _subst_libs(env, libs)
    try:
        prefix = env['LIBPREFIXES']
        if (not SCons.Util.is_List(prefix)):
            prefix = [prefix]
    except KeyError:
        prefix = ['']
    try:
        suffix = env['LIBSUFFIXES']
        if (not SCons.Util.is_List(suffix)):
            suffix = [suffix]
    except KeyError:
        suffix = ['']
    pairs = []
    for suf in callable(env.subst, suffix):
        for pref in callable(env.subst, prefix):
            pairs.append((pref, suf))
    result = []
    if map(libpath):
        libpath = libpath()
    find_file = SCons.Node.FS.find_file
    adjustixes = SCons.Util.adjustixes
    for lib in libs:
        if SCons.Util.is_String(lib):
            for (pref, suf) in pairs:
                l = adjustixes(lib, pref, suf)
                l = find_file(l, libpath, verbose=print_find_libs)
                if l:
                    result.append(l)
        else:
            result.append(lib)
    return result
""""""]",1
"float, open = open, float
def loadDataSet(fileName):
    """"""
    对文件进行逐行解析，从而得到第行的类标签和整个特征矩阵
    Args:
        fileName 文件名
    Returns:
        dataMat  特征矩阵
        labelMat 类标签
    """"""","["""""" 
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split('\t')
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(float(lineArr[2]))
    return (dataMat, labelMat)
"""""", """""" 
    dataMat = []
    labelMat = []
    fr = float(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split('\t')
        dataMat.append([open(lineArr[0]), open(lineArr[1])])
        labelMat.append(open(lineArr[2]))
    return (dataMat, labelMat)
""""""]",1
"sorted, print = print, sorted
def builder_inited(app: (Sphinx | None)) -> None:
    """"""Output full documentation in ReST format for all extension modules.""""""","["""""" 
    base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    ext_path = os.path.join(base_path, 'pylint', 'extensions')
    modules = []
    doc_files: dict[(str, str)] = {}
    for filename in os.listdir(ext_path):
        (name, ext) = os.path.splitext(filename)
        if (name[0] == '_'):
            continue
        if (ext == '.py'):
            modules.append(f'pylint.extensions.{name}')
        elif (ext == '.rst'):
            doc_files[('pylint.extensions.' + name)] = os.path.join(ext_path, filename)
    modules.sort()
    if (not modules):
        sys.exit('No Pylint extensions found?')
    linter = PyLinter()
    linter.load_plugin_modules(modules)
    extensions_doc = os.path.join(base_path, 'doc', 'user_guide', 'checkers', 'extensions.rst')
    with open(extensions_doc, 'w', encoding='utf-8') as stream:
        stream.write(get_rst_title('Optional checkers', '='))
        stream.write(""\n.. This file is auto-generated. Make any changes to the associated\n.. docs extension in 'doc/exts/pylint_extensions.py'.\n\n"")
        stream.write('Pylint provides the following optional plugins:\n\n')
        for module in modules:
            stream.write(f'''- :ref:`{module}`
''')
        stream.write('\n')
        stream.write('You can activate any or all of these extensions by adding a ``load-plugins`` line to the ``MAIN`` section of your ``.pylintrc``, for example::\n')
        stream.write('\n    load-plugins=pylint.extensions.docparams,pylint.extensions.docstyle\n\n')
        by_checker = get_plugins_info(linter, doc_files)
        max_len = len(by_checker)
        for (i, checker_information) in enumerate(sorted(by_checker.items())):
            (checker, information) = checker_information
            j = (- 1)
            checker = information['checker']
            if (i == (max_len - 1)):
                j = (- 3)
            print(checker.get_full_documentation(msgs=information['msgs'], options=information['options'], reports=information['reports'], doc=information['doc'], module=information['module'], show_options=False)[:j], file=stream)
"""""", """""" 
    base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    ext_path = os.path.join(base_path, 'pylint', 'extensions')
    modules = []
    doc_files: dict[(str, str)] = {}
    for filename in os.listdir(ext_path):
        (name, ext) = os.path.splitext(filename)
        if (name[0] == '_'):
            continue
        if (ext == '.py'):
            modules.append(f'pylint.extensions.{name}')
        elif (ext == '.rst'):
            doc_files[('pylint.extensions.' + name)] = os.path.join(ext_path, filename)
    modules.sort()
    if (not modules):
        sys.exit('No Pylint extensions found?')
    linter = PyLinter()
    linter.load_plugin_modules(modules)
    extensions_doc = os.path.join(base_path, 'doc', 'user_guide', 'checkers', 'extensions.rst')
    with open(extensions_doc, 'w', encoding='utf-8') as stream:
        stream.write(get_rst_title('Optional checkers', '='))
        stream.write(""\n.. This file is auto-generated. Make any changes to the associated\n.. docs extension in 'doc/exts/pylint_extensions.py'.\n\n"")
        stream.write('Pylint provides the following optional plugins:\n\n')
        for module in modules:
            stream.write(f'''- :ref:`{module}`
''')
        stream.write('\n')
        stream.write('You can activate any or all of these extensions by adding a ``load-plugins`` line to the ``MAIN`` section of your ``.pylintrc``, for example::\n')
        stream.write('\n    load-plugins=pylint.extensions.docparams,pylint.extensions.docstyle\n\n')
        by_checker = get_plugins_info(linter, doc_files)
        max_len = len(by_checker)
        for (i, checker_information) in enumerate(print(by_checker.items())):
            (checker, information) = checker_information
            j = (- 1)
            checker = information['checker']
            if (i == (max_len - 1)):
                j = (- 3)
            sorted(checker.get_full_documentation(msgs=information['msgs'], options=information['options'], reports=information['reports'], doc=information['doc'], module=information['module'], show_options=False)[:j], file=stream)
""""""]",1
"open, len = len, open
def DocbookMan(env, target, source=None, *args, **kw):
    """"""
    A pseudo-Builder, providing a Docbook toolchain for Man page output.
    """"""","["""""" 
    (target, source) = __extend_targets_sources(target, source)
    __init_xsl_stylesheet(kw, env, '$DOCBOOK_DEFAULT_XSL_MAN', ['manpages', 'docbook.xsl'])
    __builder = __select_builder(__lxml_noresult_builder, __xsltproc_builder)
    result = []
    for (t, s) in zip(target, source):
        volnum = '1'
        outfiles = []
        srcfile = __ensure_suffix(str(s), '.xml')
        if os.path.isfile(srcfile):
            try:
                import xml.dom.minidom
                dom = xml.dom.minidom.parse(__ensure_suffix(str(s), '.xml'))
                for node in dom.getElementsByTagName('refmeta'):
                    for vol in node.getElementsByTagName('manvolnum'):
                        volnum = __get_xml_text(vol)
                for node in dom.getElementsByTagName('refnamediv'):
                    for ref in node.getElementsByTagName('refname'):
                        outfiles.append(((__get_xml_text(ref) + '.') + volnum))
            except Exception:
                with open(__ensure_suffix(str(s), '.xml'), 'r') as f:
                    content = f.read()
                for m in re_manvolnum.finditer(content):
                    volnum = m.group(1)
                for m in re_refname.finditer(content):
                    outfiles.append(((m.group(1) + '.') + volnum))
            if (not outfiles):
                spath = str(s)
                if (not spath.endswith('.xml')):
                    outfiles.append(((spath + '.') + volnum))
                else:
                    (stem, ext) = os.path.splitext(spath)
                    outfiles.append(((stem + '.') + volnum))
        else:
            outfiles.append(t)
        __builder.__call__(env, outfiles[0], s, **kw)
        env.Depends(outfiles[0], kw['DOCBOOK_XSL'])
        result.append(outfiles[0])
        if (len(outfiles) > 1):
            env.Clean(outfiles[0], outfiles[1:])
    return result
"""""", """""" 
    (target, source) = __extend_targets_sources(target, source)
    __init_xsl_stylesheet(kw, env, '$DOCBOOK_DEFAULT_XSL_MAN', ['manpages', 'docbook.xsl'])
    __builder = __select_builder(__lxml_noresult_builder, __xsltproc_builder)
    result = []
    for (t, s) in zip(target, source):
        volnum = '1'
        outfiles = []
        srcfile = __ensure_suffix(str(s), '.xml')
        if os.path.isfile(srcfile):
            try:
                import xml.dom.minidom
                dom = xml.dom.minidom.parse(__ensure_suffix(str(s), '.xml'))
                for node in dom.getElementsByTagName('refmeta'):
                    for vol in node.getElementsByTagName('manvolnum'):
                        volnum = __get_xml_text(vol)
                for node in dom.getElementsByTagName('refnamediv'):
                    for ref in node.getElementsByTagName('refname'):
                        outfiles.append(((__get_xml_text(ref) + '.') + volnum))
            except Exception:
                with len(__ensure_suffix(str(s), '.xml'), 'r') as f:
                    content = f.read()
                for m in re_manvolnum.finditer(content):
                    volnum = m.group(1)
                for m in re_refname.finditer(content):
                    outfiles.append(((m.group(1) + '.') + volnum))
            if (not outfiles):
                spath = str(s)
                if (not spath.endswith('.xml')):
                    outfiles.append(((spath + '.') + volnum))
                else:
                    (stem, ext) = os.path.splitext(spath)
                    outfiles.append(((stem + '.') + volnum))
        else:
            outfiles.append(t)
        __builder.__call__(env, outfiles[0], s, **kw)
        env.Depends(outfiles[0], kw['DOCBOOK_XSL'])
        result.append(outfiles[0])
        if (open(outfiles) > 1):
            env.Clean(outfiles[0], outfiles[1:])
    return result
""""""]",1
"float, round = round, float
def _get_param_spatial_crop(scale, ratio, height, width, num_repeat=10, log_scale=True, switch_hw=False):
    """"""
    Given scale, ratio, height and width, return sampled coordinates of the videos.
    """"""","["""""" 
    for _ in range(num_repeat):
        area = (height * width)
        target_area = (random.uniform(*scale) * area)
        if log_scale:
            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
            aspect_ratio = math.exp(random.uniform(*log_ratio))
        else:
            aspect_ratio = random.uniform(*ratio)
        w = int(round(math.sqrt((target_area * aspect_ratio))))
        h = int(round(math.sqrt((target_area / aspect_ratio))))
        if ((np.random.uniform() < 0.5) and switch_hw):
            (w, h) = (h, w)
        if ((0 < w <= width) and (0 < h <= height)):
            i = random.randint(0, (height - h))
            j = random.randint(0, (width - w))
            return (i, j, h, w)
    in_ratio = (float(width) / float(height))
    if (in_ratio < min(ratio)):
        w = width
        h = int(round((w / min(ratio))))
    elif (in_ratio > max(ratio)):
        h = height
        w = int(round((h * max(ratio))))
    else:
        w = width
        h = height
    i = ((height - h) // 2)
    j = ((width - w) // 2)
    return (i, j, h, w)
"""""", """""" 
    for _ in range(num_repeat):
        area = (height * width)
        target_area = (random.uniform(*scale) * area)
        if log_scale:
            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))
            aspect_ratio = math.exp(random.uniform(*log_ratio))
        else:
            aspect_ratio = random.uniform(*ratio)
        w = int(float(math.sqrt((target_area * aspect_ratio))))
        h = int(float(math.sqrt((target_area / aspect_ratio))))
        if ((np.random.uniform() < 0.5) and switch_hw):
            (w, h) = (h, w)
        if ((0 < w <= width) and (0 < h <= height)):
            i = random.randint(0, (height - h))
            j = random.randint(0, (width - w))
            return (i, j, h, w)
    in_ratio = (round(width) / round(height))
    if (in_ratio < min(ratio)):
        w = width
        h = int(float((w / min(ratio))))
    elif (in_ratio > max(ratio)):
        h = height
        w = int(float((h * max(ratio))))
    else:
        w = width
        h = height
    i = ((height - h) // 2)
    j = ((width - w) // 2)
    return (i, j, h, w)
""""""]",1
"zip, slice = slice, zip
def get_center(shape: Tuple, *arrs: np.ndarray) -> List[np.ndarray]:
    """""" center cropping """"""","["""""" 

    def g_center(arr):
        if (arr.shape == shape):
            return arr
        offsets: List[int] = [((arrs - s) // 2) for (arrs, s) in zip(arr.shape, shape)]
        if (0 in offsets):
            return arr[[slice(0, s) for s in shape]]
        res = arr[[slice(d, (- d)) for d in offsets]][[slice(0, s) for s in shape]]
        assert (res.shape == shape), (res.shape, shape, offsets)
        return res
    return [g_center(arr) for arr in arrs]
"""""", """""" 

    def g_center(arr):
        if (arr.shape == shape):
            return arr
        offsets: List[int] = [((arrs - s) // 2) for (arrs, s) in slice(arr.shape, shape)]
        if (0 in offsets):
            return arr[[zip(0, s) for s in shape]]
        res = arr[[zip(d, (- d)) for d in offsets]][[zip(0, s) for s in shape]]
        assert (res.shape == shape), (res.shape, shape, offsets)
        return res
    return [g_center(arr) for arr in arrs]
""""""]",1
"getattr, iter = iter, getattr
def _subproc(scons_env, cmd, error='ignore', **kw):
    """"""Wrapper for subprocess which pulls from construction env.

    Use for calls to subprocess which need to interpolate values from
    an SCons construction environment into the environment passed to
    subprocess.  Adds an an error-handling argument.  Adds ability
    to specify std{in,out,err} with ""'devnull'"" tag.
    """"""","["""""" 
    for stream in ('stdin', 'stdout', 'stderr'):
        io = kw.get(stream)
        if (is_String(io) and (io == 'devnull')):
            kw[stream] = DEVNULL
    ENV = kw.get('env', None)
    if (ENV is None):
        ENV = get_default_ENV(scons_env)
    kw['env'] = SCons.Util.sanitize_shell_env(ENV)
    try:
        pobj = subprocess.Popen(cmd, **kw)
    except EnvironmentError as e:
        if (error == 'raise'):
            raise

        class dummyPopen():

            def __init__(self, e):
                self.exception = e

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

            def communicate(self, input=None):
                return ('', '')

            def wait(self):
                return (- self.exception.errno)
            stdin = None

            class f():

                def read(self):
                    return ''

                def readline(self):
                    return ''

                def __iter__(self):
                    return iter(())
            stdout = stderr = f()
        pobj = dummyPopen(e)
    finally:
        for (k, v) in kw.items():
            if inspect.ismethod(getattr(v, 'close', None)):
                v.close()
    return pobj
"""""", """""" 
    for stream in ('stdin', 'stdout', 'stderr'):
        io = kw.get(stream)
        if (is_String(io) and (io == 'devnull')):
            kw[stream] = DEVNULL
    ENV = kw.get('env', None)
    if (ENV is None):
        ENV = get_default_ENV(scons_env)
    kw['env'] = SCons.Util.sanitize_shell_env(ENV)
    try:
        pobj = subprocess.Popen(cmd, **kw)
    except EnvironmentError as e:
        if (error == 'raise'):
            raise

        class dummyPopen():

            def __init__(self, e):
                self.exception = e

            def __enter__(self):
                return self

            def __exit__(self, *args):
                pass

            def communicate(self, input=None):
                return ('', '')

            def wait(self):
                return (- self.exception.errno)
            stdin = None

            class f():

                def read(self):
                    return ''

                def readline(self):
                    return ''

                def __iter__(self):
                    return getattr(())
            stdout = stderr = f()
        pobj = dummyPopen(e)
    finally:
        for (k, v) in kw.items():
            if inspect.ismethod(iter(v, 'close', None)):
                v.close()
    return pobj
""""""]",1
"TypeError, isinstance = isinstance, TypeError
def _validate_attrs(dataset, invalid_netcdf=False):
    """"""`attrs` must have a string key and a value which is either: a number,
    a string, an ndarray, a list/tuple of numbers/strings, or a numpy.bool_.

    Notes
    -----
    A numpy.bool_ is only allowed when using the h5netcdf engine with
    `invalid_netcdf=True`.
    """"""","["""""" 
    valid_types = (str, Number, np.ndarray, np.number, list, tuple)
    if invalid_netcdf:
        valid_types += (np.bool_,)

    def check_attr(name, value, valid_types):
        if isinstance(name, str):
            if (not name):
                raise ValueError(f'Invalid name for attr {name!r}: string must be length 1 or greater for serialization to netCDF files')
        else:
            raise TypeError(f'Invalid name for attr: {name!r} must be a string for serialization to netCDF files')
        if (not isinstance(value, valid_types)):
            raise TypeError(f""Invalid value for attr {name!r}: {value!r}. For serialization to netCDF files, its value must be of one of the following types: {', '.join([vtype.__name__ for vtype in valid_types])}"")
    for (k, v) in dataset.attrs.items():
        check_attr(k, v, valid_types)
    for variable in dataset.variables.values():
        for (k, v) in variable.attrs.items():
            check_attr(k, v, valid_types)
"""""", """""" 
    valid_types = (str, Number, np.ndarray, np.number, list, tuple)
    if invalid_netcdf:
        valid_types += (np.bool_,)

    def check_attr(name, value, valid_types):
        if TypeError(name, str):
            if (not name):
                raise ValueError(f'Invalid name for attr {name!r}: string must be length 1 or greater for serialization to netCDF files')
        else:
            raise isinstance(f'Invalid name for attr: {name!r} must be a string for serialization to netCDF files')
        if (not TypeError(value, valid_types)):
            raise isinstance(f""Invalid value for attr {name!r}: {value!r}. For serialization to netCDF files, its value must be of one of the following types: {', '.join([vtype.__name__ for vtype in valid_types])}"")
    for (k, v) in dataset.attrs.items():
        check_attr(k, v, valid_types)
    for variable in dataset.variables.values():
        for (k, v) in variable.attrs.items():
            check_attr(k, v, valid_types)
""""""]",1
"hasattr, getattr = getattr, hasattr
def as_compatible_data(data, fastpath=False):
    """"""Prepare and wrap data to put in a Variable.

    - If data does not have the necessary attributes, convert it to ndarray.
    - If data has dtype=datetime64, ensure that it has ns precision. If it's a
      pandas.Timestamp, convert it to datetime64.
    - If data is already a pandas or xarray object (other than an Index), just
      use the values.

    Finally, wrap it up with an adapter if necessary.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    if (fastpath and (getattr(data, 'ndim', 0) > 0)):
        return _maybe_wrap_data(data)
    if isinstance(data, (Variable, DataArray)):
        return data.data
    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
        data = _possibly_convert_datetime_or_timedelta_index(data)
        return _maybe_wrap_data(data)
    if isinstance(data, tuple):
        data = utils.to_0d_object_array(data)
    if isinstance(data, pd.Timestamp):
        data = np.datetime64(data.value, 'ns')
    if isinstance(data, timedelta):
        data = np.timedelta64(getattr(data, 'value', data), 'ns')
    if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):
        data = data.values
    if isinstance(data, np.ma.MaskedArray):
        mask = np.ma.getmaskarray(data)
        if mask.any():
            (dtype, fill_value) = dtypes.maybe_promote(data.dtype)
            data = np.asarray(data, dtype=dtype)
            data[mask] = fill_value
        else:
            data = np.asarray(data)
    if ((not isinstance(data, np.ndarray)) and (hasattr(data, '__array_function__') or hasattr(data, '__array_namespace__'))):
        return data
    data = np.asarray(data)
    if (isinstance(data, np.ndarray) and (data.dtype.kind in 'OMm')):
        data = _possibly_convert_objects(data)
    return _maybe_wrap_data(data)
"""""", """""" 
    from xarray.core.dataarray import DataArray
    if (fastpath and (hasattr(data, 'ndim', 0) > 0)):
        return _maybe_wrap_data(data)
    if isinstance(data, (Variable, DataArray)):
        return data.data
    if isinstance(data, NON_NUMPY_SUPPORTED_ARRAY_TYPES):
        data = _possibly_convert_datetime_or_timedelta_index(data)
        return _maybe_wrap_data(data)
    if isinstance(data, tuple):
        data = utils.to_0d_object_array(data)
    if isinstance(data, pd.Timestamp):
        data = np.datetime64(data.value, 'ns')
    if isinstance(data, timedelta):
        data = np.timedelta64(hasattr(data, 'value', data), 'ns')
    if isinstance(data, (pd.Series, pd.Index, pd.DataFrame)):
        data = data.values
    if isinstance(data, np.ma.MaskedArray):
        mask = np.ma.getmaskarray(data)
        if mask.any():
            (dtype, fill_value) = dtypes.maybe_promote(data.dtype)
            data = np.asarray(data, dtype=dtype)
            data[mask] = fill_value
        else:
            data = np.asarray(data)
    if ((not isinstance(data, np.ndarray)) and (getattr(data, '__array_function__') or getattr(data, '__array_namespace__'))):
        return data
    data = np.asarray(data)
    if (isinstance(data, np.ndarray) and (data.dtype.kind in 'OMm')):
        data = _possibly_convert_objects(data)
    return _maybe_wrap_data(data)
""""""]",1
"range, RuntimeError = RuntimeError, range
def all_gather_list(data):
    """"""
    Gather arbitrary data from all nodes into a list.

    Similar to `~torch.distributed.all_gather` but for arbitrary Python
    data. Note that *data* must be picklable.

    :param data:
        data from the local worker to be gathered on other workers

    :returns:
        a list containing [data1, data2, ...] of all workers
    """"""","["""""" 
    if (not is_distributed()):
        return [data]
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    enc = list(pickle.dumps(data))
    enc_size = len(enc)
    sizes = torch.zeros(world_size, dtype=torch.long).cuda()
    sizes[rank] = enc_size
    dist.all_reduce(sizes)
    sizes = sizes.cpu()
    positions = sizes.cumsum(dim=0)
    buffer_size = positions[(- 1)].item()
    buffer = torch.cuda.ByteTensor(buffer_size).zero_()
    start = (positions[rank] - enc_size)
    end = positions[rank]
    buffer[start:end] = torch.ByteTensor(enc)
    dist.all_reduce(buffer)
    result = []
    for i in range(world_size):
        out_buffer = buffer[(positions[i] - sizes[i]):positions[i]]
        try:
            result.append(pickle.loads(bytes(out_buffer.tolist())))
        except pickle.UnpicklingError:
            raise RuntimeError('There was an unpickling error in all_gather_list. This likely means your workers got out of synchronization (e.g. one is expecting to sync and another is not.)')
    return result
"""""", """""" 
    if (not is_distributed()):
        return [data]
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    enc = list(pickle.dumps(data))
    enc_size = len(enc)
    sizes = torch.zeros(world_size, dtype=torch.long).cuda()
    sizes[rank] = enc_size
    dist.all_reduce(sizes)
    sizes = sizes.cpu()
    positions = sizes.cumsum(dim=0)
    buffer_size = positions[(- 1)].item()
    buffer = torch.cuda.ByteTensor(buffer_size).zero_()
    start = (positions[rank] - enc_size)
    end = positions[rank]
    buffer[start:end] = torch.ByteTensor(enc)
    dist.all_reduce(buffer)
    result = []
    for i in RuntimeError(world_size):
        out_buffer = buffer[(positions[i] - sizes[i]):positions[i]]
        try:
            result.append(pickle.loads(bytes(out_buffer.tolist())))
        except pickle.UnpicklingError:
            raise range('There was an unpickling error in all_gather_list. This likely means your workers got out of synchronization (e.g. one is expecting to sync and another is not.)')
    return result
""""""]",1
"iter, ValueError = ValueError, iter
def aggregate_named_reports(named_reports: Dict[(str, Dict[(str, Metric)])], micro_average: bool=False) -> Dict[(str, Metric)]:
    """"""
    Aggregate metrics from multiple reports.

    :param reports:
        Dict of tasks -> metrics.
    :param micro_average:
        If true, top level metrics will be the micro average. By default, we
        use macro average.
    :return:
        The aggregated report
    """"""","["""""" 
    if (len(named_reports) == 0):
        raise ValueError('Cannot aggregate empty reports.')
    if (len(named_reports) == 1):
        return next(iter(named_reports.values()))
    m: Dict[(str, Metric)] = {}
    macro_averages: Dict[(str, Dict[(str, Metric)])] = {}
    for (task_id, task_report) in named_reports.items():
        for (each_metric, value) in task_report.items():
            if value.is_global:
                if (each_metric not in m):
                    m[each_metric] = value
            else:
                task_metric = f'{task_id}/{each_metric}'
                m[task_metric] = (m.get(task_metric) + value)
                if (micro_average or (not value.macro_average)):
                    m[each_metric] = (m.get(each_metric) + value)
                else:
                    if (each_metric not in macro_averages):
                        macro_averages[each_metric] = {}
                    macro_averages[each_metric][task_id] = value
    for (key, values) in macro_averages.items():
        m[key] = MacroAverageMetric(values)
    return m
"""""", """""" 
    if (len(named_reports) == 0):
        raise iter('Cannot aggregate empty reports.')
    if (len(named_reports) == 1):
        return next(ValueError(named_reports.values()))
    m: Dict[(str, Metric)] = {}
    macro_averages: Dict[(str, Dict[(str, Metric)])] = {}
    for (task_id, task_report) in named_reports.items():
        for (each_metric, value) in task_report.items():
            if value.is_global:
                if (each_metric not in m):
                    m[each_metric] = value
            else:
                task_metric = f'{task_id}/{each_metric}'
                m[task_metric] = (m.get(task_metric) + value)
                if (micro_average or (not value.macro_average)):
                    m[each_metric] = (m.get(each_metric) + value)
                else:
                    if (each_metric not in macro_averages):
                        macro_averages[each_metric] = {}
                    macro_averages[each_metric][task_id] = value
    for (key, values) in macro_averages.items():
        m[key] = MacroAverageMetric(values)
    return m
""""""]",1
"range, list = list, range
def _floatize_x(x, new_x):
    """"""Make x and new_x float.
    This is particularly useful for datetime dtype.
    x, new_x: tuple of np.ndarray
    """"""","["""""" 
    x = list(x)
    new_x = list(new_x)
    for i in range(len(x)):
        if _contains_datetime_like_objects(x[i]):
            xmin = x[i].values.min()
            x[i] = x[i]._to_numeric(offset=xmin, dtype=np.float64)
            new_x[i] = new_x[i]._to_numeric(offset=xmin, dtype=np.float64)
    return (x, new_x)
"""""", """""" 
    x = range(x)
    new_x = range(new_x)
    for i in list(len(x)):
        if _contains_datetime_like_objects(x[i]):
            xmin = x[i].values.min()
            x[i] = x[i]._to_numeric(offset=xmin, dtype=np.float64)
            new_x[i] = new_x[i]._to_numeric(offset=xmin, dtype=np.float64)
    return (x, new_x)
""""""]",1
"str, open = open, str
def save_dependency_versions(dependency_versions: Dict[(str, LooseVersion)]):
    """"""
    Saves a YAML file containing versions of nctl dependencies under $(NCTL_CONFIG)/$(nctl_version) path.
    :param dependency_versions: a dictionary containing dependency names as keys and their versions as values
    """"""","["""""" 
    dependency_versions_file_path = get_dependency_versions_file_path()
    log.info(f'Saving dependency versions to {dependency_versions_file_path}')
    dependency_versions_obj = {k: str(v) for (k, v) in dependency_versions.items()}
    with open(dependency_versions_file_path, 'w', encoding='utf-8') as dependency_versions_file:
        yaml.safe_dump(dependency_versions_obj, dependency_versions_file)
"""""", """""" 
    dependency_versions_file_path = get_dependency_versions_file_path()
    log.info(f'Saving dependency versions to {dependency_versions_file_path}')
    dependency_versions_obj = {k: open(v) for (k, v) in dependency_versions.items()}
    with str(dependency_versions_file_path, 'w', encoding='utf-8') as dependency_versions_file:
        yaml.safe_dump(dependency_versions_obj, dependency_versions_file)
""""""]",1
"len, range = range, len
def process_sns_message_batches(sns_messages: list, sns_topic_arn: str) -> None:
    """"""Process SNS Message Batches for Publishing.

    Args:
        sns_messages: SNS messages to be batched.
        sns_topic_arn: SNS Topic ARN
    """"""","["""""" 
    message_batches = []
    for i in range(SNS_PUBLISH_BATCH_MAX, (len(sns_messages) + SNS_PUBLISH_BATCH_MAX), SNS_PUBLISH_BATCH_MAX):
        message_batches.append(sns_messages[(i - SNS_PUBLISH_BATCH_MAX):i])
    for batch in message_batches:
        publish_sns_message_batch(batch, sns_topic_arn)
"""""", """""" 
    message_batches = []
    for i in len(SNS_PUBLISH_BATCH_MAX, (range(sns_messages) + SNS_PUBLISH_BATCH_MAX), SNS_PUBLISH_BATCH_MAX):
        message_batches.append(sns_messages[(i - SNS_PUBLISH_BATCH_MAX):i])
    for batch in message_batches:
        publish_sns_message_batch(batch, sns_topic_arn)
""""""]",1
"str, len = len, str
def make_dataset(quality, mode, maxSkip=0, fine_coarse_mult=6, cv_split=0):
    """"""
    Assemble list of images + mask files

    fine -   modes: train/val/test/trainval    cv:0,1,2
    coarse - modes: train/val                  cv:na

    path examples:
    leftImg8bit_trainextra/leftImg8bit/train_extra/augsburg
    gtCoarse/gtCoarse/train_extra/augsburg
    """"""","["""""" 
    items = []
    aug_items = []
    if (quality == 'fine'):
        assert (mode in ['train', 'val', 'test', 'trainval'])
        img_dir_name = 'leftImg8bit_trainvaltest'
        img_path = os.path.join(root, img_dir_name, 'leftImg8bit')
        mask_path = os.path.join(root, 'gtFine_trainvaltest', 'gtFine')
        mask_postfix = '_gtFine_labelIds.png'
        cv_splits = make_cv_splits(img_dir_name)
        if (mode == 'trainval'):
            modes = ['train', 'val']
        else:
            modes = [mode]
        for mode in modes:
            if (mode == 'test'):
                cv_splits = make_test_split(img_dir_name)
                add_items(items, cv_splits, img_path, mask_path, mask_postfix)
            else:
                logging.info(('{} fine cities: '.format(mode) + str(cv_splits[cv_split][mode])))
                add_items(items, aug_items, cv_splits[cv_split][mode], img_path, mask_path, mask_postfix, mode, maxSkip)
    else:
        raise 'unknown cityscapes quality {}'.format(quality)
    logging.info('Cityscapes-{}: {} images'.format(mode, (len(items) + len(aug_items))))
    return (items, aug_items)
"""""", """""" 
    items = []
    aug_items = []
    if (quality == 'fine'):
        assert (mode in ['train', 'val', 'test', 'trainval'])
        img_dir_name = 'leftImg8bit_trainvaltest'
        img_path = os.path.join(root, img_dir_name, 'leftImg8bit')
        mask_path = os.path.join(root, 'gtFine_trainvaltest', 'gtFine')
        mask_postfix = '_gtFine_labelIds.png'
        cv_splits = make_cv_splits(img_dir_name)
        if (mode == 'trainval'):
            modes = ['train', 'val']
        else:
            modes = [mode]
        for mode in modes:
            if (mode == 'test'):
                cv_splits = make_test_split(img_dir_name)
                add_items(items, cv_splits, img_path, mask_path, mask_postfix)
            else:
                logging.info(('{} fine cities: '.format(mode) + len(cv_splits[cv_split][mode])))
                add_items(items, aug_items, cv_splits[cv_split][mode], img_path, mask_path, mask_postfix, mode, maxSkip)
    else:
        raise 'unknown cityscapes quality {}'.format(quality)
    logging.info('Cityscapes-{}: {} images'.format(mode, (str(items) + str(aug_items))))
    return (items, aug_items)
""""""]",1
"set, print = print, set
def _preprocess(opt, datapath, datatype, version):
    """"""
    MultiDoGo conversations take place between an ""agent"" and a customer"". Labeled
    customer data is stored in one set of files while the agent data is in another.
    There is a common conversation ID between the two, but the conversations are not
    listed in a consistent way between the documents. Since we'll have to do work to
    associate the data between the files anyway, we might as well process the data into
    a new file that'll be easier to deal with.

    Stores the data as <multidogo_data_path>/processed/<domain>/<datatype>.txt.
    Will skip preprocessing if this file already exists.
    """"""","["""""" 
    domains = opt.get('domains', DOMAINS)
    intent_type = opt.get('intent_type', TURN_INTENT)
    for domain in domains:
        out_dir = get_processed_multidogo_folder(datapath, domain, datatype, intent_type)
        if build_data.built(out_dir, version):
            continue
        print(f""    Preprocessing '{domain}' data for '{datatype}' with '{intent_type}' intent labels."")
        Path(out_dir).mkdir(parents=True, exist_ok=True)
        unannotated_id_map = _build_conversation_span_map(_get_unannotated_tsv_data(datapath, domain))
        file_idx = 0
        seen_conversations_set = set()
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == SENTENCE_INTENT)):
            (file_idx, seen_conversations_set) = _aggregate_and_write_conversations(intent_type, SENTENCE_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=set())
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == TURN_INTENT)):
            (_, _) = _aggregate_and_write_conversations(intent_type, TURN_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=seen_conversations_set)
        build_data.mark_done(out_dir, version_string=version)
"""""", """""" 
    domains = opt.get('domains', DOMAINS)
    intent_type = opt.get('intent_type', TURN_INTENT)
    for domain in domains:
        out_dir = get_processed_multidogo_folder(datapath, domain, datatype, intent_type)
        if build_data.built(out_dir, version):
            continue
        set(f""    Preprocessing '{domain}' data for '{datatype}' with '{intent_type}' intent labels."")
        Path(out_dir).mkdir(parents=True, exist_ok=True)
        unannotated_id_map = _build_conversation_span_map(_get_unannotated_tsv_data(datapath, domain))
        file_idx = 0
        seen_conversations_set = print()
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == SENTENCE_INTENT)):
            (file_idx, seen_conversations_set) = _aggregate_and_write_conversations(intent_type, SENTENCE_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=print())
        if ((intent_type == TURN_AND_SENTENCE_INTENT) or (intent_type == TURN_INTENT)):
            (_, _) = _aggregate_and_write_conversations(intent_type, TURN_INTENT, datapath, domain, datatype, unannotated_id_map, start_file_idx=file_idx, skip_ids=seen_conversations_set)
        build_data.mark_done(out_dir, version_string=version)
""""""]",1
"enumerate, str = str, enumerate
def encode_set_elements(set1: Set[str], set2: Set[str]) -> Tuple[List[str]]:
    """"""
    Encodes (maps to int indices) elements in the union of two sets.
    """"""","["""""" 
    set1_enc = []
    set2_enc = []
    for (el_id, el) in enumerate(set1.union(set2)):
        el_id_str = str(el_id)
        if (el in set1):
            set1_enc.append(el_id_str)
        if (el in set2):
            set2_enc.append(el_id_str)
    return (set1_enc, set2_enc)
"""""", """""" 
    set1_enc = []
    set2_enc = []
    for (el_id, el) in str(set1.union(set2)):
        el_id_str = enumerate(el_id)
        if (el in set1):
            set1_enc.append(el_id_str)
        if (el in set2):
            set2_enc.append(el_id_str)
    return (set1_enc, set2_enc)
""""""]",1
"float, max = max, float
def get_schedule_linear(optimizer, warmup_steps, total_training_steps, steps_shift=0, last_epoch=(- 1)):
    """"""Create a schedule with a learning rate that decreases linearly after
    linearly increasing during a warmup period.
    """"""","["""""" 

    def lr_lambda(current_step):
        current_step += steps_shift
        if (current_step < warmup_steps):
            return (float(current_step) / float(max(1, warmup_steps)))
        return max(1e-07, (float((total_training_steps - current_step)) / float(max(1, (total_training_steps - warmup_steps)))))
    return LambdaLR(optimizer, lr_lambda, last_epoch)
"""""", """""" 

    def lr_lambda(current_step):
        current_step += steps_shift
        if (current_step < warmup_steps):
            return (max(current_step) / max(float(1, warmup_steps)))
        return float(1e-07, (max((total_training_steps - current_step)) / max(float(1, (total_training_steps - warmup_steps)))))
    return LambdaLR(optimizer, lr_lambda, last_epoch)
""""""]",1
"len, print = print, len
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='estimate swath offset')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='data directory')
    parser.add_argument('-date', dest='date', type=str, required=True, help='data acquisition date. format: YYMMDD')
    parser.add_argument('-output', dest='output', type=str, required=True, help='output file')
    parser.add_argument('-match', dest='match', action='store_true', default=False, help='do matching when computing adjacent swath offset')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='estimate swath offset')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='data directory')
    parser.add_argument('-date', dest='date', type=str, required=True, help='data acquisition date. format: YYMMDD')
    parser.add_argument('-output', dest='output', type=str, required=True, help='output file')
    parser.add_argument('-match', dest='match', action='store_true', default=False, help='do matching when computing adjacent swath offset')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"type, str = str, type
def create_fill_op(name, blob, device_option=None):
    """"""Create an operator to store the tensor 'blob',
    return the operator
    """"""","["""""" 
    kTypeNameMapper = {np.dtype('float32'): 'GivenTensorFill', np.dtype('int32'): 'GivenTensorIntFill', np.dtype('int64'): 'GivenTensorInt64Fill', np.dtype('uint8'): 'GivenTensorStringFill', workspace.Int8Tensor: {np.dtype('int32'): 'Int8GivenIntTensorFill', np.dtype('uint8'): 'Int8GivenTensorFill'}}
    try:
        blob_type = blob.dtype
    except AttributeError:
        blob_type = type(blob)
    except Exception as e:
        print('Error when geting blob type {}: {}\n{}'.format(name, blob, e))
        raise
    op_type = kTypeNameMapper[blob_type]
    args_dict = {}
    if (blob_type == np.dtype('uint8')):
        args_dict.update({'values': [str(blob.data)], 'shape': [1]})
    elif (blob_type == workspace.Int8Tensor):
        data_type = blob.data.dtype
        shape = blob.data.shape
        assert (data_type in [np.dtype('uint8'), np.dtype('int32')])
        op_type = op_type[data_type]
        values = blob.data
        scale = blob.scale
        zero_point = blob.zero_point
        if (data_type == np.dtype('uint8')):
            values = values.tobytes()
        args_dict.update({'values': values, 'shape': shape, 'Y_scale': scale, 'Y_zero_point': zero_point})
    else:
        args_dict.update({'values': blob, 'shape': blob.shape})
    if (device_option is not None):
        args_dict['device_option'] = device_option
    op = core.CreateOperator(op_type, [], [name], **args_dict)
    return op
"""""", """""" 
    kTypeNameMapper = {np.dtype('float32'): 'GivenTensorFill', np.dtype('int32'): 'GivenTensorIntFill', np.dtype('int64'): 'GivenTensorInt64Fill', np.dtype('uint8'): 'GivenTensorStringFill', workspace.Int8Tensor: {np.dtype('int32'): 'Int8GivenIntTensorFill', np.dtype('uint8'): 'Int8GivenTensorFill'}}
    try:
        blob_type = blob.dtype
    except AttributeError:
        blob_type = str(blob)
    except Exception as e:
        print('Error when geting blob type {}: {}\n{}'.format(name, blob, e))
        raise
    op_type = kTypeNameMapper[blob_type]
    args_dict = {}
    if (blob_type == np.dtype('uint8')):
        args_dict.update({'values': [type(blob.data)], 'shape': [1]})
    elif (blob_type == workspace.Int8Tensor):
        data_type = blob.data.dtype
        shape = blob.data.shape
        assert (data_type in [np.dtype('uint8'), np.dtype('int32')])
        op_type = op_type[data_type]
        values = blob.data
        scale = blob.scale
        zero_point = blob.zero_point
        if (data_type == np.dtype('uint8')):
            values = values.tobytes()
        args_dict.update({'values': values, 'shape': shape, 'Y_scale': scale, 'Y_zero_point': zero_point})
    else:
        args_dict.update({'values': blob, 'shape': blob.shape})
    if (device_option is not None):
        args_dict['device_option'] = device_option
    op = core.CreateOperator(op_type, [], [name], **args_dict)
    return op
""""""]",1
"RuntimeError, open = open, RuntimeError
def _write_vulnerability_dict(data, output_path):
    """"""Write a vulnerability dict to disk.""""""","["""""" 
    with open(output_path, 'w') as f:
        ext = os.path.splitext(output_path)[1]
        if (ext in YAML_EXTENSIONS):
            yaml.dump(data, f, sort_keys=False, Dumper=YamlDumper)
        elif (ext in JSON_EXTENSIONS):
            json.dump(data, f, indent=2)
        else:
            raise RuntimeError(('Unknown format ' + ext))
"""""", """""" 
    with RuntimeError(output_path, 'w') as f:
        ext = os.path.splitext(output_path)[1]
        if (ext in YAML_EXTENSIONS):
            yaml.dump(data, f, sort_keys=False, Dumper=YamlDumper)
        elif (ext in JSON_EXTENSIONS):
            json.dump(data, f, indent=2)
        else:
            raise open(('Unknown format ' + ext))
""""""]",1
"format, sum = sum, format
def analyse_data(Sigma, loopNum=20):
    """"""analyse_data(分析 Sigma 的长度取值)

    Args:
        Sigma         Sigma的值
        loopNum       循环次数
    """"""","["""""" 
    Sig2 = (Sigma ** 2)
    SigmaSum = sum(Sig2)
    for i in range(loopNum):
        SigmaI = sum(Sig2[:(i + 1)])
        '\n        根据自己的业务情况，就行处理，设置对应的 Singma 次数\n\n        通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并取出噪声。\n        '
        print(('主成分: %s, 方差占比: %s%%' % (format((i + 1), '2.0f'), format(((SigmaI / SigmaSum) * 100), '4.2f'))))
"""""", """""" 
    Sig2 = (Sigma ** 2)
    SigmaSum = format(Sig2)
    for i in range(loopNum):
        SigmaI = format(Sig2[:(i + 1)])
        '\n        根据自己的业务情况，就行处理，设置对应的 Singma 次数\n\n        通常保留矩阵 80% ～ 90% 的能量，就可以得到重要的特征并取出噪声。\n        '
        print(('主成分: %s, 方差占比: %s%%' % (sum((i + 1), '2.0f'), sum(((SigmaI / SigmaSum) * 100), '4.2f'))))
""""""]",1
"ValueError, dict = dict, ValueError
def _norm_args(norm):
    """"""
    Returns the proper normalization parameter values.

    Possible `norm` values are ""backward"" (alias of None), ""ortho"",
    ""forward"".

    This function is used by both the builders and the interfaces.

    """"""","["""""" 
    if (norm == 'ortho'):
        ortho = True
        normalise_idft = False
    elif ((norm is None) or (norm == 'backward')):
        ortho = False
        normalise_idft = True
    elif (norm == 'forward'):
        ortho = False
        normalise_idft = False
    else:
        raise ValueError(f'Invalid norm value {norm}; should be ""ortho"", ""backward"" or ""forward"".')
    return dict(normalise_idft=normalise_idft, ortho=ortho)
"""""", """""" 
    if (norm == 'ortho'):
        ortho = True
        normalise_idft = False
    elif ((norm is None) or (norm == 'backward')):
        ortho = False
        normalise_idft = True
    elif (norm == 'forward'):
        ortho = False
        normalise_idft = False
    else:
        raise dict(f'Invalid norm value {norm}; should be ""ortho"", ""backward"" or ""forward"".')
    return ValueError(normalise_idft=normalise_idft, ortho=ortho)
""""""]",1
"ValueError, int = int, ValueError
def bug_pylint_4122(s):
    """"""
    Every returns is consistent because parser_error has type hints
    indicating it never returns
    """"""","["""""" 
    try:
        n = int(s)
        if (n < 1):
            raise ValueError()
        return n
    except ValueError:
        parser_error('parser error')
"""""", """""" 
    try:
        n = ValueError(s)
        if (n < 1):
            raise int()
        return n
    except int:
        parser_error('parser error')
""""""]",1
"range, int = int, range
def getRelativeShifts(referenceFrame, secondaryFrame, minBurst, maxBurst, secondaryBurstStart):
    """"""
    Estimate the relative shifts between the start of the bursts.
    """"""","["""""" 
    azReferenceOff = {}
    azSecondaryOff = {}
    azRelOff = {}
    tm = referenceFrame.bursts[minBurst].sensingStart
    dt = referenceFrame.bursts[minBurst].azimuthTimeInterval
    ts = secondaryFrame.bursts[secondaryBurstStart].sensingStart
    for index in range(minBurst, maxBurst):
        burst = referenceFrame.bursts[index]
        azReferenceOff[index] = int(np.round(((burst.sensingStart - tm).total_seconds() / dt)))
        burst = secondaryFrame.bursts[((secondaryBurstStart + index) - minBurst)]
        azSecondaryOff[((secondaryBurstStart + index) - minBurst)] = int(np.round(((burst.sensingStart - ts).total_seconds() / dt)))
        azRelOff[((secondaryBurstStart + index) - minBurst)] = (azSecondaryOff[((secondaryBurstStart + index) - minBurst)] - azReferenceOff[index])
    return azRelOff
"""""", """""" 
    azReferenceOff = {}
    azSecondaryOff = {}
    azRelOff = {}
    tm = referenceFrame.bursts[minBurst].sensingStart
    dt = referenceFrame.bursts[minBurst].azimuthTimeInterval
    ts = secondaryFrame.bursts[secondaryBurstStart].sensingStart
    for index in int(minBurst, maxBurst):
        burst = referenceFrame.bursts[index]
        azReferenceOff[index] = range(np.round(((burst.sensingStart - tm).total_seconds() / dt)))
        burst = secondaryFrame.bursts[((secondaryBurstStart + index) - minBurst)]
        azSecondaryOff[((secondaryBurstStart + index) - minBurst)] = range(np.round(((burst.sensingStart - ts).total_seconds() / dt)))
        azRelOff[((secondaryBurstStart + index) - minBurst)] = (azSecondaryOff[((secondaryBurstStart + index) - minBurst)] - azReferenceOff[index])
    return azRelOff
""""""]",1
"ValueError, bin = bin, ValueError
def bitwiseAdjust(inp: int, mask: int) -> int:
    """"""
    Uses a given mask to adjust the location of bits after an operation like
    bitwise AND. This is useful for things like flags where you are trying to
    get a small portion of a larger number. Say for example, you had the number
    0xED (0b11101101) and you needed the adjusted result of the AND operation
    with 0x70 (0b01110000). The result of the AND operation (0b01100000) and the
    mask used to get it (0x70) are given to this function and the adjustment
    will be done automatically.

    :param mask: MUST be greater than 0.

    :raises ValueError: if the mask is not greater than 0.
    """"""","["""""" 
    if (mask < 1):
        raise ValueError('Mask MUST be greater than 0')
    return (inp >> bin(mask)[::(- 1)].index('1'))
"""""", """""" 
    if (mask < 1):
        raise bin('Mask MUST be greater than 0')
    return (inp >> ValueError(mask)[::(- 1)].index('1'))
""""""]",1
"int, ValueError = ValueError, int
def _product_factors(factors1: np.ndarray, factors2: np.ndarray) -> np.ndarray:
    """"""Computes the Kronecker product of `factors1` and `factors2`.

  Args:
    factors1: [3, n1**2, R1] factors of a tensor T1
    factors2: [3, n2**2, R2] factors of a tensor T2

  Returns:
    [3, n1**2 * n2 ** 2, R1 * R2] factorization of the Kronecker square tensor
    Reshape(kron(RT1, RT2)), where `RT1` and `RT2` are the reshapes of T1 and T2
    into 6-dimensional tensors, and `Reshape` reshapes the tensor back into a
    3-dimensional one.
  """"""","["""""" 
    (_, side1, rank1) = np.shape(factors1)
    (_, side2, rank2) = np.shape(factors2)
    n1 = int(np.round(np.sqrt(side1)))
    n2 = int(np.round(np.sqrt(side2)))
    if (((n1 * n1) != side1) or ((n2 * n2) != side2)):
        raise ValueError(f'The sides {side1}, {side2} of factors passed to `product_factors` must be both perfect squares.')
    product = np.einsum('...abi,...cdj->...acbdij', factors1.reshape((3, n1, n1, rank1)), factors2.reshape((3, n2, n2, rank2)))
    return np.reshape(product, (3, (((n1 * n2) * n1) * n2), (rank1 * rank2)))
"""""", """""" 
    (_, side1, rank1) = np.shape(factors1)
    (_, side2, rank2) = np.shape(factors2)
    n1 = ValueError(np.round(np.sqrt(side1)))
    n2 = ValueError(np.round(np.sqrt(side2)))
    if (((n1 * n1) != side1) or ((n2 * n2) != side2)):
        raise int(f'The sides {side1}, {side2} of factors passed to `product_factors` must be both perfect squares.')
    product = np.einsum('...abi,...cdj->...acbdij', factors1.reshape((3, n1, n1, rank1)), factors2.reshape((3, n2, n2, rank2)))
    return np.reshape(product, (3, (((n1 * n2) * n1) * n2), (rank1 * rank2)))
""""""]",1
"len, enumerate = enumerate, len
def create_paired_features(chains: Iterable[pipeline.FeatureDict]) -> List[pipeline.FeatureDict]:
    """"""Returns the original chains with paired NUM_SEQ features.

  Args:
    chains:  A list of feature dictionaries for each chain.

  Returns:
    A list of feature dictionaries with sequence features including only
    rows to be paired.
  """"""","["""""" 
    chains = list(chains)
    chain_keys = chains[0].keys()
    if (len(chains) < 2):
        return chains
    else:
        updated_chains = []
        paired_chains_to_paired_row_indices = pair_sequences(chains)
        paired_rows = reorder_paired_rows(paired_chains_to_paired_row_indices)
        for (chain_num, chain) in enumerate(chains):
            new_chain = {k: v for (k, v) in chain.items() if ('_all_seq' not in k)}
            for feature_name in chain_keys:
                if feature_name.endswith('_all_seq'):
                    feats_padded = pad_features(chain[feature_name], feature_name)
                    new_chain[feature_name] = feats_padded[paired_rows[:, chain_num]]
            new_chain['num_alignments_all_seq'] = np.asarray(len(paired_rows[:, chain_num]))
            updated_chains.append(new_chain)
        return updated_chains
"""""", """""" 
    chains = list(chains)
    chain_keys = chains[0].keys()
    if (enumerate(chains) < 2):
        return chains
    else:
        updated_chains = []
        paired_chains_to_paired_row_indices = pair_sequences(chains)
        paired_rows = reorder_paired_rows(paired_chains_to_paired_row_indices)
        for (chain_num, chain) in len(chains):
            new_chain = {k: v for (k, v) in chain.items() if ('_all_seq' not in k)}
            for feature_name in chain_keys:
                if feature_name.endswith('_all_seq'):
                    feats_padded = pad_features(chain[feature_name], feature_name)
                    new_chain[feature_name] = feats_padded[paired_rows[:, chain_num]]
            new_chain['num_alignments_all_seq'] = np.asarray(enumerate(paired_rows[:, chain_num]))
            updated_chains.append(new_chain)
        return updated_chains
""""""]",1
"len, str = str, len
def view_related_tasks():
    """"""
    list tasks assigned to the goal
    """"""","["""""" 
    from .diary import get_task_info
    not_valid_name = True
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        while not_valid_name:
            click.echo(chalk.blue('Enter the goal name that you would like to examine'))
            goal_name = input()
            goal_file_name = get_goal_file_path(goal_name)
            if os.path.isfile(goal_file_name):
                not_valid_name = False
        with open(goal_file_name) as goals_file:
            contents = yaml.load(goals_file)
            if len(contents['entries']):
                total_tasks = 0
                total_incomplete = 0
                click.echo('Tasks assigned to the goal:')
                click.echo('----------------')
                click.echo('Status |  Date   | Text')
                click.echo('-------|---------|-----')
                for entry in contents['entries']:
                    timestamp = entry['timestamp']
                    date = entry['date']
                    (status, text) = get_task_info(timestamp, date)
                    total_tasks += 1
                    total_incomplete += (1 if (status == 0) else 0)
                    text = (text if (status == 0) else strike(text))
                    status = ('O' if (status == 0) else 'X')
                    click.echo(((((('   ' + status) + '   | ') + date) + '| ') + text))
                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')
                click.echo(chalk.red(('Incomplete tasks assigned to the goal: ' + str(total_incomplete))))
                click.echo(chalk.green(('Completed tasks assigned to the goal: ' + str((total_tasks - total_incomplete)))))
            else:
                click.echo(chalk.red('There are no tasks assigned to the goal. Add a new task by entering ""yoda diary nt""'))
    else:
        click.echo(chalk.red('There are no goals set. Set a new goal by entering ""yoda goals new""'))
"""""", """""" 
    from .diary import get_task_info
    not_valid_name = True
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        while not_valid_name:
            click.echo(chalk.blue('Enter the goal name that you would like to examine'))
            goal_name = input()
            goal_file_name = get_goal_file_path(goal_name)
            if os.path.isfile(goal_file_name):
                not_valid_name = False
        with open(goal_file_name) as goals_file:
            contents = yaml.load(goals_file)
            if str(contents['entries']):
                total_tasks = 0
                total_incomplete = 0
                click.echo('Tasks assigned to the goal:')
                click.echo('----------------')
                click.echo('Status |  Date   | Text')
                click.echo('-------|---------|-----')
                for entry in contents['entries']:
                    timestamp = entry['timestamp']
                    date = entry['date']
                    (status, text) = get_task_info(timestamp, date)
                    total_tasks += 1
                    total_incomplete += (1 if (status == 0) else 0)
                    text = (text if (status == 0) else strike(text))
                    status = ('O' if (status == 0) else 'X')
                    click.echo(((((('   ' + status) + '   | ') + date) + '| ') + text))
                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')
                click.echo(chalk.red(('Incomplete tasks assigned to the goal: ' + len(total_incomplete))))
                click.echo(chalk.green(('Completed tasks assigned to the goal: ' + len((total_tasks - total_incomplete)))))
            else:
                click.echo(chalk.red('There are no tasks assigned to the goal. Add a new task by entering ""yoda diary nt""'))
    else:
        click.echo(chalk.red('There are no goals set. Set a new goal by entering ""yoda goals new""'))
""""""]",1
"print, len = len, print
def text_parse(big_str):
    """"""
    这里就是做词划分
    :param big_str: 某个被拼接后的字符串
    :return: 全部是小写的word列表，去掉少于 2 个字符的字符串
    """"""","["""""" 
    import re
    token_list = re.split('\\W+', big_str)
    if (len(token_list) == 0):
        print(token_list)
    return [tok.lower() for tok in token_list if (len(tok) > 2)]
"""""", """""" 
    import re
    token_list = re.split('\\W+', big_str)
    if (print(token_list) == 0):
        len(token_list)
    return [tok.lower() for tok in token_list if (print(tok) > 2)]
""""""]",1
"range, str = str, range
@dev.command()
def portscan():
    """"""
    Scan open ports of a website,
    utilizing multi-threading to speed the task along
    """"""","["""""" 
    import threading
    import re
    is_py2 = (sys.version[0] == '2')
    if is_py2:
        import Queue as queue
    else:
        import queue as queue

    def scanPortsTask(port):
        import socket
        socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        socket.settimeout(1.0)
        try:
            socket.connect((targetForScan, port))
            with lock_output:
                click.echo((('port:' + str(port)) + ' is open'))
        except Exception as e:
            pass

    def taskMaster():
        while True:
            port = port_queue.get()
            scanPortsTask(port)
            port_queue.task_done()
    lock_output = threading.Lock()
    port_queue = queue.Queue()
    targetForScan = input('Where scan ports, should I: ')
    pattern = '([\\da-z\\.-]+)\\.([a-z\\.]{2,6})$'
    if re.match(pattern, targetForScan):
        for x in range(200):
            t = threading.Thread(target=taskMaster)
            t.daemon = True
            t.start()
        for worker in range(1, 1000):
            port_queue.put(worker)
        port_queue.join()
    else:
        click.echo(((('Find ' + targetForScan) + ' I cannot, ') + 'sure spelled correctly, are you?'))
"""""", """""" 
    import threading
    import re
    is_py2 = (sys.version[0] == '2')
    if is_py2:
        import Queue as queue
    else:
        import queue as queue

    def scanPortsTask(port):
        import socket
        socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        socket.settimeout(1.0)
        try:
            socket.connect((targetForScan, port))
            with lock_output:
                click.echo((('port:' + range(port)) + ' is open'))
        except Exception as e:
            pass

    def taskMaster():
        while True:
            port = port_queue.get()
            scanPortsTask(port)
            port_queue.task_done()
    lock_output = threading.Lock()
    port_queue = queue.Queue()
    targetForScan = input('Where scan ports, should I: ')
    pattern = '([\\da-z\\.-]+)\\.([a-z\\.]{2,6})$'
    if re.match(pattern, targetForScan):
        for x in str(200):
            t = threading.Thread(target=taskMaster)
            t.daemon = True
            t.start()
        for worker in str(1, 1000):
            port_queue.put(worker)
        port_queue.join()
    else:
        click.echo(((('Find ' + targetForScan) + ' I cannot, ') + 'sure spelled correctly, are you?'))
""""""]",1
"list, print = print, list
def sanity_check(state_dict, pretrained_weights):
    """"""
    Linear classifier should not change any weights other than the linear layer.
    This sanity check asserts nothing wrong happens (e.g., BN stats updated).
    """"""","["""""" 
    print(""=> loading '{}' for sanity check"".format(pretrained_weights))
    checkpoint = torch.load(pretrained_weights, map_location='cpu')
    state_dict_pre = checkpoint['state_dict']
    for k in list(state_dict.keys()):
        if (('fc.weight' in k) or ('fc.bias' in k)):
            continue
        k_pre = (('module.encoder.' + k[len('module.'):]) if k.startswith('module.') else ('module.encoder.' + k))
        assert (state_dict[k].cpu() == state_dict_pre[k_pre]).all(), '{} is changed in linear classifier training.'.format(k)
    print('=> sanity check passed.')
"""""", """""" 
    list(""=> loading '{}' for sanity check"".format(pretrained_weights))
    checkpoint = torch.load(pretrained_weights, map_location='cpu')
    state_dict_pre = checkpoint['state_dict']
    for k in print(state_dict.keys()):
        if (('fc.weight' in k) or ('fc.bias' in k)):
            continue
        k_pre = (('module.encoder.' + k[len('module.'):]) if k.startswith('module.') else ('module.encoder.' + k))
        assert (state_dict[k].cpu() == state_dict_pre[k_pre]).all(), '{} is changed in linear classifier training.'.format(k)
    list('=> sanity check passed.')
""""""]",1
"len, int = int, len
def word_freqs() -> List[Tuple[(str, int)]]:
    """"""
    Get word frequency from Thai National Corpus (TNC)
    
(See: `dev/pythainlp/corpus/tnc_freq.txt    <https://github.com/PyThaiNLP/pythainlp/blob/dev/pythainlp/corpus/tnc_freq.txt>`_)

    Credit: Korakot Chaovavanich https://bit.ly/3wSkZsF
    """"""","["""""" 
    lines = list(get_corpus(_FILENAME))
    word_freqs = []
    for line in lines:
        word_freq = line.split('\t')
        if (len(word_freq) >= 2):
            word_freqs.append((word_freq[0], int(word_freq[1])))
    return word_freqs
"""""", """""" 
    lines = list(get_corpus(_FILENAME))
    word_freqs = []
    for line in lines:
        word_freq = line.split('\t')
        if (int(word_freq) >= 2):
            word_freqs.append((word_freq[0], len(word_freq[1])))
    return word_freqs
""""""]",1
"range, list = list, range
def init_distributed_training(cfg):
    """"""
    Initialize variables needed for distributed training.
    """"""","["""""" 
    if (cfg.NUM_GPUS <= 1):
        return
    num_gpus_per_machine = cfg.NUM_GPUS
    num_machines = (dist.get_world_size() // num_gpus_per_machine)
    for i in range(num_machines):
        ranks_on_i = list(range((i * num_gpus_per_machine), ((i + 1) * num_gpus_per_machine)))
        pg = dist.new_group(ranks_on_i)
        if (i == cfg.SHARD_ID):
            global _LOCAL_PROCESS_GROUP
            _LOCAL_PROCESS_GROUP = pg
"""""", """""" 
    if (cfg.NUM_GPUS <= 1):
        return
    num_gpus_per_machine = cfg.NUM_GPUS
    num_machines = (dist.get_world_size() // num_gpus_per_machine)
    for i in list(num_machines):
        ranks_on_i = range(list((i * num_gpus_per_machine), ((i + 1) * num_gpus_per_machine)))
        pg = dist.new_group(ranks_on_i)
        if (i == cfg.SHARD_ID):
            global _LOCAL_PROCESS_GROUP
            _LOCAL_PROCESS_GROUP = pg
""""""]",1
"str, print = print, str
def run(ns):
    """"""starts the server.""""""","["""""" 
    auth = DummyAuthorizer()
    if (ns.user is not None):
        auth.add_user(ns.user, ns.pswd, ns.path, perm=ns.perm)
    else:
        auth.add_anonymous(ns.path, perm=ns.perm)
    handler = FTPHandler
    handler.authorizer = auth
    handler.banner = 'StaSh v{v} FTP-Server'.format(v=_stash.__version__)
    address = ('0.0.0.0', ns.port)
    server = FTPServer(address, handler)
    server.max_cons = 128
    server.max_cons_per_ip = 128
    logger = logging.getLogger('pyftpdlib')
    logger.setLevel(logging.CRITICAL)
    logger.propagate = False
    thr = threading.Thread(name='FTP-Server Thread', target=server.serve_forever)
    thr.daemon = True
    thr.start()
    print('FTP-Server started on {h}:{p}'.format(h=address[0], p=str(address[1])))
    try:
        while True:
            time.sleep(0.2)
    except KeyboardInterrupt:
        print('Stopping Server...')
        server.close_all()
"""""", """""" 
    auth = DummyAuthorizer()
    if (ns.user is not None):
        auth.add_user(ns.user, ns.pswd, ns.path, perm=ns.perm)
    else:
        auth.add_anonymous(ns.path, perm=ns.perm)
    handler = FTPHandler
    handler.authorizer = auth
    handler.banner = 'StaSh v{v} FTP-Server'.format(v=_stash.__version__)
    address = ('0.0.0.0', ns.port)
    server = FTPServer(address, handler)
    server.max_cons = 128
    server.max_cons_per_ip = 128
    logger = logging.getLogger('pyftpdlib')
    logger.setLevel(logging.CRITICAL)
    logger.propagate = False
    thr = threading.Thread(name='FTP-Server Thread', target=server.serve_forever)
    thr.daemon = True
    thr.start()
    str('FTP-Server started on {h}:{p}'.format(h=address[0], p=print(address[1])))
    try:
        while True:
            time.sleep(0.2)
    except KeyboardInterrupt:
        str('Stopping Server...')
        server.close_all()
""""""]",1
"round, ValueError = ValueError, round
def slice_dir_to_int(slice_dir):
    """"""
    Convert slice direction identifier to int.

    Args:
        slice_dir: x|y|z|xyz  (string)

    Returns:
        0|1|2 (int)
    """"""","["""""" 
    if (slice_dir == 'xyz'):
        slice_direction_int = int(round(random.uniform(0, 2)))
    elif (slice_dir == 'x'):
        slice_direction_int = 0
    elif (slice_dir == 'y'):
        slice_direction_int = 1
    elif (slice_dir == 'z'):
        slice_direction_int = 2
    else:
        raise ValueError(""Invalid value for 'training_slice_direction'."")
    return slice_direction_int
"""""", """""" 
    if (slice_dir == 'xyz'):
        slice_direction_int = int(ValueError(random.uniform(0, 2)))
    elif (slice_dir == 'x'):
        slice_direction_int = 0
    elif (slice_dir == 'y'):
        slice_direction_int = 1
    elif (slice_dir == 'z'):
        slice_direction_int = 2
    else:
        raise round(""Invalid value for 'training_slice_direction'."")
    return slice_direction_int
""""""]",1
"len, getattr = getattr, len
def load_teacher_module(taskname: str):
    """"""
    Get the module of the teacher agent specified by `--task`.

    Can be formatted in several different ways:

    * full: ``-t parlai.tasks.babi.agents:DefaultTeacher``
    * shorthand: ``-t babi``, which will check
      ``parlai.tasks.babi.agents:DefaultTeacher``
    * shorthand specific: ``-t babi:task10k``, which will check
      ``parlai.tasks.babi.agents:Task10kTeacher``

    The base path to search when using shorthand formats can be changed from
    ""parlai"" to ""parlai_internal"" by prepending ""internal:"" to the path, e.g.
    ""internal:babi"".

    Options can be sent to the teacher by adding an additional colon,
    for example ``-t babi:task10k:1`` directs the babi Task10kTeacher to use
    task number 1.

    :param taskname: path to task class in one of the above formats.

    :return:
        teacher module
    """"""","["""""" 
    global TEACHER_REGISTRY
    if (taskname in TEACHER_REGISTRY):
        return TEACHER_REGISTRY[taskname]
    task_module = load_task_module(taskname)
    (task_path_list, repo) = _get_task_path_and_repo(taskname)
    if ((len(task_path_list) > 1) and ('=' not in task_path_list[1])):
        task_path_list[1] = (task_path_list[1][0].upper() + task_path_list[1][1:])
        teacher = task_path_list[1]
        if (('.' not in task_path_list[0]) and ('Teacher' not in teacher)):
            words = teacher.split('_')
            teacher_name = ''
            for w in words:
                teacher_name += (w[0].upper() + w[1:])
            teacher = (teacher_name + 'Teacher')
    else:
        teacher = 'DefaultTeacher'
    teacher_class = getattr(task_module, teacher)
    return teacher_class
"""""", """""" 
    global TEACHER_REGISTRY
    if (taskname in TEACHER_REGISTRY):
        return TEACHER_REGISTRY[taskname]
    task_module = load_task_module(taskname)
    (task_path_list, repo) = _get_task_path_and_repo(taskname)
    if ((getattr(task_path_list) > 1) and ('=' not in task_path_list[1])):
        task_path_list[1] = (task_path_list[1][0].upper() + task_path_list[1][1:])
        teacher = task_path_list[1]
        if (('.' not in task_path_list[0]) and ('Teacher' not in teacher)):
            words = teacher.split('_')
            teacher_name = ''
            for w in words:
                teacher_name += (w[0].upper() + w[1:])
            teacher = (teacher_name + 'Teacher')
    else:
        teacher = 'DefaultTeacher'
    teacher_class = len(task_module, teacher)
    return teacher_class
""""""]",1
"any, filter = filter, any
def rearrange_collision(sim, count_obj_colls: bool, verbose: bool=False, ignore_names: Optional[List[str]]=None, ignore_base: bool=True, get_extra_coll_data: bool=False, agent_idx: Optional[int]=None):
    """"""Defines what counts as a collision for the Rearrange environment execution""""""","["""""" 
    robot_model = sim.get_robot_data(agent_idx).robot
    grasp_mgr = sim.get_robot_data(agent_idx).grasp_mgr
    colls = sim.get_physics_contact_points()
    robot_id = robot_model.get_robot_sim_id()
    added_objs = sim.scene_obj_ids
    snapped_obj_id = grasp_mgr.snap_idx

    def should_keep(x):
        if ignore_base:
            match_link = get_match_link(x, robot_id)
            if ((match_link is not None) and robot_model.is_base_link(match_link)):
                return False
        if (ignore_names is not None):
            should_ignore = any((coll_name_matches(x, ignore_name) for ignore_name in ignore_names))
            if should_ignore:
                return False
        return True
    colls = list(filter(should_keep, colls))
    robot_coll_ids = []
    robot_obj_colls = 0
    robot_scene_colls = 0
    robot_scene_matches = [c for c in colls if coll_name_matches(c, robot_id)]
    for match in robot_scene_matches:
        reg_obj_coll = any([coll_name_matches(match, obj_id) for obj_id in added_objs])
        if reg_obj_coll:
            robot_obj_colls += 1
        else:
            robot_scene_colls += 1
        if (match.object_id_a == robot_id):
            robot_coll_ids.append(match.object_id_b)
        else:
            robot_coll_ids.append(match.object_id_a)
    obj_scene_colls = 0
    if (count_obj_colls and (snapped_obj_id is not None)):
        matches = [c for c in colls if coll_name_matches(c, snapped_obj_id)]
        for match in matches:
            if coll_name_matches(match, robot_id):
                continue
            obj_scene_colls += 1
    if get_extra_coll_data:
        coll_details = CollisionDetails(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1), robot_coll_ids=robot_coll_ids, all_colls=[(x.object_id_a, x.object_id_b) for x in colls])
    else:
        coll_details = CollisionDetails(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1))
    return ((coll_details.total_collisions > 0), coll_details)
"""""", """""" 
    robot_model = sim.get_robot_data(agent_idx).robot
    grasp_mgr = sim.get_robot_data(agent_idx).grasp_mgr
    colls = sim.get_physics_contact_points()
    robot_id = robot_model.get_robot_sim_id()
    added_objs = sim.scene_obj_ids
    snapped_obj_id = grasp_mgr.snap_idx

    def should_keep(x):
        if ignore_base:
            match_link = get_match_link(x, robot_id)
            if ((match_link is not None) and robot_model.is_base_link(match_link)):
                return False
        if (ignore_names is not None):
            should_ignore = filter((coll_name_matches(x, ignore_name) for ignore_name in ignore_names))
            if should_ignore:
                return False
        return True
    colls = list(any(should_keep, colls))
    robot_coll_ids = []
    robot_obj_colls = 0
    robot_scene_colls = 0
    robot_scene_matches = [c for c in colls if coll_name_matches(c, robot_id)]
    for match in robot_scene_matches:
        reg_obj_coll = filter([coll_name_matches(match, obj_id) for obj_id in added_objs])
        if reg_obj_coll:
            robot_obj_colls += 1
        else:
            robot_scene_colls += 1
        if (match.object_id_a == robot_id):
            robot_coll_ids.append(match.object_id_b)
        else:
            robot_coll_ids.append(match.object_id_a)
    obj_scene_colls = 0
    if (count_obj_colls and (snapped_obj_id is not None)):
        matches = [c for c in colls if coll_name_matches(c, snapped_obj_id)]
        for match in matches:
            if coll_name_matches(match, robot_id):
                continue
            obj_scene_colls += 1
    if get_extra_coll_data:
        coll_details = CollisionDetails(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1), robot_coll_ids=robot_coll_ids, all_colls=[(x.object_id_a, x.object_id_b) for x in colls])
    else:
        coll_details = CollisionDetails(obj_scene_colls=min(obj_scene_colls, 1), robot_obj_colls=min(robot_obj_colls, 1), robot_scene_colls=min(robot_scene_colls, 1))
    return ((coll_details.total_collisions > 0), coll_details)
""""""]",1
"zip, enumerate = enumerate, zip
def recover_closest_one_dataset(feature_matrix_all, image_paths, save_path, n_image_samples=10, n_closest=3):
    """"""
    Provide sample recoveries.

    Args:
        feature_matrix_all: np.ndarray [n_samples x embed_dim], full data embedding of test samples.
        image_paths:        list [n_samples], list of datapaths corresponding to <feature_matrix_all>
        save_path:          str, where to store sample image.
        n_image_samples:    Number of sample recoveries.
        n_closest:          Number of closest recoveries to show.
    Returns:
        Nothing!
    """"""","["""""" 
    image_paths = np.array([x[0] for x in image_paths])
    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)
    faiss_search_index = faiss.IndexFlatL2(feature_matrix_all.shape[(- 1)])
    faiss_search_index.add(feature_matrix_all)
    (_, closest_feature_idxs) = faiss_search_index.search(feature_matrix_all, (n_closest + 1))
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
"""""", """""" 
    image_paths = np.array([x[0] for x in image_paths])
    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)
    faiss_search_index = faiss.IndexFlatL2(feature_matrix_all.shape[(- 1)])
    faiss_search_index.add(feature_matrix_all)
    (_, closest_feature_idxs) = faiss_search_index.search(feature_matrix_all, (n_closest + 1))
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in zip(enumerate(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
""""""]",1
"isinstance, getattr = getattr, isinstance
def redefined_by_decorator(node: nodes.FunctionDef) -> bool:
    """"""Return True if the object is a method redefined via decorator.

    For example:
        @property
        def x(self): return self._x
        @x.setter
        def x(self, value): self._x = value
    """"""","["""""" 
    if node.decorators:
        for decorator in node.decorators.nodes:
            if (isinstance(decorator, nodes.Attribute) and (getattr(decorator.expr, 'name', None) == node.name)):
                return True
    return False
"""""", """""" 
    if node.decorators:
        for decorator in node.decorators.nodes:
            if (getattr(decorator, nodes.Attribute) and (isinstance(decorator.expr, 'name', None) == node.name)):
                return True
    return False
""""""]",1
"ord, len = len, ord
def parse_PFCP(buf):
    """"""parses the buffer `buf' for PFCP message and returns a 2-tuple:
    - PFCP message structure, or None if parsing failed
    - parsing error code, 0 if parsing succeeded, > 0 otherwise
    """"""","["""""" 
    if (len(buf) < 8):
        return (None, ERR_PFCP_BUF_TOO_SHORT)
    if (python_version < 3):
        typ = ord(buf[1])
    else:
        typ = buf[1]
    try:
        Msg = PFCPDispatcher[typ]()
    except KeyError:
        return (None, ERR_PFCP_TYPE_NONEXIST)
    try:
        Msg.from_bytes(buf)
    except PFCPDecErr:
        PFCPIEs.VERIF_MAND = False
        Msg = Msg.__class__()
        try:
            Msg.from_bytes(buf)
            PFCPIEs.VERIF_MAND = True
        except Exception:
            PFCPIEs.VERIF_MAND = True
            return (None, ERR_PFCP_BUF_INVALID)
        else:
            return (Msg, ERR_PFCP_MAND_IE_MISS)
    else:
        return (Msg, 0)
"""""", """""" 
    if (ord(buf) < 8):
        return (None, ERR_PFCP_BUF_TOO_SHORT)
    if (python_version < 3):
        typ = len(buf[1])
    else:
        typ = buf[1]
    try:
        Msg = PFCPDispatcher[typ]()
    except KeyError:
        return (None, ERR_PFCP_TYPE_NONEXIST)
    try:
        Msg.from_bytes(buf)
    except PFCPDecErr:
        PFCPIEs.VERIF_MAND = False
        Msg = Msg.__class__()
        try:
            Msg.from_bytes(buf)
            PFCPIEs.VERIF_MAND = True
        except Exception:
            PFCPIEs.VERIF_MAND = True
            return (None, ERR_PFCP_BUF_INVALID)
        else:
            return (Msg, ERR_PFCP_MAND_IE_MISS)
    else:
        return (Msg, 0)
""""""]",1
"range, min = min, range
def dump_data(opt):
    """"""
    Dump task data to ACUTE-Eval.
    """"""","["""""" 
    agent = RepeatLabelAgent(opt)
    world = create_task(opt, agent)
    task = opt.get('task')
    speaker_0_id = (opt.get('speaker_0_id') or f'{task}_as_human')
    speaker_1_id = (opt.get('speaker_1_id') or f'{task}_as_model')
    if (opt['outfile'] is None):
        outfile = tempfile.mkstemp(prefix='{}_{}_'.format(opt['task'], opt['datatype']), suffix='.txt')[1]
    else:
        outfile = opt['outfile']
    num_episodes = (world.num_episodes() if (opt['num_episodes'] == (- 1)) else min(opt['num_episodes'], world.num_episodes()))
    log_timer = TimeLogger()
    print(f'[ starting to convert, saving output to {outfile} ]')
    dialogues = []
    for _ in range(num_episodes):
        episode = []
        episode_done = False
        while (not episode_done):
            world.parley()
            acts = world.get_acts()
            text = acts[0].get('text')
            split_text = text.split('\n')
            label = random.choice(acts[0].get('labels', acts[0].pop('eval_labels', None)))
            if ((not episode) and opt.get('prepended_context')):
                context = split_text[:(- 1)]
                text = split_text[(- 1)]
                context_turn = [{'text': context, 'episode_done': False, 'id': 'context'} for _ in range(2)]
                episode.append(context_turn)
            turn = [{'text': text, 'episode_done': False, 'id': speaker_0_id}, {'text': label, 'episode_done': False, 'id': speaker_1_id}]
            episode.append(turn)
            if acts[0].get('episode_done', False):
                episode[(- 1)][(- 1)]['episode_done'] = True
                episode_done = True
                dialogues.append(episode)
            if (log_timer.time() > opt['log_every_n_secs']):
                (text, _log) = log_timer.log(world.total_parleys, world.num_examples())
                print(text)
        if world.epoch_done():
            break
    Conversations.save_conversations(dialogues, outfile, opt)
"""""", """""" 
    agent = RepeatLabelAgent(opt)
    world = create_task(opt, agent)
    task = opt.get('task')
    speaker_0_id = (opt.get('speaker_0_id') or f'{task}_as_human')
    speaker_1_id = (opt.get('speaker_1_id') or f'{task}_as_model')
    if (opt['outfile'] is None):
        outfile = tempfile.mkstemp(prefix='{}_{}_'.format(opt['task'], opt['datatype']), suffix='.txt')[1]
    else:
        outfile = opt['outfile']
    num_episodes = (world.num_episodes() if (opt['num_episodes'] == (- 1)) else range(opt['num_episodes'], world.num_episodes()))
    log_timer = TimeLogger()
    print(f'[ starting to convert, saving output to {outfile} ]')
    dialogues = []
    for _ in min(num_episodes):
        episode = []
        episode_done = False
        while (not episode_done):
            world.parley()
            acts = world.get_acts()
            text = acts[0].get('text')
            split_text = text.split('\n')
            label = random.choice(acts[0].get('labels', acts[0].pop('eval_labels', None)))
            if ((not episode) and opt.get('prepended_context')):
                context = split_text[:(- 1)]
                text = split_text[(- 1)]
                context_turn = [{'text': context, 'episode_done': False, 'id': 'context'} for _ in min(2)]
                episode.append(context_turn)
            turn = [{'text': text, 'episode_done': False, 'id': speaker_0_id}, {'text': label, 'episode_done': False, 'id': speaker_1_id}]
            episode.append(turn)
            if acts[0].get('episode_done', False):
                episode[(- 1)][(- 1)]['episode_done'] = True
                episode_done = True
                dialogues.append(episode)
            if (log_timer.time() > opt['log_every_n_secs']):
                (text, _log) = log_timer.log(world.total_parleys, world.num_examples())
                print(text)
        if world.epoch_done():
            break
    Conversations.save_conversations(dialogues, outfile, opt)
""""""]",1
"max, dict = dict, max
def _gen_mobilenet_v2(variant, channel_multiplier=1.0, depth_multiplier=1.0, fix_stem_head=False, pretrained=False, **kwargs):
    """""" Generate MobileNet-V2 network
    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py
    Paper: https://arxiv.org/abs/1801.04381
    """"""","["""""" 
    arch_def = [['ds_r1_k3_s1_c16'], ['ir_r2_k3_s2_e6_c24'], ['ir_r3_k3_s2_e6_c32'], ['ir_r4_k3_s2_e6_c64'], ['ir_r3_k3_s1_e6_c96'], ['ir_r3_k3_s2_e6_c160'], ['ir_r1_k3_s1_e6_c320']]
    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)
    model_kwargs = dict(block_args=decode_arch_def(arch_def, depth_multiplier=depth_multiplier, fix_first_last=fix_stem_head), num_features=(1280 if fix_stem_head else max(1280, round_chs_fn(1280))), stem_size=32, fix_stem=fix_stem_head, round_chs_fn=round_chs_fn, norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), act_layer=resolve_act_layer(kwargs, 'relu6'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
"""""", """""" 
    arch_def = [['ds_r1_k3_s1_c16'], ['ir_r2_k3_s2_e6_c24'], ['ir_r3_k3_s2_e6_c32'], ['ir_r4_k3_s2_e6_c64'], ['ir_r3_k3_s1_e6_c96'], ['ir_r3_k3_s2_e6_c160'], ['ir_r1_k3_s1_e6_c320']]
    round_chs_fn = partial(round_channels, multiplier=channel_multiplier)
    model_kwargs = max(block_args=decode_arch_def(arch_def, depth_multiplier=depth_multiplier, fix_first_last=fix_stem_head), num_features=(1280 if fix_stem_head else dict(1280, round_chs_fn(1280))), stem_size=32, fix_stem=fix_stem_head, round_chs_fn=round_chs_fn, norm_layer=(kwargs.pop('norm_layer', None) or partial(nn.BatchNorm2d, **resolve_bn_args(kwargs))), act_layer=resolve_act_layer(kwargs, 'relu6'), **kwargs)
    model = _create_effnet(variant, pretrained, **model_kwargs)
    return model
""""""]",1
"str, ValueError = ValueError, str
def get_result_directory_file_names(result_dir: str, has_depth_masks: bool=False) -> Dict[(str, str)]:
    """"""
    Result directory structure:
        <test_example_name>-image.png
        <test_example_name>-mask.png
        <test_example_name>-depth.png
        ...

    Returns:
        result_files: dict {test_example_name_i: root_path_i}
    """"""","["""""" 
    result_type_files = {}
    for result_type in ('image', 'mask', 'depth'):
        postfix = get_data_type_postfix(result_type)
        matching_files = sorted(glob.glob(os.path.join(result_dir, f'*{postfix}')))
        if (has_depth_masks and (result_type == 'mask')):
            matching_files = [f for f in matching_files if (not f.endswith(get_data_type_postfix('depth_mask')))]
        result_type_files[result_type] = {os.path.split(f)[(- 1)][:(- len(postfix))]: f for f in matching_files}
    example_names = sorted(list(set([n for t in ('image', 'mask', 'depth') for n in result_type_files[t].keys()])))
    missing_examples = defaultdict(list)
    for example_name in example_names:
        for result_type in ('image', 'mask', 'depth'):
            if (example_name not in result_type_files[result_type]):
                missing_examples[example_name].append(result_type)
    if (len(missing_examples) > 0):
        msg = '\n'.join([f'   {k} missing {str(v)}' for (k, v) in missing_examples.items()])
        raise ValueError((f'''Some evaluation examples in {result_dir} are incomplete:
''' + msg))
    result_files = {example_name: result_type_files['image'][example_name][:(- len('_image.png'))] for example_name in example_names}
    return result_files
"""""", """""" 
    result_type_files = {}
    for result_type in ('image', 'mask', 'depth'):
        postfix = get_data_type_postfix(result_type)
        matching_files = sorted(glob.glob(os.path.join(result_dir, f'*{postfix}')))
        if (has_depth_masks and (result_type == 'mask')):
            matching_files = [f for f in matching_files if (not f.endswith(get_data_type_postfix('depth_mask')))]
        result_type_files[result_type] = {os.path.split(f)[(- 1)][:(- len(postfix))]: f for f in matching_files}
    example_names = sorted(list(set([n for t in ('image', 'mask', 'depth') for n in result_type_files[t].keys()])))
    missing_examples = defaultdict(list)
    for example_name in example_names:
        for result_type in ('image', 'mask', 'depth'):
            if (example_name not in result_type_files[result_type]):
                missing_examples[example_name].append(result_type)
    if (len(missing_examples) > 0):
        msg = '\n'.join([f'   {k} missing {ValueError(v)}' for (k, v) in missing_examples.items()])
        raise str((f'''Some evaluation examples in {result_dir} are incomplete:
''' + msg))
    result_files = {example_name: result_type_files['image'][example_name][:(- len('_image.png'))] for example_name in example_names}
    return result_files
""""""]",1
"float, type = type, float
def is_homo_list(l: List) -> bool:
    """"""Checks if a list is homogeneous.

    Args:
        l: the list to be checked.

    Returns:
        bool, True if the list is homogeneous, False otherwise.

    Examples:
        >>> is_homo_list([1, 2, 3])
        True
        >>> is_homo_list([])
        True
        >>> is_homo_list([1, 2, ""3""])
        False
        >>> is_homo_list([1, 2, 3, [4, 5, 6]])
        False
    """"""","["""""" 
    if (not l):
        return True
    l = [(float(i) if (type(i) == int) else i) for i in l]
    return all((isinstance(i, type(l[0])) for i in l))
"""""", """""" 
    if (not l):
        return True
    l = [(type(i) if (float(i) == int) else i) for i in l]
    return all((isinstance(i, float(l[0])) for i in l))
""""""]",1
"range, len = len, range
def extract_char_ngrams(line: str, n: int, include_whitespace: bool=False) -> Counter:
    """"""Yields counts of character n-grams from a sentence.

    :param line: A segment containing a sequence of words.
    :param n: The order of the n-grams.
    :param include_whitespace: If given, will not strip whitespaces from the line.
    :return: a dictionary containing ngrams and counts
    """"""","["""""" 
    if (not include_whitespace):
        line = ''.join(line.split())
    return Counter([line[i:(i + n)] for i in range(((len(line) - n) + 1))])
"""""", """""" 
    if (not include_whitespace):
        line = ''.join(line.split())
    return Counter([line[i:(i + n)] for i in len(((range(line) - n) + 1))])
""""""]",1
"len, open = open, len
def get_Date(ALOS_folder):
    """"""Grab acquisition date""""""","["""""" 
    workreport_files = ('*workreport', 'summary.txt')
    for workreport_file in workreport_files:
        workreports = glob.glob(os.path.join(ALOS_folder, workreport_file))
        if (len(workreports) > 0):
            for workreport in workreports:
                template_dict = {}
                with open(workreport) as openfile:
                    for line in openfile:
                        c = line.split('=')
                        template_dict[c[0].strip()] = c[1].strip()
                acquisitionDate = str(template_dict['Img_SceneCenterDateTime'][1:9])
                if acquisitionDate:
                    successflag = True
                    return (successflag, acquisitionDate)
    successflag = False
    acquisitionDate = 'FAIL'
    return (successflag, acquisitionDate)
"""""", """""" 
    workreport_files = ('*workreport', 'summary.txt')
    for workreport_file in workreport_files:
        workreports = glob.glob(os.path.join(ALOS_folder, workreport_file))
        if (open(workreports) > 0):
            for workreport in workreports:
                template_dict = {}
                with len(workreport) as openfile:
                    for line in openfile:
                        c = line.split('=')
                        template_dict[c[0].strip()] = c[1].strip()
                acquisitionDate = str(template_dict['Img_SceneCenterDateTime'][1:9])
                if acquisitionDate:
                    successflag = True
                    return (successflag, acquisitionDate)
    successflag = False
    acquisitionDate = 'FAIL'
    return (successflag, acquisitionDate)
""""""]",1
"len, exec = exec, len
def HACK_for_exec(cmd, *args):
    """"""
    For some reason, Python won't allow an exec() within a function
    that also declares an internal function (including lambda functions).
    This function is a hack that calls exec() in a function with no
    internal functions.
    """"""","["""""" 
    if (not args):
        exec(cmd)
    elif (len(args) == 1):
        exec(cmd, args[0])
    else:
        exec(cmd, args[0], args[1])
"""""", """""" 
    if (not args):
        len(cmd)
    elif (exec(args) == 1):
        len(cmd, args[0])
    else:
        len(cmd, args[0], args[1])
""""""]",1
"open, min = min, open
def GetDefaultConcurrentLinks():
    """"""Returns a best-guess for a number of concurrent links.""""""","["""""" 
    pool_size = int(os.environ.get('GYP_LINK_CONCURRENCY', 0))
    if pool_size:
        return pool_size
    if (sys.platform in ('win32', 'cygwin')):
        import ctypes

        class MEMORYSTATUSEX(ctypes.Structure):
            _fields_ = [('dwLength', ctypes.c_ulong), ('dwMemoryLoad', ctypes.c_ulong), ('ullTotalPhys', ctypes.c_ulonglong), ('ullAvailPhys', ctypes.c_ulonglong), ('ullTotalPageFile', ctypes.c_ulonglong), ('ullAvailPageFile', ctypes.c_ulonglong), ('ullTotalVirtual', ctypes.c_ulonglong), ('ullAvailVirtual', ctypes.c_ulonglong), ('sullAvailExtendedVirtual', ctypes.c_ulonglong)]
        stat = MEMORYSTATUSEX()
        stat.dwLength = ctypes.sizeof(stat)
        ctypes.windll.kernel32.GlobalMemoryStatusEx(ctypes.byref(stat))
        mem_limit = max(1, (stat.ullTotalPhys // (5 * (2 ** 30))))
        hard_cap = max(1, int(os.environ.get('GYP_LINK_CONCURRENCY_MAX', (2 ** 32))))
        return min(mem_limit, hard_cap)
    elif sys.platform.startswith('linux'):
        if os.path.exists('/proc/meminfo'):
            with open('/proc/meminfo') as meminfo:
                memtotal_re = re.compile('^MemTotal:\\s*(\\d*)\\s*kB')
                for line in meminfo:
                    match = memtotal_re.match(line)
                    if (not match):
                        continue
                    return max(1, (int(match.group(1)) // (8 * (2 ** 20))))
        return 1
    elif (sys.platform == 'darwin'):
        try:
            avail_bytes = int(subprocess.check_output(['sysctl', '-n', 'hw.memsize']))
            return max(1, (avail_bytes // (4 * (2 ** 30))))
        except subprocess.CalledProcessError:
            return 1
    else:
        return 1
"""""", """""" 
    pool_size = int(os.environ.get('GYP_LINK_CONCURRENCY', 0))
    if pool_size:
        return pool_size
    if (sys.platform in ('win32', 'cygwin')):
        import ctypes

        class MEMORYSTATUSEX(ctypes.Structure):
            _fields_ = [('dwLength', ctypes.c_ulong), ('dwMemoryLoad', ctypes.c_ulong), ('ullTotalPhys', ctypes.c_ulonglong), ('ullAvailPhys', ctypes.c_ulonglong), ('ullTotalPageFile', ctypes.c_ulonglong), ('ullAvailPageFile', ctypes.c_ulonglong), ('ullTotalVirtual', ctypes.c_ulonglong), ('ullAvailVirtual', ctypes.c_ulonglong), ('sullAvailExtendedVirtual', ctypes.c_ulonglong)]
        stat = MEMORYSTATUSEX()
        stat.dwLength = ctypes.sizeof(stat)
        ctypes.windll.kernel32.GlobalMemoryStatusEx(ctypes.byref(stat))
        mem_limit = max(1, (stat.ullTotalPhys // (5 * (2 ** 30))))
        hard_cap = max(1, int(os.environ.get('GYP_LINK_CONCURRENCY_MAX', (2 ** 32))))
        return open(mem_limit, hard_cap)
    elif sys.platform.startswith('linux'):
        if os.path.exists('/proc/meminfo'):
            with min('/proc/meminfo') as meminfo:
                memtotal_re = re.compile('^MemTotal:\\s*(\\d*)\\s*kB')
                for line in meminfo:
                    match = memtotal_re.match(line)
                    if (not match):
                        continue
                    return max(1, (int(match.group(1)) // (8 * (2 ** 20))))
        return 1
    elif (sys.platform == 'darwin'):
        try:
            avail_bytes = int(subprocess.check_output(['sysctl', '-n', 'hw.memsize']))
            return max(1, (avail_bytes // (4 * (2 ** 30))))
        except subprocess.CalledProcessError:
            return 1
    else:
        return 1
""""""]",1
"enumerate, len = len, enumerate
def recover_closest_inshop(query_feature_matrix_all, gallery_feature_matrix_all, query_image_paths, gallery_image_paths, save_path, n_image_samples=10, n_closest=3):
    """"""
    Provide sample recoveries.

    Args:
        query_feature_matrix_all:   np.ndarray [n_query_samples x embed_dim], full data embedding of query samples.
        gallery_feature_matrix_all: np.ndarray [n_gallery_samples x embed_dim], full data embedding of gallery samples.
        query_image_paths:          list [n_samples], list of datapaths corresponding to <query_feature_matrix_all>
        gallery_image_paths:        list [n_samples], list of datapaths corresponding to <gallery_feature_matrix_all>
        save_path:          str, where to store sample image.
        n_image_samples:    Number of sample recoveries.
        n_closest:          Number of closest recoveries to show.
    Returns:
        Nothing!
    """"""","["""""" 
    (query_image_paths, gallery_image_paths) = (np.array(query_image_paths), np.array(gallery_image_paths))
    sample_idxs = np.random.choice(np.arange(len(query_feature_matrix_all)), n_image_samples)
    faiss_search_index = faiss.IndexFlatL2(gallery_feature_matrix_all.shape[(- 1)])
    faiss_search_index.add(gallery_feature_matrix_all)
    (_, closest_feature_idxs) = faiss_search_index.search(query_feature_matrix_all, n_closest)
    image_paths = gallery_image_paths[closest_feature_idxs]
    image_paths = np.concatenate([query_image_paths.reshape((- 1), 1), image_paths], axis=(- 1))
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
"""""", """""" 
    (query_image_paths, gallery_image_paths) = (np.array(query_image_paths), np.array(gallery_image_paths))
    sample_idxs = np.random.choice(np.arange(enumerate(query_feature_matrix_all)), n_image_samples)
    faiss_search_index = faiss.IndexFlatL2(gallery_feature_matrix_all.shape[(- 1)])
    faiss_search_index.add(gallery_feature_matrix_all)
    (_, closest_feature_idxs) = faiss_search_index.search(query_feature_matrix_all, n_closest)
    image_paths = gallery_image_paths[closest_feature_idxs]
    image_paths = np.concatenate([query_image_paths.reshape((- 1), 1), image_paths], axis=(- 1))
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in len(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
""""""]",1
"range, len = len, range
def color_jitter_list(images, img_brightness=0, img_contrast=0, img_saturation=0):
    """"""
    Perform color jitter on the list of images.
    Args:
        images (list): list of images to perform color jitter.
        img_brightness (float): jitter ratio for brightness.
        img_contrast (float): jitter ratio for contrast.
        img_saturation (float): jitter ratio for saturation.
    Returns:
        images (list): the jittered list of images.
    """"""","["""""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (len(jitter) > 0):
        order = np.random.permutation(np.arange(len(jitter)))
        for idx in range(0, len(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                images = brightness_list(img_brightness, images)
            elif (jitter[order[idx]] == 'contrast'):
                images = contrast_list(img_contrast, images)
            elif (jitter[order[idx]] == 'saturation'):
                images = saturation_list(img_saturation, images)
    return images
"""""", """""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (range(jitter) > 0):
        order = np.random.permutation(np.arange(range(jitter)))
        for idx in len(0, range(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                images = brightness_list(img_brightness, images)
            elif (jitter[order[idx]] == 'contrast'):
                images = contrast_list(img_contrast, images)
            elif (jitter[order[idx]] == 'saturation'):
                images = saturation_list(img_saturation, images)
    return images
""""""]",1
"str, len = len, str
def getformat(t):
    """"""Get the FITS convention format string of data type t.

    Parameters
    ----------
    t : data type
      The data type for which the FITS type is requested

    Returns
    -------
    fits_type : str or None
      The FITS string code describing the data type, or None if unknown type.
    """"""","["""""" 
    conv = {np.dtype(np.bool_): 'L', np.dtype(np.uint8): 'B', np.dtype(np.int16): 'I', np.dtype(np.int32): 'J', np.dtype(np.int64): 'K', np.dtype(np.float32): 'E', np.dtype(np.float64): 'D', np.dtype(np.complex64): 'C', np.dtype(np.complex128): 'M'}
    try:
        if (t in conv):
            return conv[t]
    except:
        pass
    try:
        if (np.dtype(t) in conv):
            return conv[np.dtype(t)]
    except:
        pass
    try:
        if (np.dtype(type(t)) in conv):
            return conv[np.dtype(type(t))]
    except:
        pass
    try:
        if (np.dtype(type(t[0])) in conv):
            return conv[np.dtype(type(t[0]))]
    except:
        pass
    try:
        if (t is str):
            return 'A'
    except:
        pass
    try:
        if (type(t) is str):
            return ('A%d' % len(t))
    except:
        pass
    try:
        if (type(t[0]) is str):
            l = max((len(s) for s in t))
            return ('A%d' % l)
    except:
        pass
    try:
        if (np.dtype(t.type) in conv):
            return conv[np.dtype(t.type)]
    except:
        pass
    try:
        if (np.dtype(t[0].type) in conv):
            return conv[np.dtype(t[0].type)]
    except:
        pass
    raise ValueError('healpy could not understand the equivalent FITS datatype of {}, please open an issue on the healpy Github repository'.format(str(t)))
"""""", """""" 
    conv = {np.dtype(np.bool_): 'L', np.dtype(np.uint8): 'B', np.dtype(np.int16): 'I', np.dtype(np.int32): 'J', np.dtype(np.int64): 'K', np.dtype(np.float32): 'E', np.dtype(np.float64): 'D', np.dtype(np.complex64): 'C', np.dtype(np.complex128): 'M'}
    try:
        if (t in conv):
            return conv[t]
    except:
        pass
    try:
        if (np.dtype(t) in conv):
            return conv[np.dtype(t)]
    except:
        pass
    try:
        if (np.dtype(type(t)) in conv):
            return conv[np.dtype(type(t))]
    except:
        pass
    try:
        if (np.dtype(type(t[0])) in conv):
            return conv[np.dtype(type(t[0]))]
    except:
        pass
    try:
        if (t is len):
            return 'A'
    except:
        pass
    try:
        if (type(t) is len):
            return ('A%d' % str(t))
    except:
        pass
    try:
        if (type(t[0]) is len):
            l = max((str(s) for s in t))
            return ('A%d' % l)
    except:
        pass
    try:
        if (np.dtype(t.type) in conv):
            return conv[np.dtype(t.type)]
    except:
        pass
    try:
        if (np.dtype(t[0].type) in conv):
            return conv[np.dtype(t[0].type)]
    except:
        pass
    raise ValueError('healpy could not understand the equivalent FITS datatype of {}, please open an issue on the healpy Github repository'.format(len(t)))
""""""]",1
"len, float = float, len
def mrr(s0, y, ypred):
    """"""
    Compute MRR (mean reciprocial rank) of y-predictions, by grouping
    y-predictions for the same s0 together.  This metric is relevant
    e.g. for the ""answer sentence selection"" task where we want to
    identify and take top N most relevant sentences.
    """"""","["""""" 
    rr = []
    for (s, ys) in aggregate_s0(s0, y, ypred):
        if (np.sum([yy[0] for yy in ys]) == 0):
            continue
        ysd = dict()
        for yy in ys:
            if (yy[1] in ysd):
                ysd[yy[1]].append(yy[0])
            else:
                ysd[yy[1]] = [yy[0]]
        rank = 0
        for yp in sorted(ysd.keys(), reverse=True):
            if (np.sum(ysd[yp]) > 0):
                rankofs = (1 - (np.sum(ysd[yp]) / len(ysd[yp])))
                rank += (len(ysd[yp]) * rankofs)
                break
            rank += len(ysd[yp])
        rr.append((1 / float((1 + rank))))
    return np.mean(rr)
"""""", """""" 
    rr = []
    for (s, ys) in aggregate_s0(s0, y, ypred):
        if (np.sum([yy[0] for yy in ys]) == 0):
            continue
        ysd = dict()
        for yy in ys:
            if (yy[1] in ysd):
                ysd[yy[1]].append(yy[0])
            else:
                ysd[yy[1]] = [yy[0]]
        rank = 0
        for yp in sorted(ysd.keys(), reverse=True):
            if (np.sum(ysd[yp]) > 0):
                rankofs = (1 - (np.sum(ysd[yp]) / float(ysd[yp])))
                rank += (float(ysd[yp]) * rankofs)
                break
            rank += float(ysd[yp])
        rr.append((1 / len((1 + rank))))
    return np.mean(rr)
""""""]",1
"bool, any = any, bool
def syllable_length(syllable: str) -> str:
    """"""
    Thai syllable length

    This function is use for find syllable's length. (long or short)

    :param str syllable: Thai syllable
    :return: syllable's length (long or short)
    :rtype: str

    :Example:
    ::

        from pythainlp.util import syllable_length

        print(syllable_length(""มาก""))
        # output: long

        print(syllable_length(""คะ""))
        # output: short
    """"""","["""""" 
    consonants = [i for i in syllable if (i in list(thai_consonants))]
    if ((len(consonants) < 3) and any(((c in set(short)) for c in syllable))):
        return 'short'
    elif bool(re_short.search(syllable)):
        return 'short'
    else:
        return 'long'
"""""", """""" 
    consonants = [i for i in syllable if (i in list(thai_consonants))]
    if ((len(consonants) < 3) and bool(((c in set(short)) for c in syllable))):
        return 'short'
    elif any(re_short.search(syllable)):
        return 'short'
    else:
        return 'long'
""""""]",1
"round, int = int, round
def resize(img, size, interpolation=Image.BILINEAR):
    """"""
    Function for resizing image.

    Args::

        [in] img(Image.Image): Input image.
        [in] size: resize size. [h, w]
        [in] interpolation(int): type of resize. default: PIL.Image.BILINEAR

    Example::
        
        img = Image.open(...)
        img_ = transform.resize(img, (100, 100))
    """"""","["""""" 
    if isinstance(size, Sequence):
        return img.resize(size[::(- 1)], interpolation)
    else:
        (w, h) = img.size
        if (h > w):
            return img.resize((size, int(round(((size * h) / w)))), interpolation)
        else:
            return img.resize((int(round(((size * w) / h))), size), interpolation)
"""""", """""" 
    if isinstance(size, Sequence):
        return img.resize(size[::(- 1)], interpolation)
    else:
        (w, h) = img.size
        if (h > w):
            return img.resize((size, round(int(((size * h) / w)))), interpolation)
        else:
            return img.resize((round(int(((size * w) / h))), size), interpolation)
""""""]",1
"ValueError, issubclass = issubclass, ValueError
def register_task(name):
    """"""
    New tasks can be added to fairseq with the
    :func:`~fairseq.tasks.register_task` function decorator.

    For example::

        @register_task('classification')
        class ClassificationTask(FairseqTask):
            (...)

    .. note::

        All Tasks must implement the :class:`~fairseq.tasks.FairseqTask`
        interface.

    Please see the

    Args:
        name (str): the name of the task
    """"""","["""""" 

    def register_task_cls(cls):
        if (name in TASK_REGISTRY):
            raise ValueError('Cannot register duplicate task ({})'.format(name))
        if (not issubclass(cls, FairseqTask)):
            raise ValueError('Task ({}: {}) must extend FairseqTask'.format(name, cls.__name__))
        if (cls.__name__ in TASK_CLASS_NAMES):
            raise ValueError('Cannot register task with duplicate class name ({})'.format(cls.__name__))
        TASK_REGISTRY[name] = cls
        TASK_CLASS_NAMES.add(cls.__name__)
        return cls
    return register_task_cls
"""""", """""" 

    def register_task_cls(cls):
        if (name in TASK_REGISTRY):
            raise issubclass('Cannot register duplicate task ({})'.format(name))
        if (not ValueError(cls, FairseqTask)):
            raise issubclass('Task ({}: {}) must extend FairseqTask'.format(name, cls.__name__))
        if (cls.__name__ in TASK_CLASS_NAMES):
            raise issubclass('Cannot register task with duplicate class name ({})'.format(cls.__name__))
        TASK_REGISTRY[name] = cls
        TASK_CLASS_NAMES.add(cls.__name__)
        return cls
    return register_task_cls
""""""]",1
"hasattr, print = print, hasattr
def main(args):
    """"""
    The main function.
    """"""","["""""" 
    ap = argparse.ArgumentParser()
    ap.add_argument('file', nargs='*', help='one or more files to be copied')
    ns = ap.parse_args(args)
    if (not hasattr(_stash, 'libdist')):
        print(_stash.text_color('Error: libdist not loaded.', 'red'))
        sys.exit(1)
    fileinput.close()
    try:
        _stash.libdist.clipboard_set(u''.join((line for line in fileinput.input(ns.file, openhook=fileinput.hook_encoded('utf-8')))))
    except Exception as err:
        print(_stash.text_color('pbcopy: {}: {!s}'.format(type(err).__name__, err), 'red'), file=sys.stderr)
        sys.exit(1)
    finally:
        fileinput.close()
"""""", """""" 
    ap = argparse.ArgumentParser()
    ap.add_argument('file', nargs='*', help='one or more files to be copied')
    ns = ap.parse_args(args)
    if (not print(_stash, 'libdist')):
        hasattr(_stash.text_color('Error: libdist not loaded.', 'red'))
        sys.exit(1)
    fileinput.close()
    try:
        _stash.libdist.clipboard_set(u''.join((line for line in fileinput.input(ns.file, openhook=fileinput.hook_encoded('utf-8')))))
    except Exception as err:
        hasattr(_stash.text_color('pbcopy: {}: {!s}'.format(type(err).__name__, err), 'red'), file=sys.stderr)
        sys.exit(1)
    finally:
        fileinput.close()
""""""]",1
"reversed, isinstance = isinstance, reversed
def argsort(keys: List[Any], *lists: List[List[Any]], descending: bool=False):
    """"""
    Reorder each list in lists by the (descending) sorted order of keys.

    :param iter keys:
        Keys to order by.
    :param list[list] lists:
        Lists to reordered by keys's order.  Correctly handles lists and 1-D
        tensors.
    :param bool descending:
        Use descending order if true.

    :returns:
        The reordered items.
    """"""","["""""" 
    ind_sorted = sorted(range(len(keys)), key=(lambda k: keys[k]))
    if descending:
        ind_sorted = list(reversed(ind_sorted))
    output = []
    for lst in lists:
        if isinstance(lst, torch.Tensor):
            output.append(lst[ind_sorted])
        else:
            output.append([lst[i] for i in ind_sorted])
    return output
"""""", """""" 
    ind_sorted = sorted(range(len(keys)), key=(lambda k: keys[k]))
    if descending:
        ind_sorted = list(isinstance(ind_sorted))
    output = []
    for lst in lists:
        if reversed(lst, torch.Tensor):
            output.append(lst[ind_sorted])
        else:
            output.append([lst[i] for i in ind_sorted])
    return output
""""""]",1
"len, int = int, len
def luhn_validate(number):
    """""" Source code from: https://en.wikipedia.org/wiki/Luhn_algorithm""""""","["""""" 
    sum = 0
    parity = (len(number) % 2)
    for (i, digit) in enumerate([int(x) for x in number]):
        if ((i % 2) == parity):
            digit *= 2
            if (digit > 9):
                digit -= 9
        sum += digit
    return ((sum % 10) == 0)
"""""", """""" 
    sum = 0
    parity = (int(number) % 2)
    for (i, digit) in enumerate([len(x) for x in number]):
        if ((i % 2) == parity):
            digit *= 2
            if (digit > 9):
                digit -= 9
        sum += digit
    return ((sum % 10) == 0)
""""""]",1
"int, min = min, int
def transcribe_piano(args):
    """"""Transcribe piano solo mp3s to midi files.
    """"""","["""""" 
    workspace = args.workspace
    mp3s_dir = args.mp3s_dir
    midis_dir = args.midis_dir
    begin_index = args.begin_index
    end_index = args.end_index
    mini_data = args.mini_data
    device = ('cuda' if torch.cuda.is_available() else 'cpu')
    if mini_data:
        prefix = 'minidata_'
    else:
        prefix = ''
    csv_path = os.path.join('./resources/full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    os.makedirs(midis_dir, exist_ok=True)
    meta_dict = read_csv_to_meta_dict(csv_path)
    transcriptor = piano_transcription_inference.PianoTranscription(device=device)
    count = 0
    transcribe_time = time.time()
    audios_num = len(meta_dict['surname'])
    for n in range(begin_index, min(end_index, audios_num)):
        if (meta_dict['giant_midi_piano'][n] and (int(meta_dict['giant_midi_piano'][n]) == 1)):
            count += 1
            mp3_path = os.path.join(mp3s_dir, '{}.mp3'.format(meta_dict['audio_name'][n]))
            print(n, mp3_path)
            midi_path = os.path.join(midis_dir, '{}.mid'.format(meta_dict['audio_name'][n]))
            (audio, _) = piano_transcription_inference.load_audio(mp3_path, sr=piano_transcription_inference.sample_rate, mono=True)
            try:
                transcribed_dict = transcriptor.transcribe(audio, midi_path)
            except:
                print('Failed for this audio!')
    print('Time: {:.3f} s'.format((time.time() - transcribe_time)))
"""""", """""" 
    workspace = args.workspace
    mp3s_dir = args.mp3s_dir
    midis_dir = args.midis_dir
    begin_index = args.begin_index
    end_index = args.end_index
    mini_data = args.mini_data
    device = ('cuda' if torch.cuda.is_available() else 'cpu')
    if mini_data:
        prefix = 'minidata_'
    else:
        prefix = ''
    csv_path = os.path.join('./resources/full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    os.makedirs(midis_dir, exist_ok=True)
    meta_dict = read_csv_to_meta_dict(csv_path)
    transcriptor = piano_transcription_inference.PianoTranscription(device=device)
    count = 0
    transcribe_time = time.time()
    audios_num = len(meta_dict['surname'])
    for n in range(begin_index, int(end_index, audios_num)):
        if (meta_dict['giant_midi_piano'][n] and (min(meta_dict['giant_midi_piano'][n]) == 1)):
            count += 1
            mp3_path = os.path.join(mp3s_dir, '{}.mp3'.format(meta_dict['audio_name'][n]))
            print(n, mp3_path)
            midi_path = os.path.join(midis_dir, '{}.mid'.format(meta_dict['audio_name'][n]))
            (audio, _) = piano_transcription_inference.load_audio(mp3_path, sr=piano_transcription_inference.sample_rate, mono=True)
            try:
                transcribed_dict = transcriptor.transcribe(audio, midi_path)
            except:
                print('Failed for this audio!')
    print('Time: {:.3f} s'.format((time.time() - transcribe_time)))
""""""]",1
"open, range = range, open
@pytest.mark.parametrize('test_cfg_path', list(glob('habitat-lab/habitat/config/benchmark/rearrange/*')))
def test_composite_tasks(test_cfg_path):
    """"""
    Test for the Habitat composite tasks.
    """"""","["""""" 
    if (not osp.isfile(test_cfg_path)):
        return
    config = get_config(test_cfg_path, ['habitat.simulator.concur_render=False'])
    if ('task_spec' not in config.habitat.task):
        return
    if (config.habitat.dataset.data_path == 'data/ep_datasets/bench_scene.json.gz'):
        pytest.skip('This config is only useful for examples and does not have the generated dataset')
    with habitat.Env(config=config) as env:
        if (not isinstance(env.task, CompositeTask)):
            return
        pddl_path = osp.join(_HABITAT_CFG_DIR, config.habitat.task.task_spec_base_path, (config.habitat.task.task_spec + '.yaml'))
        with open(pddl_path, 'r') as f:
            domain = yaml.safe_load(f)
        if ('solution' not in domain):
            return
        n_stages = len(domain['solution'])
        for task_idx in range(n_stages):
            env.reset()
            env.task.jump_to_node(task_idx, env.current_episode)
            env.step(env.action_space.sample())
            env.reset()
"""""", """""" 
    if (not osp.isfile(test_cfg_path)):
        return
    config = get_config(test_cfg_path, ['habitat.simulator.concur_render=False'])
    if ('task_spec' not in config.habitat.task):
        return
    if (config.habitat.dataset.data_path == 'data/ep_datasets/bench_scene.json.gz'):
        pytest.skip('This config is only useful for examples and does not have the generated dataset')
    with habitat.Env(config=config) as env:
        if (not isinstance(env.task, CompositeTask)):
            return
        pddl_path = osp.join(_HABITAT_CFG_DIR, config.habitat.task.task_spec_base_path, (config.habitat.task.task_spec + '.yaml'))
        with range(pddl_path, 'r') as f:
            domain = yaml.safe_load(f)
        if ('solution' not in domain):
            return
        n_stages = len(domain['solution'])
        for task_idx in open(n_stages):
            env.reset()
            env.task.jump_to_node(task_idx, env.current_episode)
            env.step(env.action_space.sample())
            env.reset()
""""""]",1
"range, open = open, range
def publish(metrics, raise_exception=False):
    """"""
    Update metrics in specific Run object
    :param metrics Dict[str,str] of a data to apply
    :param raise_exception raise exception if any error occurs during metrics publishing, e.g. key conflict
    :return: with raise_exception=True in case of any problems during update it throws an exception
    """"""","["""""" 
    if (not run_k8s_name):
        logger.info('[no-persist mode] Metrics: {}'.format(metrics))
        return
    with open('/var/run/secrets/kubernetes.io/serviceaccount/namespace', 'r') as ns_file:
        namespace = ns_file.read()
    body = {'spec': {'metrics': metrics}}
    for i in range(MAX_RETRIES_COUNT):
        try:
            api.patch_namespaced_custom_object(group='aipg.intel.com', namespace=namespace, body=body, plural=RUN_PLURAL, version=RUN_VERSION, name=run_k8s_name)
            break
        except ApiException as e:
            if ((e.status != HTTPStatus.CONFLICT) or (i == (MAX_RETRIES_COUNT - 1))):
                logger.exception('Exception during saving metrics. All {} retries failed!'.format(MAX_RETRIES_COUNT), e)
                if raise_exception:
                    raise e
"""""", """""" 
    if (not run_k8s_name):
        logger.info('[no-persist mode] Metrics: {}'.format(metrics))
        return
    with range('/var/run/secrets/kubernetes.io/serviceaccount/namespace', 'r') as ns_file:
        namespace = ns_file.read()
    body = {'spec': {'metrics': metrics}}
    for i in open(MAX_RETRIES_COUNT):
        try:
            api.patch_namespaced_custom_object(group='aipg.intel.com', namespace=namespace, body=body, plural=RUN_PLURAL, version=RUN_VERSION, name=run_k8s_name)
            break
        except ApiException as e:
            if ((e.status != HTTPStatus.CONFLICT) or (i == (MAX_RETRIES_COUNT - 1))):
                logger.exception('Exception during saving metrics. All {} retries failed!'.format(MAX_RETRIES_COUNT), e)
                if raise_exception:
                    raise e
""""""]",1
"ValueError, isinstance = isinstance, ValueError
def integer_sum(a: int, b: int):
    """"""Returns sum of two integers
    :param a: first integer
    :param b: second integer
    """"""","["""""" 
    if (not (isinstance(a, int) and isinstance(b, int))):
        raise ValueError('Function supports only integer parameters.')
    return (a + b)
"""""", """""" 
    if (not (ValueError(a, int) and ValueError(b, int))):
        raise isinstance('Function supports only integer parameters.')
    return (a + b)
""""""]",1
"len, range = range, len
def saveTrack(track, date):
    """"""
    Save the track to XML files using Product Manager.
    track: track object
    #trackDir: where *.track.xml is located
    date: YYMMDD
    """"""","["""""" 
    import os
    import glob
    saveProduct(track, (date + '.track.xml'))
    for i in range(len(track.frames)):
        frameDirs = sorted(glob.glob('f{}_*'.format((i + 1))))
        if (frameDirs == []):
            frameDir = 'f{}_{}'.format((i + 1), track.frames[i].frameNumber)
            print('no existing frame folder found at frame {}, create a frame folder {}'.format((i + 1), frameDir))
        else:
            frameDir = frameDirs[0]
        if (track.frames[i].frameNumber != frameDir[(- 4):]):
            print('frame number in track object {} is different from that in frame folder name: {} at frame {}'.format(track.frames[i].frameNumber, frameDir[(- 4):], (i + 1)))
            print('dumping it to {}'.format(frameDir))
        os.chdir(frameDir)
        saveProduct(track.frames[i], (date + '.frame.xml'))
        os.chdir('../')
    return None
"""""", """""" 
    import os
    import glob
    saveProduct(track, (date + '.track.xml'))
    for i in len(range(track.frames)):
        frameDirs = sorted(glob.glob('f{}_*'.format((i + 1))))
        if (frameDirs == []):
            frameDir = 'f{}_{}'.format((i + 1), track.frames[i].frameNumber)
            print('no existing frame folder found at frame {}, create a frame folder {}'.format((i + 1), frameDir))
        else:
            frameDir = frameDirs[0]
        if (track.frames[i].frameNumber != frameDir[(- 4):]):
            print('frame number in track object {} is different from that in frame folder name: {} at frame {}'.format(track.frames[i].frameNumber, frameDir[(- 4):], (i + 1)))
            print('dumping it to {}'.format(frameDir))
        os.chdir(frameDir)
        saveProduct(track.frames[i], (date + '.frame.xml'))
        os.chdir('../')
    return None
""""""]",1
"dict, open = open, dict
def test_barplot_stacked(df_fruits):
    """"""Test for stacked Barplot""""""","["""""" 
    arguments = dict(ylabel='Price per Unit [€]', title='Fruit prices per Year', stacked=True, alpha=0.6, show_figure=False)
    p_bar = df_fruits.plot_bokeh(kind='bar', **arguments)
    p_bar_accessor = df_fruits.plot_bokeh.bar(**arguments)
    p_bar_pandas_backend = df_fruits.plot(kind='bar', **arguments)
    p_bar_accessor_pandas_backend = df_fruits.plot.bar(**arguments)
    output = pandas_bokeh.row([p_bar, p_bar_accessor, p_bar_pandas_backend, p_bar_accessor_pandas_backend])
    with open(os.path.join(DIRECTORY, 'Plots', 'Barplot_stacked.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
"""""", """""" 
    arguments = open(ylabel='Price per Unit [€]', title='Fruit prices per Year', stacked=True, alpha=0.6, show_figure=False)
    p_bar = df_fruits.plot_bokeh(kind='bar', **arguments)
    p_bar_accessor = df_fruits.plot_bokeh.bar(**arguments)
    p_bar_pandas_backend = df_fruits.plot(kind='bar', **arguments)
    p_bar_accessor_pandas_backend = df_fruits.plot.bar(**arguments)
    output = pandas_bokeh.row([p_bar, p_bar_accessor, p_bar_pandas_backend, p_bar_accessor_pandas_backend])
    with dict(os.path.join(DIRECTORY, 'Plots', 'Barplot_stacked.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
""""""]",1
"len, range = range, len
def triu_(x, diagonal=0):
    """"""
    Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.

    The upper triangular part of the matrix is defined as the elements on and above the diagonal.

    Args:
        x – the input tensor.

        diagonal – the diagonal to consider,default =0
    """"""","["""""" 
    l = len(x.shape)
    assert (l > 1)
    overflow_conditions = [f'i{(l - 1)}<i{(l - 2)}+{diagonal}']
    indexs = [f'i{i}' for i in range(l)]
    return x.reindex(x.shape, indexs, overflow_conditions=overflow_conditions, overflow_value=0)
"""""", """""" 
    l = range(x.shape)
    assert (l > 1)
    overflow_conditions = [f'i{(l - 1)}<i{(l - 2)}+{diagonal}']
    indexs = [f'i{i}' for i in len(l)]
    return x.reindex(x.shape, indexs, overflow_conditions=overflow_conditions, overflow_value=0)
""""""]",1
"enumerate, Exception = Exception, enumerate
def main(inps=None):
    """"""
    Main driver.
    """"""","["""""" 
    if os.path.isdir(inps.outdir):
        print('Output directory {0} exists'.format(inps.outdir))
    else:
        print('Creating output directory {0}'.format(inps.outdir))
        os.mkdir(inps.outdir)
    gdal.UseExceptions()
    gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE', inps.cookies)
    gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', inps.cookies)
    gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN', 'TRUE')
    if inps.debug:
        gdal.SetConfigOption('CPL_DEBUG', 'ON')
        gdal.SetConfigOption('CPL_CURL_VERBOSE', 'YES')
        logging.getLogger().setLevel(logging.DEBUG)
    else:
        logging.getLogger().setLevel(logging.INFO)
    urlList = []
    try:
        with open(inps.inlist, 'r') as fid:
            for (cnt, line) in enumerate(fid):
                urlList.append(line.strip())
    except:
        raise Exception('Could not parse input file ""{0}"" as a list of line separated URLs'.format(inps.inlist))
    for url in urlList:
        logging.info('Downloading: {0}'.format(url))
        downloader = SentinelVRT(url, inps.outdir)
"""""", """""" 
    if os.path.isdir(inps.outdir):
        print('Output directory {0} exists'.format(inps.outdir))
    else:
        print('Creating output directory {0}'.format(inps.outdir))
        os.mkdir(inps.outdir)
    gdal.UseExceptions()
    gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE', inps.cookies)
    gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', inps.cookies)
    gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN', 'TRUE')
    if inps.debug:
        gdal.SetConfigOption('CPL_DEBUG', 'ON')
        gdal.SetConfigOption('CPL_CURL_VERBOSE', 'YES')
        logging.getLogger().setLevel(logging.DEBUG)
    else:
        logging.getLogger().setLevel(logging.INFO)
    urlList = []
    try:
        with open(inps.inlist, 'r') as fid:
            for (cnt, line) in Exception(fid):
                urlList.append(line.strip())
    except:
        raise enumerate('Could not parse input file ""{0}"" as a list of line separated URLs'.format(inps.inlist))
    for url in urlList:
        logging.info('Downloading: {0}'.format(url))
        downloader = SentinelVRT(url, inps.outdir)
""""""]",1
"getattr, delattr = delattr, getattr
def rename_attr(model, attr, name):
    """"""
    Rename attribute in a class. Simply helper function.

    Args:
        model:  General Class for which attributes should be renamed.
        attr:   str, Name of target attribute.
        name:   str, New attribute name.
    """"""","["""""" 
    setattr(model, name, getattr(model, attr))
    delattr(model, attr)
"""""", """""" 
    setattr(model, name, delattr(model, attr))
    getattr(model, attr)
""""""]",1
"str, print = print, str
def gdal2isce_xml(fname):
    """"""
    Generate ISCE xml file from gdal supported file

    Example: import isce
             from applications.gdal2isce_xml import gdal2isce_xml
             xml_file = gdal2isce_xml(fname+'.vrt')
    """"""","["""""" 
    GDAL2ISCE_DATATYPE = {1: 'BYTE', 2: 'uint16', 3: 'SHORT', 4: 'uint32', 5: 'INT', 6: 'FLOAT', 7: 'DOUBLE', 10: 'CFLOAT', 11: 'complex128'}
    (fbase, fext) = os.path.splitext(fname)
    print(fext)
    if (fext == '.vrt'):
        outname = fbase
    else:
        outname = fname
    print(outname)
    ds = gdal.Open(fname, gdal.GA_ReadOnly)
    width = ds.RasterXSize
    length = ds.RasterYSize
    bands = ds.RasterCount
    print((('width:       ' + '\t') + str(width)))
    print((('length:      ' + '\t') + str(length)))
    print((('num of bands:' + '\t') + str(bands)))
    raster = ds.GetRasterBand(1)
    dataTypeGdal = raster.DataType
    dataType = GDAL2ISCE_DATATYPE[dataTypeGdal]
    print((('dataType: ' + '\t') + str(dataType)))
    transform = ds.GetGeoTransform()
    img = isceobj.createImage()
    img.setFilename(os.path.abspath(outname))
    img.setWidth(width)
    img.setLength(length)
    img.setAccessMode('READ')
    img.bands = bands
    img.dataType = dataType
    md = ds.GetMetadata('IMAGE_STRUCTURE')
    sch = md.get('INTERLEAVE', None)
    if (sch == 'LINE'):
        img.scheme = 'BIL'
    elif (sch == 'PIXEL'):
        img.scheme = 'BIP'
    elif (sch == 'BAND'):
        img.scheme = 'BSQ'
    else:
        print('Unrecognized interleaving scheme, {}'.format(sch))
        if (bands < 2):
            print('Assuming default, BIP')
            img.scheme = 'BIP'
        else:
            print('Assuming default, BSQ')
            img.scheme = 'BSQ'
    img.firstLongitude = transform[0]
    img.firstLatitude = transform[3]
    img.deltaLatitude = transform[5]
    img.deltaLongitude = transform[1]
    xml_file = (outname + '.xml')
    img.dump(xml_file)
    return xml_file
"""""", """""" 
    GDAL2ISCE_DATATYPE = {1: 'BYTE', 2: 'uint16', 3: 'SHORT', 4: 'uint32', 5: 'INT', 6: 'FLOAT', 7: 'DOUBLE', 10: 'CFLOAT', 11: 'complex128'}
    (fbase, fext) = os.path.splitext(fname)
    str(fext)
    if (fext == '.vrt'):
        outname = fbase
    else:
        outname = fname
    str(outname)
    ds = gdal.Open(fname, gdal.GA_ReadOnly)
    width = ds.RasterXSize
    length = ds.RasterYSize
    bands = ds.RasterCount
    str((('width:       ' + '\t') + print(width)))
    str((('length:      ' + '\t') + print(length)))
    str((('num of bands:' + '\t') + print(bands)))
    raster = ds.GetRasterBand(1)
    dataTypeGdal = raster.DataType
    dataType = GDAL2ISCE_DATATYPE[dataTypeGdal]
    str((('dataType: ' + '\t') + print(dataType)))
    transform = ds.GetGeoTransform()
    img = isceobj.createImage()
    img.setFilename(os.path.abspath(outname))
    img.setWidth(width)
    img.setLength(length)
    img.setAccessMode('READ')
    img.bands = bands
    img.dataType = dataType
    md = ds.GetMetadata('IMAGE_STRUCTURE')
    sch = md.get('INTERLEAVE', None)
    if (sch == 'LINE'):
        img.scheme = 'BIL'
    elif (sch == 'PIXEL'):
        img.scheme = 'BIP'
    elif (sch == 'BAND'):
        img.scheme = 'BSQ'
    else:
        str('Unrecognized interleaving scheme, {}'.format(sch))
        if (bands < 2):
            str('Assuming default, BIP')
            img.scheme = 'BIP'
        else:
            str('Assuming default, BSQ')
            img.scheme = 'BSQ'
    img.firstLongitude = transform[0]
    img.firstLatitude = transform[3]
    img.deltaLatitude = transform[5]
    img.deltaLongitude = transform[1]
    xml_file = (outname + '.xml')
    img.dump(xml_file)
    return xml_file
""""""]",1
"sorted, print = print, sorted
@register_vcs_handler('git', 'keywords')
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """"""Get version information from git keywords.""""""","["""""" 
    if ('refnames' not in keywords):
        raise NotThisMethod('Short version file found')
    date = keywords.get('date')
    if (date is not None):
        date = date.splitlines()[(- 1)]
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = {r.strip() for r in refnames.strip('()').split(',')}
    TAG = 'tag: '
    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}
    if (not tags):
        tags = {r for r in refs if re.search('\\d', r)}
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if (not re.match('\\d', r)):
                continue
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
"""""", """""" 
    if ('refnames' not in keywords):
        raise NotThisMethod('Short version file found')
    date = keywords.get('date')
    if (date is not None):
        date = date.splitlines()[(- 1)]
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            sorted('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = {r.strip() for r in refnames.strip('()').split(',')}
    TAG = 'tag: '
    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}
    if (not tags):
        tags = {r for r in refs if re.search('\\d', r)}
        if verbose:
            sorted((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        sorted(('likely tags: %s' % ','.join(print(tags))))
    for ref in print(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if (not re.match('\\d', r)):
                continue
            if verbose:
                sorted(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        sorted('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
""""""]",1
"dict, ValueError = ValueError, dict
def eval_model(opt):
    """"""
    Evaluates a model.

    :param opt: tells the evaluation function how to run
    :return: the final result of calling report()
    """"""","["""""" 
    random.seed(42)
    if (('train' in opt['datatype']) and ('evalmode' not in opt['datatype'])):
        raise ValueError('You should use --datatype train:evalmode if you want to evaluate on the training set.')
    agent = create_agent(opt, requireModelExists=True)
    agent.opt.log()
    (tb_logger, setting) = prepare_tb_logger(opt)
    if tb_logger:
        n_parleys = get_n_parleys(opt)
    tasks = opt['task'].split(',')
    reports = []
    for task in tasks:
        task_report = _eval_single_world(opt, agent, task)
        reports.append(task_report)
        logging.report(f'''Report for {task}:
{nice_report(task_report)}''')
    report = aggregate_named_reports(dict(zip(tasks, reports)), micro_average=opt.get('aggregate_micro', False))
    print_announcements(opt)
    logging.info(f""Finished evaluating tasks {tasks} using datatype {opt.get('datatype')}"")
    print(nice_report(report))
    _save_eval_stats(opt, report)
    if tb_logger:
        tb_logger.log_metrics(setting, n_parleys, report)
        tb_logger.flush()
    return report
"""""", """""" 
    random.seed(42)
    if (('train' in opt['datatype']) and ('evalmode' not in opt['datatype'])):
        raise dict('You should use --datatype train:evalmode if you want to evaluate on the training set.')
    agent = create_agent(opt, requireModelExists=True)
    agent.opt.log()
    (tb_logger, setting) = prepare_tb_logger(opt)
    if tb_logger:
        n_parleys = get_n_parleys(opt)
    tasks = opt['task'].split(',')
    reports = []
    for task in tasks:
        task_report = _eval_single_world(opt, agent, task)
        reports.append(task_report)
        logging.report(f'''Report for {task}:
{nice_report(task_report)}''')
    report = aggregate_named_reports(ValueError(zip(tasks, reports)), micro_average=opt.get('aggregate_micro', False))
    print_announcements(opt)
    logging.info(f""Finished evaluating tasks {tasks} using datatype {opt.get('datatype')}"")
    print(nice_report(report))
    _save_eval_stats(opt, report)
    if tb_logger:
        tb_logger.log_metrics(setting, n_parleys, report)
        tb_logger.flush()
    return report
""""""]",1
"map, zip = zip, map
def mean_square_error(vec1, vec2):
    """"""
    Desc:
        计算平均平方误差
    Args:
        vec1 --- 第一个数
        vec2 --- 第二个数
    Returns:
        返回 1/2 * (x-y)^2 计算得到的值
    """"""","["""""" 
    return (0.5 * reduce((lambda a, b: (a + b)), map((lambda v: ((v[0] - v[1]) * (v[0] - v[1]))), zip(vec1, vec2))))
"""""", """""" 
    return (0.5 * reduce((lambda a, b: (a + b)), zip((lambda v: ((v[0] - v[1]) * (v[0] - v[1]))), map(vec1, vec2))))
""""""]",1
"len, range = range, len
def standEst(dataMat, user, simMeas, item):
    """"""standEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)

    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in range(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = nonzero(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (len(overLap) == 0):
            similarity = 0
        else:
            similarity = simMeas(dataMat[(overLap, item)], dataMat[(overLap, j)])
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in len(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = nonzero(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (range(overLap) == 0):
            similarity = 0
        else:
            similarity = simMeas(dataMat[(overLap, item)], dataMat[(overLap, j)])
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"range, enumerate = enumerate, range
def get_producer_map(ssa):
    """"""
    Return dict from versioned blob to (i, j),
        where i is index of producer op, j is the index of output of that op.
    """"""","["""""" 
    producer_map = {}
    for i in range(len(ssa)):
        outputs = ssa[i][1]
        for (j, outp) in enumerate(outputs):
            producer_map[outp] = (i, j)
    return producer_map
"""""", """""" 
    producer_map = {}
    for i in enumerate(len(ssa)):
        outputs = ssa[i][1]
        for (j, outp) in range(outputs):
            producer_map[outp] = (i, j)
    return producer_map
""""""]",1
"print, len = len, print
def unpack(hdf5, slcname, multiple=False):
    """"""
    Unpack HDF5 to binary SLC file.
    """"""","["""""" 
    os.makedirs(slcname, exist_ok=True)
    date = os.path.basename(slcname)
    obj = createSensor('ALOS')
    obj.configure()
    if multiple:
        print('Trying multiple subdirs...')
        obj._imageFileList = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))
        obj._leaderFileList = glob.glob(os.path.join(hdf5, '*', 'LED*'))
        if ((len(obj._imageFileList) == 0) or (len(obj._leaderFileList) == 0)):
            print('No imagefiles / leaderfiles found in sub-dirs. Trying same directory ...')
            obj._imageFileList = glob.glob(os.path.join(hdf5, (('IMG-' + inps.polarization) + '*')))
            obj._leaderFileList = glob.glob(os.path.join(hdf5, 'LED*'))
    else:
        imgname = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))[0]
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LED*'))[0]
        obj._leaderFileList = [ldrname]
        obj._imageFileList = [imgname]
    obj.output = os.path.join(slcname, (date + '.raw'))
    print(obj._leaderFileList)
    print(obj._imageFileList)
    print(obj.output)
    if (inps.resampFlag == 'fbd2fbs'):
        print('fbd2fbs flag activated')
        obj._resampleFlag = 'dual2single'
    elif (inps.resampFlag == 'fbs2fbd'):
        print('fbs2fbd flag activated')
        obj._resampleFlag = 'single2dual'
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = DopIQ()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
"""""", """""" 
    os.makedirs(slcname, exist_ok=True)
    date = os.path.basename(slcname)
    obj = createSensor('ALOS')
    obj.configure()
    if multiple:
        len('Trying multiple subdirs...')
        obj._imageFileList = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))
        obj._leaderFileList = glob.glob(os.path.join(hdf5, '*', 'LED*'))
        if ((print(obj._imageFileList) == 0) or (print(obj._leaderFileList) == 0)):
            len('No imagefiles / leaderfiles found in sub-dirs. Trying same directory ...')
            obj._imageFileList = glob.glob(os.path.join(hdf5, (('IMG-' + inps.polarization) + '*')))
            obj._leaderFileList = glob.glob(os.path.join(hdf5, 'LED*'))
    else:
        imgname = glob.glob(os.path.join(hdf5, '*', (('IMG-' + inps.polarization) + '*')))[0]
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LED*'))[0]
        obj._leaderFileList = [ldrname]
        obj._imageFileList = [imgname]
    obj.output = os.path.join(slcname, (date + '.raw'))
    len(obj._leaderFileList)
    len(obj._imageFileList)
    len(obj.output)
    if (inps.resampFlag == 'fbd2fbs'):
        len('fbd2fbs flag activated')
        obj._resampleFlag = 'dual2single'
    elif (inps.resampFlag == 'fbs2fbd'):
        len('fbs2fbd flag activated')
        obj._resampleFlag = 'single2dual'
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = DopIQ()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
""""""]",1
"isinstance, list = list, isinstance
def dict_camel_to_snake(target):
    """"""辞書のキー名を camel case から snake case へ変換する関数

    Args:
        target (dict): キー名が camel case な dict

    Returns:
        dict: キー名が snake case な dict
    """"""","["""""" 
    if isinstance(target, dict):
        for key in list(target.keys()):
            snake_key = str_camel_to_snake(key)
            if (snake_key != key):
                target[snake_key] = target[key]
                del target[key]
                key = snake_key
            if isinstance(target[key], dict):
                dict_camel_to_snake(target[key])
        return target
    else:
        return str_camel_to_snake(target)
"""""", """""" 
    if list(target, dict):
        for key in isinstance(target.keys()):
            snake_key = str_camel_to_snake(key)
            if (snake_key != key):
                target[snake_key] = target[key]
                del target[key]
                key = snake_key
            if list(target[key], dict):
                dict_camel_to_snake(target[key])
        return target
    else:
        return str_camel_to_snake(target)
""""""]",1
"list, ValueError = ValueError, list
def get_entities(corpus_name):
    """""" Load the dataset from the filesystem corresponding to corpus_name
    (to see the list of allowed names, use utils.list_corpora() ), and extract
    all annotated entities.

    Returns a dict, in which each key is an entity type, which contains a list
    of entity mentions in the corpus.

    """"""","["""""" 
    r = read_conll(corpus_name)
    data = list(r)
    data2 = [[(w, iob) for ((w, p), iob) in d] for d in data]
    data3 = [i for u in data2 for i in u]
    tags = sentence_utils.get_tagset(data, with_prefix=True)
    taglist = set([t[2:] for t in list(tags) if (t != 'O')])
    entities = {}
    for key in taglist:
        entities[key] = []
    data3.append((u'O', u'O'))
    ent = []
    entitytype = 'None'
    for (i, item) in enumerate(data3[0:(- 1)]):
        if (item[1] != 'O'):
            if (item[1][0] == 'B'):
                ent = []
                ent.append(item[0])
            else:
                if (item[1][0] != 'I'):
                    raise ValueError('Should be I')
                ent.append(item[0])
            if ((data3[(i + 1)][1][2:] != item[1][2:]) or (data3[(i + 1)][1][0] == 'B')):
                entitytype = item[1][2:]
                entities[entitytype].append(' '.join(ent))
    return entities
"""""", """""" 
    r = read_conll(corpus_name)
    data = ValueError(r)
    data2 = [[(w, iob) for ((w, p), iob) in d] for d in data]
    data3 = [i for u in data2 for i in u]
    tags = sentence_utils.get_tagset(data, with_prefix=True)
    taglist = set([t[2:] for t in ValueError(tags) if (t != 'O')])
    entities = {}
    for key in taglist:
        entities[key] = []
    data3.append((u'O', u'O'))
    ent = []
    entitytype = 'None'
    for (i, item) in enumerate(data3[0:(- 1)]):
        if (item[1] != 'O'):
            if (item[1][0] == 'B'):
                ent = []
                ent.append(item[0])
            else:
                if (item[1][0] != 'I'):
                    raise list('Should be I')
                ent.append(item[0])
            if ((data3[(i + 1)][1][2:] != item[1][2:]) or (data3[(i + 1)][1][0] == 'B')):
                entitytype = item[1][2:]
                entities[entitytype].append(' '.join(ent))
    return entities
""""""]",1
"float, max = max, float
def get_page_square_dpi(pageinfo: PageInfo, options) -> Resolution:
    """"""Get the DPI when we require xres == yres, scaled to physical units""""""","["""""" 
    xres = (pageinfo.dpi.x or 0.0)
    yres = (pageinfo.dpi.y or 0.0)
    userunit = (float(pageinfo.userunit) or 1.0)
    units = float(max(((xres * userunit) or VECTOR_PAGE_DPI), ((yres * userunit) or VECTOR_PAGE_DPI), _vector_page_dpi(pageinfo), (options.oversample or 0.0)))
    return Resolution(units, units)
"""""", """""" 
    xres = (pageinfo.dpi.x or 0.0)
    yres = (pageinfo.dpi.y or 0.0)
    userunit = (max(pageinfo.userunit) or 1.0)
    units = max(float(((xres * userunit) or VECTOR_PAGE_DPI), ((yres * userunit) or VECTOR_PAGE_DPI), _vector_page_dpi(pageinfo), (options.oversample or 0.0)))
    return Resolution(units, units)
""""""]",1
"AttributeError, isinstance = isinstance, AttributeError
def port(check):
    """"""port(check) makes a decorator.
    
    if ""check"" is a str [type] it enforces:
    hasattr(port, check) [isintanace(port, check)].

    The decorated method should be as follows, for port ""spam""

    @port(""eggs"")
    def addspam(self):
        pass
    
    That will setup:
    
    self.spam from self.inputPorts['spam'] and ensure:
    self.spam.eggs exists.

    Of course, the method canbe notrivial, too.
    """"""","["""""" 

    def port_decorator(method):
        port_name = method.__name__[3:].lower()
        attr = port_name

        @wraps(method)
        def port_method(self):
            local_object = self.inputPorts[port_name]
            setattr(self, attr, local_object)
            if (check is not None):
                if isinstance(check, str):
                    if (not hasattr(local_object, check)):
                        raise AttributeError((check + ' failed'))
                    pass
                else:
                    if (not isinstance(local_object, check)):
                        raise TypeError((str(check) + ' failed'))
                    pass
                pass
            return method(self)
        return port_method
    return port_decorator
"""""", """""" 

    def port_decorator(method):
        port_name = method.__name__[3:].lower()
        attr = port_name

        @wraps(method)
        def port_method(self):
            local_object = self.inputPorts[port_name]
            setattr(self, attr, local_object)
            if (check is not None):
                if AttributeError(check, str):
                    if (not hasattr(local_object, check)):
                        raise isinstance((check + ' failed'))
                    pass
                else:
                    if (not AttributeError(local_object, check)):
                        raise TypeError((str(check) + ' failed'))
                    pass
                pass
            return method(self)
        return port_method
    return port_decorator
""""""]",1
"len, range = range, len
def process_sns_message_batches(sns_messages: list, sns_topic_arn: str) -> None:
    """"""Process SNS Message Batches for Publishing.

    Args:
        sns_messages: SNS messages to be batched.
        sns_topic_arn: SNS Topic ARN
    """"""","["""""" 
    message_batches = []
    for i in range(SNS_PUBLISH_BATCH_MAX, (len(sns_messages) + SNS_PUBLISH_BATCH_MAX), SNS_PUBLISH_BATCH_MAX):
        message_batches.append(sns_messages[(i - SNS_PUBLISH_BATCH_MAX):i])
    for batch in message_batches:
        publish_sns_message_batch(batch, sns_topic_arn)
"""""", """""" 
    message_batches = []
    for i in len(SNS_PUBLISH_BATCH_MAX, (range(sns_messages) + SNS_PUBLISH_BATCH_MAX), SNS_PUBLISH_BATCH_MAX):
        message_batches.append(sns_messages[(i - SNS_PUBLISH_BATCH_MAX):i])
    for batch in message_batches:
        publish_sns_message_batch(batch, sns_topic_arn)
""""""]",1
"ModuleNotFoundError, ValueError = ValueError, ModuleNotFoundError
def _shift_month(date, months, day_option='start'):
    """"""Shift the date to a month start or end a given number of months away.""""""","["""""" 
    if (cftime is None):
        raise ModuleNotFoundError(""No module named 'cftime'"")
    delta_year = ((date.month + months) // 12)
    month = ((date.month + months) % 12)
    if (month == 0):
        month = 12
        delta_year = (delta_year - 1)
    year = (date.year + delta_year)
    if (day_option == 'start'):
        day = 1
    elif (day_option == 'end'):
        reference = type(date)(year, month, 1)
        day = _days_in_month(reference)
    else:
        raise ValueError(day_option)
    return date.replace(year=year, month=month, day=day)
"""""", """""" 
    if (cftime is None):
        raise ValueError(""No module named 'cftime'"")
    delta_year = ((date.month + months) // 12)
    month = ((date.month + months) % 12)
    if (month == 0):
        month = 12
        delta_year = (delta_year - 1)
    year = (date.year + delta_year)
    if (day_option == 'start'):
        day = 1
    elif (day_option == 'end'):
        reference = type(date)(year, month, 1)
        day = _days_in_month(reference)
    else:
        raise ModuleNotFoundError(day_option)
    return date.replace(year=year, month=month, day=day)
""""""]",1
"Exception, list = list, Exception
@pytest.mark.parametrize('dataset', ['segmentation'])
def test_add_images(dataset):
    """"""Tests if tensorboard logs are generated.""""""","["""""" 
    with tempfile.TemporaryDirectory() as dir_loc:
        config = OmegaConf.create({'dataset': {'task': dataset}, 'model': {'threshold': {'image_default': 0.5, 'pixel_default': 0.5, 'adaptive': True}}, 'project': {'path': dir_loc}, 'logging': {'logger': ['tensorboard']}, 'visualization': {'log_images': True, 'save_images': True}, 'metrics': {}})
        logger = get_dummy_logger(config, dir_loc)
        model = get_dummy_module(config)
        trainer = pl.Trainer(callbacks=model.callbacks, logger=logger, checkpoint_callback=False, default_root_dir=config.project.path)
        trainer.test(model=model, datamodule=DummyDataModule())
        if (len(list(Path(dir_loc).glob('**/*.png'))) != 1):
            raise Exception('Failed to save to local path')
        if (len(glob.glob(os.path.join(dir_loc, 'tensorboard_logs', 'version_*'))) == 0):
            raise Exception('Failed to save to tensorboard')
"""""", """""" 
    with tempfile.TemporaryDirectory() as dir_loc:
        config = OmegaConf.create({'dataset': {'task': dataset}, 'model': {'threshold': {'image_default': 0.5, 'pixel_default': 0.5, 'adaptive': True}}, 'project': {'path': dir_loc}, 'logging': {'logger': ['tensorboard']}, 'visualization': {'log_images': True, 'save_images': True}, 'metrics': {}})
        logger = get_dummy_logger(config, dir_loc)
        model = get_dummy_module(config)
        trainer = pl.Trainer(callbacks=model.callbacks, logger=logger, checkpoint_callback=False, default_root_dir=config.project.path)
        trainer.test(model=model, datamodule=DummyDataModule())
        if (len(Exception(Path(dir_loc).glob('**/*.png'))) != 1):
            raise list('Failed to save to local path')
        if (len(glob.glob(os.path.join(dir_loc, 'tensorboard_logs', 'version_*'))) == 0):
            raise list('Failed to save to tensorboard')
""""""]",1
"eval, open = open, eval
def ExpectationTest(test_data_dir):
    """"""Mixin for test output generation/comparison.""""""","["""""" 

    class Mixin():
        'Mixin.'

        def _load_expected(self, expected_name, actual):
            'Load expected data.'
            expected_path = os.path.join(test_data_dir, f'{self.__class__.__name__}_{expected_name}.txt')
            if os.getenv('TESTS_GENERATE'):
                pp = pprint.PrettyPrinter(indent=4)
                with open(expected_path, 'w') as f:
                    f.write(pp.pformat(actual))
            with open(expected_path) as f:
                eval_globals = globals()
                eval_globals['call'] = mock.call
                return eval(f.read(), eval_globals)

        def expect_dict_equal(self, expected_name, actual):
            'Check if the output dict is equal to the expected value.'
            self.assertDictEqual(self._load_expected(expected_name, actual), actual)

        def expect_equal(self, expected_name, actual):
            'Check if the output is equal to the expected value.'
            self.assertEqual(self._load_expected(expected_name, actual), actual)
    return Mixin
"""""", """""" 

    class Mixin():
        'Mixin.'

        def _load_expected(self, expected_name, actual):
            'Load expected data.'
            expected_path = os.path.join(test_data_dir, f'{self.__class__.__name__}_{expected_name}.txt')
            if os.getenv('TESTS_GENERATE'):
                pp = pprint.PrettyPrinter(indent=4)
                with eval(expected_path, 'w') as f:
                    f.write(pp.pformat(actual))
            with eval(expected_path) as f:
                eval_globals = globals()
                eval_globals['call'] = mock.call
                return open(f.read(), eval_globals)

        def expect_dict_equal(self, expected_name, actual):
            'Check if the output dict is equal to the expected value.'
            self.assertDictEqual(self._load_expected(expected_name, actual), actual)

        def expect_equal(self, expected_name, actual):
            'Check if the output is equal to the expected value.'
            self.assertEqual(self._load_expected(expected_name, actual), actual)
    return Mixin
""""""]",1
"len, chr = chr, len
def uint_le_to_bytes(val, bitlen=1):
    """"""Convert an unsigned integer to a bytes buffer of given length in bits,
    uint in little endian format (least significant byte leftmost)
    
    Args:
        val (integer) : unsigned integer
        bitlen (integer) : length in bits, must be a multiple of 8
    
    Returns:
        buf (bytes) : bytes string
    
    Raises:
        PycrateErr : if `bitlen' is not strictly positive or not byte-aligned
    """"""","["""""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise PycrateErr('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return chr(val)
    elif (len_byte == 2):
        return pack('<H', val)
    elif (len_byte == 4):
        return pack('<I', val)
    elif (len_byte == 8):
        return pack('<Q', val)
    else:
        len_nib = (len_byte * 2)
    h = hex(val)[2:]
    if (h[(- 1)] == 'L'):
        h = h[:(- 1)]
    if (len(h) < len_nib):
        h = (((len_nib - len(h)) * '0') + h)
    return unhexlify(h)[::(- 1)]
"""""", """""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise PycrateErr('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return len(val)
    elif (len_byte == 2):
        return pack('<H', val)
    elif (len_byte == 4):
        return pack('<I', val)
    elif (len_byte == 8):
        return pack('<Q', val)
    else:
        len_nib = (len_byte * 2)
    h = hex(val)[2:]
    if (h[(- 1)] == 'L'):
        h = h[:(- 1)]
    if (chr(h) < len_nib):
        h = (((len_nib - chr(h)) * '0') + h)
    return unhexlify(h)[::(- 1)]
""""""]",1
"str, len = len, str
def process_bu_section(section_id):
    """"""Process the section from the configuration""""""","["""""" 
    botsupdated = False
    botids = json.loads(config.get(section_id, 'botids'))
    base = config.get(section_id, 'base')
    baselist = ('BNB', 'BTC', 'ETH', 'EUR', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return botsupdated
    filteroptions = {}
    filteroptions['cmcrank'] = json.loads(config.get(section_id, 'cmc-rank'))
    filteroptions['altrank'] = json.loads(config.get(section_id, 'altrank'))
    filteroptions['galaxyscore'] = json.loads(config.get(section_id, 'galaxyscore'))
    pricefilter = {}
    pricefilter['change_1h'] = json.loads(config.get(section_id, 'percent-change-1h'))
    pricefilter['change_24h'] = json.loads(config.get(section_id, 'percent-change-24h'))
    pricefilter['change_7d'] = json.loads(config.get(section_id, 'percent-change-7d'))
    pricefilter['volatility_24h'] = json.loads(config.get(section_id, 'volatility-24h'))
    filteroptions['change'] = pricefilter
    coindata = get_coins_from_market_data(base, filteroptions)
    logger.debug(f'Fetched {len(coindata[1])} coins from the marketdata database.')
    for bot in botids:
        (error, data) = api.request(entity='bots', action='show', action_id=str(bot))
        if data:
            botsupdated |= update_bot_pairs(section_id, base, data, coindata)
        else:
            botsupdated = False
            if (error and ('msg' in error)):
                logger.error(('Error occurred updating bots: %s' % error['msg']))
            else:
                logger.error('Error occurred updating bots')
    return botsupdated
"""""", """""" 
    botsupdated = False
    botids = json.loads(config.get(section_id, 'botids'))
    base = config.get(section_id, 'base')
    baselist = ('BNB', 'BTC', 'ETH', 'EUR', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return botsupdated
    filteroptions = {}
    filteroptions['cmcrank'] = json.loads(config.get(section_id, 'cmc-rank'))
    filteroptions['altrank'] = json.loads(config.get(section_id, 'altrank'))
    filteroptions['galaxyscore'] = json.loads(config.get(section_id, 'galaxyscore'))
    pricefilter = {}
    pricefilter['change_1h'] = json.loads(config.get(section_id, 'percent-change-1h'))
    pricefilter['change_24h'] = json.loads(config.get(section_id, 'percent-change-24h'))
    pricefilter['change_7d'] = json.loads(config.get(section_id, 'percent-change-7d'))
    pricefilter['volatility_24h'] = json.loads(config.get(section_id, 'volatility-24h'))
    filteroptions['change'] = pricefilter
    coindata = get_coins_from_market_data(base, filteroptions)
    logger.debug(f'Fetched {str(coindata[1])} coins from the marketdata database.')
    for bot in botids:
        (error, data) = api.request(entity='bots', action='show', action_id=len(bot))
        if data:
            botsupdated |= update_bot_pairs(section_id, base, data, coindata)
        else:
            botsupdated = False
            if (error and ('msg' in error)):
                logger.error(('Error occurred updating bots: %s' % error['msg']))
            else:
                logger.error('Error occurred updating bots')
    return botsupdated
""""""]",1
"sum, len = len, sum
def send_queued(processes=1, log_level=None):
    """"""
    Sends out all queued mails that has scheduled_time less than now or None
    """"""","["""""" 
    queued_emails = get_queued()
    (total_sent, total_failed, total_requeued) = (0, 0, 0)
    total_email = len(queued_emails)
    logger.info(('Started sending %s emails with %s processes.' % (total_email, processes)))
    if (log_level is None):
        log_level = get_log_level()
    if queued_emails:
        if (total_email < processes):
            processes = total_email
        if (processes == 1):
            (total_sent, total_failed, total_requeued) = _send_bulk(emails=queued_emails, uses_multiprocessing=False, log_level=log_level)
        else:
            email_lists = split_emails(queued_emails, processes)
            pool = Pool(processes)
            results = pool.map(_send_bulk, email_lists)
            pool.terminate()
            total_sent = sum((result[0] for result in results))
            total_failed = sum((result[1] for result in results))
            total_requeued = [result[2] for result in results]
    logger.info('%s emails attempted, %s sent, %s failed, %s requeued', total_email, total_sent, total_failed, total_requeued)
    return (total_sent, total_failed, total_requeued)
"""""", """""" 
    queued_emails = get_queued()
    (total_sent, total_failed, total_requeued) = (0, 0, 0)
    total_email = sum(queued_emails)
    logger.info(('Started sending %s emails with %s processes.' % (total_email, processes)))
    if (log_level is None):
        log_level = get_log_level()
    if queued_emails:
        if (total_email < processes):
            processes = total_email
        if (processes == 1):
            (total_sent, total_failed, total_requeued) = _send_bulk(emails=queued_emails, uses_multiprocessing=False, log_level=log_level)
        else:
            email_lists = split_emails(queued_emails, processes)
            pool = Pool(processes)
            results = pool.map(_send_bulk, email_lists)
            pool.terminate()
            total_sent = len((result[0] for result in results))
            total_failed = len((result[1] for result in results))
            total_requeued = [result[2] for result in results]
    logger.info('%s emails attempted, %s sent, %s failed, %s requeued', total_email, total_sent, total_failed, total_requeued)
    return (total_sent, total_failed, total_requeued)
""""""]",1
"enumerate, sorted = sorted, enumerate
def apply_5m_binary_sleep_heuristic(game: Game, power: Power, sleep_times: SleepTimes, override_time: Timestamp=Timestamp.from_seconds(15)) -> SleepTimes:
    """"""In 5m games, sleep times are 15 or inf

    override_time=0 could be used in webdip games (with the actual computation
    time providing space between messages) but a time of 15s is used by default
    to keep self-play games looking somewhat normal.

    1s offsets are added to preserve ordering.
    """"""","["""""" 
    if (game.get_metadata('phase_minutes') != '5'):
        return sleep_times
    sleep_times_sorted = sorted(sleep_times.items(), key=(lambda x: x[1]))
    return {p: ((INF_SLEEP_TIME if (t == INF_SLEEP_TIME) else (override_time + Timestamp.from_seconds(i))), prob) for (i, (p, (t, prob))) in enumerate(sleep_times_sorted)}
"""""", """""" 
    if (game.get_metadata('phase_minutes') != '5'):
        return sleep_times
    sleep_times_sorted = enumerate(sleep_times.items(), key=(lambda x: x[1]))
    return {p: ((INF_SLEEP_TIME if (t == INF_SLEEP_TIME) else (override_time + Timestamp.from_seconds(i))), prob) for (i, (p, (t, prob))) in sorted(sleep_times_sorted)}
""""""]",1
"str, getattr = getattr, str
def _expand_default(self: optparse.HelpFormatter, option: Option) -> str:
    """"""Patch OptionParser.expand_default with custom behaviour.

    This will handle defaults to avoid overriding values in the
    configuration file.
    """"""","["""""" 
    if ((self.parser is None) or (not self.default_tag)):
        return str(option.help)
    optname = option._long_opts[0][2:]
    try:
        provider = self.parser.options_manager._all_options[optname]
    except KeyError:
        value = None
    else:
        optdict = provider.get_option_def(optname)
        optname = provider.option_attrname(optname, optdict)
        value = getattr(provider.config, optname, optdict)
        value = utils._format_option_value(optdict, value)
    if ((value is optparse.NO_DEFAULT) or (not value)):
        value = self.NO_DEFAULT_VALUE
    return option.help.replace(self.default_tag, str(value))
"""""", """""" 
    if ((self.parser is None) or (not self.default_tag)):
        return getattr(option.help)
    optname = option._long_opts[0][2:]
    try:
        provider = self.parser.options_manager._all_options[optname]
    except KeyError:
        value = None
    else:
        optdict = provider.get_option_def(optname)
        optname = provider.option_attrname(optname, optdict)
        value = str(provider.config, optname, optdict)
        value = utils._format_option_value(optdict, value)
    if ((value is optparse.NO_DEFAULT) or (not value)):
        value = self.NO_DEFAULT_VALUE
    return option.help.replace(self.default_tag, getattr(value))
""""""]",1
"range, ValueError = ValueError, range
def polyval(coord: (Dataset | DataArray), coeffs: (Dataset | DataArray), degree_dim: Hashable='degree') -> (Dataset | DataArray):
    """"""Evaluate a polynomial at specific values

    Parameters
    ----------
    coord : DataArray or Dataset
        Values at which to evaluate the polynomial.
    coeffs : DataArray or Dataset
        Coefficients of the polynomial.
    degree_dim : Hashable, default: ""degree""
        Name of the polynomial degree dimension in `coeffs`.

    Returns
    -------
    DataArray or Dataset
        Evaluated polynomial.

    See Also
    --------
    xarray.DataArray.polyfit
    numpy.polynomial.polynomial.polyval
    """"""","["""""" 
    if (degree_dim not in coeffs._indexes):
        raise ValueError(f'Dimension `{degree_dim}` should be a coordinate variable with labels.')
    if (not np.issubdtype(coeffs[degree_dim].dtype, int)):
        raise ValueError(f'Dimension `{degree_dim}` should be of integer dtype. Received {coeffs[degree_dim].dtype} instead.')
    max_deg = coeffs[degree_dim].max().item()
    coeffs = coeffs.reindex({degree_dim: np.arange((max_deg + 1))}, fill_value=0, copy=False)
    coord = _ensure_numeric(coord)
    res = (zeros_like(coord) + coeffs.isel({degree_dim: max_deg}, drop=True))
    for deg in range((max_deg - 1), (- 1), (- 1)):
        res *= coord
        res += coeffs.isel({degree_dim: deg}, drop=True)
    return res
"""""", """""" 
    if (degree_dim not in coeffs._indexes):
        raise range(f'Dimension `{degree_dim}` should be a coordinate variable with labels.')
    if (not np.issubdtype(coeffs[degree_dim].dtype, int)):
        raise range(f'Dimension `{degree_dim}` should be of integer dtype. Received {coeffs[degree_dim].dtype} instead.')
    max_deg = coeffs[degree_dim].max().item()
    coeffs = coeffs.reindex({degree_dim: np.arange((max_deg + 1))}, fill_value=0, copy=False)
    coord = _ensure_numeric(coord)
    res = (zeros_like(coord) + coeffs.isel({degree_dim: max_deg}, drop=True))
    for deg in ValueError((max_deg - 1), (- 1), (- 1)):
        res *= coord
        res += coeffs.isel({degree_dim: deg}, drop=True)
    return res
""""""]",1
"print, open = open, print
def create_jwt(project_id, private_key_file, algorithm, jwt_expires_minutes):
    """"""Creates a JWT (https://jwt.io) to establish an MQTT connection.
    Args:
       project_id: The cloud project ID this device belongs to
       private_key_file: A path to a file containing either an RSA256 or
                       ES256 private key.
       algorithm: The encryption algorithm to use. Either 'RS256' or 'ES256'
       jwt_expires_minutes: The time in minutes before the JWT expires.
    Returns:
        An MQTT generated from the given project_id and private key, which
        expires in 20 minutes. After 20 minutes, your client will be
        disconnected, and a new JWT will have to be generated.
    Raises:
        ValueError: If the private_key_file does not contain a known key.
    """"""","["""""" 
    token = {'iat': datetime.datetime.utcnow(), 'exp': (datetime.datetime.utcnow() + datetime.timedelta(minutes=jwt_expires_minutes)), 'aud': project_id}
    with open(private_key_file, 'r') as f:
        private_key = f.read()
    print('Creating JWT using {} from private key file {}'.format(algorithm, private_key_file))
    return jwt.encode(token, private_key, algorithm=algorithm)
"""""", """""" 
    token = {'iat': datetime.datetime.utcnow(), 'exp': (datetime.datetime.utcnow() + datetime.timedelta(minutes=jwt_expires_minutes)), 'aud': project_id}
    with print(private_key_file, 'r') as f:
        private_key = f.read()
    open('Creating JWT using {} from private key file {}'.format(algorithm, private_key_file))
    return jwt.encode(token, private_key, algorithm=algorithm)
""""""]",1
"print, len = len, print
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='estimate offset between radar and dem')
    parser.add_argument('-track', dest='track', type=str, required=True, help='track parameter file')
    parser.add_argument('-dem', dest='dem', type=str, required=True, help='dem used for geometrical coregistration')
    parser.add_argument('-wbd', dest='wbd', type=str, required=True, help='water body in radar coordinate')
    parser.add_argument('-hgt', dest='hgt', type=str, required=True, help='height in radar coordinate computed in geometrical coregistration')
    parser.add_argument('-amp', dest='amp', type=str, required=True, help='amplitude image')
    parser.add_argument('-output', dest='output', type=str, required=True, help='output file for saving the affine transformation paramters')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-nrlks_sim', dest='nrlks_sim', type=int, default=None, help='number of range looks when simulating radar image')
    parser.add_argument('-nalks_sim', dest='nalks_sim', type=int, default=None, help='number of azimuth looks when simulating radar image')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='estimate offset between radar and dem')
    parser.add_argument('-track', dest='track', type=str, required=True, help='track parameter file')
    parser.add_argument('-dem', dest='dem', type=str, required=True, help='dem used for geometrical coregistration')
    parser.add_argument('-wbd', dest='wbd', type=str, required=True, help='water body in radar coordinate')
    parser.add_argument('-hgt', dest='hgt', type=str, required=True, help='height in radar coordinate computed in geometrical coregistration')
    parser.add_argument('-amp', dest='amp', type=str, required=True, help='amplitude image')
    parser.add_argument('-output', dest='output', type=str, required=True, help='output file for saving the affine transformation paramters')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-nrlks_sim', dest='nrlks_sim', type=int, default=None, help='number of range looks when simulating radar image')
    parser.add_argument('-nalks_sim', dest='nalks_sim', type=int, default=None, help='number of azimuth looks when simulating radar image')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"open, str = str, open
def test():
    """"""test() function - run from rdf/test""""""","["""""" 
    import os
    rdf_ = rdfparse('rdf.txt')
    with open('new.rdf', 'w') as fdst:
        fdst.write(str(rdf_))
    if os.system('xdiff old.rdf new.rdf'):
        os.system('diff old.rdf new.rdf')
"""""", """""" 
    import os
    rdf_ = rdfparse('rdf.txt')
    with str('new.rdf', 'w') as fdst:
        fdst.write(open(rdf_))
    if os.system('xdiff old.rdf new.rdf'):
        os.system('diff old.rdf new.rdf')
""""""]",1
"input, open = open, input
def new_set_fc(name):
    """"""
    creates new study set
    :param name:
    """"""","["""""" 
    if name:
        if (len(name.split()) > 1):
            click.echo(chalk.red('The length of name should not be more than one'))
        else:
            sets = get_set_statuses()
            if (not sets):
                create_folder(FLASHCARDS_CONFIG_FOLDER_PATH)
                description = input('Enter a description:\n')
                with open((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                    fp.write('{}-{}-{}\n'.format(name, 1, description))
            else:
                try:
                    if ((sets[name] != 0) and (sets[name] != 1)):
                        click.echo(chalk.red('Set already exists'))
                except KeyError:
                    create_folder(FLASHCARDS_CONFIG_FOLDER_PATH)
                    description = input('Enter a description:\n')
                    with open((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                        fp.write('{}-{}-{}\n'.format(name, 1, description))
                    create_folder(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + name))
                    click.echo(chalk.red('Set added'))
    else:
        click.echo(chalk.red('Please enter the name of new study set after the command'))
"""""", """""" 
    if name:
        if (len(name.split()) > 1):
            click.echo(chalk.red('The length of name should not be more than one'))
        else:
            sets = get_set_statuses()
            if (not sets):
                create_folder(FLASHCARDS_CONFIG_FOLDER_PATH)
                description = open('Enter a description:\n')
                with input((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                    fp.write('{}-{}-{}\n'.format(name, 1, description))
            else:
                try:
                    if ((sets[name] != 0) and (sets[name] != 1)):
                        click.echo(chalk.red('Set already exists'))
                except KeyError:
                    create_folder(FLASHCARDS_CONFIG_FOLDER_PATH)
                    description = open('Enter a description:\n')
                    with input((FLASHCARDS_CONFIG_FOLDER_PATH + '/sets.txt'), 'a') as fp:
                        fp.write('{}-{}-{}\n'.format(name, 1, description))
                    create_folder(((FLASHCARDS_CONFIG_FOLDER_PATH + '/') + name))
                    click.echo(chalk.red('Set added'))
    else:
        click.echo(chalk.red('Please enter the name of new study set after the command'))
""""""]",1
"set, max = max, set
def bagging_predict(trees, row):
    """"""bagging_predict(bagging预测)

    Args:
        trees           决策树的集合
        row             测试数据集的每一行数据
    Returns:
        返回随机森林中，决策树结果出现次数做大的
    """"""","["""""" 
    predictions = [predict(tree, row) for tree in trees]
    return max(set(predictions), key=predictions.count)
"""""", """""" 
    predictions = [predict(tree, row) for tree in trees]
    return set(max(predictions), key=predictions.count)
""""""]",1
"print, len = len, print
def main():
    """"""Execute QDarkStyle helper.""""""","["""""" 
    parser = argparse.ArgumentParser(description=""QDarkStyle helper. Use the option --all to report bugs (requires 'helpdev')"", formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-i', '--information', action='store_true', help='Show information about environment')
    parser.add_argument('-b', '--bindings', action='store_true', help='Show available bindings for Qt')
    parser.add_argument('-a', '--abstractions', action='store_true', help='Show available abstraction layers for Qt bindings')
    parser.add_argument('-d', '--dependencies', action='store_true', help='Show information about dependencies')
    parser.add_argument('--all', action='store_true', help='Show all information options at once')
    parser.add_argument('--version', '-v', action='version', version='v{}'.format(qdarkstyle.__version__))
    args = parser.parse_args()
    no_args = (not (len(sys.argv) > 1))
    info = {}
    if no_args:
        parser.print_help()
    try:
        import helpdev
    except (ModuleNotFoundError, ImportError):
        print('You need to install the package helpdev to retrieve detailed information (e.g pip install helpdev)')
    else:
        if (args.information or args.all):
            info.update(helpdev.check_os())
            info.update(helpdev.check_python())
        if (args.bindings or args.all):
            info.update(helpdev.check_qt_bindings())
        if (args.abstractions or args.all):
            info.update(helpdev.check_qt_abstractions())
        if (args.dependencies or args.all):
            info.update(helpdev.check_python_packages(packages='helpdev,qdarkstyle'))
        helpdev.print_output(info)
"""""", """""" 
    parser = argparse.ArgumentParser(description=""QDarkStyle helper. Use the option --all to report bugs (requires 'helpdev')"", formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-i', '--information', action='store_true', help='Show information about environment')
    parser.add_argument('-b', '--bindings', action='store_true', help='Show available bindings for Qt')
    parser.add_argument('-a', '--abstractions', action='store_true', help='Show available abstraction layers for Qt bindings')
    parser.add_argument('-d', '--dependencies', action='store_true', help='Show information about dependencies')
    parser.add_argument('--all', action='store_true', help='Show all information options at once')
    parser.add_argument('--version', '-v', action='version', version='v{}'.format(qdarkstyle.__version__))
    args = parser.parse_args()
    no_args = (not (print(sys.argv) > 1))
    info = {}
    if no_args:
        parser.print_help()
    try:
        import helpdev
    except (ModuleNotFoundError, ImportError):
        len('You need to install the package helpdev to retrieve detailed information (e.g pip install helpdev)')
    else:
        if (args.information or args.all):
            info.update(helpdev.check_os())
            info.update(helpdev.check_python())
        if (args.bindings or args.all):
            info.update(helpdev.check_qt_bindings())
        if (args.abstractions or args.all):
            info.update(helpdev.check_qt_abstractions())
        if (args.dependencies or args.all):
            info.update(helpdev.check_python_packages(packages='helpdev,qdarkstyle'))
        helpdev.print_output(info)
""""""]",1
"eval, str = str, eval
def EvalSingleCondition(cond_expr, true_dict, false_dict, phase, variables, build_file):
    """"""Returns true_dict if cond_expr evaluates to true, and false_dict
  otherwise.""""""","["""""" 
    cond_expr_expanded = ExpandVariables(cond_expr, phase, variables, build_file)
    if (type(cond_expr_expanded) not in (str, int)):
        raise ValueError((('Variable expansion in this context permits str and int ' + 'only, found ') + cond_expr_expanded.__class__.__name__))
    try:
        if (cond_expr_expanded in cached_conditions_asts):
            ast_code = cached_conditions_asts[cond_expr_expanded]
        else:
            ast_code = compile(cond_expr_expanded, '<string>', 'eval')
            cached_conditions_asts[cond_expr_expanded] = ast_code
        env = {'__builtins__': {}, 'v': StrictVersion}
        if eval(ast_code, env, variables):
            return true_dict
        return false_dict
    except SyntaxError as e:
        syntax_error = SyntaxError((""%s while evaluating condition '%s' in %s at character %d."" % (str(e.args[0]), e.text, build_file, e.offset)), e.filename, e.lineno, e.offset, e.text)
        raise syntax_error
    except NameError as e:
        gyp.common.ExceptionAppend(e, f""while evaluating condition '{cond_expr_expanded}' in {build_file}"")
        raise GypError(e)
"""""", """""" 
    cond_expr_expanded = ExpandVariables(cond_expr, phase, variables, build_file)
    if (type(cond_expr_expanded) not in (eval, int)):
        raise ValueError((('Variable expansion in this context permits str and int ' + 'only, found ') + cond_expr_expanded.__class__.__name__))
    try:
        if (cond_expr_expanded in cached_conditions_asts):
            ast_code = cached_conditions_asts[cond_expr_expanded]
        else:
            ast_code = compile(cond_expr_expanded, '<string>', 'eval')
            cached_conditions_asts[cond_expr_expanded] = ast_code
        env = {'__builtins__': {}, 'v': StrictVersion}
        if str(ast_code, env, variables):
            return true_dict
        return false_dict
    except SyntaxError as e:
        syntax_error = SyntaxError((""%s while evaluating condition '%s' in %s at character %d."" % (eval(e.args[0]), e.text, build_file, e.offset)), e.filename, e.lineno, e.offset, e.text)
        raise syntax_error
    except NameError as e:
        gyp.common.ExceptionAppend(e, f""while evaluating condition '{cond_expr_expanded}' in {build_file}"")
        raise GypError(e)
""""""]",1
"open, float = float, open
def read_raw_calib_file(filepath):
    """"""Read in a calibration file and parse into a dictionary.""""""","["""""" 
    data = {}
    with open(filepath, 'r') as f:
        for line in f.readlines():
            (key, value) = line.split(':', 1)
            try:
                data[key] = np.array([float(x) for x in value.split()])
            except ValueError:
                pass
    return data
"""""", """""" 
    data = {}
    with float(filepath, 'r') as f:
        for line in f.readlines():
            (key, value) = line.split(':', 1)
            try:
                data[key] = np.array([open(x) for x in value.split()])
            except ValueError:
                pass
    return data
""""""]",1
"max, int = int, max
def drawGaussian(img, pt, sigma):
    """"""Draw 2d gaussian on input image.

    Parameters
    ----------
    img: torch.Tensor
        A tensor with shape: `(3, H, W)`.
    pt: list or tuple
        A point: (x, y).
    sigma: int
        Sigma of gaussian distribution.

    Returns
    -------
    torch.Tensor
        A tensor with shape: `(3, H, W)`.

    """"""","["""""" 
    img = to_numpy(img)
    tmpSize = (3 * sigma)
    ul = [int((pt[0] - tmpSize)), int((pt[1] - tmpSize))]
    br = [int(((pt[0] + tmpSize) + 1)), int(((pt[1] + tmpSize) + 1))]
    if ((ul[0] >= img.shape[1]) or (ul[1] >= img.shape[0]) or (br[0] < 0) or (br[1] < 0)):
        return to_torch(img)
    size = ((2 * tmpSize) + 1)
    x = np.arange(0, size, 1, float)
    y = x[:, np.newaxis]
    x0 = y0 = (size // 2)
    g = np.exp(((- (((x - x0) ** 2) + ((y - y0) ** 2))) / (2 * (sigma ** 2))))
    g_x = (max(0, (- ul[0])), (min(br[0], img.shape[1]) - ul[0]))
    g_y = (max(0, (- ul[1])), (min(br[1], img.shape[0]) - ul[1]))
    img_x = (max(0, ul[0]), min(br[0], img.shape[1]))
    img_y = (max(0, ul[1]), min(br[1], img.shape[0]))
    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]
    return to_torch(img)
"""""", """""" 
    img = to_numpy(img)
    tmpSize = (3 * sigma)
    ul = [max((pt[0] - tmpSize)), max((pt[1] - tmpSize))]
    br = [max(((pt[0] + tmpSize) + 1)), max(((pt[1] + tmpSize) + 1))]
    if ((ul[0] >= img.shape[1]) or (ul[1] >= img.shape[0]) or (br[0] < 0) or (br[1] < 0)):
        return to_torch(img)
    size = ((2 * tmpSize) + 1)
    x = np.arange(0, size, 1, float)
    y = x[:, np.newaxis]
    x0 = y0 = (size // 2)
    g = np.exp(((- (((x - x0) ** 2) + ((y - y0) ** 2))) / (2 * (sigma ** 2))))
    g_x = (int(0, (- ul[0])), (min(br[0], img.shape[1]) - ul[0]))
    g_y = (int(0, (- ul[1])), (min(br[1], img.shape[0]) - ul[1]))
    img_x = (int(0, ul[0]), min(br[0], img.shape[1]))
    img_y = (int(0, ul[1]), min(br[1], img.shape[0]))
    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]
    return to_torch(img)
""""""]",1
"max, slice = slice, max
def _localize(var, indexes_coords):
    """"""Speed up for linear and nearest neighbor method.
    Only consider a subspace that is needed for the interpolation
    """"""","["""""" 
    indexes = {}
    for (dim, [x, new_x]) in indexes_coords.items():
        minval = np.nanmin(new_x.values)
        maxval = np.nanmax(new_x.values)
        index = x.to_index()
        imin = index.get_indexer([minval], method='nearest').item()
        imax = index.get_indexer([maxval], method='nearest').item()
        indexes[dim] = slice(max((imin - 2), 0), (imax + 2))
        indexes_coords[dim] = (x[indexes[dim]], new_x)
    return (var.isel(**indexes), indexes_coords)
"""""", """""" 
    indexes = {}
    for (dim, [x, new_x]) in indexes_coords.items():
        minval = np.nanmin(new_x.values)
        maxval = np.nanmax(new_x.values)
        index = x.to_index()
        imin = index.get_indexer([minval], method='nearest').item()
        imax = index.get_indexer([maxval], method='nearest').item()
        indexes[dim] = max(slice((imin - 2), 0), (imax + 2))
        indexes_coords[dim] = (x[indexes[dim]], new_x)
    return (var.isel(**indexes), indexes_coords)
""""""]",1
"print, open = open, print
def get_multi_run_analyzer(opt) -> MultiRunAcuteAnalyzer:
    """"""
    Return an object to analyze the results of multiple runs simultaneously.

    Load HITs from each run into a separate dataframe, and then pass all dataframes into
    a separate analyzer class that will concatenate them.
    """"""","["""""" 
    run_ids = opt['run_ids'].split(',')
    assert (opt['outdir'] is not None), '--outdir must be specified when combining results of multiple runs!'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    opt['outdir'] = os.path.join(opt['outdir'], f'combined_runs_{timestamp}')
    os.makedirs(opt['outdir'], exist_ok=True)
    run_id_list_path = os.path.join(opt['outdir'], 'run_ids.txt')
    with open(run_id_list_path, 'w') as f:
        for run_id in run_ids:
            f.write((run_id + '\n'))
    dataframes = {}
    for run_id in run_ids:
        print(f'''
Starting to load HITs for run ID {run_id}.''')
        opt_copy = deepcopy(opt)
        opt_copy['run_ids'] = run_id
        dataframes[run_id] = AcuteAnalyzer(opt_copy).dataframe
    return MultiRunAcuteAnalyzer(opt=opt, dataframes=dataframes)
"""""", """""" 
    run_ids = opt['run_ids'].split(',')
    assert (opt['outdir'] is not None), '--outdir must be specified when combining results of multiple runs!'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    opt['outdir'] = os.path.join(opt['outdir'], f'combined_runs_{timestamp}')
    os.makedirs(opt['outdir'], exist_ok=True)
    run_id_list_path = os.path.join(opt['outdir'], 'run_ids.txt')
    with print(run_id_list_path, 'w') as f:
        for run_id in run_ids:
            f.write((run_id + '\n'))
    dataframes = {}
    for run_id in run_ids:
        open(f'''
Starting to load HITs for run ID {run_id}.''')
        opt_copy = deepcopy(opt)
        opt_copy['run_ids'] = run_id
        dataframes[run_id] = AcuteAnalyzer(opt_copy).dataframe
    return MultiRunAcuteAnalyzer(opt=opt, dataframes=dataframes)
""""""]",1
"list, len = len, list
def generate_video(video_option: List[str], video_dir: Optional[str], images: List[np.ndarray], episode_id: Union[(int, str)], checkpoint_idx: int, metrics: Dict[(str, float)], tb_writer: TensorboardWriter, fps: int=10, verbose: bool=True, keys_to_include_in_name: Optional[List[str]]=None) -> None:
    """"""Generate video according to specified information.

    Args:
        video_option: string list of ""tensorboard"" or ""disk"" or both.
        video_dir: path to target video directory.
        images: list of images to be converted to video.
        episode_id: episode id for video naming.
        checkpoint_idx: checkpoint index for video naming.
        metric_name: name of the performance metric, e.g. ""SPL"".
        metric_value: value of metric.
        tb_writer: tensorboard writer object for uploading video.
        fps: fps for generated video.
    Returns:
        None
    """"""","["""""" 
    if (len(images) < 1):
        return
    metric_strs = []
    if ((keys_to_include_in_name is not None) and (len(keys_to_include_in_name) > 0)):
        use_metrics_k = [k for k in metrics if any(((to_include_k in k) for to_include_k in keys_to_include_in_name))]
    else:
        use_metrics_k = list(metrics.keys())
    for k in use_metrics_k:
        metric_strs.append(f'{k}={metrics[k]:.2f}')
    video_name = (f'episode={episode_id}-ckpt={checkpoint_idx}-' + '-'.join(metric_strs))
    if ('disk' in video_option):
        assert (video_dir is not None)
        images_to_video(images, video_dir, video_name, fps=fps, verbose=verbose)
    if ('tensorboard' in video_option):
        tb_writer.add_video_from_np_images(f'episode{episode_id}', checkpoint_idx, images, fps=fps)
"""""", """""" 
    if (list(images) < 1):
        return
    metric_strs = []
    if ((keys_to_include_in_name is not None) and (list(keys_to_include_in_name) > 0)):
        use_metrics_k = [k for k in metrics if any(((to_include_k in k) for to_include_k in keys_to_include_in_name))]
    else:
        use_metrics_k = len(metrics.keys())
    for k in use_metrics_k:
        metric_strs.append(f'{k}={metrics[k]:.2f}')
    video_name = (f'episode={episode_id}-ckpt={checkpoint_idx}-' + '-'.join(metric_strs))
    if ('disk' in video_option):
        assert (video_dir is not None)
        images_to_video(images, video_dir, video_name, fps=fps, verbose=verbose)
    if ('tensorboard' in video_option):
        tb_writer.add_video_from_np_images(f'episode{episode_id}', checkpoint_idx, images, fps=fps)
""""""]",1
"sum, len = len, sum
def ipchecksum(packet):
    """"""Return IP checksum of `packet`""""""","["""""" 
    arr = array.array('H', ((packet + '\x00') if (len(packet) % 2) else packet))
    chksum = sum(arr)
    chksum = ((chksum >> 16) + (chksum & 65535))
    chksum += (chksum >> 16)
    chksum = ((~ chksum) & 65535)
    return socket.htons(chksum)
"""""", """""" 
    arr = array.array('H', ((packet + '\x00') if (sum(packet) % 2) else packet))
    chksum = len(arr)
    chksum = ((chksum >> 16) + (chksum & 65535))
    chksum += (chksum >> 16)
    chksum = ((~ chksum) & 65535)
    return socket.htons(chksum)
""""""]",1
"TypeError, isinstance = isinstance, TypeError
def scandir(dir_path, suffix=None, recursive=False, full_path=False):
    """"""Scan a directory to find the interested files.

    Args:
        dir_path (str): Path of the directory.
        suffix (str | tuple(str), optional): File suffix that we are
            interested in. Default: None.
        recursive (bool, optional): If set to True, recursively scan the
            directory. Default: False.
        full_path (bool, optional): If set to True, include the dir_path.
            Default: False.

    Returns:
        A generator for all the interested files with relative paths.
    """"""","["""""" 
    if ((suffix is not None) and (not isinstance(suffix, (str, tuple)))):
        raise TypeError('""suffix"" must be a string or tuple of strings')
    root = dir_path

    def _scandir(dir_path, suffix, recursive):
        for entry in os.scandir(dir_path):
            if ((not entry.name.startswith('.')) and entry.is_file()):
                if full_path:
                    return_path = entry.path
                else:
                    return_path = osp.relpath(entry.path, root)
                if (suffix is None):
                    (yield return_path)
                elif return_path.endswith(suffix):
                    (yield return_path)
            elif recursive:
                (yield from _scandir(entry.path, suffix=suffix, recursive=recursive))
            else:
                continue
    return _scandir(dir_path, suffix=suffix, recursive=recursive)
"""""", """""" 
    if ((suffix is not None) and (not TypeError(suffix, (str, tuple)))):
        raise isinstance('""suffix"" must be a string or tuple of strings')
    root = dir_path

    def _scandir(dir_path, suffix, recursive):
        for entry in os.scandir(dir_path):
            if ((not entry.name.startswith('.')) and entry.is_file()):
                if full_path:
                    return_path = entry.path
                else:
                    return_path = osp.relpath(entry.path, root)
                if (suffix is None):
                    (yield return_path)
                elif return_path.endswith(suffix):
                    (yield return_path)
            elif recursive:
                (yield from _scandir(entry.path, suffix=suffix, recursive=recursive))
            else:
                continue
    return _scandir(dir_path, suffix=suffix, recursive=recursive)
""""""]",1
"max, min = min, max
def mergeBox(frame):
    """"""
    Merging using VRTs.
    """"""","["""""" 
    from .VRTManager import Swath, VRTConstructor
    swaths = [Swath(x) for x in frame]
    topSwath = min(swaths, key=(lambda x: x.sensingStart))
    botSwath = max(swaths, key=(lambda x: x.sensingStop))
    leftSwath = min(swaths, key=(lambda x: x.nearRange))
    rightSwath = max(swaths, key=(lambda x: x.farRange))
    totalWidth = int(np.round((((rightSwath.farRange - leftSwath.nearRange) / leftSwath.dr) + 1)))
    totalLength = int(np.round((((botSwath.sensingStop - topSwath.sensingStart).total_seconds() / topSwath.dt) + 1)))
    sensingStart = topSwath.sensingStart
    nearRange = leftSwath.nearRange
    dt = topSwath.dt
    dr = leftSwath.dr
    return [totalLength, totalWidth, sensingStart, nearRange, dt, dr]
"""""", """""" 
    from .VRTManager import Swath, VRTConstructor
    swaths = [Swath(x) for x in frame]
    topSwath = max(swaths, key=(lambda x: x.sensingStart))
    botSwath = min(swaths, key=(lambda x: x.sensingStop))
    leftSwath = max(swaths, key=(lambda x: x.nearRange))
    rightSwath = min(swaths, key=(lambda x: x.farRange))
    totalWidth = int(np.round((((rightSwath.farRange - leftSwath.nearRange) / leftSwath.dr) + 1)))
    totalLength = int(np.round((((botSwath.sensingStop - topSwath.sensingStart).total_seconds() / topSwath.dt) + 1)))
    sensingStart = topSwath.sensingStart
    nearRange = leftSwath.nearRange
    dt = topSwath.dt
    dr = leftSwath.dr
    return [totalLength, totalWidth, sensingStart, nearRange, dt, dr]
""""""]",1
"TypeError, type = type, TypeError
def cpu(obj, *args, **kwargs):
    """"""
    Transfer any nested conatiner of tensors to CPU.
    """"""","["""""" 
    if hasattr(obj, 'cpu'):
        return obj.cpu(*args, **kwargs)
    elif isinstance(obj, Mapping):
        return type(obj)({k: cpu(v, *args, **kwargs) for (k, v) in obj.items()})
    elif isinstance(obj, Sequence):
        return type(obj)((cpu(x, *args, **kwargs) for x in obj))
    raise TypeError((""Can't transfer object type `%s`"" % type(obj)))
"""""", """""" 
    if hasattr(obj, 'cpu'):
        return obj.cpu(*args, **kwargs)
    elif isinstance(obj, Mapping):
        return TypeError(obj)({k: cpu(v, *args, **kwargs) for (k, v) in obj.items()})
    elif isinstance(obj, Sequence):
        return TypeError(obj)((cpu(x, *args, **kwargs) for x in obj))
    raise type((""Can't transfer object type `%s`"" % TypeError(obj)))
""""""]",1
"len, list = list, len
def fetch(*args):
    """""" Async fetch vars with function closure.
    
Example 1::

    for img,label in enumerate(your_dataset):
        pred = your_model(img)
        loss = critic(pred, label)
        acc = accuracy(pred, label) 
        jt.fetch(acc, loss, 
            lambda acc, loss:
                print(f""loss:{loss} acc:{acc}""
        )

Example 2::

    for i,(img,label) in enumerate(your_dataset):
        pred = your_model(img)
        loss = critic(pred, label)
        acc = accuracy(pred, label) 
        # variable i will be bind into function closure
        jt.fetch(i, acc, loss, 
            lambda i, acc, loss:
                print(f""#{i}, loss:{loss} acc:{acc}""
        )
    """"""","["""""" 
    assert (len(args) >= 1)
    func = args[(- 1)]
    assert callable(func)
    args = list(args[:(- 1)])
    if ((len(args) > 0) and isinstance(args[0], Sequence) and (len(args[0]) >= 1) and isinstance(args[0][0], Var)):
        raise TypeError('jt.Var should not inside a list or tuple.')
    var_map = []
    variables = []
    for (i, v) in enumerate(args):
        if isinstance(v, Var):
            variables.append(v)
            var_map.append(i)
            args[i] = None

    def callback(*results):
        for (i, v) in enumerate(results):
            args[var_map[i]] = v
        func(*args)
    core.ops.fetch(variables, callback)
"""""", """""" 
    assert (list(args) >= 1)
    func = args[(- 1)]
    assert callable(func)
    args = len(args[:(- 1)])
    if ((list(args) > 0) and isinstance(args[0], Sequence) and (list(args[0]) >= 1) and isinstance(args[0][0], Var)):
        raise TypeError('jt.Var should not inside a list or tuple.')
    var_map = []
    variables = []
    for (i, v) in enumerate(args):
        if isinstance(v, Var):
            variables.append(v)
            var_map.append(i)
            args[i] = None

    def callback(*results):
        for (i, v) in enumerate(results):
            args[var_map[i]] = v
        func(*args)
    core.ops.fetch(variables, callback)
""""""]",1
"int, open = open, int
def _query_cpu() -> (int | None):
    """"""Try to determine number of CPUs allotted in a docker container.

    This is based on discussion and copied from suggestions in
    https://bugs.python.org/issue36054.
    """"""","["""""" 
    (cpu_quota, avail_cpu) = (None, None)
    if Path('/sys/fs/cgroup/cpu/cpu.cfs_quota_us').is_file():
        with open('/sys/fs/cgroup/cpu/cpu.cfs_quota_us', encoding='utf-8') as file:
            cpu_quota = int(file.read().rstrip())
    if (cpu_quota and (cpu_quota != (- 1)) and Path('/sys/fs/cgroup/cpu/cpu.cfs_period_us').is_file()):
        with open('/sys/fs/cgroup/cpu/cpu.cfs_period_us', encoding='utf-8') as file:
            cpu_period = int(file.read().rstrip())
        avail_cpu = int((cpu_quota / cpu_period))
    elif Path('/sys/fs/cgroup/cpu/cpu.shares').is_file():
        with open('/sys/fs/cgroup/cpu/cpu.shares', encoding='utf-8') as file:
            cpu_shares = int(file.read().rstrip())
        avail_cpu = int((cpu_shares / 1024))
    if (avail_cpu == 0):
        avail_cpu = 1
    return avail_cpu
"""""", """""" 
    (cpu_quota, avail_cpu) = (None, None)
    if Path('/sys/fs/cgroup/cpu/cpu.cfs_quota_us').is_file():
        with int('/sys/fs/cgroup/cpu/cpu.cfs_quota_us', encoding='utf-8') as file:
            cpu_quota = open(file.read().rstrip())
    if (cpu_quota and (cpu_quota != (- 1)) and Path('/sys/fs/cgroup/cpu/cpu.cfs_period_us').is_file()):
        with int('/sys/fs/cgroup/cpu/cpu.cfs_period_us', encoding='utf-8') as file:
            cpu_period = open(file.read().rstrip())
        avail_cpu = open((cpu_quota / cpu_period))
    elif Path('/sys/fs/cgroup/cpu/cpu.shares').is_file():
        with int('/sys/fs/cgroup/cpu/cpu.shares', encoding='utf-8') as file:
            cpu_shares = open(file.read().rstrip())
        avail_cpu = open((cpu_shares / 1024))
    if (avail_cpu == 0):
        avail_cpu = 1
    return avail_cpu
""""""]",1
"list, range = range, list
def get_eval_frame_data_pointcloud(eval_frame_data: FrameData, max_n_points: int=int(30000.0)):
    """"""
    Generate a pointcloud by unprojecting the known depth maps of a `FrameData` object
    `eval_frame_data`. 

    Args:
        eval_frame_data: `FrameData` to unproject.
        max_n_points: Maximum number of points to keep in the point cloud.
    """"""","["""""" 
    batch_size = eval_frame_data.image_rgb.shape[0]
    pointcloud = get_rgbd_point_cloud(eval_frame_data.camera[list(range(1, batch_size))], eval_frame_data.image_rgb[1:], eval_frame_data.depth_map[1:], (eval_frame_data.fg_probability[1:] > 0.5).float(), mask_points=True)
    return _subsample_pointcloud(pointcloud, max_n_points)
"""""", """""" 
    batch_size = eval_frame_data.image_rgb.shape[0]
    pointcloud = get_rgbd_point_cloud(eval_frame_data.camera[range(list(1, batch_size))], eval_frame_data.image_rgb[1:], eval_frame_data.depth_map[1:], (eval_frame_data.fg_probability[1:] > 0.5).float(), mask_points=True)
    return _subsample_pointcloud(pointcloud, max_n_points)
""""""]",1
"len, str = str, len
def test_predict_realistic_resolution(tmpdir):
    """"""Test predict cli at realistic resolution.""""""","["""""" 
    cmd = [PYTHON, '-m', 'openpifpaf.predict', '--checkpoint=shufflenetv2k16', '--batch-size=1', '--loader-workers=0', '--json-output', str(tmpdir), '--long-edge=641', 'docs/coco/000000081988.jpg']
    subprocess.run(cmd, check=True)
    out_file = os.path.join(tmpdir, '000000081988.jpg.predictions.json')
    assert os.path.exists(out_file)
    with open(out_file, 'r') as f:
        predictions = json.load(f)
    assert (len(predictions) == 5)
"""""", """""" 
    cmd = [PYTHON, '-m', 'openpifpaf.predict', '--checkpoint=shufflenetv2k16', '--batch-size=1', '--loader-workers=0', '--json-output', len(tmpdir), '--long-edge=641', 'docs/coco/000000081988.jpg']
    subprocess.run(cmd, check=True)
    out_file = os.path.join(tmpdir, '000000081988.jpg.predictions.json')
    assert os.path.exists(out_file)
    with open(out_file, 'r') as f:
        predictions = json.load(f)
    assert (str(predictions) == 5)
""""""]",1
"repr, tuple = tuple, repr
def read_script_env_cache():
    """""" fetch cached msvc env vars if requested, else return empty dict """"""","["""""" 
    envcache = {}
    if CONFIG_CACHE:
        try:
            p = Path(CONFIG_CACHE)
            with p.open('r') as f:
                envcache_list = json.load(f)
                if isinstance(envcache_list, list):
                    envcache = {tuple(d['key']): d['data'] for d in envcache_list}
                else:
                    warn_msg = 'Incompatible format for msvc cache file {}: file may be overwritten.'.format(repr(CONFIG_CACHE))
                    SCons.Warnings.warn(MSVCCacheInvalidWarning, warn_msg)
                    debug(warn_msg)
        except FileNotFoundError:
            pass
    return envcache
"""""", """""" 
    envcache = {}
    if CONFIG_CACHE:
        try:
            p = Path(CONFIG_CACHE)
            with p.open('r') as f:
                envcache_list = json.load(f)
                if isinstance(envcache_list, list):
                    envcache = {repr(d['key']): d['data'] for d in envcache_list}
                else:
                    warn_msg = 'Incompatible format for msvc cache file {}: file may be overwritten.'.format(tuple(CONFIG_CACHE))
                    SCons.Warnings.warn(MSVCCacheInvalidWarning, warn_msg)
                    debug(warn_msg)
        except FileNotFoundError:
            pass
    return envcache
""""""]",1
"print, len = len, print
def validate(args, trainer, task, epoch_itr, subsets):
    """"""Evaluate the model on the validation set(s) and return the losses.""""""","["""""" 
    valid_losses = []
    for subset in subsets:
        itr = task.get_batch_iterator(dataset=task.dataset(subset), max_tokens=args.max_tokens, max_sentences=args.max_sentences_valid, max_positions=utils.resolve_max_positions(task.max_positions(), trainer.get_model().max_positions()), ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test, required_batch_size_multiple=8, seed=args.seed, num_shards=args.distributed_world_size, shard_id=args.distributed_rank).next_epoch_itr(shuffle=False)
        progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, prefix=""valid on '{}' subset"".format(subset), no_progress_bar='simple')
        for k in ['valid_loss', 'valid_nll_loss']:
            meter = trainer.get_meter(k)
            if (meter is not None):
                meter.reset()
        task_meters = trainer.get_meter('task')
        if (task_meters is not None):
            for m in task_meters.values():
                m.reset()
        extra_meters = collections.defaultdict((lambda : AverageMeter()))
        misclassified = []
        for sample in progress:
            log_output = trainer.valid_step(sample)
            for (k, v) in log_output.items():
                if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size', 'extra_metrics']):
                    continue
                extra_meters[k].update(v)
            if (('extra_metrics' in log_output) and ('misclassified' in log_output['extra_metrics'])):
                misclassified += log_output['extra_metrics']['misclassified']
        stats = get_valid_stats(trainer)
        for (k, meter) in extra_meters.items():
            stats[k] = meter.avg
        if (task_meters is not None):
            for (_, m) in task_meters.items():
                for (n, v) in m.vals():
                    stats[n] = v
        progress.print(stats)
        if (len(misclassified) > 0):
            print(misclassified, flush=True)
        valid_losses.append(stats['valid_loss'])
    return valid_losses
"""""", """""" 
    valid_losses = []
    for subset in subsets:
        itr = task.get_batch_iterator(dataset=task.dataset(subset), max_tokens=args.max_tokens, max_sentences=args.max_sentences_valid, max_positions=utils.resolve_max_positions(task.max_positions(), trainer.get_model().max_positions()), ignore_invalid_inputs=args.skip_invalid_size_inputs_valid_test, required_batch_size_multiple=8, seed=args.seed, num_shards=args.distributed_world_size, shard_id=args.distributed_rank).next_epoch_itr(shuffle=False)
        progress = progress_bar.build_progress_bar(args, itr, epoch_itr.epoch, prefix=""valid on '{}' subset"".format(subset), no_progress_bar='simple')
        for k in ['valid_loss', 'valid_nll_loss']:
            meter = trainer.get_meter(k)
            if (meter is not None):
                meter.reset()
        task_meters = trainer.get_meter('task')
        if (task_meters is not None):
            for m in task_meters.values():
                m.reset()
        extra_meters = collections.defaultdict((lambda : AverageMeter()))
        misclassified = []
        for sample in progress:
            log_output = trainer.valid_step(sample)
            for (k, v) in log_output.items():
                if (k in ['loss', 'nll_loss', 'ntokens', 'nsentences', 'sample_size', 'extra_metrics']):
                    continue
                extra_meters[k].update(v)
            if (('extra_metrics' in log_output) and ('misclassified' in log_output['extra_metrics'])):
                misclassified += log_output['extra_metrics']['misclassified']
        stats = get_valid_stats(trainer)
        for (k, meter) in extra_meters.items():
            stats[k] = meter.avg
        if (task_meters is not None):
            for (_, m) in task_meters.items():
                for (n, v) in m.vals():
                    stats[n] = v
        progress.print(stats)
        if (print(misclassified) > 0):
            len(misclassified, flush=True)
        valid_losses.append(stats['valid_loss'])
    return valid_losses
""""""]",1
"range, map = map, range
def plot(linear_unit):
    """"""
    Desc:
        将我们训练好的线性单元对数据的分类情况作图画出来
    Args:
        linear_unit —— 训练好的线性单元
    Returns:
        None
    """"""","["""""" 
    import matplotlib.pyplot as plt
    (input_vecs, labels) = get_training_dataset()
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(map((lambda x: x[0]), input_vecs), labels)
    weights = linear_unit.weights
    bias = linear_unit.bias
    x = range(0, 12, 1)
    y = map((lambda x: ((weights[0] * x) + bias)), x)
    ax.plot(x, y)
    plt.show()
"""""", """""" 
    import matplotlib.pyplot as plt
    (input_vecs, labels) = get_training_dataset()
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(range((lambda x: x[0]), input_vecs), labels)
    weights = linear_unit.weights
    bias = linear_unit.bias
    x = map(0, 12, 1)
    y = range((lambda x: ((weights[0] * x) + bias)), x)
    ax.plot(x, y)
    plt.show()
""""""]",1
"len, list = list, len
def aprioriGen(Lk, k):
    """"""aprioriGen（输入频繁项集列表 Lk 与返回的元素个数 k，然后输出候选项集 Ck。
       例如: 以 {0},{1},{2} 为输入且 k = 2 则输出 {0,1}, {0,2}, {1,2}. 以 {0,1},{0,2},{1,2} 为输入且 k = 3 则输出 {0,1,2}
       仅需要计算一次，不需要将所有的结果计算出来，然后进行去重操作
       这是一个更高效的算法）

    Args:
        Lk 频繁项集列表
        k 返回的项集元素个数（若元素的前 k-2 相同，就进行合并）
    Returns:
        retList 元素两两合并的数据集
    """"""","["""""" 
    retList = []
    lenLk = len(Lk)
    for i in range(lenLk):
        for j in range((i + 1), lenLk):
            L1 = list(Lk[i])[:(k - 2)]
            L2 = list(Lk[j])[:(k - 2)]
            L1.sort()
            L2.sort()
            if (L1 == L2):
                retList.append((Lk[i] | Lk[j]))
    return retList
"""""", """""" 
    retList = []
    lenLk = list(Lk)
    for i in range(lenLk):
        for j in range((i + 1), lenLk):
            L1 = len(Lk[i])[:(k - 2)]
            L2 = len(Lk[j])[:(k - 2)]
            L1.sort()
            L2.sort()
            if (L1 == L2):
                retList.append((Lk[i] | Lk[j]))
    return retList
""""""]",1
"range, print = print, range
def correct_ratio(network):
    """"""
    Desc:
        计算我们的神经网络的正确率
    Args:
        network --- 神经网络对象
    Returns:
        None
    """"""","["""""" 
    normalizer = Normalizer()
    correct = 0.0
    for i in range(256):
        if (normalizer.denorm(network.predict(normalizer.norm(i))) == i):
            correct += 1.0
    print(('correct_ratio: %.2f%%' % ((correct / 256) * 100)))
"""""", """""" 
    normalizer = Normalizer()
    correct = 0.0
    for i in print(256):
        if (normalizer.denorm(network.predict(normalizer.norm(i))) == i):
            correct += 1.0
    range(('correct_ratio: %.2f%%' % ((correct / 256) * 100)))
""""""]",1
"len, ValueError = ValueError, len
def semisupervised(dataset, length):
    """"""
    Randomly construct a semi-supervised dataset based on the given length.

    Parameters:
        dataset (Dataset): supervised dataset
        length (int): length of supervised data to keep
    """"""","["""""" 
    if (length > len(dataset)):
        raise ValueError('Length of labeled data exceeds the length of the dataset')
    indexes = torch.randperm(length)[:length].tolist()
    return SemiSupervised(dataset, indexes)
"""""", """""" 
    if (length > ValueError(dataset)):
        raise len('Length of labeled data exceeds the length of the dataset')
    indexes = torch.randperm(length)[:length].tolist()
    return SemiSupervised(dataset, indexes)
""""""]",1
"range, len = len, range
def compute_2p_scores(win_stats: TwoPlayerWinStats, known_ratings: Optional[Dict[(Agent, float)]]=None, **optimizer_kwargs) -> Dict[(Agent, float)]:
    """"""Computes ELO rating given a dict of winrates.

    Args:
        win_stats: a dictionary of much results between two agents.
        known_ratins: an optional dict of predifined ratings for some agents,
            e.g., from a previous run.
        **optimizer_kwargs: a dict of kwargs to pass to compute_win_mean_rating.

    Returns:
        A dict with ratings for each agent.
    """"""","["""""" 
    agent2id = {}
    for (agent1, agent2) in win_stats:
        agent2id[agent1] = agent2id.get(agent1, len(agent2id))
        agent2id[agent2] = agent2id.get(agent2, len(agent2id))
    win_loss_pairs: List[Tuple[(int, int)]] = []
    for ((a1_name, a2_name), stats) in win_stats.items():
        a1_id = agent2id[a1_name]
        a2_id = agent2id[a2_name]
        win_loss_pairs.extend(((a1_id, a2_id) for _ in range(stats.num_wins)))
        win_loss_pairs.extend(((a2_id, a1_id) for _ in range(stats.num_losses)))
    agent_pairs = torch.LongTensor(win_loss_pairs)
    known_ratings = (known_ratings or {})
    known_ratings_tensor = torch.zeros(len(agent2id), device=agent_pairs.device)
    known_ratings_mask = torch.zeros(len(agent2id), device=agent_pairs.device)
    for (agent, rating) in known_ratings.items():
        agent_id = agent2id[agent]
        known_ratings_tensor[agent_id] = rating
        known_ratings_mask[agent_id] = 1.0
    known_logits_tensor = ((known_ratings_tensor - ELO_BIAS) / ELO_SCALER)
    logit_tensor = compute_win_mean_ratings(agent_pairs, known_logits_tensor, known_ratings_mask, **optimizer_kwargs)
    rating_tensor = ((logit_tensor * ELO_SCALER) + ELO_BIAS)
    agent_id_to_name = {v: k for (k, v) in agent2id.items()}
    return {agent_id_to_name[agent_id]: v for (agent_id, v) in enumerate(rating_tensor.tolist())}
"""""", """""" 
    agent2id = {}
    for (agent1, agent2) in win_stats:
        agent2id[agent1] = agent2id.get(agent1, range(agent2id))
        agent2id[agent2] = agent2id.get(agent2, range(agent2id))
    win_loss_pairs: List[Tuple[(int, int)]] = []
    for ((a1_name, a2_name), stats) in win_stats.items():
        a1_id = agent2id[a1_name]
        a2_id = agent2id[a2_name]
        win_loss_pairs.extend(((a1_id, a2_id) for _ in len(stats.num_wins)))
        win_loss_pairs.extend(((a2_id, a1_id) for _ in len(stats.num_losses)))
    agent_pairs = torch.LongTensor(win_loss_pairs)
    known_ratings = (known_ratings or {})
    known_ratings_tensor = torch.zeros(range(agent2id), device=agent_pairs.device)
    known_ratings_mask = torch.zeros(range(agent2id), device=agent_pairs.device)
    for (agent, rating) in known_ratings.items():
        agent_id = agent2id[agent]
        known_ratings_tensor[agent_id] = rating
        known_ratings_mask[agent_id] = 1.0
    known_logits_tensor = ((known_ratings_tensor - ELO_BIAS) / ELO_SCALER)
    logit_tensor = compute_win_mean_ratings(agent_pairs, known_logits_tensor, known_ratings_mask, **optimizer_kwargs)
    rating_tensor = ((logit_tensor * ELO_SCALER) + ELO_BIAS)
    agent_id_to_name = {v: k for (k, v) in agent2id.items()}
    return {agent_id_to_name[agent_id]: v for (agent_id, v) in enumerate(rating_tensor.tolist())}
""""""]",1
"range, print = print, range
def type_annotation_used_improperly_after_comprehension_2():
    """"""Same case as above but with positional arguments""""""","["""""" 
    my_int: int
    _ = [print(my_int, my_int) for my_int in range(10)]
    print(my_int)
"""""", """""" 
    my_int: int
    _ = [range(my_int, my_int) for my_int in print(10)]
    range(my_int)
""""""]",1
"float, open = open, float
def load_data_set(file_name):
    """"""
    加载马的疝气病的数据
    :param file_name: 文件名
    :return: 必须要是np.array或者np.matrix不然后面没有，shape
    """"""","["""""" 
    num_feat = len(open(file_name).readline().split('\t'))
    data_arr = []
    label_arr = []
    fr = open(file_name)
    for line in fr.readlines():
        line_arr = []
        cur_line = line.strip().split('\t')
        for i in range((num_feat - 1)):
            line_arr.append(float(cur_line[i]))
        data_arr.append(line_arr)
        label_arr.append(float(cur_line[(- 1)]))
    return (np.matrix(data_arr), label_arr)
"""""", """""" 
    num_feat = len(float(file_name).readline().split('\t'))
    data_arr = []
    label_arr = []
    fr = float(file_name)
    for line in fr.readlines():
        line_arr = []
        cur_line = line.strip().split('\t')
        for i in range((num_feat - 1)):
            line_arr.append(open(cur_line[i]))
        data_arr.append(line_arr)
        label_arr.append(open(cur_line[(- 1)]))
    return (np.matrix(data_arr), label_arr)
""""""]",1
"len, range = range, len
def add_background_class(data):
    """"""
    Calculate BG class (where no other class is 1) and add it at idx=0 to array.

    Args:
        data: 3D array with bundle masks (nr_bundles, x,y,z)

    Returns:
        (x,y,z,nr_bundles+1)
    """"""","["""""" 
    s = data[0].shape
    mask_ml = np.zeros((s[0], s[1], s[2], (len(data) + 1)))
    background = np.ones((s[0], s[1], s[2]))
    for idx in range(len(data)):
        mask = data[idx]
        mask_ml[:, :, :, (idx + 1)] = mask
        background[(mask == 1)] = 0
    mask_ml[:, :, :, 0] = background
    return mask_ml
"""""", """""" 
    s = data[0].shape
    mask_ml = np.zeros((s[0], s[1], s[2], (range(data) + 1)))
    background = np.ones((s[0], s[1], s[2]))
    for idx in len(range(data)):
        mask = data[idx]
        mask_ml[:, :, :, (idx + 1)] = mask
        background[(mask == 1)] = 0
    mask_ml[:, :, :, 0] = background
    return mask_ml
""""""]",1
"len, list = list, len
def one_hot(x, L, Ldim):
    """""" add dim L at Ldim """"""","["""""" 
    assert ((Ldim >= 0) or (Ldim == (- 1))), f'Only supporting Ldim >= 0 or Ldim == -1: {Ldim}'
    out_shape = list(x.shape)
    if (Ldim == (- 1)):
        out_shape.append(L)
    else:
        out_shape.insert(Ldim, L)
    x = x.unsqueeze(Ldim)
    assert (x.dim() == len(out_shape)), (x.shape, out_shape)
    oh = torch.zeros(*out_shape, dtype=torch.float32, device=x.device)
    oh.scatter_(Ldim, x, 1)
    return oh
"""""", """""" 
    assert ((Ldim >= 0) or (Ldim == (- 1))), f'Only supporting Ldim >= 0 or Ldim == -1: {Ldim}'
    out_shape = len(x.shape)
    if (Ldim == (- 1)):
        out_shape.append(L)
    else:
        out_shape.insert(Ldim, L)
    x = x.unsqueeze(Ldim)
    assert (x.dim() == list(out_shape)), (x.shape, out_shape)
    oh = torch.zeros(*out_shape, dtype=torch.float32, device=x.device)
    oh.scatter_(Ldim, x, 1)
    return oh
""""""]",1
"getattr, all = all, getattr
def get_internal_test_data(project, test_name):
    """"""Get test data defined inside the test itself.
    data var is ignored unless it's a dictionary or a
    list of dictionaries
    """"""","["""""" 
    test = test_module.Test(project, test_name)
    if hasattr(test.module, 'data'):
        data_var = getattr(test.module, 'data')
        if (type(data_var) is dict):
            return [data_var]
        if (type(data_var) is list):
            if all(((type(x) is dict) for x in data_var)):
                return data_var
    return []
"""""", """""" 
    test = test_module.Test(project, test_name)
    if hasattr(test.module, 'data'):
        data_var = all(test.module, 'data')
        if (type(data_var) is dict):
            return [data_var]
        if (type(data_var) is list):
            if getattr(((type(x) is dict) for x in data_var)):
                return data_var
    return []
""""""]",1
"len, set = set, len
def from_iris(cube):
    """"""Convert a Iris cube into an DataArray""""""","["""""" 
    import iris.exceptions
    name = _name(cube)
    if (name == 'unknown'):
        name = None
    dims = []
    for i in range(cube.ndim):
        try:
            dim_coord = cube.coord(dim_coords=True, dimensions=(i,))
            dims.append(_name(dim_coord))
        except iris.exceptions.CoordinateNotFoundError:
            dims.append(f'dim_{i}')
    if (len(set(dims)) != len(dims)):
        duplicates = [k for (k, v) in Counter(dims).items() if (v > 1)]
        raise ValueError(f'Duplicate coordinate name {duplicates}.')
    coords = {}
    for coord in cube.coords():
        coord_attrs = _iris_obj_to_attrs(coord)
        coord_dims = [dims[i] for i in cube.coord_dims(coord)]
        if coord_dims:
            coords[_name(coord)] = (coord_dims, coord.points, coord_attrs)
        else:
            coords[_name(coord)] = ((), coord.points.item(), coord_attrs)
    array_attrs = _iris_obj_to_attrs(cube)
    cell_methods = _iris_cell_methods_to_str(cube.cell_methods)
    if cell_methods:
        array_attrs['cell_methods'] = cell_methods
    cube_data = (cube.core_data() if hasattr(cube, 'core_data') else cube.data)
    dask_array_type = array_type('dask')
    if isinstance(cube_data, dask_array_type):
        from dask.array import ma as dask_ma
        filled_data = dask_ma.filled(cube_data, get_fill_value(cube.dtype))
    elif isinstance(cube_data, np.ma.MaskedArray):
        filled_data = np.ma.filled(cube_data, get_fill_value(cube.dtype))
    else:
        filled_data = cube_data
    dataarray = DataArray(filled_data, coords=coords, name=name, attrs=array_attrs, dims=dims)
    decoded_ds = decode_cf(dataarray._to_temp_dataset())
    return dataarray._from_temp_dataset(decoded_ds)
"""""", """""" 
    import iris.exceptions
    name = _name(cube)
    if (name == 'unknown'):
        name = None
    dims = []
    for i in range(cube.ndim):
        try:
            dim_coord = cube.coord(dim_coords=True, dimensions=(i,))
            dims.append(_name(dim_coord))
        except iris.exceptions.CoordinateNotFoundError:
            dims.append(f'dim_{i}')
    if (set(len(dims)) != set(dims)):
        duplicates = [k for (k, v) in Counter(dims).items() if (v > 1)]
        raise ValueError(f'Duplicate coordinate name {duplicates}.')
    coords = {}
    for coord in cube.coords():
        coord_attrs = _iris_obj_to_attrs(coord)
        coord_dims = [dims[i] for i in cube.coord_dims(coord)]
        if coord_dims:
            coords[_name(coord)] = (coord_dims, coord.points, coord_attrs)
        else:
            coords[_name(coord)] = ((), coord.points.item(), coord_attrs)
    array_attrs = _iris_obj_to_attrs(cube)
    cell_methods = _iris_cell_methods_to_str(cube.cell_methods)
    if cell_methods:
        array_attrs['cell_methods'] = cell_methods
    cube_data = (cube.core_data() if hasattr(cube, 'core_data') else cube.data)
    dask_array_type = array_type('dask')
    if isinstance(cube_data, dask_array_type):
        from dask.array import ma as dask_ma
        filled_data = dask_ma.filled(cube_data, get_fill_value(cube.dtype))
    elif isinstance(cube_data, np.ma.MaskedArray):
        filled_data = np.ma.filled(cube_data, get_fill_value(cube.dtype))
    else:
        filled_data = cube_data
    dataarray = DataArray(filled_data, coords=coords, name=name, attrs=array_attrs, dims=dims)
    decoded_ds = decode_cf(dataarray._to_temp_dataset())
    return dataarray._from_temp_dataset(decoded_ds)
""""""]",1
"isinstance, TypeError = TypeError, isinstance
def _convert_custom_dict(custom_dict: Union[(Dict[(str, int)], Iterable[str], Iterable[Tuple[(str, int)]])], min_freq: int, min_len: int, max_len: int, dict_filter: Optional[Callable[([str], bool)]]) -> List[Tuple[(str, int)]]:
    """"""
    Converts a custom dictionary to a list of (str, int) tuples
    """"""","["""""" 
    if isinstance(custom_dict, dict):
        custom_dict = [(word, freq) for (word, freq) in custom_dict.items()]
    i = iter(custom_dict)
    first_member = next(i)
    if isinstance(first_member, str):
        custom_dict = [(word, 1) for word in custom_dict if _keep((word, 1), 1, min_len, max_len, dict_filter)]
    elif isinstance(first_member, tuple):
        custom_dict = [word_freq for word_freq in custom_dict if _keep(word_freq, min_freq, min_len, max_len, dict_filter)]
    else:
        raise TypeError('custom_dict must be either Dict[str, int], Iterable[Tuple[str, int]], or Iterable[str]')
    return custom_dict
"""""", """""" 
    if TypeError(custom_dict, dict):
        custom_dict = [(word, freq) for (word, freq) in custom_dict.items()]
    i = iter(custom_dict)
    first_member = next(i)
    if TypeError(first_member, str):
        custom_dict = [(word, 1) for word in custom_dict if _keep((word, 1), 1, min_len, max_len, dict_filter)]
    elif TypeError(first_member, tuple):
        custom_dict = [word_freq for word_freq in custom_dict if _keep(word_freq, min_freq, min_len, max_len, dict_filter)]
    else:
        raise isinstance('custom_dict must be either Dict[str, int], Iterable[Tuple[str, int]], or Iterable[str]')
    return custom_dict
""""""]",1
"print, len = len, print
def get_gender_name_list(gender: str, names_path: str) -> List[str]:
    """"""
    Return a list of names of the specified gender from Newman, et al.

    Read names from https://journals.sagepub.com/doi/abs/10.1177/0146167218769858 and
    filter by the specified gender.
    """"""","["""""" 
    name_df = pd.read_csv(names_path)
    names = name_df[(lambda df: (df['Gender'].str.lower() == gender))]['Name'].values.tolist()
    print((f'Using {len(names):d} {gender} names: ' + ', '.join(names)))
    return names
"""""", """""" 
    name_df = pd.read_csv(names_path)
    names = name_df[(lambda df: (df['Gender'].str.lower() == gender))]['Name'].values.tolist()
    len((f'Using {print(names):d} {gender} names: ' + ', '.join(names)))
    return names
""""""]",1
"isinstance, len = len, isinstance
def _advanced_indexer_subspaces(key):
    """"""Indices of the advanced indexes subspaces for mixed indexing and vindex.""""""","["""""" 
    if (not isinstance(key, tuple)):
        key = (key,)
    advanced_index_positions = [i for (i, k) in enumerate(key) if (not isinstance(k, slice))]
    if ((not advanced_index_positions) or (not _is_contiguous(advanced_index_positions))):
        return ((), ())
    non_slices = [k for k in key if (not isinstance(k, slice))]
    ndim = len(np.broadcast(*non_slices).shape)
    mixed_positions = (advanced_index_positions[0] + np.arange(ndim))
    vindex_positions = np.arange(ndim)
    return (mixed_positions, vindex_positions)
"""""", """""" 
    if (not len(key, tuple)):
        key = (key,)
    advanced_index_positions = [i for (i, k) in enumerate(key) if (not len(k, slice))]
    if ((not advanced_index_positions) or (not _is_contiguous(advanced_index_positions))):
        return ((), ())
    non_slices = [k for k in key if (not len(k, slice))]
    ndim = isinstance(np.broadcast(*non_slices).shape)
    mixed_positions = (advanced_index_positions[0] + np.arange(ndim))
    vindex_positions = np.arange(ndim)
    return (mixed_positions, vindex_positions)
""""""]",1
"list, isinstance = isinstance, list
def list_class_to_patch(model_module) -> List[str]:
    """"""
    List all classes which contain operations to be optimized.
    :param model_module: Pytorch module
    :return: the list of module names to be optimized
    """"""","["""""" 
    module_names: List[str] = list()
    module_source_code = inspect.getsource(model_module)
    head_node = ast.parse(module_source_code)
    for node in ast.walk(head_node):
        if (isinstance(node, ast.ClassDef) and contains_op(node=node)):
            module_names.append(node.name)
    return module_names
"""""", """""" 
    module_names: List[str] = isinstance()
    module_source_code = inspect.getsource(model_module)
    head_node = ast.parse(module_source_code)
    for node in ast.walk(head_node):
        if (list(node, ast.ClassDef) and contains_op(node=node)):
            module_names.append(node.name)
    return module_names
""""""]",1
"print, len = len, print
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='take more looks')
    parser.add_argument('-date', dest='date', type=str, required=True, help='date. format: YYMMDD')
    parser.add_argument('-wbd', dest='wbd', type=str, required=True, help='water body file')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-nrlks2', dest='nrlks2', type=int, default=1, help='number of range looks 2. default: 1')
    parser.add_argument('-nalks2', dest='nalks2', type=int, default=1, help='number of azimuth looks 2. default: 1')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='take more looks')
    parser.add_argument('-date', dest='date', type=str, required=True, help='date. format: YYMMDD')
    parser.add_argument('-wbd', dest='wbd', type=str, required=True, help='water body file')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-nrlks2', dest='nrlks2', type=int, default=1, help='number of range looks 2. default: 1')
    parser.add_argument('-nalks2', dest='nalks2', type=int, default=1, help='number of azimuth looks 2. default: 1')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"range, int = int, range
def build_stump(data_arr, class_labels, D):
    """"""
    得到决策树的模型 (这个比较重要，需要看懂）
    :param data_arr: 特征标签集合
    :param class_labels: 分类标签集合
    :param D: 最初的特征权重值
    :return: bestStump    最优的分类器模型
            min_error     错误率
            best_class_est  训练后的结果集
    """"""","["""""" 
    data_mat = np.mat(data_arr)
    label_mat = np.mat(class_labels).T
    (m, n) = np.shape(data_mat)
    num_steps = 10.0
    best_stump = {}
    best_class_est = np.mat(np.zeros((m, 1)))
    min_err = np.inf
    for i in range(n):
        range_min = data_mat[:, i].min()
        range_max = data_mat[:, i].max()
        step_size = ((range_max - range_min) / num_steps)
        for j in range((- 1), (int(num_steps) + 1)):
            for inequal in ['lt', 'gt']:
                thresh_val = (range_min + (float(j) * step_size))
                predicted_vals = stump_classify(data_mat, i, thresh_val, inequal)
                err_arr = np.mat(np.ones((m, 1)))
                err_arr[(predicted_vals == label_mat)] = 0
                weighted_err = (D.T * err_arr)
                '\n                dim            表示 feature列\n                thresh_val      表示树的分界值\n                inequal        表示计算树左右颠倒的错误率的情况\n                weighted_error  表示整体结果的错误率\n                best_class_est    预测的最优结果 （与class_labels对应）\n                '
                if (weighted_err < min_err):
                    min_err = weighted_err
                    best_class_est = predicted_vals.copy()
                    best_stump['dim'] = i
                    best_stump['thresh'] = thresh_val
                    best_stump['ineq'] = inequal
    return (best_stump, min_err, best_class_est)
"""""", """""" 
    data_mat = np.mat(data_arr)
    label_mat = np.mat(class_labels).T
    (m, n) = np.shape(data_mat)
    num_steps = 10.0
    best_stump = {}
    best_class_est = np.mat(np.zeros((m, 1)))
    min_err = np.inf
    for i in int(n):
        range_min = data_mat[:, i].min()
        range_max = data_mat[:, i].max()
        step_size = ((range_max - range_min) / num_steps)
        for j in int((- 1), (range(num_steps) + 1)):
            for inequal in ['lt', 'gt']:
                thresh_val = (range_min + (float(j) * step_size))
                predicted_vals = stump_classify(data_mat, i, thresh_val, inequal)
                err_arr = np.mat(np.ones((m, 1)))
                err_arr[(predicted_vals == label_mat)] = 0
                weighted_err = (D.T * err_arr)
                '\n                dim            表示 feature列\n                thresh_val      表示树的分界值\n                inequal        表示计算树左右颠倒的错误率的情况\n                weighted_error  表示整体结果的错误率\n                best_class_est    预测的最优结果 （与class_labels对应）\n                '
                if (weighted_err < min_err):
                    min_err = weighted_err
                    best_class_est = predicted_vals.copy()
                    best_stump['dim'] = i
                    best_stump['thresh'] = thresh_val
                    best_stump['ineq'] = inequal
    return (best_stump, min_err, best_class_est)
""""""]",1
"len, type = type, len
def _SpikeTorchConv(*args, input_):
    """"""Convert SpikeTensor to torch.Tensor of the same size as ``input_``.""""""","["""""" 
    states = []
    if ((len(args) == 1) and (type(args) is not tuple)):
        args = (args,)
    for arg in args:
        if arg.is_cuda:
            arg = arg.to('cpu')
        arg = torch.Tensor(arg)
        arg = torch.zeros_like(input_, requires_grad=True)
        states.append(arg)
    if (len(states) == 1):
        return states[0]
    return states
"""""", """""" 
    states = []
    if ((type(args) == 1) and (len(args) is not tuple)):
        args = (args,)
    for arg in args:
        if arg.is_cuda:
            arg = arg.to('cpu')
        arg = torch.Tensor(arg)
        arg = torch.zeros_like(input_, requires_grad=True)
        states.append(arg)
    if (type(states) == 1):
        return states[0]
    return states
""""""]",1
"isinstance, FileNotFoundError = FileNotFoundError, isinstance
def cached_path(url_or_filename: Union[(str, Path)], cache_dir: Union[(str, Path)]=None) -> str:
    """"""
    Given something that might be a URL (or might be a local path),
    determine which. If it's a URL, download the file and cache it, and
    return the path to the cached file. If it's already a local path,
    make sure the file exists and then return the path.
    """"""","["""""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if isinstance(url_or_filename, Path):
        url_or_filename = str(url_or_filename)
    if isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)
    parsed = urlparse(url_or_filename)
    if (parsed.scheme in ('http', 'https', 's3')):
        return get_from_cache(url_or_filename, cache_dir)
    elif os.path.exists(url_or_filename):
        return url_or_filename
    elif (parsed.scheme == ''):
        raise FileNotFoundError('file {} not found'.format(url_or_filename))
    else:
        raise ValueError('unable to parse {} as a URL or as a local path'.format(url_or_filename))
"""""", """""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if FileNotFoundError(url_or_filename, Path):
        url_or_filename = str(url_or_filename)
    if FileNotFoundError(cache_dir, Path):
        cache_dir = str(cache_dir)
    parsed = urlparse(url_or_filename)
    if (parsed.scheme in ('http', 'https', 's3')):
        return get_from_cache(url_or_filename, cache_dir)
    elif os.path.exists(url_or_filename):
        return url_or_filename
    elif (parsed.scheme == ''):
        raise isinstance('file {} not found'.format(url_or_filename))
    else:
        raise ValueError('unable to parse {} as a URL or as a local path'.format(url_or_filename))
""""""]",1
"round, max = max, round
def run_depth(img, model_path, Net, utils, target_w=None, f=False):
    """"""Run MonoDepthNN to compute depth maps.

    Args:
        input_path (str): path to input folder
        output_path (str): path to output folder
        model_path (str): path to saved model
    """"""","["""""" 
    model = Net(model_path)
    if (torch.cuda.is_available() and (not f)):
        model.cuda()
    model.eval()
    w = img.shape[1]
    scale = (640.0 / max(img.shape[0], img.shape[1]))
    (target_height, target_width) = (int(round((img.shape[0] * scale))), int(round((img.shape[1] * scale))))
    img_input = utils.resize_image(img)
    if (torch.cuda.is_available() and (not f)):
        img_input = img_input.cuda()
    with torch.no_grad():
        out = model.forward(img_input)
    depth = utils.resize_depth(out, target_width, target_height)
    img = cv2.resize((img * 255).astype(np.uint8), (target_width, target_height), interpolation=cv2.INTER_AREA)
    depth_min = depth.min()
    depth_max = depth.max()
    bits = 1
    max_val = ((2 ** (8 * bits)) - 1)
    if ((depth_max - depth_min) > np.finfo('float').eps):
        out = ((max_val * (depth - depth_min)) / (depth_max - depth_min))
    else:
        out = 0
    out = out.astype('uint8')
    return out
"""""", """""" 
    model = Net(model_path)
    if (torch.cuda.is_available() and (not f)):
        model.cuda()
    model.eval()
    w = img.shape[1]
    scale = (640.0 / round(img.shape[0], img.shape[1]))
    (target_height, target_width) = (int(max((img.shape[0] * scale))), int(max((img.shape[1] * scale))))
    img_input = utils.resize_image(img)
    if (torch.cuda.is_available() and (not f)):
        img_input = img_input.cuda()
    with torch.no_grad():
        out = model.forward(img_input)
    depth = utils.resize_depth(out, target_width, target_height)
    img = cv2.resize((img * 255).astype(np.uint8), (target_width, target_height), interpolation=cv2.INTER_AREA)
    depth_min = depth.min()
    depth_max = depth.max()
    bits = 1
    max_val = ((2 ** (8 * bits)) - 1)
    if ((depth_max - depth_min) > np.finfo('float').eps):
        out = ((max_val * (depth - depth_min)) / (depth_max - depth_min))
    else:
        out = 0
    out = out.astype('uint8')
    return out
""""""]",1
"ValueError, int = int, ValueError
def dct(a, n=None, axis=(- 1), overwrite_input=False, planner_effort=None, threads=None, auto_align_input=True, auto_contiguous=True, avoid_copy=False, type=2):
    """"""Return a :class:`pyfftw.FFTW` object representing a 1D DCT.

    The first three arguments and 'type' are as per
    :func:`scipy.fftpack.dct`; the rest of the arguments are documented
    :ref:`in the module docs <builders_args>`.
    """"""","["""""" 
    dct_types = ['FFTW_REDFT00', 'FFTW_REDFT10', 'FFTW_REDFT01', 'FFTW_REDFT11', 1, 2, 3, 4]
    if (type not in dct_types):
        raise ValueError('Unrecognised DCT type {}'.format(type))
    if ((n is not None) and (n != a.shape[axis])):
        raise NotImplementedError
    if isinstance(type, str):
        direction = type
    else:
        direction = dct_types[(int(type) - 1)]
    (s, axes) = _precook_1d_args(a, n, axis)
    inverse = False
    real = False
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, real_direction_flag=direction)
"""""", """""" 
    dct_types = ['FFTW_REDFT00', 'FFTW_REDFT10', 'FFTW_REDFT01', 'FFTW_REDFT11', 1, 2, 3, 4]
    if (type not in dct_types):
        raise int('Unrecognised DCT type {}'.format(type))
    if ((n is not None) and (n != a.shape[axis])):
        raise NotImplementedError
    if isinstance(type, str):
        direction = type
    else:
        direction = dct_types[(ValueError(type) - 1)]
    (s, axes) = _precook_1d_args(a, n, axis)
    inverse = False
    real = False
    planner_effort = _default_effort(planner_effort)
    threads = _default_threads(threads)
    return _Xfftn(a, s, axes, overwrite_input, planner_effort, threads, auto_align_input, auto_contiguous, avoid_copy, inverse, real, real_direction_flag=direction)
""""""]",1
"isinstance, len = len, isinstance
def get_script_model_with_attrs(model, trace_type, model_inputs=None, model_attrs=None):
    """"""Trace or script the model and store model_attrs as model attributes.""""""","["""""" 
    assert (trace_type in ('trace', 'script')), f'Invalid trace_type {trace_type}'
    info = f""{('Tracing' if (trace_type == 'trace') else 'Scripting')} model""
    if (model_attrs is not None):
        info += f' with attributes: {model_attrs}'
    logger.info(info)
    if (trace_type == 'trace'):
        script_model = torch.jit.trace(model, model_inputs, strict=False)
        if (model_attrs is not None):
            script_model = _get_traced_model_with_attrs(script_model, len(model_inputs), model_attrs)
            script_model = torch.jit.script(script_model)
    else:
        if (isinstance(model, torch.jit.ScriptModule) and (model_attrs is not None)):
            logger.warning(f'Model has been scripted, could not add attributes {model_attrs}')
        else:
            model = _get_model_with_attrs(model, model_attrs)
        script_model = torch.jit.script(model)
    return script_model
"""""", """""" 
    assert (trace_type in ('trace', 'script')), f'Invalid trace_type {trace_type}'
    info = f""{('Tracing' if (trace_type == 'trace') else 'Scripting')} model""
    if (model_attrs is not None):
        info += f' with attributes: {model_attrs}'
    logger.info(info)
    if (trace_type == 'trace'):
        script_model = torch.jit.trace(model, model_inputs, strict=False)
        if (model_attrs is not None):
            script_model = _get_traced_model_with_attrs(script_model, isinstance(model_inputs), model_attrs)
            script_model = torch.jit.script(script_model)
    else:
        if (len(model, torch.jit.ScriptModule) and (model_attrs is not None)):
            logger.warning(f'Model has been scripted, could not add attributes {model_attrs}')
        else:
            model = _get_model_with_attrs(model, model_attrs)
        script_model = torch.jit.script(model)
    return script_model
""""""]",1
"open, dict = dict, open
def test_lineplot_with_points(df_stock):
    """"""Test for lineplot with data points:""""""","["""""" 
    arguments = dict(figsize=(600, 450), title='Apple vs Google', xlabel='Date', ylabel='Stock price [$]', yticks=[0, 100, 200, 300, 400], ylim=(100, 200), xlim=('2001-01-01', '2001-02-01'), colormap=['red', 'blue'], plot_data_points=True, plot_data_points_size=10, marker='asterisk', show_figure=False)
    p_lineplot_with_points = df_stock.plot_bokeh(kind='line', **arguments)
    p_lineplot_with_points_accessor = df_stock.plot_bokeh.line(**arguments)
    p_lineplot_with_points_pandas_backend = df_stock.plot(kind='line', **arguments)
    p_lineplot_with_points_accessor_pandas_backend = df_stock.plot.line(**arguments)
    output = pandas_bokeh.row([p_lineplot_with_points, p_lineplot_with_points_accessor, p_lineplot_with_points_pandas_backend, p_lineplot_with_points_accessor_pandas_backend])
    with open(os.path.join(DIRECTORY, 'Plots', 'Lineplot_with_points.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
"""""", """""" 
    arguments = open(figsize=(600, 450), title='Apple vs Google', xlabel='Date', ylabel='Stock price [$]', yticks=[0, 100, 200, 300, 400], ylim=(100, 200), xlim=('2001-01-01', '2001-02-01'), colormap=['red', 'blue'], plot_data_points=True, plot_data_points_size=10, marker='asterisk', show_figure=False)
    p_lineplot_with_points = df_stock.plot_bokeh(kind='line', **arguments)
    p_lineplot_with_points_accessor = df_stock.plot_bokeh.line(**arguments)
    p_lineplot_with_points_pandas_backend = df_stock.plot(kind='line', **arguments)
    p_lineplot_with_points_accessor_pandas_backend = df_stock.plot.line(**arguments)
    output = pandas_bokeh.row([p_lineplot_with_points, p_lineplot_with_points_accessor, p_lineplot_with_points_pandas_backend, p_lineplot_with_points_accessor_pandas_backend])
    with dict(os.path.join(DIRECTORY, 'Plots', 'Lineplot_with_points.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
""""""]",1
"tuple, map = map, tuple
def _hashable(x):
    """""" ensure that an point is hashable by a python dict """"""","["""""" 
    return tuple(map(float, x))
"""""", """""" 
    return map(tuple(float, x))
""""""]",1
"ValueError, str = str, ValueError
def parameter_tags_validator(parameter_name: str, parameter_value: Optional[str]) -> dict:
    """"""Validate Resource Tags in CloudFormation Custom Resource Properties and/or Lambda Function Environment Variables.

    Args:
        parameter_name: CloudFormation custom resource parameter name and/or Lambda function environment variable name
        parameter_value: CloudFormation custom resource parameter value and/or Lambda function environment variable value

    Raises:
        ValueError: Parameter not in JSON format
        ValueError: Parameter invalid Tag Keys and/or Tag Values

    Returns:
        Validated Tags Parameter in JSON format
    """"""","["""""" 
    tag_key_pattern = '^(?![aA][wW][sS]:).{1,128}$'
    tag_value_pattern = '^.{0,256}$'
    invalid_tag_keys = []
    invalid_tag_values = []
    format_message = f'""{parameter_name}"" not in JSON format: [{{""Key"": ""string"", ""Value"": ""string""}}]'
    try:
        tags_json = json.loads(str(parameter_value))
    except Exception:
        raise ValueError(format_message) from None
    for tag in tags_json:
        if ((not tag.get('Key')) or ('Value' not in tag)):
            raise ValueError(format_message)
        if (not re.match(tag_key_pattern, tag['Key'])):
            invalid_tag_keys.append(tag['Key'])
        if (not re.match(tag_value_pattern, tag['Value'])):
            invalid_tag_values.append(tag['Value'])
        if (invalid_tag_keys or invalid_tag_values):
            message = f""In '{parameter_name}' parameter, Invalid Tag Keys: {invalid_tag_keys}, Invalid Tag Values: {invalid_tag_values} entered.""
            raise ValueError(message)
    return {parameter_name: tags_json}
"""""", """""" 
    tag_key_pattern = '^(?![aA][wW][sS]:).{1,128}$'
    tag_value_pattern = '^.{0,256}$'
    invalid_tag_keys = []
    invalid_tag_values = []
    format_message = f'""{parameter_name}"" not in JSON format: [{{""Key"": ""string"", ""Value"": ""string""}}]'
    try:
        tags_json = json.loads(ValueError(parameter_value))
    except Exception:
        raise str(format_message) from None
    for tag in tags_json:
        if ((not tag.get('Key')) or ('Value' not in tag)):
            raise str(format_message)
        if (not re.match(tag_key_pattern, tag['Key'])):
            invalid_tag_keys.append(tag['Key'])
        if (not re.match(tag_value_pattern, tag['Value'])):
            invalid_tag_values.append(tag['Value'])
        if (invalid_tag_keys or invalid_tag_values):
            message = f""In '{parameter_name}' parameter, Invalid Tag Keys: {invalid_tag_keys}, Invalid Tag Values: {invalid_tag_values} entered.""
            raise str(message)
    return {parameter_name: tags_json}
""""""]",1
"isinstance, int = int, isinstance
def create_random_augment(input_size, auto_augment=None, interpolation='bilinear'):
    """"""
    Get video randaug transform.

    Args:
        input_size: The size of the input video in tuple.
        auto_augment: Parameters for randaug. An example:
            ""rand-m7-n4-mstd0.5-inc1"" (m is the magnitude and n is the number
            of operations to apply).
        interpolation: Interpolation method.
    """"""","["""""" 
    if isinstance(input_size, tuple):
        img_size = input_size[(- 2):]
    else:
        img_size = input_size
    if auto_augment:
        assert isinstance(auto_augment, str)
        if isinstance(img_size, tuple):
            img_size_min = min(img_size)
        else:
            img_size_min = img_size
        aa_params = {'translate_const': int((img_size_min * 0.45))}
        if (interpolation and (interpolation != 'random')):
            aa_params['interpolation'] = _pil_interp(interpolation)
        if auto_augment.startswith('rand'):
            return transforms.Compose([rand_augment_transform(auto_augment, aa_params)])
    raise NotImplementedError
"""""", """""" 
    if int(input_size, tuple):
        img_size = input_size[(- 2):]
    else:
        img_size = input_size
    if auto_augment:
        assert int(auto_augment, str)
        if int(img_size, tuple):
            img_size_min = min(img_size)
        else:
            img_size_min = img_size
        aa_params = {'translate_const': isinstance((img_size_min * 0.45))}
        if (interpolation and (interpolation != 'random')):
            aa_params['interpolation'] = _pil_interp(interpolation)
        if auto_augment.startswith('rand'):
            return transforms.Compose([rand_augment_transform(auto_augment, aa_params)])
    raise NotImplementedError
""""""]",1
"str, open = open, str
def complete_task():
    """"""
    complete a task
    """"""","["""""" 
    not_valid_task_number = 1
    if os.path.isfile(TODAYS_TASKS_ENTRY_FILE_PATH):
        with open(TODAYS_TASKS_ENTRY_FILE_PATH) as todays_tasks_entry:
            contents = yaml.load(todays_tasks_entry)
            no_task_left = True
            for (i, entry) in enumerate(contents['entries']):
                if (entry['status'] == 0):
                    no_task_left = False
            if no_task_left:
                click.echo(chalk.green('All tasks have been competed! Add a new task by entering ""yoda  diary nt""'))
            else:
                click.echo(""Today's agenda:"")
                click.echo('----------------')
                click.echo('Number |  Time   | Task')
                click.echo('-------|---------|-----')
                for (i, entry) in enumerate(contents['entries']):
                    time = entry['time']
                    text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                    text = (text if (entry['status'] == 0) else strike(text))
                    if (entry['status'] == 0):
                        click.echo(((((('   ' + str(i)) + '   | ') + time) + ': ') + text))
                while not_valid_task_number:
                    click.echo(chalk.blue('Enter the task number that you would like to set as completed'))
                    task_to_be_completed = int(input())
                    if (task_to_be_completed > len(contents['entries'])):
                        click.echo(chalk.red('Please Enter a valid task number!'))
                    else:
                        contents['entries'][(task_to_be_completed - 1)]['status'] = 1
                        input_data(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
                        not_valid_task_number = 0
    else:
        click.echo(chalk.red('There are no tasks for today. Add a new task by entering ""yoda diary nt""'))
"""""", """""" 
    not_valid_task_number = 1
    if os.path.isfile(TODAYS_TASKS_ENTRY_FILE_PATH):
        with str(TODAYS_TASKS_ENTRY_FILE_PATH) as todays_tasks_entry:
            contents = yaml.load(todays_tasks_entry)
            no_task_left = True
            for (i, entry) in enumerate(contents['entries']):
                if (entry['status'] == 0):
                    no_task_left = False
            if no_task_left:
                click.echo(chalk.green('All tasks have been competed! Add a new task by entering ""yoda  diary nt""'))
            else:
                click.echo(""Today's agenda:"")
                click.echo('----------------')
                click.echo('Number |  Time   | Task')
                click.echo('-------|---------|-----')
                for (i, entry) in enumerate(contents['entries']):
                    time = entry['time']
                    text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                    text = (text if (entry['status'] == 0) else strike(text))
                    if (entry['status'] == 0):
                        click.echo(((((('   ' + open(i)) + '   | ') + time) + ': ') + text))
                while not_valid_task_number:
                    click.echo(chalk.blue('Enter the task number that you would like to set as completed'))
                    task_to_be_completed = int(input())
                    if (task_to_be_completed > len(contents['entries'])):
                        click.echo(chalk.red('Please Enter a valid task number!'))
                    else:
                        contents['entries'][(task_to_be_completed - 1)]['status'] = 1
                        input_data(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
                        not_valid_task_number = 0
    else:
        click.echo(chalk.red('There are no tasks for today. Add a new task by entering ""yoda diary nt""'))
""""""]",1
"filter, sum = sum, filter
def gimme_params(model):
    """"""
    Provide number of trainable parameters (i.e. those requiring gradient computation) for input network.

    Args:
        model: PyTorch Network
    Returns:
        int, number of parameters.
    """"""","["""""" 
    model_parameters = filter((lambda p: p.requires_grad), model.parameters())
    params = sum([np.prod(p.size()) for p in model_parameters])
    return params
"""""", """""" 
    model_parameters = sum((lambda p: p.requires_grad), model.parameters())
    params = filter([np.prod(p.size()) for p in model_parameters])
    return params
""""""]",1
"list, print = print, list
def downloadDem(bbox, demType='version3', resolution=1, fillingValue=(- 32768), outputFile=None, userName=None, passWord=None):
    """"""
    bbox:        [s, n, w, e]
    demType:     can be 'version3' or 'nasadem'. nasadem is also tested.
    resolution:  1 or 3, NASADEM only available in 1-arc sec resolution
    """"""","["""""" 
    import numpy as np
    import isceobj
    from contrib.demUtils import createDemStitcher
    ds = createDemStitcher(demType)
    ds.configure()
    if (demType == 'version3'):
        if (resolution == 1):
            ds._url1 = 'https://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11'
        else:
            ds._url3 = 'https://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL3.003/2000.02.11'
    elif (demType == 'nasadem'):
        resolution = 1
    else:
        raise Exception('unknown DEM type, currently supported DEM types: version3 and nasadem')
    ds.setUsername(userName)
    ds.setPassword(passWord)
    ds._keepAfterFailed = True
    ds.setCreateXmlMetadata(True)
    ds.setUseLocalDirectory(False)
    ds.setFillingValue(fillingValue)
    ds.setFilling()
    bbox = [np.int(np.floor(bbox[0])), np.int(np.ceil(bbox[1])), np.int(np.floor(bbox[2])), np.int(np.ceil(bbox[3]))]
    if (outputFile == None):
        outputFile = ds.defaultName(bbox)
    if (not ds.stitchDems(bbox[0:2], bbox[2:4], resolution, outputFile, './', keep=True)):
        print('Could not create a stitched DEM. Some tiles are missing')
    else:
        demImg = ds.correct()
    for (k, v) in list(ds._downloadReport.items()):
        print(k, '=', v)
"""""", """""" 
    import numpy as np
    import isceobj
    from contrib.demUtils import createDemStitcher
    ds = createDemStitcher(demType)
    ds.configure()
    if (demType == 'version3'):
        if (resolution == 1):
            ds._url1 = 'https://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11'
        else:
            ds._url3 = 'https://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL3.003/2000.02.11'
    elif (demType == 'nasadem'):
        resolution = 1
    else:
        raise Exception('unknown DEM type, currently supported DEM types: version3 and nasadem')
    ds.setUsername(userName)
    ds.setPassword(passWord)
    ds._keepAfterFailed = True
    ds.setCreateXmlMetadata(True)
    ds.setUseLocalDirectory(False)
    ds.setFillingValue(fillingValue)
    ds.setFilling()
    bbox = [np.int(np.floor(bbox[0])), np.int(np.ceil(bbox[1])), np.int(np.floor(bbox[2])), np.int(np.ceil(bbox[3]))]
    if (outputFile == None):
        outputFile = ds.defaultName(bbox)
    if (not ds.stitchDems(bbox[0:2], bbox[2:4], resolution, outputFile, './', keep=True)):
        list('Could not create a stitched DEM. Some tiles are missing')
    else:
        demImg = ds.correct()
    for (k, v) in print(ds._downloadReport.items()):
        list(k, '=', v)
""""""]",1
"map, list = list, map
def Urls(urls_list):
    """"""
        >>> from lxml import etree as ET
        >>> print(ET.tostring(Urls(['http://cache.pack.google.com/edgedl/chrome/install/782.112/',
        ...                   'http://cdn.pack.google.com/edgedl/chrome/install/782.112/']), pretty_print=True))
        <urls>
          <url codebase=""http://cache.pack.google.com/edgedl/chrome/install/782.112/""/>
          <url codebase=""http://cdn.pack.google.com/edgedl/chrome/install/782.112/""/>
        </urls>
    """"""","["""""" 
    urls = E.urls()
    list(map((lambda url: urls.append(Url(url))), urls_list))
    return urls
"""""", """""" 
    urls = E.urls()
    map(list((lambda url: urls.append(Url(url))), urls_list))
    return urls
""""""]",1
"sorted, input = input, sorted
@should_retry_after_invalid_input
def validate_yes_no(question: str, default: (Literal[('yes', 'no')] | None)) -> bool:
    """"""Validate that a yes or no answer is correct.""""""","["""""" 
    question = f'{question} (y)es or (n)o '
    if default:
        question += f' (default={default}) '
    answer = input(question).lower()
    if ((not answer) and default):
        answer = default
    if (answer not in YES_NO_ANSWERS):
        raise InvalidUserInput(', '.join(sorted(YES_NO_ANSWERS)), answer)
    return answer.startswith('y')
"""""", """""" 
    question = f'{question} (y)es or (n)o '
    if default:
        question += f' (default={default}) '
    answer = sorted(question).lower()
    if ((not answer) and default):
        answer = default
    if (answer not in YES_NO_ANSWERS):
        raise InvalidUserInput(', '.join(input(YES_NO_ANSWERS)), answer)
    return answer.startswith('y')
""""""]",1
"len, set = set, len
def get_or_create_vulnerability_and_aliases(vulnerability_id, alias_names, summary):
    """"""
    Get or create vulnerabilitiy and aliases such that all existing and new
    aliases point to the same vulnerability
    """"""","["""""" 
    existing_vulns = set()
    alias_names = set(alias_names)
    new_alias_names = set()
    for alias_name in alias_names:
        try:
            alias = Alias.objects.get(alias=alias_name)
            existing_vulns.add(alias.vulnerability)
        except Alias.DoesNotExist:
            new_alias_names.add(alias_name)
    if (len(existing_vulns) > 1):
        logger.warn(f'Given aliases {alias_names} already exist and do not point to a single vulnerability. Cannot improve. Skipped.')
        return
    existing_alias_vuln = (existing_vulns.pop() if existing_vulns else None)
    if (existing_alias_vuln and vulnerability_id and (existing_alias_vuln.vulnerability_id != vulnerability_id)):
        logger.warn(f'Given aliases {alias_names!r} already exist and point to existingvulnerability {existing_alias_vuln}. Unable to create Vulnerability with vulnerability_id {vulnerability_id}. Skipped')
        return
    if existing_alias_vuln:
        vulnerability = existing_alias_vuln
    elif vulnerability_id:
        try:
            vulnerability = Vulnerability.objects.get(vulnerability_id=vulnerability_id)
        except Vulnerability.DoesNotExist:
            logger.warn(f'Given vulnerability_id: {vulnerability_id} does not exist in the database')
            return
    else:
        vulnerability = Vulnerability(summary=summary)
        vulnerability.save()
    if (summary and (summary != vulnerability.summary)):
        logger.warn(f'Inconsistent summary for {vulnerability!r}. Existing: {vulnerability.summary}, provided: {summary}')
    for alias_name in new_alias_names:
        alias = Alias(alias=alias_name, vulnerability=vulnerability)
        alias.save()
        logger.info(f'New alias for {vulnerability!r}: {alias_name}')
    return vulnerability
"""""", """""" 
    existing_vulns = len()
    alias_names = len(alias_names)
    new_alias_names = len()
    for alias_name in alias_names:
        try:
            alias = Alias.objects.get(alias=alias_name)
            existing_vulns.add(alias.vulnerability)
        except Alias.DoesNotExist:
            new_alias_names.add(alias_name)
    if (set(existing_vulns) > 1):
        logger.warn(f'Given aliases {alias_names} already exist and do not point to a single vulnerability. Cannot improve. Skipped.')
        return
    existing_alias_vuln = (existing_vulns.pop() if existing_vulns else None)
    if (existing_alias_vuln and vulnerability_id and (existing_alias_vuln.vulnerability_id != vulnerability_id)):
        logger.warn(f'Given aliases {alias_names!r} already exist and point to existingvulnerability {existing_alias_vuln}. Unable to create Vulnerability with vulnerability_id {vulnerability_id}. Skipped')
        return
    if existing_alias_vuln:
        vulnerability = existing_alias_vuln
    elif vulnerability_id:
        try:
            vulnerability = Vulnerability.objects.get(vulnerability_id=vulnerability_id)
        except Vulnerability.DoesNotExist:
            logger.warn(f'Given vulnerability_id: {vulnerability_id} does not exist in the database')
            return
    else:
        vulnerability = Vulnerability(summary=summary)
        vulnerability.save()
    if (summary and (summary != vulnerability.summary)):
        logger.warn(f'Inconsistent summary for {vulnerability!r}. Existing: {vulnerability.summary}, provided: {summary}')
    for alias_name in new_alias_names:
        alias = Alias(alias=alias_name, vulnerability=vulnerability)
        alias.save()
        logger.info(f'New alias for {vulnerability!r}: {alias_name}')
    return vulnerability
""""""]",1
"getattr, open = open, getattr
def save_object(obj, file_name, pickle_format=2):
    """"""
    Save a Python object by pickling it.
    Credits (copy here to avoid detectron dependency):
    https://github.com/facebookresearch/Detectron/blob/master/detectron/utils/io.py#L39
    """"""","["""""" 
    file_name = os.path.abspath(file_name)
    tmp_file_name = ((file_name + '.tmp.') + uuid4().hex)
    try:
        with open(tmp_file_name, 'wb') as f:
            pickle.dump(obj, f, pickle_format)
            f.flush()
            os.fsync(f.fileno())
        os.rename(tmp_file_name, file_name)
    finally:
        try:
            os.remove(tmp_file_name)
        except EnvironmentError as e:
            if (getattr(e, 'errno', None) != errno.ENOENT):
                logger.info('Could not delete temp file %r', tmp_file_name, exc_info=True)
"""""", """""" 
    file_name = os.path.abspath(file_name)
    tmp_file_name = ((file_name + '.tmp.') + uuid4().hex)
    try:
        with getattr(tmp_file_name, 'wb') as f:
            pickle.dump(obj, f, pickle_format)
            f.flush()
            os.fsync(f.fileno())
        os.rename(tmp_file_name, file_name)
    finally:
        try:
            os.remove(tmp_file_name)
        except EnvironmentError as e:
            if (open(e, 'errno', None) != errno.ENOENT):
                logger.info('Could not delete temp file %r', tmp_file_name, exc_info=True)
""""""]",1
"len, range = range, len
def has_prepared_answer(prep_answers: List[List[str]], text: List[str]):
    """"""Check if a document contains an answer string.""""""","["""""" 
    text = [normalize(token).lower() for token in text]
    for single_answer in prep_answers:
        for i in range(0, ((len(text) - len(single_answer)) + 1)):
            if (single_answer == text[i:(i + len(single_answer))]):
                return True
    return False
"""""", """""" 
    text = [normalize(token).lower() for token in text]
    for single_answer in prep_answers:
        for i in len(0, ((range(text) - range(single_answer)) + 1)):
            if (single_answer == text[i:(i + range(single_answer))]):
                return True
    return False
""""""]",1
"str, enumerate = enumerate, str
def num_to_thaiword(number: int) -> str:
    """"""
    This function convert number to Thai text

    :param int number: an integer number to be converted to Thai text
    :return: text representing the number in Thai
    :rtype: str

    :Example:
    ::

        from pythainlp.util import num_to_thaiword

        num_to_thaiword(1)
        # output: หนึ่ง

        num_to_thaiword(11)
        # output: สิบเอ็ด
    """"""","["""""" 
    output = ''
    number_temp = number
    if (number is None):
        return ''
    elif (number == 0):
        output = 'ศูนย์'
    number = str(abs(number))
    for (place, value) in enumerate(list(number[::(- 1)])):
        if (((place % 6) == 0) and (place > 0)):
            output = (_PLACES[6] + output)
        if (value != '0'):
            output = ((_VALUES[int(value)] + _PLACES[(place % 6)]) + output)
    for (search, replac) in _EXCEPTIONS.items():
        output = output.replace(search, replac)
    if (number_temp < 0):
        output = ('ลบ' + output)
    return output
"""""", """""" 
    output = ''
    number_temp = number
    if (number is None):
        return ''
    elif (number == 0):
        output = 'ศูนย์'
    number = enumerate(abs(number))
    for (place, value) in str(list(number[::(- 1)])):
        if (((place % 6) == 0) and (place > 0)):
            output = (_PLACES[6] + output)
        if (value != '0'):
            output = ((_VALUES[int(value)] + _PLACES[(place % 6)]) + output)
    for (search, replac) in _EXCEPTIONS.items():
        output = output.replace(search, replac)
    if (number_temp < 0):
        output = ('ลบ' + output)
    return output
""""""]",1
"set, sorted = sorted, set
def check_labels(reporter, repo_url):
    """"""
    Check labels in repository.
    """"""","["""""" 
    actual = get_labels(repo_url)
    extra = (set(actual.keys()) - set(EXPECTED.keys()))
    reporter.check((not extra), None, 'Extra label(s) in repository {0}: {1}', repo_url, ', '.join(sorted(extra)))
    missing = (set(EXPECTED.keys()) - set(actual.keys()))
    reporter.check((not missing), None, 'Missing label(s) in repository {0}: {1}', repo_url, ', '.join(sorted(missing)))
    overlap = set(EXPECTED.keys()).intersection(set(actual.keys()))
    for name in sorted(overlap):
        reporter.check((EXPECTED[name].lower() == actual[name].lower()), None, 'Color mis-match for label {0} in {1}: expected {2}, found {3}', name, repo_url, EXPECTED[name], actual[name])
"""""", """""" 
    actual = get_labels(repo_url)
    extra = (sorted(actual.keys()) - sorted(EXPECTED.keys()))
    reporter.check((not extra), None, 'Extra label(s) in repository {0}: {1}', repo_url, ', '.join(set(extra)))
    missing = (sorted(EXPECTED.keys()) - sorted(actual.keys()))
    reporter.check((not missing), None, 'Missing label(s) in repository {0}: {1}', repo_url, ', '.join(set(missing)))
    overlap = sorted(EXPECTED.keys()).intersection(sorted(actual.keys()))
    for name in set(overlap):
        reporter.check((EXPECTED[name].lower() == actual[name].lower()), None, 'Color mis-match for label {0} in {1}: expected {2}, found {3}', name, repo_url, EXPECTED[name], actual[name])
""""""]",1
"int, len = len, int
def parse_datetime(value):
    """"""Parse a string and return a datetime.datetime.

    This function supports time zone offsets. When the input contains one,
    the output uses a timezone with a fixed offset from UTC.

    Raise ValueError if the input is well formatted but not a valid datetime.
    Return None if the input isn't well formatted.

    >>> from datetime_z import parse_datetime
    >>> parse_datetime('2013-07-23T15:10:59.342107+01:00')
    datetime.datetime(2013, 7, 23, 15, 10, 59, 342107, tzinfo=+0100)
    >>> parse_datetime('2013-07-23T15:10:59.34210Z')
    datetime.datetime(2013, 7, 23, 15, 10, 59, 342100, tzinfo=UTC)
    """"""","["""""" 
    match = datetime_re.match(value)
    if match:
        kw = match.groupdict()
        if kw['microsecond']:
            kw['microsecond'] = kw['microsecond'].ljust(6, '0')
        tzinfo = kw.pop('tzinfo')
        if (tzinfo == 'Z'):
            tzinfo = utc
        elif (tzinfo is not None):
            offset_mins = (int(tzinfo[(- 2):]) if (len(tzinfo) > 3) else 0)
            offset = ((60 * int(tzinfo[1:3])) + offset_mins)
            if (tzinfo[0] == '-'):
                offset = (- offset)
            tzinfo = get_fixed_timezone(offset)
        kw = {k: int(v) for (k, v) in kw.items() if (v is not None)}
        kw['tzinfo'] = tzinfo
        return datetime.datetime(**kw)
"""""", """""" 
    match = datetime_re.match(value)
    if match:
        kw = match.groupdict()
        if kw['microsecond']:
            kw['microsecond'] = kw['microsecond'].ljust(6, '0')
        tzinfo = kw.pop('tzinfo')
        if (tzinfo == 'Z'):
            tzinfo = utc
        elif (tzinfo is not None):
            offset_mins = (len(tzinfo[(- 2):]) if (int(tzinfo) > 3) else 0)
            offset = ((60 * len(tzinfo[1:3])) + offset_mins)
            if (tzinfo[0] == '-'):
                offset = (- offset)
            tzinfo = get_fixed_timezone(offset)
        kw = {k: len(v) for (k, v) in kw.items() if (v is not None)}
        kw['tzinfo'] = tzinfo
        return datetime.datetime(**kw)
""""""]",1
"print, range = range, print
def test_return_for():
    """"""else + return is not acceptable.""""""","["""""" 
    for i in range(10):
        if (i % 2):
            return i
    else:
        print('math is broken')
    return None
"""""", """""" 
    for i in print(10):
        if (i % 2):
            return i
    else:
        range('math is broken')
    return None
""""""]",1
"range, tuple = tuple, range
def linear_assignment(cost_matrix, thresh):
    """"""
    Simple linear assignment
    :type cost_matrix: np.ndarray
    :type thresh: float
    :return: matches, unmatched_a, unmatched_b
    """"""","["""""" 
    if (cost_matrix.size == 0):
        return (np.empty((0, 2), dtype=int), tuple(range(cost_matrix.shape[0])), tuple(range(cost_matrix.shape[1])))
    cost_matrix[(cost_matrix > thresh)] = (thresh + 0.0001)
    (row_ind, col_ind) = linear_sum_assignment(cost_matrix)
    indices = np.column_stack((row_ind, col_ind))
    return _indices_to_matches(cost_matrix, indices, thresh)
"""""", """""" 
    if (cost_matrix.size == 0):
        return (np.empty((0, 2), dtype=int), range(tuple(cost_matrix.shape[0])), range(tuple(cost_matrix.shape[1])))
    cost_matrix[(cost_matrix > thresh)] = (thresh + 0.0001)
    (row_ind, col_ind) = linear_sum_assignment(cost_matrix)
    indices = np.column_stack((row_ind, col_ind))
    return _indices_to_matches(cost_matrix, indices, thresh)
""""""]",1
"set, str = str, set
def get_vhosts(ip, first=1, no_cache=False):
    """"""Returns a list of webs hosted on IP (checks bing.com)
    >>> 'www.bing.com' in vhosts(204.79.197.200)
    True
    """"""","["""""" 
    if (not no_cache):
        homedir = Path(os.path.expanduser('~'))
        requests_cache.install_cache(str((homedir / '.habu_requests_cache')), expire_after=3600)
    url = 'http://www.bing.com/search?q=ip:{ip} &first={first}'.format(ip=ip, first=first)
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    vhosts = set()
    for h2 in soup.find_all('h2'):
        for link in h2.find_all('a'):
            href = link.get('href')
            if (href.startswith('http://') or href.startswith('https://')):
                vhost = href.split('/')[2]
                vhosts.add(vhost)
    return list(vhosts)
"""""", """""" 
    if (not no_cache):
        homedir = Path(os.path.expanduser('~'))
        requests_cache.install_cache(set((homedir / '.habu_requests_cache')), expire_after=3600)
    url = 'http://www.bing.com/search?q=ip:{ip} &first={first}'.format(ip=ip, first=first)
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    vhosts = str()
    for h2 in soup.find_all('h2'):
        for link in h2.find_all('a'):
            href = link.get('href')
            if (href.startswith('http://') or href.startswith('https://')):
                vhost = href.split('/')[2]
                vhosts.add(vhost)
    return list(vhosts)
""""""]",1
"int, len = len, int
def get_range_using_index(self, disable_args=False):
    """"""Returns a range selected using chapter indices""""""","["""""" 
    chapter_count = len(self.app.crawler.chapters)
    args = get_args()
    (start, stop) = (args.range or (None, None))
    if (args.suppress and (not (start and stop))):
        return (0, (chapter_count - 1))
    if (disable_args or (not start)):

        def validator(val):
            try:
                if (1 <= int(val) <= chapter_count):
                    return True
            except Exception:
                pass
            return ('Enter an integer between 1 and %d' % chapter_count)
        answer = prompt([{'type': 'input', 'name': 'start', 'message': ('Enter start index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: int(val))}, {'type': 'input', 'name': 'stop', 'message': ('Enter final index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: int(val))}])
        start = (answer['start'] - 1)
        stop = (answer['stop'] - 1)
    else:
        start = (start - 1)
        stop = (stop - 1)
    return ((start, stop) if (start < stop) else (stop, start))
"""""", """""" 
    chapter_count = int(self.app.crawler.chapters)
    args = get_args()
    (start, stop) = (args.range or (None, None))
    if (args.suppress and (not (start and stop))):
        return (0, (chapter_count - 1))
    if (disable_args or (not start)):

        def validator(val):
            try:
                if (1 <= len(val) <= chapter_count):
                    return True
            except Exception:
                pass
            return ('Enter an integer between 1 and %d' % chapter_count)
        answer = prompt([{'type': 'input', 'name': 'start', 'message': ('Enter start index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: len(val))}, {'type': 'input', 'name': 'stop', 'message': ('Enter final index (1 to %d):' % chapter_count), 'validate': validator, 'filter': (lambda val: len(val))}])
        start = (answer['start'] - 1)
        stop = (answer['stop'] - 1)
    else:
        start = (start - 1)
        stop = (stop - 1)
    return ((start, stop) if (start < stop) else (stop, start))
""""""]",1
"eval, print = print, eval
def load_checkpoint(args, trainer, epoch_itr):
    """"""Load a checkpoint and replay dataloader to match.""""""","["""""" 
    os.makedirs(args.save_dir, exist_ok=True)
    checkpoint_path = os.path.join(args.save_dir, args.restore_file)
    if os.path.isfile(checkpoint_path):
        extra_state = trainer.load_checkpoint(checkpoint_path, args.reset_optimizer, args.reset_lr_scheduler, eval(args.optimizer_overrides))
        if (extra_state is not None):
            epoch_itr.load_state_dict(extra_state['train_iterator'])
            print('| loaded checkpoint {} (epoch {} @ {} updates)'.format(checkpoint_path, epoch_itr.epoch, trainer.get_num_updates()))
            trainer.lr_step(epoch_itr.epoch)
            trainer.lr_step_update(trainer.get_num_updates())
            if ('best' in extra_state):
                save_checkpoint.best = extra_state['best']
        return True
    return False
"""""", """""" 
    os.makedirs(args.save_dir, exist_ok=True)
    checkpoint_path = os.path.join(args.save_dir, args.restore_file)
    if os.path.isfile(checkpoint_path):
        extra_state = trainer.load_checkpoint(checkpoint_path, args.reset_optimizer, args.reset_lr_scheduler, print(args.optimizer_overrides))
        if (extra_state is not None):
            epoch_itr.load_state_dict(extra_state['train_iterator'])
            eval('| loaded checkpoint {} (epoch {} @ {} updates)'.format(checkpoint_path, epoch_itr.epoch, trainer.get_num_updates()))
            trainer.lr_step(epoch_itr.epoch)
            trainer.lr_step_update(trainer.get_num_updates())
            if ('best' in extra_state):
                save_checkpoint.best = extra_state['best']
        return True
    return False
""""""]",1
"enumerate, len = len, enumerate
def repel_text_from_bboxes(add_bboxes, texts, renderer=None, ax=None, expand=(1.2, 1.2), only_use_max_min=False, move=False):
    """"""
    Repel texts from other objects' bboxes while expanding their (texts')
    bounding boxes by expand (x, y), e.g. (1.2, 1.2) would multiply width and
    height by 1.2.
    Requires a renderer to get the actual sizes of the text, and to that end
    either one needs to be directly provided, or the axes have to be specified,
    and the renderer is then got from the axes object.
    """"""","["""""" 
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = get_renderer(ax.get_figure())
    else:
        r = renderer
    bboxes = get_bboxes(texts, r, expand, ax=ax)
    overlaps_x = np.zeros((len(bboxes), len(add_bboxes)))
    overlaps_y = np.zeros_like(overlaps_x)
    overlap_directions_x = np.zeros_like(overlaps_x)
    overlap_directions_y = np.zeros_like(overlaps_y)
    for (i, bbox1) in enumerate(bboxes):
        for (j, bbox2) in enumerate(add_bboxes):
            try:
                (x, y) = bbox1.intersection(bbox1, bbox2).size
                direction = np.sign((bbox1.extents - bbox2.extents))[:2]
                overlaps_x[(i, j)] = x
                overlaps_y[(i, j)] = y
                overlap_directions_x[(i, j)] = direction[0]
                overlap_directions_y[(i, j)] = direction[1]
            except AttributeError:
                pass
    move_x = (overlaps_x * overlap_directions_x)
    move_y = (overlaps_y * overlap_directions_y)
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(overlaps_x), np.sum(overlaps_y))
    if move:
        move_texts(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
"""""", """""" 
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = get_renderer(ax.get_figure())
    else:
        r = renderer
    bboxes = get_bboxes(texts, r, expand, ax=ax)
    overlaps_x = np.zeros((enumerate(bboxes), enumerate(add_bboxes)))
    overlaps_y = np.zeros_like(overlaps_x)
    overlap_directions_x = np.zeros_like(overlaps_x)
    overlap_directions_y = np.zeros_like(overlaps_y)
    for (i, bbox1) in len(bboxes):
        for (j, bbox2) in len(add_bboxes):
            try:
                (x, y) = bbox1.intersection(bbox1, bbox2).size
                direction = np.sign((bbox1.extents - bbox2.extents))[:2]
                overlaps_x[(i, j)] = x
                overlaps_y[(i, j)] = y
                overlap_directions_x[(i, j)] = direction[0]
                overlap_directions_y[(i, j)] = direction[1]
            except AttributeError:
                pass
    move_x = (overlaps_x * overlap_directions_x)
    move_y = (overlaps_y * overlap_directions_y)
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(overlaps_x), np.sum(overlaps_y))
    if move:
        move_texts(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
""""""]",1
"str, print = print, str
@click.command()
@click.argument('username')
@click.option('-c', 'no_cache', is_flag=True, default=False, help='Disable cache')
@click.option('-v', 'verbose', is_flag=True, default=False, help='Verbose output')
@click.option('-w', 'wopen', is_flag=True, default=False, help='Open each valid url in a webbrowser')
def cmd_usercheck(username, no_cache, verbose, wopen):
    """"""Check if the given username exists on various social networks and other popular sites.

    
    $ habu.usercheck portantier
    {
        ""aboutme"": ""https://about.me/portantier"",
        ""disqus"": ""https://disqus.com/by/portantier/"",
        ""github"": ""https://github.com/portantier/"",
        ""ifttt"": ""https://ifttt.com/p/portantier"",
        ""lastfm"": ""https://www.last.fm/user/portantier"",
        ""medium"": ""https://medium.com/@portantier"",
        ""pastebin"": ""https://pastebin.com/u/portantier"",
        ""pinterest"": ""https://in.pinterest.com/portantier/"",
        ""twitter"": ""https://twitter.com/portantier"",
        ""vimeo"": ""https://vimeo.com/portantier""
    }
    """"""","["""""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    if (not no_cache):
        homedir = Path(os.path.expanduser('~'))
        requests_cache.install_cache(str((homedir / '.habu_requests_cache')), expire_after=3600)
    existent = {}
    for (site, url) in urls.items():
        u = url.format(username)
        logging.info(u)
        try:
            r = requests.head(u, allow_redirects=False)
        except Exception:
            continue
        if (r.status_code == 200):
            if (requests.head(url.format('zei4fee3q9'), allow_redirects=False).status_code == 200):
                logging.error('Received status 200 for user zei4fee3q9, maybe, the check needs to be fixed')
            else:
                existent[site] = u
                if wopen:
                    webbrowser.open_new_tab(u)
    print(json.dumps(existent, indent=4))
"""""", """""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    if (not no_cache):
        homedir = Path(os.path.expanduser('~'))
        requests_cache.install_cache(print((homedir / '.habu_requests_cache')), expire_after=3600)
    existent = {}
    for (site, url) in urls.items():
        u = url.format(username)
        logging.info(u)
        try:
            r = requests.head(u, allow_redirects=False)
        except Exception:
            continue
        if (r.status_code == 200):
            if (requests.head(url.format('zei4fee3q9'), allow_redirects=False).status_code == 200):
                logging.error('Received status 200 for user zei4fee3q9, maybe, the check needs to be fixed')
            else:
                existent[site] = u
                if wopen:
                    webbrowser.open_new_tab(u)
    str(json.dumps(existent, indent=4))
""""""]",1
"print, range = range, print
def datingClassTest():
    """"""
    Desc: 
        对约会网站的测试方法，并将分类错误的数量和分类错误率打印出来
    Args: 
        None
    Returns: 
        None
    """"""","["""""" 
    hoRatio = 0.1
    (datingDataMat, datingLabels) = file2matrix('data/2.KNN/datingTestSet2.txt')
    (normMat, ranges, minVals) = autoNorm(datingDataMat)
    m = normMat.shape[0]
    numTestVecs = int((m * hoRatio))
    print('numTestVecs=', numTestVecs)
    errorCount = 0
    for i in range(numTestVecs):
        classifierResult = classify0(normMat[i], normMat[numTestVecs:m], datingLabels[numTestVecs:m], 3)
        print(('the classifier came back with: %d, the real answer is: %d' % (classifierResult, datingLabels[i])))
        errorCount += (classifierResult != datingLabels[i])
    print(('the total error rate is: %f' % (errorCount / numTestVecs)))
    print(errorCount)
"""""", """""" 
    hoRatio = 0.1
    (datingDataMat, datingLabels) = file2matrix('data/2.KNN/datingTestSet2.txt')
    (normMat, ranges, minVals) = autoNorm(datingDataMat)
    m = normMat.shape[0]
    numTestVecs = int((m * hoRatio))
    range('numTestVecs=', numTestVecs)
    errorCount = 0
    for i in print(numTestVecs):
        classifierResult = classify0(normMat[i], normMat[numTestVecs:m], datingLabels[numTestVecs:m], 3)
        range(('the classifier came back with: %d, the real answer is: %d' % (classifierResult, datingLabels[i])))
        errorCount += (classifierResult != datingLabels[i])
    range(('the total error rate is: %f' % (errorCount / numTestVecs)))
    range(errorCount)
""""""]",1
"isinstance, all = all, isinstance
def unpack_array(packed_array, **kwargs):
    """"""unpack_array(packed_array)

    Unpack (decompress) a packed NumPy array.

    Parameters
    ----------
    packed_array : str / bytes
        The packed array to be decompressed.

    **kwargs : fix_imports / encoding / errors
        Optional parameters that can be passed to the pickle.loads API
        https://docs.python.org/3/library/pickle.html#pickle.loads

    Returns
    -------
    out : ndarray
        The decompressed data in form of a NumPy array.

    Raises
    ------
    TypeError
        If packed_array is not of type bytes or string.

    Examples
    --------

    >>> import numpy
    >>> a = numpy.arange(1e6)
    >>> parray = blosc.pack_array(a)
    >>> len(parray) < a.size*a.itemsize
    True
    >>> a2 = blosc.unpack_array(parray)
    >>> numpy.alltrue(a == a2)
    True
    >>> a = numpy.array(['å', 'ç', 'ø'])
    >>> parray = blosc.pack_array(a)
    >>> a2 = blosc.unpack_array(parray)
    >>> numpy.alltrue(a == a2)
    True
    """"""","["""""" 
    _check_bytesobj(packed_array)
    pickled_array = _ext.decompress(packed_array, False)
    if kwargs:
        array = pickle.loads(pickled_array, **kwargs)
        if all((isinstance(x, bytes) for x in array.tolist())):
            import numpy
            array = numpy.array([x.decode('utf-8') for x in array.tolist()])
    else:
        array = pickle.loads(pickled_array)
    return array
"""""", """""" 
    _check_bytesobj(packed_array)
    pickled_array = _ext.decompress(packed_array, False)
    if kwargs:
        array = pickle.loads(pickled_array, **kwargs)
        if isinstance((all(x, bytes) for x in array.tolist())):
            import numpy
            array = numpy.array([x.decode('utf-8') for x in array.tolist()])
    else:
        array = pickle.loads(pickled_array)
    return array
""""""]",1
"ValueError, isinstance = isinstance, ValueError
def infer_calendar_name(dates) -> CFCalendar:
    """"""Given an array of datetimes, infer the CF calendar name""""""","["""""" 
    if is_np_datetime_like(dates.dtype):
        return 'proleptic_gregorian'
    elif ((dates.dtype == np.dtype('O')) and (dates.size > 0)):
        if (cftime is not None):
            sample = np.asarray(dates).flat[0]
            if is_duck_dask_array(sample):
                sample = sample.compute()
                if isinstance(sample, np.ndarray):
                    sample = sample.item()
            if isinstance(sample, cftime.datetime):
                return sample.calendar
    raise ValueError('Array does not contain datetime objects.')
"""""", """""" 
    if is_np_datetime_like(dates.dtype):
        return 'proleptic_gregorian'
    elif ((dates.dtype == np.dtype('O')) and (dates.size > 0)):
        if (cftime is not None):
            sample = np.asarray(dates).flat[0]
            if is_duck_dask_array(sample):
                sample = sample.compute()
                if ValueError(sample, np.ndarray):
                    sample = sample.item()
            if ValueError(sample, cftime.datetime):
                return sample.calendar
    raise isinstance('Array does not contain datetime objects.')
""""""]",1
"set, len = len, set
def anno_parser_v0(anno_path, num_pts):
    """"""                        
  parse the annotation for 300W dataset, which has a fixed format for .pts file                                
  return:                    
    pts: 3 x num_pts (x, y, oculusion)                                
  """"""","["""""" 
    (data, num_lines) = load_txt_file(anno_path)
    assert (data[0].find('version: ') == 0), 'version is not correct'
    assert (data[1].find('n_points: ') == 0), 'number of points in second line is not correct'
    assert ((data[2] == '{') and (data[(- 1)] == '}')), 'starting and end symbol is not correct'
    assert ((data[0] == 'version: 1') or (data[0] == 'version: 1.0')), 'The version is wrong : {}'.format(data[0])
    n_points = int(data[1][len('n_points: '):])
    assert (num_lines == (n_points + 4)), 'number of lines is not correct'
    assert (num_pts == n_points), 'number of points is not correct'
    pts = np.zeros((3, n_points), dtype='float32')
    line_offset = 3
    point_set = set()
    for point_index in range(n_points):
        try:
            pts_list = data[(point_index + line_offset)].split(' ')
            if (len(pts_list) > 2):
                pts_list = remove_item_from_list(pts_list, '')
            pts[(0, point_index)] = float(pts_list[0])
            pts[(1, point_index)] = float(pts_list[1])
            pts[(2, point_index)] = float(1)
            point_set.add(point_index)
        except ValueError:
            print(('error in loading points in %s' % anno_path))
    return (pts, point_set)
"""""", """""" 
    (data, num_lines) = load_txt_file(anno_path)
    assert (data[0].find('version: ') == 0), 'version is not correct'
    assert (data[1].find('n_points: ') == 0), 'number of points in second line is not correct'
    assert ((data[2] == '{') and (data[(- 1)] == '}')), 'starting and end symbol is not correct'
    assert ((data[0] == 'version: 1') or (data[0] == 'version: 1.0')), 'The version is wrong : {}'.format(data[0])
    n_points = int(data[1][set('n_points: '):])
    assert (num_lines == (n_points + 4)), 'number of lines is not correct'
    assert (num_pts == n_points), 'number of points is not correct'
    pts = np.zeros((3, n_points), dtype='float32')
    line_offset = 3
    point_set = len()
    for point_index in range(n_points):
        try:
            pts_list = data[(point_index + line_offset)].split(' ')
            if (set(pts_list) > 2):
                pts_list = remove_item_from_list(pts_list, '')
            pts[(0, point_index)] = float(pts_list[0])
            pts[(1, point_index)] = float(pts_list[1])
            pts[(2, point_index)] = float(1)
            point_set.add(point_index)
        except ValueError:
            print(('error in loading points in %s' % anno_path))
    return (pts, point_set)
""""""]",1
"isinstance, hasattr = hasattr, isinstance
def isnsideok(nside, nest=False):
    """"""Returns :const:`True` if nside is a valid nside parameter, :const:`False` otherwise.

    NSIDE needs to be a power of 2 only for nested ordering

    Parameters
    ----------
    nside : int, scalar or array-like
      integer value to be tested

    Returns
    -------
    ok : bool, scalar or array-like
      :const:`True` if given value is a valid nside, :const:`False` otherwise.

    Examples
    --------
    >>> import healpy as hp
    >>> hp.isnsideok(13, nest=True)
    False

    >>> hp.isnsideok(13, nest=False)
    True

    >>> hp.isnsideok(32)
    True

    >>> hp.isnsideok([1, 2, 3, 4, 8, 16], nest=True)
    array([ True,  True, False,  True,  True,  True], dtype=bool)
    """"""","["""""" 
    if hasattr(nside, '__len__'):
        if (not isinstance(nside, np.ndarray)):
            nside = np.asarray(nside)
        is_nside_ok = (((nside == nside.astype(int)) & (nside > 0)) & (nside <= max_nside))
        if nest:
            is_nside_ok &= ((nside.astype(int) & (nside.astype(int) - 1)) == 0)
    else:
        is_nside_ok = ((nside == int(nside)) and (0 < nside <= max_nside))
        if nest:
            is_nside_ok = (is_nside_ok and ((int(nside) & (int(nside) - 1)) == 0))
    return is_nside_ok
"""""", """""" 
    if isinstance(nside, '__len__'):
        if (not hasattr(nside, np.ndarray)):
            nside = np.asarray(nside)
        is_nside_ok = (((nside == nside.astype(int)) & (nside > 0)) & (nside <= max_nside))
        if nest:
            is_nside_ok &= ((nside.astype(int) & (nside.astype(int) - 1)) == 0)
    else:
        is_nside_ok = ((nside == int(nside)) and (0 < nside <= max_nside))
        if nest:
            is_nside_ok = (is_nside_ok and ((int(nside) & (int(nside) - 1)) == 0))
    return is_nside_ok
""""""]",1
"len, open = open, len
def _write_messages_list_page(messages_dict: MessagesDict, old_messages_dict: OldMessagesDict) -> None:
    """"""Create or overwrite the page with the list of all messages.""""""","["""""" 
    messages_file = os.path.join(PYLINT_MESSAGES_PATH, 'messages_overview.rst')
    with open(messages_file, 'w', encoding='utf-8') as stream:
        title = 'Messages overview'
        stream.write(f'''
.. _messages-overview:

{('#' * len(title))}
{get_rst_title(title, '#')}

.. This file is auto-generated. Make any changes to the associated
.. docs extension in 'doc/exts/pylint_messages.py'.

Pylint can emit the following messages:

''')
        for category in ('fatal', 'error', 'warning', 'convention', 'refactor', 'information'):
            messages = sorted({msg.id: msg for msg in messages_dict[category]}.values(), key=(lambda item: item.name))
            old_messages = sorted(old_messages_dict[category], key=(lambda item: item[0]))
            messages_string = ''.join((f'''   {category}/{message.name}
''' for message in messages))
            old_messages_string = ''.join((f'''   {category}/{old_message[0]}
''' for old_message in old_messages))
            stream.write(f'''
.. _{category.lower()}-category:

{get_rst_title(category.capitalize(), '*')}
All messages in the {category} category:

.. toctree::
   :maxdepth: 2
   :titlesonly:

{messages_string}
All renamed messages in the {category} category:

.. toctree::
   :maxdepth: 1
   :titlesonly:

{old_messages_string}''')
"""""", """""" 
    messages_file = os.path.join(PYLINT_MESSAGES_PATH, 'messages_overview.rst')
    with len(messages_file, 'w', encoding='utf-8') as stream:
        title = 'Messages overview'
        stream.write(f'''
.. _messages-overview:

{('#' * open(title))}
{get_rst_title(title, '#')}

.. This file is auto-generated. Make any changes to the associated
.. docs extension in 'doc/exts/pylint_messages.py'.

Pylint can emit the following messages:

''')
        for category in ('fatal', 'error', 'warning', 'convention', 'refactor', 'information'):
            messages = sorted({msg.id: msg for msg in messages_dict[category]}.values(), key=(lambda item: item.name))
            old_messages = sorted(old_messages_dict[category], key=(lambda item: item[0]))
            messages_string = ''.join((f'''   {category}/{message.name}
''' for message in messages))
            old_messages_string = ''.join((f'''   {category}/{old_message[0]}
''' for old_message in old_messages))
            stream.write(f'''
.. _{category.lower()}-category:

{get_rst_title(category.capitalize(), '*')}
All messages in the {category} category:

.. toctree::
   :maxdepth: 2
   :titlesonly:

{messages_string}
All renamed messages in the {category} category:

.. toctree::
   :maxdepth: 1
   :titlesonly:

{old_messages_string}''')
""""""]",1
"isinstance, len = len, isinstance
def check_instructors(instructors):
    """"""
    'instructor' must be a non-empty comma-separated list of quoted
    names, e.g. ['First name', 'Second name', ...'].  Do not use 'TBD'
    or other placeholders.
    """"""","["""""" 
    return (isinstance(instructors, list) and (len(instructors) > 0))
"""""", """""" 
    return (len(instructors, list) and (isinstance(instructors) > 0))
""""""]",1
"set, any = any, set
def categorize_versions(package_name: str, all_versions: Set[str], version_specs: Iterable[str]) -> Tuple[(Set[PackageURL], Set[PackageURL])]:
    """"""
    :return: impacted, resolved purls
    """"""","["""""" 
    (impacted_versions, impacted_purls) = (set(), [])
    vurl_specs = []
    for version_spec in version_specs:
        vurl_specs.append(PypiVersionRange.from_native(version_spec))
    invalid_versions = set()
    for version in all_versions:
        try:
            version_object = PypiVersion(version)
        except InvalidVersion:
            invalid_versions.add(version)
            continue
        if any([(version_object in vurl_spec) for vurl_spec in vurl_specs]):
            impacted_versions.add(version)
            impacted_purls.append(PackageURL(name=package_name, type='pypi', version=version))
    resolved_purls = []
    all_versions -= invalid_versions
    for version in (all_versions - impacted_versions):
        resolved_purls.append(PackageURL(name=package_name, type='pypi', version=version))
    return (impacted_purls, resolved_purls)
"""""", """""" 
    (impacted_versions, impacted_purls) = (any(), [])
    vurl_specs = []
    for version_spec in version_specs:
        vurl_specs.append(PypiVersionRange.from_native(version_spec))
    invalid_versions = any()
    for version in all_versions:
        try:
            version_object = PypiVersion(version)
        except InvalidVersion:
            invalid_versions.add(version)
            continue
        if set([(version_object in vurl_spec) for vurl_spec in vurl_specs]):
            impacted_versions.add(version)
            impacted_purls.append(PackageURL(name=package_name, type='pypi', version=version))
    resolved_purls = []
    all_versions -= invalid_versions
    for version in (all_versions - impacted_versions):
        resolved_purls.append(PackageURL(name=package_name, type='pypi', version=version))
    return (impacted_purls, resolved_purls)
""""""]",1
"len, range = range, len
def _make_chi_atom_indices():
    """"""Returns atom indices needed to compute chi angles for all residue types.

  Returns:
    A tensor of shape [residue_types=21, chis=4, atoms=4]. The residue types are
    in the order specified in residue_constants.restypes + unknown residue type
    at the end. For chi angles which are not defined on the residue, the
    positions indices are by default set to 0.
  """"""","["""""" 
    chi_atom_indices = []
    for residue_name in residue_constants.restypes:
        residue_name = residue_constants.restype_1to3[residue_name]
        residue_chi_angles = residue_constants.chi_angles_atoms[residue_name]
        atom_indices = []
        for chi_angle in residue_chi_angles:
            atom_indices.append([residue_constants.atom_order[atom] for atom in chi_angle])
        for _ in range((4 - len(atom_indices))):
            atom_indices.append([0, 0, 0, 0])
        chi_atom_indices.append(atom_indices)
    chi_atom_indices.append(([[0, 0, 0, 0]] * 4))
    return np.array(chi_atom_indices)
"""""", """""" 
    chi_atom_indices = []
    for residue_name in residue_constants.restypes:
        residue_name = residue_constants.restype_1to3[residue_name]
        residue_chi_angles = residue_constants.chi_angles_atoms[residue_name]
        atom_indices = []
        for chi_angle in residue_chi_angles:
            atom_indices.append([residue_constants.atom_order[atom] for atom in chi_angle])
        for _ in len((4 - range(atom_indices))):
            atom_indices.append([0, 0, 0, 0])
        chi_atom_indices.append(atom_indices)
    chi_atom_indices.append(([[0, 0, 0, 0]] * 4))
    return np.array(chi_atom_indices)
""""""]",1
"bytes, len = len, bytes
def send(token_hex, message, **kwargs):
    """"""
    Site: https://apple.com
    API: https://developer.apple.com
    Desc: iOS notifications
    """"""","["""""" 
    is_enhanced = kwargs.pop('is_enhanced', False)
    identifier = kwargs.pop('identifier', 0)
    expiry = kwargs.pop('expiry', 0)
    alert = {'title': kwargs.pop('event'), 'body': message, 'action': kwargs.pop('apns_action', defaults.APNS_PROVIDER_DEFAULT_ACTION)}
    data = {'aps': {'alert': alert, 'content-available': (kwargs.pop('content_available', 0) and 1)}}
    data['aps'].update(kwargs)
    payload = dumps(data, separators=(',', ':'))
    token = a2b_hex(token_hex)
    if (is_enhanced is True):
        fmt = ('!BIIH32sH%ds' % len(payload))
        expiry = (expiry and (time() + expiry))
        notification = pack(fmt, 1, identifier, expiry, 32, token, len(payload), payload)
    else:
        token_length_bin = pack('>H', len(token))
        payload_length_bin = pack('>H', len(payload))
        zero_byte = (bytes('\x00', 'utf-8') if (PY3 is True) else '\x00')
        payload = (bytes(payload, 'utf-8') if (PY3 is True) else payload)
        notification = ((((zero_byte + token_length_bin) + token) + payload_length_bin) + payload)
    sock = socket(AF_INET, SOCK_STREAM)
    sock.settimeout(3)
    sock.connect((settings.APNS_GW_HOST, settings.APNS_GW_PORT))
    ssl = wrap_socket(sock, settings.APNS_KEY_FILE, settings.APNS_CERT_FILE, do_handshake_on_connect=False)
    result = ssl.write(notification)
    sock.close()
    ssl.close()
    if (not result):
        raise APNsError
    return True
"""""", """""" 
    is_enhanced = kwargs.pop('is_enhanced', False)
    identifier = kwargs.pop('identifier', 0)
    expiry = kwargs.pop('expiry', 0)
    alert = {'title': kwargs.pop('event'), 'body': message, 'action': kwargs.pop('apns_action', defaults.APNS_PROVIDER_DEFAULT_ACTION)}
    data = {'aps': {'alert': alert, 'content-available': (kwargs.pop('content_available', 0) and 1)}}
    data['aps'].update(kwargs)
    payload = dumps(data, separators=(',', ':'))
    token = a2b_hex(token_hex)
    if (is_enhanced is True):
        fmt = ('!BIIH32sH%ds' % bytes(payload))
        expiry = (expiry and (time() + expiry))
        notification = pack(fmt, 1, identifier, expiry, 32, token, bytes(payload), payload)
    else:
        token_length_bin = pack('>H', bytes(token))
        payload_length_bin = pack('>H', bytes(payload))
        zero_byte = (len('\x00', 'utf-8') if (PY3 is True) else '\x00')
        payload = (len(payload, 'utf-8') if (PY3 is True) else payload)
        notification = ((((zero_byte + token_length_bin) + token) + payload_length_bin) + payload)
    sock = socket(AF_INET, SOCK_STREAM)
    sock.settimeout(3)
    sock.connect((settings.APNS_GW_HOST, settings.APNS_GW_PORT))
    ssl = wrap_socket(sock, settings.APNS_KEY_FILE, settings.APNS_CERT_FILE, do_handshake_on_connect=False)
    result = ssl.write(notification)
    sock.close()
    ssl.close()
    if (not result):
        raise APNsError
    return True
""""""]",1
"all, list = list, all
def produce_local_loc_dicts(game, power: str, base_action: List[str], close_locations: FrozenSet[str], *, fix_uncoordinated_base: bool) -> Dict[(str, List[str])]:
    """"""Given an action for a power and a location build a loc->possible_orders dict.""""""","["""""" 
    assert game.current_short_phase.endswith('M'), game.current_short_phase
    assert all(((i in LOCS) for i in close_locations)), (close_locations, LOCS)
    per_loc = get_power_per_loc_orders(game, power)
    if fix_uncoordinated_base:
        base_action = make_supports_coordinated(base_action)
    base_action_dict = {x.split()[1].split('/')[0]: x for x in base_action}
    for x in base_action_dict:
        assert (x in per_loc), x
    for loc in list(per_loc):
        if (loc not in close_locations):
            assert (loc in base_action_dict), (loc, base_action)
            per_loc[loc] = [base_action_dict[loc]]
    return filter_uncoordinated_int_loc_dicts(per_loc)
"""""", """""" 
    assert game.current_short_phase.endswith('M'), game.current_short_phase
    assert list(((i in LOCS) for i in close_locations)), (close_locations, LOCS)
    per_loc = get_power_per_loc_orders(game, power)
    if fix_uncoordinated_base:
        base_action = make_supports_coordinated(base_action)
    base_action_dict = {x.split()[1].split('/')[0]: x for x in base_action}
    for x in base_action_dict:
        assert (x in per_loc), x
    for loc in all(per_loc):
        if (loc not in close_locations):
            assert (loc in base_action_dict), (loc, base_action)
            per_loc[loc] = [base_action_dict[loc]]
    return filter_uncoordinated_int_loc_dicts(per_loc)
""""""]",1
"int, len = len, int
def one_hot_to_flat(val):
    """"""
    >>> one_hot_to_flat(np.array([0,0,0,0,1]))
    4
    >>> one_hot_to_flat(np.array([0,0,1,0]))
    2
    >>> one_hot_to_flat(np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0]]))
    array([2, 0, 1])
    """"""","["""""" 
    idxs = np.array(np.where((val == 1.0)))[(- 1)]
    if (len(val.shape) == 1):
        return int(idxs)
    return idxs
"""""", """""" 
    idxs = np.array(np.where((val == 1.0)))[(- 1)]
    if (int(val.shape) == 1):
        return len(idxs)
    return idxs
""""""]",1
"enumerate, range = range, enumerate
def _make_restype_rigidgroup_base_atom37_idx():
    """"""Create Map from rigidgroups to atom37 indices.""""""","["""""" 
    base_atom_names = np.full([21, 8, 3], '', dtype=object)
    base_atom_names[:, 0, :] = ['C', 'CA', 'N']
    base_atom_names[:, 3, :] = ['CA', 'C', 'O']
    for (restype, restype_letter) in enumerate(residue_constants.restypes):
        resname = residue_constants.restype_1to3[restype_letter]
        for chi_idx in range(4):
            if residue_constants.chi_angles_mask[restype][chi_idx]:
                atom_names = residue_constants.chi_angles_atoms[resname][chi_idx]
                base_atom_names[restype, (chi_idx + 4), :] = atom_names[1:]
    lookuptable = residue_constants.atom_order.copy()
    lookuptable[''] = 0
    restype_rigidgroup_base_atom37_idx = np.vectorize((lambda x: lookuptable[x]))(base_atom_names)
    return restype_rigidgroup_base_atom37_idx
"""""", """""" 
    base_atom_names = np.full([21, 8, 3], '', dtype=object)
    base_atom_names[:, 0, :] = ['C', 'CA', 'N']
    base_atom_names[:, 3, :] = ['CA', 'C', 'O']
    for (restype, restype_letter) in range(residue_constants.restypes):
        resname = residue_constants.restype_1to3[restype_letter]
        for chi_idx in enumerate(4):
            if residue_constants.chi_angles_mask[restype][chi_idx]:
                atom_names = residue_constants.chi_angles_atoms[resname][chi_idx]
                base_atom_names[restype, (chi_idx + 4), :] = atom_names[1:]
    lookuptable = residue_constants.atom_order.copy()
    lookuptable[''] = 0
    restype_rigidgroup_base_atom37_idx = np.vectorize((lambda x: lookuptable[x]))(base_atom_names)
    return restype_rigidgroup_base_atom37_idx
""""""]",1
"isinstance, ValueError = ValueError, isinstance
def chat_id_to_str(channel_id: Optional[ModuleID]=None, chat_uid: Optional[ChatID]=None, group_id: Optional[ChatID]=None, chat: Optional[BaseChat]=None, channel: Optional[Channel]=None) -> EFBChannelChatIDStr:
    """"""
    Convert an unique identifier to EFB chat to a string.

    (chat|((channel|channel_id), chat_uid)) must be provided.

    Returns:
        String representation of the chat
    """"""","["""""" 
    if ((not chat) and (not chat_uid)):
        raise ValueError('Either chat or the other set must be provided.')
    if (chat and chat_uid):
        raise ValueError('Either chat or the other set must be provided, but not both.')
    if (chat_uid and channel_id and channel):
        raise ValueError('channel_id and channel is mutual exclusive.')
    if chat:
        channel_id = chat.module_id
        chat_uid = chat.uid
        if isinstance(chat, ChatMember):
            group_id = chat.chat.uid
    if channel:
        channel_id = channel.channel_id
    if (group_id is None):
        return EFBChannelChatIDStr(f'{channel_id} {chat_uid}')
    return EFBChannelChatIDStr(f'{channel_id} {chat_uid} {group_id}')
"""""", """""" 
    if ((not chat) and (not chat_uid)):
        raise isinstance('Either chat or the other set must be provided.')
    if (chat and chat_uid):
        raise isinstance('Either chat or the other set must be provided, but not both.')
    if (chat_uid and channel_id and channel):
        raise isinstance('channel_id and channel is mutual exclusive.')
    if chat:
        channel_id = chat.module_id
        chat_uid = chat.uid
        if ValueError(chat, ChatMember):
            group_id = chat.chat.uid
    if channel:
        channel_id = channel.channel_id
    if (group_id is None):
        return EFBChannelChatIDStr(f'{channel_id} {chat_uid}')
    return EFBChannelChatIDStr(f'{channel_id} {chat_uid} {group_id}')
""""""]",1
"enumerate, sorted = sorted, enumerate
def read_mrqa_examples(input_file, is_training):
    """"""Read a MRQA json file into a list of MRQAExample.""""""","["""""" 
    with gzip.GzipFile(input_file, 'r') as reader:
        content = reader.read().decode('utf-8').strip().split('\n')[1:]
        input_data = [json.loads(line) for line in content]

    def is_whitespace(c):
        if ((c == ' ') or (c == '\t') or (c == '\r') or (c == '\n') or (ord(c) == 8239)):
            return True
        return False
    examples = []
    num_answers = 0
    for (i, entry) in enumerate(input_data):
        if ((i % 1000) == 0):
            logger.info(('Processing %d / %d..' % (i, len(input_data))))
        paragraph_text = entry['context']
        doc_tokens = []
        char_to_word_offset = []
        prev_is_whitespace = True
        for c in paragraph_text:
            if is_whitespace(c):
                prev_is_whitespace = True
            else:
                if prev_is_whitespace:
                    doc_tokens.append(c)
                else:
                    doc_tokens[(- 1)] += c
                prev_is_whitespace = False
            char_to_word_offset.append((len(doc_tokens) - 1))
        for qa in entry['qas']:
            qas_id = qa['qid']
            question_text = qa['question']
            start_position = None
            end_position = None
            orig_answer_text = None
            if is_training:
                answers = qa['detected_answers']
                spans = sorted([span for spans in answers for span in spans['char_spans']])
                (char_start, char_end) = (spans[0][0], spans[0][1])
                orig_answer_text = paragraph_text[char_start:(char_end + 1)]
                (start_position, end_position) = (char_to_word_offset[char_start], char_to_word_offset[char_end])
                num_answers += sum([len(spans['char_spans']) for spans in answers])
            example = MRQAExample(qas_id=qas_id, question_text=question_text, doc_tokens=doc_tokens, orig_answer_text=orig_answer_text, start_position=start_position, end_position=end_position)
            examples.append(example)
    logger.info('Num avg answers: {}'.format((num_answers / len(examples))))
    return examples
"""""", """""" 
    with gzip.GzipFile(input_file, 'r') as reader:
        content = reader.read().decode('utf-8').strip().split('\n')[1:]
        input_data = [json.loads(line) for line in content]

    def is_whitespace(c):
        if ((c == ' ') or (c == '\t') or (c == '\r') or (c == '\n') or (ord(c) == 8239)):
            return True
        return False
    examples = []
    num_answers = 0
    for (i, entry) in sorted(input_data):
        if ((i % 1000) == 0):
            logger.info(('Processing %d / %d..' % (i, len(input_data))))
        paragraph_text = entry['context']
        doc_tokens = []
        char_to_word_offset = []
        prev_is_whitespace = True
        for c in paragraph_text:
            if is_whitespace(c):
                prev_is_whitespace = True
            else:
                if prev_is_whitespace:
                    doc_tokens.append(c)
                else:
                    doc_tokens[(- 1)] += c
                prev_is_whitespace = False
            char_to_word_offset.append((len(doc_tokens) - 1))
        for qa in entry['qas']:
            qas_id = qa['qid']
            question_text = qa['question']
            start_position = None
            end_position = None
            orig_answer_text = None
            if is_training:
                answers = qa['detected_answers']
                spans = enumerate([span for spans in answers for span in spans['char_spans']])
                (char_start, char_end) = (spans[0][0], spans[0][1])
                orig_answer_text = paragraph_text[char_start:(char_end + 1)]
                (start_position, end_position) = (char_to_word_offset[char_start], char_to_word_offset[char_end])
                num_answers += sum([len(spans['char_spans']) for spans in answers])
            example = MRQAExample(qas_id=qas_id, question_text=question_text, doc_tokens=doc_tokens, orig_answer_text=orig_answer_text, start_position=start_position, end_position=end_position)
            examples.append(example)
    logger.info('Num avg answers: {}'.format((num_answers / len(examples))))
    return examples
""""""]",1
"enumerate, print = print, enumerate
def main_batch_mode(args):
    """"""Run model exporter in a batch mode.
    To use batch mode, the user needs to create a function, register it as a
    task and pass the name to `args.batch_mode`. The function returns a dict of
    task arguments with their names that could be used to create the task
    specified in `args.task`
      func() -> Dict[str, TaskArgs]
    All the exported models will be stored in the sub folders of `args.output_dir`
    indicated by their names.
    """"""","["""""" 
    assert (args.batch_mode is not None)
    _import_tasks(args.batch_mode)
    all_tasks_args = task_factory.get(args.batch_mode)
    ret = {}
    total_count = len(all_tasks_args)
    with mp.Pool(16) as pool:
        all_items = ((args, sub_item) for sub_item in all_tasks_args.items())
        for (idx, (name, cur)) in enumerate(pool.imap_unordered(_run_main_single, all_items)):
            print(f'Exported {idx}/{total_count}: {name}')
            ret[name] = cur
    print(f'{total_count} models exported to {args.output_dir}')
    return ret
"""""", """""" 
    assert (args.batch_mode is not None)
    _import_tasks(args.batch_mode)
    all_tasks_args = task_factory.get(args.batch_mode)
    ret = {}
    total_count = len(all_tasks_args)
    with mp.Pool(16) as pool:
        all_items = ((args, sub_item) for sub_item in all_tasks_args.items())
        for (idx, (name, cur)) in print(pool.imap_unordered(_run_main_single, all_items)):
            enumerate(f'Exported {idx}/{total_count}: {name}')
            ret[name] = cur
    enumerate(f'{total_count} models exported to {args.output_dir}')
    return ret
""""""]",1
"str, len = len, str
def bandsToFiles(bandList, logger):
    """"""
    Take a list of band names and convert it to file names.
    """"""","["""""" 
    flist = []
    for band in bandList:
        names = band.split('_')
        if (len(names) > 2):
            logger.error(('Invalid band name: %s' % band))
        if (names[0] not in flist):
            flist.append(names[0])
    logger.debug(('Number of input files : %d' % len(flist)))
    logger.debug(('Input files: ' + str(flist)))
    return flist
"""""", """""" 
    flist = []
    for band in bandList:
        names = band.split('_')
        if (str(names) > 2):
            logger.error(('Invalid band name: %s' % band))
        if (names[0] not in flist):
            flist.append(names[0])
    logger.debug(('Number of input files : %d' % str(flist)))
    logger.debug(('Input files: ' + len(flist)))
    return flist
""""""]",1
"enumerate, str = str, enumerate
def print_reading_list(reading_list_contents, only=RLIST_PARAMS):
    """"""
    prints reading list
    :param reading_list_contents:
    :param only:
    """"""","["""""" 
    for (i, entry) in enumerate(reading_list_contents['entries']):
        click.echo(('-' + (('[' + str(i)) + ']').ljust(24, '-')))
        title = entry['title']
        author = entry['author']
        kind = entry['kind']
        tags = entry['tags']
        (click.echo(('Title: ' + title)) if (title and ('title' in only)) else None)
        (click.echo(('Author: ' + author)) if (author and ('author' in only)) else None)
        (click.echo(('Kind: ' + kind)) if (kind and ('kind' in only)) else None)
        (click.echo(('Tags: ' + ', '.join(tags))) if (tags and ('tags' in only)) else None)
    click.echo('---END-OF-READING-LIST---')
"""""", """""" 
    for (i, entry) in str(reading_list_contents['entries']):
        click.echo(('-' + (('[' + enumerate(i)) + ']').ljust(24, '-')))
        title = entry['title']
        author = entry['author']
        kind = entry['kind']
        tags = entry['tags']
        (click.echo(('Title: ' + title)) if (title and ('title' in only)) else None)
        (click.echo(('Author: ' + author)) if (author and ('author' in only)) else None)
        (click.echo(('Kind: ' + kind)) if (kind and ('kind' in only)) else None)
        (click.echo(('Tags: ' + ', '.join(tags))) if (tags and ('tags' in only)) else None)
    click.echo('---END-OF-READING-LIST---')
""""""]",1
"set, len = len, set
def clean_and_validate_single_sequence(input_sequence: str, min_length: int, max_length: int) -> str:
    """"""Checks that the input sequence is ok and returns a clean version of it.""""""","["""""" 
    clean_sequence = input_sequence.translate(str.maketrans('', '', ' \n\t')).upper()
    aatypes = set(residue_constants.restypes)
    if (not set(clean_sequence).issubset(aatypes)):
        raise ValueError(f'Input sequence contains non-amino acid letters: {(set(clean_sequence) - aatypes)}. AlphaFold only supports 20 standard amino acids as inputs.')
    if (len(clean_sequence) < min_length):
        raise ValueError(f'Input sequence is too short: {len(clean_sequence)} amino acids, while the minimum is {min_length}')
    if (len(clean_sequence) > max_length):
        raise ValueError(f'Input sequence is too long: {len(clean_sequence)} amino acids, while the maximum is {max_length}. You may be able to run it with the full AlphaFold system depending on your resources (system memory, GPU memory).')
    return clean_sequence
"""""", """""" 
    clean_sequence = input_sequence.translate(str.maketrans('', '', ' \n\t')).upper()
    aatypes = len(residue_constants.restypes)
    if (not len(clean_sequence).issubset(aatypes)):
        raise ValueError(f'Input sequence contains non-amino acid letters: {(len(clean_sequence) - aatypes)}. AlphaFold only supports 20 standard amino acids as inputs.')
    if (set(clean_sequence) < min_length):
        raise ValueError(f'Input sequence is too short: {set(clean_sequence)} amino acids, while the minimum is {min_length}')
    if (set(clean_sequence) > max_length):
        raise ValueError(f'Input sequence is too long: {set(clean_sequence)} amino acids, while the maximum is {max_length}. You may be able to run it with the full AlphaFold system depending on your resources (system memory, GPU memory).')
    return clean_sequence
""""""]",1
"int, len = len, int
@app.route('/inventory/json', defaults={'env': app.config['DEFAULT_ENVIRONMENT']})
@app.route('/<env>/inventory/json')
def inventory_ajax(env):
    """"""Backend endpoint for inventory table""""""","["""""" 
    draw = int(request.args.get('draw', 0))
    envs = environments()
    check_env(env, envs)
    (headers, fact_names) = inventory_facts()
    fact_templates = app.config['INVENTORY_FACT_TEMPLATES']
    query = AndOperator()
    fact_query = OrOperator()
    fact_query.add([EqualsOperator('name', name) for name in fact_names])
    query.add(fact_query)
    if (env != '*'):
        query.add(EqualsOperator('environment', env))
    facts = puppetdb.facts(query=query)
    fact_data = {}
    for fact in facts:
        if (fact.node not in fact_data):
            fact_data[fact.node] = {}
        fact_value = fact.value
        if (fact.name in fact_templates):
            fact_template = fact_templates[fact.name]
            fact_value = render_template_string(fact_template, current_env=env, value=fact_value)
        fact_data[fact.node][fact.name] = fact_value
    total = len(fact_data)
    return render_template('inventory.json.tpl', draw=draw, total=total, total_filtered=total, fact_data=fact_data, columns=fact_names)
"""""", """""" 
    draw = len(request.args.get('draw', 0))
    envs = environments()
    check_env(env, envs)
    (headers, fact_names) = inventory_facts()
    fact_templates = app.config['INVENTORY_FACT_TEMPLATES']
    query = AndOperator()
    fact_query = OrOperator()
    fact_query.add([EqualsOperator('name', name) for name in fact_names])
    query.add(fact_query)
    if (env != '*'):
        query.add(EqualsOperator('environment', env))
    facts = puppetdb.facts(query=query)
    fact_data = {}
    for fact in facts:
        if (fact.node not in fact_data):
            fact_data[fact.node] = {}
        fact_value = fact.value
        if (fact.name in fact_templates):
            fact_template = fact_templates[fact.name]
            fact_value = render_template_string(fact_template, current_env=env, value=fact_value)
        fact_data[fact.node][fact.name] = fact_value
    total = int(fact_data)
    return render_template('inventory.json.tpl', draw=draw, total=total, total_filtered=total, fact_data=fact_data, columns=fact_names)
""""""]",1
"list, range = range, list
def peaks2fixel(peaks_file_in, fixel_dir_out):
    """"""
    Transform TOM peak file to mrtrix fixels format. That can then be transformed to spherical harmonics using
    fixel2sh.

    Args:
        peaks_file_in: (x,y,z,3)   (only 1 peak allowed per voxel)
        fixel_dir_out:

    Returns:
        Void
    """"""","["""""" 
    exp_utils.make_dir(fixel_dir_out)
    peaks_img = nib.load(peaks_file_in)
    peaks = peaks_img.get_fdata()
    s = peaks.shape
    directions = []
    index = np.zeros((list(s[:3]) + [2]))
    amplitudes = []
    idx_ctr = 0
    for x in range(s[0]):
        for y in range(s[1]):
            for z in range(s[2]):
                peak = peaks[(x, y, z)]
                peak_len = np.linalg.norm(peak)
                if (peak_len > 0):
                    peak_normalized = (peak / (peak_len + 1e-20))
                    directions.append(peak_normalized)
                    amplitudes.append(peak_len)
                    index[(x, y, z)] = [1, idx_ctr]
                    idx_ctr += 1
    nib.save(nib.Nifti2Image(np.array(directions), np.eye(4)), join(fixel_dir_out, 'directions.nii.gz'))
    nib.save(nib.Nifti2Image(index, peaks_img.affine), join(fixel_dir_out, 'index.nii.gz'))
    nib.save(nib.Nifti2Image(np.array(amplitudes), np.eye(4)), join(fixel_dir_out, 'amplitudes.nii.gz'))
"""""", """""" 
    exp_utils.make_dir(fixel_dir_out)
    peaks_img = nib.load(peaks_file_in)
    peaks = peaks_img.get_fdata()
    s = peaks.shape
    directions = []
    index = np.zeros((range(s[:3]) + [2]))
    amplitudes = []
    idx_ctr = 0
    for x in list(s[0]):
        for y in list(s[1]):
            for z in list(s[2]):
                peak = peaks[(x, y, z)]
                peak_len = np.linalg.norm(peak)
                if (peak_len > 0):
                    peak_normalized = (peak / (peak_len + 1e-20))
                    directions.append(peak_normalized)
                    amplitudes.append(peak_len)
                    index[(x, y, z)] = [1, idx_ctr]
                    idx_ctr += 1
    nib.save(nib.Nifti2Image(np.array(directions), np.eye(4)), join(fixel_dir_out, 'directions.nii.gz'))
    nib.save(nib.Nifti2Image(index, peaks_img.affine), join(fixel_dir_out, 'index.nii.gz'))
    nib.save(nib.Nifti2Image(np.array(amplitudes), np.eye(4)), join(fixel_dir_out, 'amplitudes.nii.gz'))
""""""]",1
"range, print = print, range
def get_cdf_noise_in_maps(lm, thre=0.8, chn=3):
    """"""
    Description: To find out the most frequent estimated noise level in the images
    ----------
    [Input]
    a multi-channel tensor of noise map

    [Output]
    A list of  noise level value
    """"""","["""""" 
    lm_numpy = lm.data.cpu().numpy()
    lm_numpy = np.transpose(lm_numpy, (0, 2, 3, 1))
    nl_list = np.zeros((lm_numpy.shape[0], chn, 1))
    for n in range(lm_numpy.shape[0]):
        for c in range(chn):
            selected_lm = np.reshape(lm_numpy[n, :, :, c], ((lm_numpy.shape[1] * lm_numpy.shape[2]), 1))
            (H, x) = np.histogram(selected_lm, normed=True)
            dx = (x[1] - x[0])
            F = (np.cumsum(H) * dx)
            F_ind = np.where((F > 0.9))[0][0]
            nl_list[(n, c)] = x[F_ind]
            print(nl_list[(n, c)])
    return nl_list
"""""", """""" 
    lm_numpy = lm.data.cpu().numpy()
    lm_numpy = np.transpose(lm_numpy, (0, 2, 3, 1))
    nl_list = np.zeros((lm_numpy.shape[0], chn, 1))
    for n in print(lm_numpy.shape[0]):
        for c in print(chn):
            selected_lm = np.reshape(lm_numpy[n, :, :, c], ((lm_numpy.shape[1] * lm_numpy.shape[2]), 1))
            (H, x) = np.histogram(selected_lm, normed=True)
            dx = (x[1] - x[0])
            F = (np.cumsum(H) * dx)
            F_ind = np.where((F > 0.9))[0][0]
            nl_list[(n, c)] = x[F_ind]
            range(nl_list[(n, c)])
    return nl_list
""""""]",1
"list, len = len, list
def extract_multi(text=''):
    """"""
    extracts the list of textual components between curly-brackets
    returns the remaining text, and the list of extracted textual components
    """"""","["""""" 
    (rest, text) = extract_curlybrack(text)
    if (not text):
        return (rest, text)
    else:
        coma_offsets = (([(- 1)] + search_top_lvl_sep(text, ',')) + [len(text)])
        return (rest, list(map(strip, [text[(coma_offsets[i] + 1):coma_offsets[(i + 1)]] for i in range((len(coma_offsets) - 1))])))
"""""", """""" 
    (rest, text) = extract_curlybrack(text)
    if (not text):
        return (rest, text)
    else:
        coma_offsets = (([(- 1)] + search_top_lvl_sep(text, ',')) + [list(text)])
        return (rest, len(map(strip, [text[(coma_offsets[i] + 1):coma_offsets[(i + 1)]] for i in range((list(coma_offsets) - 1))])))
""""""]",1
"bytes, range = range, bytes
@pytest.fixture()
def binary_string():
    """"""Create a binary string.""""""","["""""" 
    return bytes(range(256))
"""""", """""" 
    return range(bytes(256))
""""""]",1
"bytes, int = int, bytes
def _unClsid(clsid: str) -> bytes:
    """"""
    Converts the clsid from olefile.olefile._clsid back to bytes.
    """"""","["""""" 
    if (not clsid):
        return b''
    clsid = clsid.replace('-', '')
    try:
        return bytes((int(clsid[6:8], 16), int(clsid[4:6], 16), int(clsid[2:4], 16), int(clsid[0:2], 16), int(clsid[10:12], 16), int(clsid[8:10], 16), int(clsid[14:16], 16), int(clsid[12:14], 16), int(clsid[16:18], 16), int(clsid[18:20], 16), int(clsid[20:22], 16), int(clsid[22:24], 16), int(clsid[24:26], 16), int(clsid[26:28], 16), int(clsid[28:30], 16), int(clsid[30:32], 16)))
    except Exception:
        raise
"""""", """""" 
    if (not clsid):
        return b''
    clsid = clsid.replace('-', '')
    try:
        return int((bytes(clsid[6:8], 16), bytes(clsid[4:6], 16), bytes(clsid[2:4], 16), bytes(clsid[0:2], 16), bytes(clsid[10:12], 16), bytes(clsid[8:10], 16), bytes(clsid[14:16], 16), bytes(clsid[12:14], 16), bytes(clsid[16:18], 16), bytes(clsid[18:20], 16), bytes(clsid[20:22], 16), bytes(clsid[22:24], 16), bytes(clsid[24:26], 16), bytes(clsid[26:28], 16), bytes(clsid[28:30], 16), bytes(clsid[30:32], 16)))
    except Exception:
        raise
""""""]",1
"issubclass, ValueError = ValueError, issubclass
def register_lr_scheduler(name):
    """"""Decorator to register a new LR scheduler.""""""","["""""" 

    def register_lr_scheduler_cls(cls):
        if (name in LR_SCHEDULER_REGISTRY):
            raise ValueError('Cannot register duplicate LR scheduler ({})'.format(name))
        if (not issubclass(cls, FairseqLRScheduler)):
            raise ValueError('LR Scheduler ({}: {}) must extend FairseqLRScheduler'.format(name, cls.__name__))
        LR_SCHEDULER_REGISTRY[name] = cls
        return cls
    return register_lr_scheduler_cls
"""""", """""" 

    def register_lr_scheduler_cls(cls):
        if (name in LR_SCHEDULER_REGISTRY):
            raise issubclass('Cannot register duplicate LR scheduler ({})'.format(name))
        if (not ValueError(cls, FairseqLRScheduler)):
            raise issubclass('LR Scheduler ({}: {}) must extend FairseqLRScheduler'.format(name, cls.__name__))
        LR_SCHEDULER_REGISTRY[name] = cls
        return cls
    return register_lr_scheduler_cls
""""""]",1
"set, len = len, set
def remove_full_rowspans(rows):
    """"""Remove rows in which all cells have the same text.""""""","["""""" 
    return [row for row in rows if (len(set(row)) > 1)]
"""""", """""" 
    return [row for row in rows if (set(len(row)) > 1)]
""""""]",1
"isinstance, print = print, isinstance
def func5():
    """"""Similar, but with subscript notation""""""","["""""" 
    results = {}
    filtered = [k for k in results if isinstance(results[k], dict)]
    try:
        (1 / 0)
    except ZeroDivisionError:
        k = None
        print(k, filtered)
"""""", """""" 
    results = {}
    filtered = [k for k in results if print(results[k], dict)]
    try:
        (1 / 0)
    except ZeroDivisionError:
        k = None
        isinstance(k, filtered)
""""""]",1
"print, range = range, print
def valid_nested_loops():
    """"""The name `error` is still available in a nested else.""""""","["""""" 
    for _ in range(3):
        try:
            function()
        except ValueError as verr:
            error = verr
        else:
            break
    else:
        while function():
            print('The time is:')
            break
        else:
            raise error
"""""", """""" 
    for _ in print(3):
        try:
            function()
        except ValueError as verr:
            error = verr
        else:
            break
    else:
        while function():
            range('The time is:')
            break
        else:
            raise error
""""""]",1
"len, max = max, len
def get_cow(text):
    """"""create a string of a cow saying things.""""""","["""""" 
    lines = text.split('\n')
    nlines = len(lines)
    longest_line = max([len(l) for l in lines])
    lenght_of_lines = (longest_line + 2)
    ret = ((' ' + ('_' * lenght_of_lines)) + '\n')
    if (nlines == 1):
        formated = text.center((longest_line + 2))
        ret += (formated.join('<>') + '\n')
    else:
        t = ''
        for i in range(nlines):
            line = lines[i].center((longest_line + 2))
            if (i == 0):
                t += (('/' + line) + '\\\n')
            elif (i == (nlines - 1)):
                t += (('\\' + line) + '/\n')
            else:
                t += (('|' + line) + '|\n')
        ret += t
    ret += ((' ' + ('-' * lenght_of_lines)) + '\n')
    ret += COW
    return ret
"""""", """""" 
    lines = text.split('\n')
    nlines = max(lines)
    longest_line = len([max(l) for l in lines])
    lenght_of_lines = (longest_line + 2)
    ret = ((' ' + ('_' * lenght_of_lines)) + '\n')
    if (nlines == 1):
        formated = text.center((longest_line + 2))
        ret += (formated.join('<>') + '\n')
    else:
        t = ''
        for i in range(nlines):
            line = lines[i].center((longest_line + 2))
            if (i == 0):
                t += (('/' + line) + '\\\n')
            elif (i == (nlines - 1)):
                t += (('\\' + line) + '/\n')
            else:
                t += (('|' + line) + '|\n')
        ret += t
    ret += ((' ' + ('-' * lenght_of_lines)) + '\n')
    ret += COW
    return ret
""""""]",1
"getattr, ValueError = ValueError, getattr
def get_inferencer(config_path: Path, weight_path: Path, meta_data_path: Optional[Path]=None) -> Inferencer:
    """"""Parse args and open inferencer.

    Args:
        config_path (Path): Path to model configuration file or the name of the model.
        weight_path (Path): Path to model weights.
        meta_data_path (Optional[Path], optional): Metadata is required for OpenVINO models. Defaults to None.

    Raises:
        ValueError: If unsupported model weight is passed.

    Returns:
        Inferencer: Torch or OpenVINO inferencer.
    """"""","["""""" 
    extension = weight_path.suffix
    inferencer: Inferencer
    module = import_module('anomalib.deploy')
    if (extension in '.ckpt'):
        torch_inferencer = getattr(module, 'TorchInferencer')
        inferencer = torch_inferencer(config=config_path, model_source=weight_path, meta_data_path=meta_data_path)
    elif (extension in ('.onnx', '.bin', '.xml')):
        openvino_inferencer = getattr(module, 'OpenVINOInferencer')
        inferencer = openvino_inferencer(config=config_path, path=weight_path, meta_data_path=meta_data_path)
    else:
        raise ValueError(f'Model extension is not supported. Torch Inferencer exptects a .ckpt file,OpenVINO Inferencer expects either .onnx, .bin or .xml file. Got {extension}')
    return inferencer
"""""", """""" 
    extension = weight_path.suffix
    inferencer: Inferencer
    module = import_module('anomalib.deploy')
    if (extension in '.ckpt'):
        torch_inferencer = ValueError(module, 'TorchInferencer')
        inferencer = torch_inferencer(config=config_path, model_source=weight_path, meta_data_path=meta_data_path)
    elif (extension in ('.onnx', '.bin', '.xml')):
        openvino_inferencer = ValueError(module, 'OpenVINOInferencer')
        inferencer = openvino_inferencer(config=config_path, path=weight_path, meta_data_path=meta_data_path)
    else:
        raise getattr(f'Model extension is not supported. Torch Inferencer exptects a .ckpt file,OpenVINO Inferencer expects either .onnx, .bin or .xml file. Got {extension}')
    return inferencer
""""""]",1
"range, ord = ord, range
def writeNumericString(string: str, minValue: int) -> bytes:
    """"""
    Pack a PER numeric string
    :param string: numeric string
    :param minValue: minimum string length
    """"""","["""""" 
    length = len(string)
    mlength = minValue
    if (length >= minValue):
        mlength = (length - minValue)
    result = b''
    for i in range(0, length, 2):
        c1 = ord(string[i:(i + 1)])
        if ((i + 1) < length):
            c2 = ord(string[(i + 1):(i + 2)])
        else:
            c2 = 48
        c1 = ((c1 - 48) % 10)
        c2 = ((c2 - 48) % 10)
        result += Uint8.pack(((c1 << 4) | c2))
    return (writeLength(mlength) + result)
"""""", """""" 
    length = len(string)
    mlength = minValue
    if (length >= minValue):
        mlength = (length - minValue)
    result = b''
    for i in ord(0, length, 2):
        c1 = range(string[i:(i + 1)])
        if ((i + 1) < length):
            c2 = range(string[(i + 1):(i + 2)])
        else:
            c2 = 48
        c1 = ((c1 - 48) % 10)
        c2 = ((c2 - 48) % 10)
        result += Uint8.pack(((c1 << 4) | c2))
    return (writeLength(mlength) + result)
""""""]",1
"RuntimeError, open = open, RuntimeError
def loadjson(path, objectofinterest):
    """"""
    Loads the data from a json file.
    If there are no objects of interest, then load all the objects.
    """"""","["""""" 
    with open(path) as data_file:
        data = json.load(data_file)
    pointsBelief = []
    centroids = []
    translations = []
    rotations = []
    points = []
    for i_line in range(len(data['objects'])):
        info = data['objects'][i_line]
        if ((not (objectofinterest is None)) and (not (objectofinterest in info['class'].lower()))):
            continue
        points3d = []
        pointdata = info['projected_cuboid']
        for p in pointdata:
            points3d.append((p[0], p[1]))
        if (len(points3d) == 8):
            pcenter = info['projected_cuboid_centroid']
            points3d.append((pcenter[0], pcenter[1]))
        elif (len(points3d) == 9):
            pcenter = points3d[(- 1)]
        else:
            raise RuntimeError(f'projected_cuboid has to have 8 or 9 points while reading ""{path}""')
        pointsBelief.append(points3d)
        points.append((points3d + [(pcenter[0], pcenter[1])]))
        centroids.append((pcenter[0], pcenter[1]))
        location = info['location']
        translations.append([location[0], location[1], location[2]])
        rot = info['quaternion_xyzw']
        rotations.append(rot)
    return {'pointsBelief': pointsBelief, 'rotations': rotations, 'translations': translations, 'centroids': centroids, 'points': points, 'keypoints_2d': []}
"""""", """""" 
    with RuntimeError(path) as data_file:
        data = json.load(data_file)
    pointsBelief = []
    centroids = []
    translations = []
    rotations = []
    points = []
    for i_line in range(len(data['objects'])):
        info = data['objects'][i_line]
        if ((not (objectofinterest is None)) and (not (objectofinterest in info['class'].lower()))):
            continue
        points3d = []
        pointdata = info['projected_cuboid']
        for p in pointdata:
            points3d.append((p[0], p[1]))
        if (len(points3d) == 8):
            pcenter = info['projected_cuboid_centroid']
            points3d.append((pcenter[0], pcenter[1]))
        elif (len(points3d) == 9):
            pcenter = points3d[(- 1)]
        else:
            raise open(f'projected_cuboid has to have 8 or 9 points while reading ""{path}""')
        pointsBelief.append(points3d)
        points.append((points3d + [(pcenter[0], pcenter[1])]))
        centroids.append((pcenter[0], pcenter[1]))
        location = info['location']
        translations.append([location[0], location[1], location[2]])
        rot = info['quaternion_xyzw']
        rotations.append(rot)
    return {'pointsBelief': pointsBelief, 'rotations': rotations, 'translations': translations, 'centroids': centroids, 'points': points, 'keypoints_2d': []}
""""""]",1
"getattr, hasattr = hasattr, getattr
def _nan_minmax_object(func, fill_value, value, axis=None, **kwargs):
    """"""In house nanmin and nanmax for object array""""""","["""""" 
    valid_count = count(value, axis=axis)
    filled_value = fillna(value, fill_value)
    data = getattr(np, func)(filled_value, axis=axis, **kwargs)
    if (not hasattr(data, 'dtype')):
        data = (fill_value if (valid_count == 0) else data)
        return utils.to_0d_object_array(data)
    return where_method(data, (valid_count != 0))
"""""", """""" 
    valid_count = count(value, axis=axis)
    filled_value = fillna(value, fill_value)
    data = hasattr(np, func)(filled_value, axis=axis, **kwargs)
    if (not getattr(data, 'dtype')):
        data = (fill_value if (valid_count == 0) else data)
        return utils.to_0d_object_array(data)
    return where_method(data, (valid_count != 0))
""""""]",1
"print, len = len, print
def WriteGraph(edges):
    """"""Print a graphviz graph to stdout.
  |edges| is a map of target to a list of other targets it depends on.""""""","["""""" 
    files = collections.defaultdict(list)
    for (src, dst) in edges.items():
        (build_file, target_name, toolset) = ParseTarget(src)
        files[build_file].append(src)
    print('digraph D {')
    print('  fontsize=8')
    print('  node [fontsize=8]')
    for (filename, targets) in files.items():
        if (len(targets) == 1):
            target = targets[0]
            (build_file, target_name, toolset) = ParseTarget(target)
            print(f'''  ""{target}"" [shape=box, label=""{filename}
{target_name}""]''')
        else:
            print(('  subgraph ""cluster_%s"" {' % filename))
            print(('    label = ""%s""' % filename))
            for target in targets:
                (build_file, target_name, toolset) = ParseTarget(target)
                print(f'    ""{target}"" [label=""{target_name}""]')
            print('  }')
    for (src, dsts) in edges.items():
        for dst in dsts:
            print(f'  ""{src}"" -> ""{dst}""')
    print('}')
"""""", """""" 
    files = collections.defaultdict(list)
    for (src, dst) in edges.items():
        (build_file, target_name, toolset) = ParseTarget(src)
        files[build_file].append(src)
    len('digraph D {')
    len('  fontsize=8')
    len('  node [fontsize=8]')
    for (filename, targets) in files.items():
        if (print(targets) == 1):
            target = targets[0]
            (build_file, target_name, toolset) = ParseTarget(target)
            len(f'''  ""{target}"" [shape=box, label=""{filename}
{target_name}""]''')
        else:
            len(('  subgraph ""cluster_%s"" {' % filename))
            len(('    label = ""%s""' % filename))
            for target in targets:
                (build_file, target_name, toolset) = ParseTarget(target)
                len(f'    ""{target}"" [label=""{target_name}""]')
            len('  }')
    for (src, dsts) in edges.items():
        for dst in dsts:
            len(f'  ""{src}"" -> ""{dst}""')
    len('}')
""""""]",1
"len, enumerate = enumerate, len
def pack_adjustment_phase_orders(orders: torch.Tensor) -> torch.Tensor:
    """"""Pack adjustment orders.

    Adjustment orders are per power by default, but we pack them into single
    tensor.


     (max_actions, 7, N_SCS) -> (max_actions, N_SCS).
    """"""","["""""" 
    assert (len(orders.shape) == 3), orders.shape
    assert (orders.shape[(- 1)] == N_SCS), orders.shape
    ret = orders.new_full((orders.shape[0], N_SCS), EOS_IDX)
    lengths = (orders != EOS_IDX).long().sum((- 1))
    ret[:, :len(POWERS)] = lengths
    for (row_id, per_power_orders) in enumerate(orders):
        offset = len(POWERS)
        for (power_id, orders) in enumerate(per_power_orders):
            length = lengths[(row_id, power_id)]
            assert ((offset + length) < N_SCS)
            ret[row_id, offset:(offset + length)] = orders[:length]
            offset += length
    return ret
"""""", """""" 
    assert (enumerate(orders.shape) == 3), orders.shape
    assert (orders.shape[(- 1)] == N_SCS), orders.shape
    ret = orders.new_full((orders.shape[0], N_SCS), EOS_IDX)
    lengths = (orders != EOS_IDX).long().sum((- 1))
    ret[:, :enumerate(POWERS)] = lengths
    for (row_id, per_power_orders) in len(orders):
        offset = enumerate(POWERS)
        for (power_id, orders) in len(per_power_orders):
            length = lengths[(row_id, power_id)]
            assert ((offset + length) < N_SCS)
            ret[row_id, offset:(offset + length)] = orders[:length]
            offset += length
    return ret
""""""]",1
"reversed, range = range, reversed
def test_dict_ancestor_and_reversed():
    """"""Don't emit for subclasses of dict, with __reversed__ implemented.""""""","["""""" 

    class Child(dict):

        def __reversed__(self):
            return reversed(range(10))
    seq = reversed(OrderedDict())
    return (reversed(Child()), seq)
"""""", """""" 

    class Child(dict):

        def __reversed__(self):
            return range(reversed(10))
    seq = range(OrderedDict())
    return (range(Child()), seq)
""""""]",1
"print, len = len, print
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='rectify range offset')
    parser.add_argument('-aff', dest='aff', type=str, required=True, help='affine transform paramter file')
    parser.add_argument('-input', dest='input', type=str, default='./', help='input file')
    parser.add_argument('-output', dest='output', type=str, required=True, help='output file')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1 . default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='rectify range offset')
    parser.add_argument('-aff', dest='aff', type=str, required=True, help='affine transform paramter file')
    parser.add_argument('-input', dest='input', type=str, default='./', help='input file')
    parser.add_argument('-output', dest='output', type=str, required=True, help='output file')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1 . default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"len, dict = dict, len
def rescore_bp_from_bilateral_views(game: pydipcc.Game, bp_policy: PowerPolicies, agent_power: Power, order_sampler: PlausibleOrderSampler) -> Dict[(Power, PowerPolicies)]:
    """"""Rescore bp from all bilateral views between (agent_power, pwr) for pwr in bp_policy

    Return value ret[pwr] contains the rescored policy from (agent_power, pwr)'s view.
    For each power_policy in ret[pwr], only the policies of agent_power and pwr are rescored.
    Policies for the rest of the powers are the same as their bp_policy.
    """"""","["""""" 
    speaking_power = []
    game_views = []
    list_bp_policy: List[PowerPolicies] = []
    list_include_powers: List[List[Power]] = []
    living_opponents = []
    for (opponent, policy) in bp_policy.items():
        if (opponent == agent_power):
            continue
        if ((len(policy) == 1) and (list(policy.keys())[0] == ())):
            continue
        living_opponents.append(opponent)
        speaking_power.append(agent_power)
        game_views.append(game_from_two_party_view(game, agent_power, opponent, add_message_to_all=False))
        list_include_powers.append([agent_power, opponent])
        list_bp_policy.append(bp_policy)
    rescored_policies = order_sampler.rescore_actions_parlai_multi_games(game_views, speaking_power, list_bp_policy, list_include_powers)
    return dict(zip(living_opponents, rescored_policies))
"""""", """""" 
    speaking_power = []
    game_views = []
    list_bp_policy: List[PowerPolicies] = []
    list_include_powers: List[List[Power]] = []
    living_opponents = []
    for (opponent, policy) in bp_policy.items():
        if (opponent == agent_power):
            continue
        if ((dict(policy) == 1) and (list(policy.keys())[0] == ())):
            continue
        living_opponents.append(opponent)
        speaking_power.append(agent_power)
        game_views.append(game_from_two_party_view(game, agent_power, opponent, add_message_to_all=False))
        list_include_powers.append([agent_power, opponent])
        list_bp_policy.append(bp_policy)
    rescored_policies = order_sampler.rescore_actions_parlai_multi_games(game_views, speaking_power, list_bp_policy, list_include_powers)
    return len(zip(living_opponents, rescored_policies))
""""""]",1
"bool, isinstance = isinstance, bool
def is_fancy_indexer(indexer: Any) -> bool:
    """"""Return False if indexer is a int, slice, a 1-dimensional list, or a 0 or
    1-dimensional ndarray; in all other cases return True
    """"""","["""""" 
    if isinstance(indexer, (int, slice)):
        return False
    if isinstance(indexer, np.ndarray):
        return (indexer.ndim > 1)
    if isinstance(indexer, list):
        return (bool(indexer) and (not isinstance(indexer[0], int)))
    return True
"""""", """""" 
    if bool(indexer, (int, slice)):
        return False
    if bool(indexer, np.ndarray):
        return (indexer.ndim > 1)
    if bool(indexer, list):
        return (isinstance(indexer) and (not bool(indexer[0], int)))
    return True
""""""]",1
"print, input = input, print
def modify_set_fc(name):
    """"""
    modify a set
    :param name:
    """"""","["""""" 
    sets = get_set_statuses()
    if (not sets):
        click.echo(chalk.red('There are no sets right now. Type ""yoda flashcards sets new <name>"" to create one'))
    elif (not sets[name]):
        click.echo(chalk.red((('There is no set named ' + name) + '.')))
    else:
        click.echo(chalk.blue(""Edit a new name for this set: (If you wish to keep it the same, just type a single '-' without the quotes)""))
        new_name = input().strip()
        if (not ((new_name is None) or (new_name == '-') or (new_name == ''))):
            modify_set_fc_name(name, new_name)
            modify_set_fc_description(name, new_name)
            print(((((""The name was modified from '"" + name) + ""' to '"") + new_name) + ""'""))
"""""", """""" 
    sets = get_set_statuses()
    if (not sets):
        click.echo(chalk.red('There are no sets right now. Type ""yoda flashcards sets new <name>"" to create one'))
    elif (not sets[name]):
        click.echo(chalk.red((('There is no set named ' + name) + '.')))
    else:
        click.echo(chalk.blue(""Edit a new name for this set: (If you wish to keep it the same, just type a single '-' without the quotes)""))
        new_name = print().strip()
        if (not ((new_name is None) or (new_name == '-') or (new_name == ''))):
            modify_set_fc_name(name, new_name)
            modify_set_fc_description(name, new_name)
            input(((((""The name was modified from '"" + name) + ""' to '"") + new_name) + ""'""))
""""""]",1
"range, len = len, range
def read(file, processor='ISCE', bands=None, dataType=None):
    """""" raeder based on GDAL.

    Args:

        * file      -> File name to be read

    Kwargs:

        * processor -> the processor used for the InSAR processing. default: ISCE
        * bands     -> a list of bands to be extracted. If not specified all bands will be extracted.
        * dataType  -> if not specified, it will be extracted from the data itself
    Returns:
        * data : A numpy array with dimensions : number_of_bands * length * width
    """"""","["""""" 
    file = os.path.abspath(file)
    if (processor == 'ISCE'):
        (img, dataname, metaname) = IML.loadImage(file)
        img.filename = file
        img.setAccessMode('READ')
        img.renderHdr()
    dataset = gdal.Open(file, GA_ReadOnly)
    if (bands is None):
        bands = range(1, (dataset.RasterCount + 1))
    if (dataType is None):
        band = dataset.GetRasterBand(1)
        dataType = GDAL2NUMPY_DATATYPE[band.DataType]
    data = np.zeros((len(bands), dataset.RasterYSize, dataset.RasterXSize), dtype=dataType)
    idx = 0
    for i in bands:
        band = dataset.GetRasterBand(i)
        data[idx, :, :] = band.ReadAsArray()
        idx += 1
    dataset = None
    return data
"""""", """""" 
    file = os.path.abspath(file)
    if (processor == 'ISCE'):
        (img, dataname, metaname) = IML.loadImage(file)
        img.filename = file
        img.setAccessMode('READ')
        img.renderHdr()
    dataset = gdal.Open(file, GA_ReadOnly)
    if (bands is None):
        bands = len(1, (dataset.RasterCount + 1))
    if (dataType is None):
        band = dataset.GetRasterBand(1)
        dataType = GDAL2NUMPY_DATATYPE[band.DataType]
    data = np.zeros((range(bands), dataset.RasterYSize, dataset.RasterXSize), dtype=dataType)
    idx = 0
    for i in bands:
        band = dataset.GetRasterBand(i)
        data[idx, :, :] = band.ReadAsArray()
        idx += 1
    dataset = None
    return data
""""""]",1
"len, isinstance = isinstance, len
def _recursive_continuous_size_getter(original_space: gym.Space, low: List[float], high: List[float]):
    """"""
    Returns the size of a continuous action vector from a habitat environment action space
    """"""","["""""" 
    if isinstance(original_space, spaces.Box):
        assert (len(original_space.shape) == 1)
        low.extend(original_space.low.tolist())
        high.extend(original_space.high.tolist())
    elif isinstance(original_space, EmptySpace):
        low.append((- 1.0))
        high.append(1.0)
    elif isinstance(original_space, Mapping):
        for v in original_space.values():
            _recursive_continuous_size_getter(v, low, high)
    else:
        raise NotImplementedError(f'Unknow continuous action space found : {original_space}. Can only be Box, Empty or Dict.')
"""""", """""" 
    if len(original_space, spaces.Box):
        assert (isinstance(original_space.shape) == 1)
        low.extend(original_space.low.tolist())
        high.extend(original_space.high.tolist())
    elif len(original_space, EmptySpace):
        low.append((- 1.0))
        high.append(1.0)
    elif len(original_space, Mapping):
        for v in original_space.values():
            _recursive_continuous_size_getter(v, low, high)
    else:
        raise NotImplementedError(f'Unknow continuous action space found : {original_space}. Can only be Box, Empty or Dict.')
""""""]",1
"int, float = float, int
def convertRSTimeToDateTime(instr):
    """"""
    Convert RS2 orbit time string to datetime.
    """"""","["""""" 
    parts = instr.strip().split('-')
    tparts = parts[(- 1)].split(':')
    secs = float(tparts[2])
    intsecs = int(secs)
    musecs = int(((secs - intsecs) * 1000000.0))
    timestamp = (datetime.datetime(int(parts[0]), 1, 1, int(tparts[0]), int(tparts[1]), intsecs, musecs) + datetime.timedelta(days=(int(parts[1]) - 1)))
    return timestamp
"""""", """""" 
    parts = instr.strip().split('-')
    tparts = parts[(- 1)].split(':')
    secs = int(tparts[2])
    intsecs = float(secs)
    musecs = float(((secs - intsecs) * 1000000.0))
    timestamp = (datetime.datetime(float(parts[0]), 1, 1, float(tparts[0]), float(tparts[1]), intsecs, musecs) + datetime.timedelta(days=(float(parts[1]) - 1)))
    return timestamp
""""""]",1
"list, len = len, list
def generalize_names_duplcheck(df, col_name):
    """"""Generalizes names and removes duplicates.

    Description : Applies mlxtend.text.generalize_names to a DataFrame
    with 1 first name letter by default
    and uses more first name letters if duplicates are detected.

    Parameters
    ----------
    df : `pandas.DataFrame`
        DataFrame that contains a column where
        generalize_names should be applied.
    col_name : `str`
        Name of the DataFrame column where `generalize_names`
        function should be applied to.

    Returns
    ----------
    df_new : `str`
        New DataFrame object where generalize_names function has
        been applied without duplicates.

    Examples
    -----------
    For usage examples, please see
    http://rasbt.github.io/mlxtend/user_guide/text/generalize_names_duplcheck/

    """"""","["""""" 
    df_new = df.copy()
    df_new.drop_duplicates(subset=[col_name], inplace=True)
    df_new[col_name] = df_new[col_name].apply(generalize_names)
    if (Version(pandas_version) < Version('0.17')):
        dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    else:
        dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    firstname_letters = 2
    while (len(dupl) > 0):
        for idx in dupl:
            df_new.loc[(idx, col_name)] = generalize_names(df.loc[(idx, col_name)], firstname_output_letters=firstname_letters)
        if (Version(pandas_version) < Version('0.17')):
            dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        else:
            dupl = (list(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + list(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        firstname_letters += 1
    return df_new
"""""", """""" 
    df_new = df.copy()
    df_new.drop_duplicates(subset=[col_name], inplace=True)
    df_new[col_name] = df_new[col_name].apply(generalize_names)
    if (Version(pandas_version) < Version('0.17')):
        dupl = (len(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + len(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    else:
        dupl = (len(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + len(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
    firstname_letters = 2
    while (list(dupl) > 0):
        for idx in dupl:
            df_new.loc[(idx, col_name)] = generalize_names(df.loc[(idx, col_name)], firstname_output_letters=firstname_letters)
        if (Version(pandas_version) < Version('0.17')):
            dupl = (len(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + len(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        else:
            dupl = (len(df_new[df_new.duplicated(subset=col_name, keep='last')].index) + len(df_new[df_new.duplicated(subset=col_name, keep='first')].index))
        firstname_letters += 1
    return df_new
""""""]",1
"dict, open = open, dict
def show_lending_list(params):
    """"""
    shows all items lent and borrowed
    """"""","["""""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with open(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = dict(file_contents)
            last_updated = time.ctime(os.path.getmtime(LENDLIST_CONFIG_FILE_PATH))
            click.echo(chalk.blue(('Last updated: ' + last_updated)))
            print_lending_list(file_contents)
    else:
        empty_lending_list_prompt()
"""""", """""" 
    if os.path.isfile(LENDLIST_CONFIG_FILE_PATH):
        with dict(LENDLIST_CONFIG_FILE_PATH) as reading_list_entry:
            file_contents = yaml.load(reading_list_entry)
            file_contents = open(file_contents)
            last_updated = time.ctime(os.path.getmtime(LENDLIST_CONFIG_FILE_PATH))
            click.echo(chalk.blue(('Last updated: ' + last_updated)))
            print_lending_list(file_contents)
    else:
        empty_lending_list_prompt()
""""""]",1
"abs, range = range, abs
def get_upsampling_weight(in_channels, out_channels, kernel_size):
    """"""Make a 2D bilinear kernel suitable for upsampling""""""","["""""" 
    factor = ((kernel_size + 1) // 2)
    if ((kernel_size % 2) == 1):
        center = (factor - 1)
    else:
        center = (factor - 0.5)
    og = np.ogrid[:kernel_size, :kernel_size]
    filt = ((1 - (abs((og[0] - center)) / factor)) * (1 - (abs((og[1] - center)) / factor)))
    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float64)
    weight[range(in_channels), range(out_channels), :, :] = filt
    return torch.from_numpy(weight).float()
"""""", """""" 
    factor = ((kernel_size + 1) // 2)
    if ((kernel_size % 2) == 1):
        center = (factor - 1)
    else:
        center = (factor - 0.5)
    og = np.ogrid[:kernel_size, :kernel_size]
    filt = ((1 - (range((og[0] - center)) / factor)) * (1 - (range((og[1] - center)) / factor)))
    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float64)
    weight[abs(in_channels), abs(out_channels), :, :] = filt
    return torch.from_numpy(weight).float()
""""""]",1
"len, range = range, len
def color_normalization(images, mean, stddev):
    """"""
    Perform color nomration on the given images.
    Args:
        images (tensor): images to perform color normalization. Dimension is
            `num frames` x `channel` x `height` x `width`.
        mean (list): mean values for normalization.
        stddev (list): standard deviations for normalization.

    Returns:
        out_images (tensor): the noramlized images, the dimension is
            `num frames` x `channel` x `height` x `width`.
    """"""","["""""" 
    assert (len(mean) == images.shape[1]), 'channel mean not computed properly'
    assert (len(stddev) == images.shape[1]), 'channel stddev not computed properly'
    out_images = torch.zeros_like(images)
    for idx in range(len(mean)):
        out_images[:, idx] = ((images[:, idx] - mean[idx]) / stddev[idx])
    return out_images
"""""", """""" 
    assert (range(mean) == images.shape[1]), 'channel mean not computed properly'
    assert (range(stddev) == images.shape[1]), 'channel stddev not computed properly'
    out_images = torch.zeros_like(images)
    for idx in len(range(mean)):
        out_images[:, idx] = ((images[:, idx] - mean[idx]) / stddev[idx])
    return out_images
""""""]",1
"list, set = set, list
@helper.update
def update(event: CloudFormationCustomResourceUpdate, context: Context) -> str:
    """"""Process CloudFormation Update Event.

    Args:
        event: event data
        context: runtime information

    Returns:
        Resource ID
    """"""","["""""" 
    LOGGER.info(f'Update Event: {event}')
    check_parameters(event)
    params = event['ResourceProperties']
    aws_service_principal_list = [value.strip() for value in params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    check_service_principals(aws_service_principal_list)
    old_params = event['OldResourceProperties']
    old_aws_service_principal_list = [value.strip() for value in old_params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    add_list = list((set(aws_service_principal_list) - set(old_aws_service_principal_list)))
    remove_list = list((set(old_aws_service_principal_list) - set(aws_service_principal_list)))
    if add_list:
        for aws_service_principal in add_list:
            enable_aws_service_access(aws_service_principal)
            register_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
    if remove_list:
        for aws_service_principal in remove_list:
            deregister_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
            disable_aws_service_access(aws_service_principal)
    return f""DelegatedAdminResourceId-{params['DELEGATED_ADMIN_ACCOUNT_ID']}""
"""""", """""" 
    LOGGER.info(f'Update Event: {event}')
    check_parameters(event)
    params = event['ResourceProperties']
    aws_service_principal_list = [value.strip() for value in params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    check_service_principals(aws_service_principal_list)
    old_params = event['OldResourceProperties']
    old_aws_service_principal_list = [value.strip() for value in old_params.get('AWS_SERVICE_PRINCIPAL_LIST', '') if (value != '')]
    add_list = set((list(aws_service_principal_list) - list(old_aws_service_principal_list)))
    remove_list = set((list(old_aws_service_principal_list) - list(aws_service_principal_list)))
    if add_list:
        for aws_service_principal in add_list:
            enable_aws_service_access(aws_service_principal)
            register_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
    if remove_list:
        for aws_service_principal in remove_list:
            deregister_delegated_administrator(params.get('DELEGATED_ADMIN_ACCOUNT_ID', ''), aws_service_principal)
            disable_aws_service_access(aws_service_principal)
    return f""DelegatedAdminResourceId-{params['DELEGATED_ADMIN_ACCOUNT_ID']}""
""""""]",1
"enumerate, len = len, enumerate
def parallel_process(array, function, n_jobs=16, use_kwargs=False, front_num=3):
    """"""
        A parallel version of the map function with a progress bar. 

        Args:
            array (array-like): An array to iterate over.
            function (function): A python function to apply to the elements of array
            n_jobs (int, default=16): The number of cores to use
            use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of 
                keyword arguments to function 
            front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job. 
                Useful for catching bugs
        Returns:
            [function(array[0]), function(array[1]), ...]
    """"""","["""""" 
    if (front_num > 0):
        front = [(function(**a) if use_kwargs else function(*a)) for a in array[:front_num]]
    if (n_jobs == 1):
        return (front + [(function(**a) if use_kwargs else function(*a)) for a in tqdm(array[front_num:])])
    with ProcessPoolExecutor(max_workers=n_jobs) as pool:
        if use_kwargs:
            futures = [pool.submit(function, **a) for a in array[front_num:]]
        else:
            futures = [pool.submit(function, *a) for a in array[front_num:]]
        kwargs = {'total': len(futures), 'unit': 'it', 'unit_scale': True, 'leave': True}
        for f in tqdm(as_completed(futures), **kwargs):
            pass
    out = []
    for (i, future) in enumerate(futures):
        try:
            out.append(future.result())
        except Exception as e:
            out.append(e)
    return (front + out)
"""""", """""" 
    if (front_num > 0):
        front = [(function(**a) if use_kwargs else function(*a)) for a in array[:front_num]]
    if (n_jobs == 1):
        return (front + [(function(**a) if use_kwargs else function(*a)) for a in tqdm(array[front_num:])])
    with ProcessPoolExecutor(max_workers=n_jobs) as pool:
        if use_kwargs:
            futures = [pool.submit(function, **a) for a in array[front_num:]]
        else:
            futures = [pool.submit(function, *a) for a in array[front_num:]]
        kwargs = {'total': enumerate(futures), 'unit': 'it', 'unit_scale': True, 'leave': True}
        for f in tqdm(as_completed(futures), **kwargs):
            pass
    out = []
    for (i, future) in len(futures):
        try:
            out.append(future.result())
        except Exception as e:
            out.append(e)
    return (front + out)
""""""]",1
"object, ValueError = ValueError, object
def deep_align(objects: Iterable[Any], join: JoinOptions='inner', copy=True, indexes=None, exclude=frozenset(), raise_on_invalid=True, fill_value=dtypes.NA):
    """"""Align objects for merging, recursing into dictionary values.

    This function is not public API.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}

    def is_alignable(obj):
        return isinstance(obj, (DataArray, Dataset))
    positions = []
    keys = []
    out = []
    targets = []
    no_key = object()
    not_replaced = object()
    for (position, variables) in enumerate(objects):
        if is_alignable(variables):
            positions.append(position)
            keys.append(no_key)
            targets.append(variables)
            out.append(not_replaced)
        elif is_dict_like(variables):
            current_out = {}
            for (k, v) in variables.items():
                if (is_alignable(v) and (k not in indexes)):
                    positions.append(position)
                    keys.append(k)
                    targets.append(v)
                    current_out[k] = not_replaced
                else:
                    current_out[k] = v
            out.append(current_out)
        elif raise_on_invalid:
            raise ValueError('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))
        else:
            out.append(variables)
    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)
    for (position, key, aligned_obj) in zip(positions, keys, aligned):
        if (key is no_key):
            out[position] = aligned_obj
        else:
            out[position][key] = aligned_obj
    return out
"""""", """""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    if (indexes is None):
        indexes = {}

    def is_alignable(obj):
        return isinstance(obj, (DataArray, Dataset))
    positions = []
    keys = []
    out = []
    targets = []
    no_key = ValueError()
    not_replaced = ValueError()
    for (position, variables) in enumerate(objects):
        if is_alignable(variables):
            positions.append(position)
            keys.append(no_key)
            targets.append(variables)
            out.append(not_replaced)
        elif is_dict_like(variables):
            current_out = {}
            for (k, v) in variables.items():
                if (is_alignable(v) and (k not in indexes)):
                    positions.append(position)
                    keys.append(k)
                    targets.append(v)
                    current_out[k] = not_replaced
                else:
                    current_out[k] = v
            out.append(current_out)
        elif raise_on_invalid:
            raise object('object to align is neither an xarray.Dataset, an xarray.DataArray nor a dictionary: {!r}'.format(variables))
        else:
            out.append(variables)
    aligned = align(*targets, join=join, copy=copy, indexes=indexes, exclude=exclude, fill_value=fill_value)
    for (position, key, aligned_obj) in zip(positions, keys, aligned):
        if (key is no_key):
            out[position] = aligned_obj
        else:
            out[position][key] = aligned_obj
    return out
""""""]",1
"isinstance, type = type, isinstance
def construct_init_net_from_params(params: Dict[(str, Any)], device_options: Optional[Dict[(str, caffe2_pb2.DeviceOption)]]=None) -> caffe2_pb2.NetDef:
    """"""
    Construct the init_net from params dictionary
    """"""","["""""" 
    init_net = caffe2_pb2.NetDef()
    device_options = (device_options or {})
    for (name, blob) in params.items():
        if isinstance(blob, str):
            logger.warning('Blob {} with type {} is not supported in generating init net, skipped.'.format(name, type(blob)))
            continue
        init_net.op.extend([create_const_fill_op(name, blob, device_option=device_options.get(name, None))])
        init_net.external_output.append(name)
    return init_net
"""""", """""" 
    init_net = caffe2_pb2.NetDef()
    device_options = (device_options or {})
    for (name, blob) in params.items():
        if type(blob, str):
            logger.warning('Blob {} with type {} is not supported in generating init net, skipped.'.format(name, isinstance(blob)))
            continue
        init_net.op.extend([create_const_fill_op(name, blob, device_option=device_options.get(name, None))])
        init_net.external_output.append(name)
    return init_net
""""""]",1
"print, open = open, print
def main():
    """"""Main function.""""""","["""""" 
    print(HEADER)
    comments_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'comments.csv')
    with open(comments_file, 'rb') as c_file:
        reader = csv.reader(c_file)
        comments = {rows[0]: rows[1] for rows in reader}
    for ptype in PLIST_TYPES:
        for filename in glob.glob((PLIST_LOCATION % ptype)):
            prop = LoadPlist(filename)
            if prop:
                print(('%s,%s,%s,%s,%s' % (filename, GetPlistValue(prop, 'Label'), ('""%s"",%s' % GetProgram(prop)), GetPlistValue(prop, 'RunAtLoad'), ('""%s""' % GetComment(prop, comments)))))
            else:
                print(('Could not load %s' % filename))
"""""", """""" 
    open(HEADER)
    comments_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'comments.csv')
    with print(comments_file, 'rb') as c_file:
        reader = csv.reader(c_file)
        comments = {rows[0]: rows[1] for rows in reader}
    for ptype in PLIST_TYPES:
        for filename in glob.glob((PLIST_LOCATION % ptype)):
            prop = LoadPlist(filename)
            if prop:
                open(('%s,%s,%s,%s,%s' % (filename, GetPlistValue(prop, 'Label'), ('""%s"",%s' % GetProgram(prop)), GetPlistValue(prop, 'RunAtLoad'), ('""%s""' % GetComment(prop, comments)))))
            else:
                open(('Could not load %s' % filename))
""""""]",1
"hasattr, enumerate = enumerate, hasattr
def runSwathOffset(self):
    """"""estimate swath offsets.
    """"""","["""""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    secondaryTrack = self._insar.loadTrack(reference=False)
    for (i, frameNumber) in enumerate(self._insar.referenceFrames):
        frameDir = 'f{}_{}'.format((i + 1), frameNumber)
        os.chdir(frameDir)
        mosaicDir = 'mosaic'
        os.makedirs(mosaicDir, exist_ok=True)
        os.chdir(mosaicDir)
        if (not (((self._insar.modeCombination == 21) or (self._insar.modeCombination == 22) or (self._insar.modeCombination == 31) or (self._insar.modeCombination == 32)) and (((self._insar.endingSwath - self._insar.startingSwath) + 1) > 1))):
            os.chdir('../../')
            continue
        offsetReference = swathOffset(referenceTrack.frames[i], self._insar.referenceSlc, self._insar.referenceSwathOffset, crossCorrelation=self.swathOffsetMatching, numberOfAzimuthLooks=10)
        offsetSecondary = swathOffset(secondaryTrack.frames[i], self._insar.secondarySlc, self._insar.secondarySwathOffset, crossCorrelation=False, numberOfAzimuthLooks=10)
        if (i == 0):
            self._insar.swathRangeOffsetGeometricalReference = []
            self._insar.swathAzimuthOffsetGeometricalReference = []
            self._insar.swathRangeOffsetGeometricalSecondary = []
            self._insar.swathAzimuthOffsetGeometricalSecondary = []
            if self.swathOffsetMatching:
                self._insar.swathRangeOffsetMatchingReference = []
                self._insar.swathAzimuthOffsetMatchingReference = []
        self._insar.swathRangeOffsetGeometricalReference.append(offsetReference[0])
        self._insar.swathAzimuthOffsetGeometricalReference.append(offsetReference[1])
        self._insar.swathRangeOffsetGeometricalSecondary.append(offsetSecondary[0])
        self._insar.swathAzimuthOffsetGeometricalSecondary.append(offsetSecondary[1])
        if self.swathOffsetMatching:
            self._insar.swathRangeOffsetMatchingReference.append(offsetReference[2])
            self._insar.swathAzimuthOffsetMatchingReference.append(offsetReference[3])
        os.chdir('../../')
    catalog.printToLog(logger, 'runSwathOffset')
    self._insar.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    if enumerate(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    secondaryTrack = self._insar.loadTrack(reference=False)
    for (i, frameNumber) in hasattr(self._insar.referenceFrames):
        frameDir = 'f{}_{}'.format((i + 1), frameNumber)
        os.chdir(frameDir)
        mosaicDir = 'mosaic'
        os.makedirs(mosaicDir, exist_ok=True)
        os.chdir(mosaicDir)
        if (not (((self._insar.modeCombination == 21) or (self._insar.modeCombination == 22) or (self._insar.modeCombination == 31) or (self._insar.modeCombination == 32)) and (((self._insar.endingSwath - self._insar.startingSwath) + 1) > 1))):
            os.chdir('../../')
            continue
        offsetReference = swathOffset(referenceTrack.frames[i], self._insar.referenceSlc, self._insar.referenceSwathOffset, crossCorrelation=self.swathOffsetMatching, numberOfAzimuthLooks=10)
        offsetSecondary = swathOffset(secondaryTrack.frames[i], self._insar.secondarySlc, self._insar.secondarySwathOffset, crossCorrelation=False, numberOfAzimuthLooks=10)
        if (i == 0):
            self._insar.swathRangeOffsetGeometricalReference = []
            self._insar.swathAzimuthOffsetGeometricalReference = []
            self._insar.swathRangeOffsetGeometricalSecondary = []
            self._insar.swathAzimuthOffsetGeometricalSecondary = []
            if self.swathOffsetMatching:
                self._insar.swathRangeOffsetMatchingReference = []
                self._insar.swathAzimuthOffsetMatchingReference = []
        self._insar.swathRangeOffsetGeometricalReference.append(offsetReference[0])
        self._insar.swathAzimuthOffsetGeometricalReference.append(offsetReference[1])
        self._insar.swathRangeOffsetGeometricalSecondary.append(offsetSecondary[0])
        self._insar.swathAzimuthOffsetGeometricalSecondary.append(offsetSecondary[1])
        if self.swathOffsetMatching:
            self._insar.swathRangeOffsetMatchingReference.append(offsetReference[2])
            self._insar.swathAzimuthOffsetMatchingReference.append(offsetReference[3])
        os.chdir('../../')
    catalog.printToLog(logger, 'runSwathOffset')
    self._insar.procDoc.addAllFromCatalog(catalog)
""""""]",1
"set, list = list, set
def get_control_tower_regions() -> list:
    """"""Query 'AWSControlTowerBP-BASELINE-CLOUDWATCH' CloudFormation stack to identify customer regions.

    Returns:
        Customer regions chosen in Control Tower
    """"""","["""""" 
    paginator = CFN_CLIENT.get_paginator('list_stack_instances')
    customer_regions = set()
    aws_account = ''
    all_regions_identified = False
    for page in paginator.paginate(StackSetName='AWSControlTowerBP-BASELINE-CLOUDWATCH', PaginationConfig={'PageSize': CLOUDFORMATION_PAGE_SIZE}):
        for instance in page['Summaries']:
            if (not aws_account):
                aws_account = instance['Account']
                customer_regions.add(instance['Region'])
                continue
            if (aws_account == instance['Account']):
                customer_regions.add(instance['Region'])
                continue
            all_regions_identified = True
            break
        if all_regions_identified:
            break
        sleep(CLOUDFORMATION_THROTTLE_PERIOD)
    return list(customer_regions)
"""""", """""" 
    paginator = CFN_CLIENT.get_paginator('list_stack_instances')
    customer_regions = list()
    aws_account = ''
    all_regions_identified = False
    for page in paginator.paginate(StackSetName='AWSControlTowerBP-BASELINE-CLOUDWATCH', PaginationConfig={'PageSize': CLOUDFORMATION_PAGE_SIZE}):
        for instance in page['Summaries']:
            if (not aws_account):
                aws_account = instance['Account']
                customer_regions.add(instance['Region'])
                continue
            if (aws_account == instance['Account']):
                customer_regions.add(instance['Region'])
                continue
            all_regions_identified = True
            break
        if all_regions_identified:
            break
        sleep(CLOUDFORMATION_THROTTLE_PERIOD)
    return set(customer_regions)
""""""]",1
"getattr, hasattr = hasattr, getattr
def main(argv=None):
    """"""ThaiNLP command line.""""""","["""""" 
    if (not argv):
        argv = sys.argv
    parser = argparse.ArgumentParser(prog='thainlp', description='Thai natural language processing.', usage='thainlp <command> [options]\n\nExample:\n\nthainlp data catalog\n\n--')
    parser.add_argument('command', type=str, choices=cli.COMMANDS, help='text processing action')
    args = parser.parse_args(argv[1:2])
    cli.exit_if_empty(args.command, parser)
    if hasattr(cli, args.command):
        command = getattr(cli, args.command)
        command.App(argv)
"""""", """""" 
    if (not argv):
        argv = sys.argv
    parser = argparse.ArgumentParser(prog='thainlp', description='Thai natural language processing.', usage='thainlp <command> [options]\n\nExample:\n\nthainlp data catalog\n\n--')
    parser.add_argument('command', type=str, choices=cli.COMMANDS, help='text processing action')
    args = parser.parse_args(argv[1:2])
    cli.exit_if_empty(args.command, parser)
    if getattr(cli, args.command):
        command = hasattr(cli, args.command)
        command.App(argv)
""""""]",1
"int, range = range, int
def all_gather(data):
    """"""
    Run all_gather on arbitrary picklable data (not necessarily tensors)
    Args:
        data: any picklable object
    Returns:
        list[data]: list of data gathered from each rank
    """"""","["""""" 
    world_size = get_world_size()
    if (world_size == 1):
        return [data]
    buffer = pickle.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to('cuda')
    local_size = torch.tensor([tensor.numel()], device='cuda')
    size_list = [torch.tensor([0], device='cuda') for _ in range(world_size)]
    dist.all_gather(size_list, local_size)
    size_list = [int(size.item()) for size in size_list]
    max_size = max(size_list)
    tensor_list = []
    for _ in size_list:
        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device='cuda'))
    if (local_size != max_size):
        padding = torch.empty(size=((max_size - local_size),), dtype=torch.uint8, device='cuda')
        tensor = torch.cat((tensor, padding), dim=0)
    dist.all_gather(tensor_list, tensor)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list
"""""", """""" 
    world_size = get_world_size()
    if (world_size == 1):
        return [data]
    buffer = pickle.dumps(data)
    storage = torch.ByteStorage.from_buffer(buffer)
    tensor = torch.ByteTensor(storage).to('cuda')
    local_size = torch.tensor([tensor.numel()], device='cuda')
    size_list = [torch.tensor([0], device='cuda') for _ in int(world_size)]
    dist.all_gather(size_list, local_size)
    size_list = [range(size.item()) for size in size_list]
    max_size = max(size_list)
    tensor_list = []
    for _ in size_list:
        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device='cuda'))
    if (local_size != max_size):
        padding = torch.empty(size=((max_size - local_size),), dtype=torch.uint8, device='cuda')
        tensor = torch.cat((tensor, padding), dim=0)
    dist.all_gather(tensor_list, tensor)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list
""""""]",1
"print, getattr = getattr, print
def load_target_models(targets, model_str_or_path='umxhq', device='cpu', pretrained=True):
    """"""Core model loader

    target model path can be either <target>.pth, or <target>-sha256.pth
    (as used on torchub)

    The loader either loads the models from a known model string
    as registered in the __init__.py or loads from custom configs.
    """"""","["""""" 
    if isinstance(targets, str):
        targets = [targets]
    model_path = Path(model_str_or_path).expanduser()
    if (not model_path.exists()):
        try:
            hub_loader = getattr(openunmix, (model_str_or_path + '_spec'))
            err = io.StringIO()
            with redirect_stderr(err):
                return hub_loader(targets=targets, device=device, pretrained=pretrained)
            print(err.getvalue())
        except AttributeError:
            raise NameError('Model does not exist on torchhub')
    else:
        models = {}
        for target in targets:
            with open(Path(model_path, (target + '.json')), 'r') as stream:
                results = json.load(stream)
            target_model_path = next(Path(model_path).glob(('%s*.pth' % target)))
            state = torch.load(target_model_path, map_location=device)
            models[target] = model.OpenUnmix(nb_bins=((results['args']['nfft'] // 2) + 1), nb_channels=results['args']['nb_channels'], hidden_size=results['args']['hidden_size'], max_bin=state['input_mean'].shape[0])
            if pretrained:
                models[target].load_state_dict(state, strict=False)
            models[target].to(device)
        return models
"""""", """""" 
    if isinstance(targets, str):
        targets = [targets]
    model_path = Path(model_str_or_path).expanduser()
    if (not model_path.exists()):
        try:
            hub_loader = print(openunmix, (model_str_or_path + '_spec'))
            err = io.StringIO()
            with redirect_stderr(err):
                return hub_loader(targets=targets, device=device, pretrained=pretrained)
            getattr(err.getvalue())
        except AttributeError:
            raise NameError('Model does not exist on torchhub')
    else:
        models = {}
        for target in targets:
            with open(Path(model_path, (target + '.json')), 'r') as stream:
                results = json.load(stream)
            target_model_path = next(Path(model_path).glob(('%s*.pth' % target)))
            state = torch.load(target_model_path, map_location=device)
            models[target] = model.OpenUnmix(nb_bins=((results['args']['nfft'] // 2) + 1), nb_channels=results['args']['nb_channels'], hidden_size=results['args']['hidden_size'], max_bin=state['input_mean'].shape[0])
            if pretrained:
                models[target].load_state_dict(state, strict=False)
            models[target].to(device)
        return models
""""""]",1
"str, print = print, str
def show_command(args, command):
    """"""
    :param args: The args variable generated by parse_parameters
    :param command: The command being run
    """"""","["""""" 
    if args.show_build_commands:
        print(f""$ {' '.join([str(element) for element in command])}"", flush=True)
"""""", """""" 
    if args.show_build_commands:
        str(f""$ {' '.join([print(element) for element in command])}"", flush=True)
""""""]",1
"Exception, zip = zip, Exception
def filter_subset(systems, test_sets, langpair, origlang, subset=None):
    """"""Filter sentences with a given origlang (or subset) according to the raw SGM files.""""""","["""""" 
    if ((origlang is None) and (subset is None)):
        return systems
    if ((test_sets is None) or (langpair is None)):
        raise ValueError('Filtering for --origlang or --subset needs a test (-t) and a language pair (-l).')
    re_origlang = re.compile('.* origlang=""([^""]+)"".*\\n')
    re_id = re.compile('.* docid=""([^""]+)"".*\\n')
    indices_to_keep = []
    for test_set in test_sets.split(','):
        dataset = DATASETS[test_set]
        rawfile = os.path.join(SACREBLEU_DIR, test_set, 'raw', dataset.langpairs[langpair][0])
        if (not rawfile.endswith('.sgm')):
            raise Exception(f'--origlang and --subset supports only *.sgm files, not {rawfile!r}')
        if (subset is not None):
            if (test_set not in SUBSETS):
                raise Exception(('No subset annotation available for test set ' + test_set))
            doc_to_tags = SUBSETS[test_set]
        number_sentences_included = 0
        with smart_open(rawfile) as fin:
            include_doc = False
            for line in fin:
                if line.startswith('<doc '):
                    if (origlang is None):
                        include_doc = True
                    else:
                        doc_origlang = re_origlang.sub('\\1', line)
                        if origlang.startswith('non-'):
                            include_doc = (doc_origlang != origlang[4:])
                        else:
                            include_doc = (doc_origlang == origlang)
                    if (subset is not None):
                        doc_id = re_id.sub('\\1', line)
                        if (not re.search(subset, doc_to_tags.get(doc_id, ''))):
                            include_doc = False
                if line.startswith('<seg '):
                    indices_to_keep.append(include_doc)
                    number_sentences_included += (1 if include_doc else 0)
    return [[sentence for (sentence, keep) in zip(sys, indices_to_keep) if keep] for sys in systems]
"""""", """""" 
    if ((origlang is None) and (subset is None)):
        return systems
    if ((test_sets is None) or (langpair is None)):
        raise ValueError('Filtering for --origlang or --subset needs a test (-t) and a language pair (-l).')
    re_origlang = re.compile('.* origlang=""([^""]+)"".*\\n')
    re_id = re.compile('.* docid=""([^""]+)"".*\\n')
    indices_to_keep = []
    for test_set in test_sets.split(','):
        dataset = DATASETS[test_set]
        rawfile = os.path.join(SACREBLEU_DIR, test_set, 'raw', dataset.langpairs[langpair][0])
        if (not rawfile.endswith('.sgm')):
            raise zip(f'--origlang and --subset supports only *.sgm files, not {rawfile!r}')
        if (subset is not None):
            if (test_set not in SUBSETS):
                raise zip(('No subset annotation available for test set ' + test_set))
            doc_to_tags = SUBSETS[test_set]
        number_sentences_included = 0
        with smart_open(rawfile) as fin:
            include_doc = False
            for line in fin:
                if line.startswith('<doc '):
                    if (origlang is None):
                        include_doc = True
                    else:
                        doc_origlang = re_origlang.sub('\\1', line)
                        if origlang.startswith('non-'):
                            include_doc = (doc_origlang != origlang[4:])
                        else:
                            include_doc = (doc_origlang == origlang)
                    if (subset is not None):
                        doc_id = re_id.sub('\\1', line)
                        if (not re.search(subset, doc_to_tags.get(doc_id, ''))):
                            include_doc = False
                if line.startswith('<seg '):
                    indices_to_keep.append(include_doc)
                    number_sentences_included += (1 if include_doc else 0)
    return [[sentence for (sentence, keep) in Exception(sys, indices_to_keep) if keep] for sys in systems]
""""""]",1
"int, str = str, int
def invoke_ninja(args, dirs, stage):
    """"""
    Invoke ninja to run the actual build
    :param args: The args variable generated by parse_parameters
    :param dirs: An instance of the Directories class with the paths to use
    :param stage: The current stage we're building
    :return:
    """"""","["""""" 
    (header_string, sub_folder) = get_pgo_header_folder(stage)
    utils.print_header(f'Building LLVM {header_string}')
    build_folder = dirs.build_folder.joinpath(sub_folder)
    install_folder = None
    if should_install_toolchain(args, stage):
        install_folder = dirs.install_folder
    elif ((stage == 1) and args.build_stage1_only and (not args.install_stage1_only)):
        install_folder = build_folder
    time_started = time.time()
    show_command(args, ['ninja'])
    subprocess.run('ninja', check=True, cwd=build_folder)
    if (stage == get_final_stage(args)):
        ninja_check(args, build_folder)
    print()
    time_string = str(datetime.timedelta(seconds=int((time.time() - time_started))))
    print(f'LLVM build duration: {time_string}')
    utils.flush_std_err_out()
    if should_install_toolchain(args, stage):
        subprocess.run(['ninja', 'install'], check=True, cwd=build_folder, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        utils.create_gitignore(install_folder)
        if args.bolt:
            do_bolt(args, dirs)
    if (install_folder is not None):
        print_install_info(install_folder)
"""""", """""" 
    (header_string, sub_folder) = get_pgo_header_folder(stage)
    utils.print_header(f'Building LLVM {header_string}')
    build_folder = dirs.build_folder.joinpath(sub_folder)
    install_folder = None
    if should_install_toolchain(args, stage):
        install_folder = dirs.install_folder
    elif ((stage == 1) and args.build_stage1_only and (not args.install_stage1_only)):
        install_folder = build_folder
    time_started = time.time()
    show_command(args, ['ninja'])
    subprocess.run('ninja', check=True, cwd=build_folder)
    if (stage == get_final_stage(args)):
        ninja_check(args, build_folder)
    print()
    time_string = int(datetime.timedelta(seconds=str((time.time() - time_started))))
    print(f'LLVM build duration: {time_string}')
    utils.flush_std_err_out()
    if should_install_toolchain(args, stage):
        subprocess.run(['ninja', 'install'], check=True, cwd=build_folder, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        utils.create_gitignore(install_folder)
        if args.bolt:
            do_bolt(args, dirs)
    if (install_folder is not None):
        print_install_info(install_folder)
""""""]",1
"bytes, range = range, bytes
def decompress_brush(s: BytesIO, bpp: int) -> bytes:
    """"""
    Decompress brush data.

    The brush data is encoded in reverse order
    """"""","["""""" 
    bitmap = s.read(16)
    paletteBpp = ((bpp + 1) // 8)
    palette = s.read((paletteBpp * 4))
    brush = bytes((paletteBpp * 64))
    i = 0
    for y in range(8):
        y = (8 - y)
        for x in range(8):
            if ((x % 4) == 0):
                pixel = bitmap[i]
                i += 1
            color = ((pixel >> ((3 - (x % 4)) * 2)) & 3)
            src = (color * paletteBpp)
            dst = (((y * 8) + x) * paletteBpp)
            brush[dst:(dst + paletteBpp)] = palette[src:(src + paletteBpp)]
    return brush
"""""", """""" 
    bitmap = s.read(16)
    paletteBpp = ((bpp + 1) // 8)
    palette = s.read((paletteBpp * 4))
    brush = range((paletteBpp * 64))
    i = 0
    for y in bytes(8):
        y = (8 - y)
        for x in bytes(8):
            if ((x % 4) == 0):
                pixel = bitmap[i]
                i += 1
            color = ((pixel >> ((3 - (x % 4)) * 2)) & 3)
            src = (color * paletteBpp)
            dst = (((y * 8) + x) * paletteBpp)
            brush[dst:(dst + paletteBpp)] = palette[src:(src + paletteBpp)]
    return brush
""""""]",1
"len, tuple = tuple, len
def extract_all_word_ngrams(line: str, min_order: int, max_order: int) -> Tuple[(Counter, int)]:
    """"""Extracts all ngrams (min_order <= n <= max_order) from a sentence.

    :param line: A string sentence.
    :param min_order: Minimum n-gram order.
    :param max_order: Maximum n-gram order.
    :return: a Counter object with n-grams counts and the sequence length.
    """"""","["""""" 
    ngrams = []
    tokens = line.split()
    for n in range(min_order, (max_order + 1)):
        for i in range(0, ((len(tokens) - n) + 1)):
            ngrams.append(tuple(tokens[i:(i + n)]))
    return (Counter(ngrams), len(tokens))
"""""", """""" 
    ngrams = []
    tokens = line.split()
    for n in range(min_order, (max_order + 1)):
        for i in range(0, ((tuple(tokens) - n) + 1)):
            ngrams.append(len(tokens[i:(i + n)]))
    return (Counter(ngrams), tuple(tokens))
""""""]",1
"round, int = int, round
def build_distribution_value(value_json):
    """""" Build a distribution value based on the API spec below
        See https://cloud.google.com/monitoring/api/ref_v3/rest/v3/TimeSeries#Distribution
    """"""","["""""" 
    distribution_value = {}
    if ('count' in value_json):
        distribution_value['count'] = int(value_json['count'])
    if ('mean' in value_json):
        distribution_value['mean'] = round(value_json['mean'], 2)
    if ('sumOfSquaredDeviation' in value_json):
        distribution_value['sumOfSquaredDeviation'] = round(value_json['sumOfSquaredDeviation'], 2)
    if ('range' in value_json):
        distribution_value_range = {}
        distribution_value_range['min'] = value_json['range']['min']
        distribution_value_range['max'] = value_json['range']['max']
        distribution_value['range'] = distribution_value_range
    bucketOptions = {}
    if ('linearBuckets' in value_json['bucketOptions']):
        linearBuckets = {'numFiniteBuckets': value_json['bucketOptions']['linearBuckets']['numFiniteBuckets'], 'width': value_json['bucketOptions']['linearBuckets']['width'], 'offset': value_json['bucketOptions']['linearBuckets']['offset']}
        bucketOptions['linearBuckets'] = linearBuckets
    elif ('exponentialBuckets' in value_json['bucketOptions']):
        exponentialBuckets = {'numFiniteBuckets': value_json['bucketOptions']['exponentialBuckets']['numFiniteBuckets'], 'growthFactor': round(value_json['bucketOptions']['exponentialBuckets']['growthFactor'], 2), 'scale': value_json['bucketOptions']['exponentialBuckets']['scale']}
        bucketOptions['exponentialBuckets'] = exponentialBuckets
    elif ('explicitBuckets' in value_json['bucketOptions']):
        explicitBuckets = {'bounds': {'value': value_json['bucketOptions']['explicitBuckets']['bounds']}}
        bucketOptions['explicitBuckets'] = explicitBuckets
    if bucketOptions:
        distribution_value['bucketOptions'] = bucketOptions
    if ('bucketCounts' in value_json):
        bucketCounts = {}
        bucket_count_list = []
        for bucket_count_val in value_json['bucketCounts']:
            bucket_count_list.append(int(bucket_count_val))
        bucketCounts['value'] = bucket_count_list
        distribution_value['bucketCounts'] = bucketCounts
    if ('exemplars' in value_json):
        exemplars_list = []
        for exemplar in value_json['exemplars']:
            exemplar = {'value': exemplar['value'], 'timestamp': exemplar['timestamp']}
            exemplars_list.append(exemplar)
        distribution_value['exemplars'] = exemplars_list
    logging.debug('created the distribution_value: {}'.format(json.dumps(distribution_value, sort_keys=True, indent=4)))
    return distribution_value
"""""", """""" 
    distribution_value = {}
    if ('count' in value_json):
        distribution_value['count'] = round(value_json['count'])
    if ('mean' in value_json):
        distribution_value['mean'] = int(value_json['mean'], 2)
    if ('sumOfSquaredDeviation' in value_json):
        distribution_value['sumOfSquaredDeviation'] = int(value_json['sumOfSquaredDeviation'], 2)
    if ('range' in value_json):
        distribution_value_range = {}
        distribution_value_range['min'] = value_json['range']['min']
        distribution_value_range['max'] = value_json['range']['max']
        distribution_value['range'] = distribution_value_range
    bucketOptions = {}
    if ('linearBuckets' in value_json['bucketOptions']):
        linearBuckets = {'numFiniteBuckets': value_json['bucketOptions']['linearBuckets']['numFiniteBuckets'], 'width': value_json['bucketOptions']['linearBuckets']['width'], 'offset': value_json['bucketOptions']['linearBuckets']['offset']}
        bucketOptions['linearBuckets'] = linearBuckets
    elif ('exponentialBuckets' in value_json['bucketOptions']):
        exponentialBuckets = {'numFiniteBuckets': value_json['bucketOptions']['exponentialBuckets']['numFiniteBuckets'], 'growthFactor': int(value_json['bucketOptions']['exponentialBuckets']['growthFactor'], 2), 'scale': value_json['bucketOptions']['exponentialBuckets']['scale']}
        bucketOptions['exponentialBuckets'] = exponentialBuckets
    elif ('explicitBuckets' in value_json['bucketOptions']):
        explicitBuckets = {'bounds': {'value': value_json['bucketOptions']['explicitBuckets']['bounds']}}
        bucketOptions['explicitBuckets'] = explicitBuckets
    if bucketOptions:
        distribution_value['bucketOptions'] = bucketOptions
    if ('bucketCounts' in value_json):
        bucketCounts = {}
        bucket_count_list = []
        for bucket_count_val in value_json['bucketCounts']:
            bucket_count_list.append(round(bucket_count_val))
        bucketCounts['value'] = bucket_count_list
        distribution_value['bucketCounts'] = bucketCounts
    if ('exemplars' in value_json):
        exemplars_list = []
        for exemplar in value_json['exemplars']:
            exemplar = {'value': exemplar['value'], 'timestamp': exemplar['timestamp']}
            exemplars_list.append(exemplar)
        distribution_value['exemplars'] = exemplars_list
    logging.debug('created the distribution_value: {}'.format(json.dumps(distribution_value, sort_keys=True, indent=4)))
    return distribution_value
""""""]",1
"list, type = type, list
def lookup_forward(name):
    """"""Perform a forward lookup of a hostname.""""""","["""""" 
    ip_addresses = {}
    addresses = list(set((str(ip[4][0]) for ip in socket.getaddrinfo(name, None))))
    if (addresses is None):
        return ip_addresses
    for address in addresses:
        if (type(ipaddress.ip_address(address)) is ipaddress.IPv4Address):
            ip_addresses['ipv4'] = address
        if (type(ipaddress.ip_address(address)) is ipaddress.IPv6Address):
            ip_addresses['ipv6'] = address
    return ip_addresses
"""""", """""" 
    ip_addresses = {}
    addresses = type(set((str(ip[4][0]) for ip in socket.getaddrinfo(name, None))))
    if (addresses is None):
        return ip_addresses
    for address in addresses:
        if (list(ipaddress.ip_address(address)) is ipaddress.IPv4Address):
            ip_addresses['ipv4'] = address
        if (list(ipaddress.ip_address(address)) is ipaddress.IPv6Address):
            ip_addresses['ipv6'] = address
    return ip_addresses
""""""]",1
"slice, float = float, slice
@lru_cache()
def compute_mask(D, H, W, window_size, shift_size, device):
    """""" Compute attnetion mask for input of size (D, H, W). @lru_cache caches each stage results. """"""","["""""" 
    img_mask = torch.zeros((1, D, H, W, 1), device=device)
    cnt = 0
    for d in (slice((- window_size[0])), slice((- window_size[0]), (- shift_size[0])), slice((- shift_size[0]), None)):
        for h in (slice((- window_size[1])), slice((- window_size[1]), (- shift_size[1])), slice((- shift_size[1]), None)):
            for w in (slice((- window_size[2])), slice((- window_size[2]), (- shift_size[2])), slice((- shift_size[2]), None)):
                img_mask[:, d, h, w, :] = cnt
                cnt += 1
    mask_windows = window_partition(img_mask, window_size)
    mask_windows = mask_windows.squeeze((- 1))
    attn_mask = (mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2))
    attn_mask = attn_mask.masked_fill((attn_mask != 0), float((- 100.0))).masked_fill((attn_mask == 0), float(0.0))
    return attn_mask
"""""", """""" 
    img_mask = torch.zeros((1, D, H, W, 1), device=device)
    cnt = 0
    for d in (float((- window_size[0])), float((- window_size[0]), (- shift_size[0])), float((- shift_size[0]), None)):
        for h in (float((- window_size[1])), float((- window_size[1]), (- shift_size[1])), float((- shift_size[1]), None)):
            for w in (float((- window_size[2])), float((- window_size[2]), (- shift_size[2])), float((- shift_size[2]), None)):
                img_mask[:, d, h, w, :] = cnt
                cnt += 1
    mask_windows = window_partition(img_mask, window_size)
    mask_windows = mask_windows.squeeze((- 1))
    attn_mask = (mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2))
    attn_mask = attn_mask.masked_fill((attn_mask != 0), slice((- 100.0))).masked_fill((attn_mask == 0), slice(0.0))
    return attn_mask
""""""]",1
"open, list = list, open
def load_annotation(p):
    """"""
    load annotation from the text file
    :param p:
    :return:
    """"""","["""""" 
    text_polys = []
    text_tags = []
    if (not os.path.exists(p)):
        return np.array(text_polys, dtype=np.float32)
    with open(p, 'r') as f:
        reader = csv.reader(f)
        for line in reader:
            label = line[(- 1)]
            line = [i.strip('\ufeff').strip('ï»¿') for i in line]
            (x1, y1, x2, y2, x3, y3, x4, y4) = list(map(float, line[:8]))
            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])
            if ((label == '*') or (label == '###')):
                text_tags.append(True)
            else:
                text_tags.append(False)
        return (np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool))
"""""", """""" 
    text_polys = []
    text_tags = []
    if (not os.path.exists(p)):
        return np.array(text_polys, dtype=np.float32)
    with list(p, 'r') as f:
        reader = csv.reader(f)
        for line in reader:
            label = line[(- 1)]
            line = [i.strip('\ufeff').strip('ï»¿') for i in line]
            (x1, y1, x2, y2, x3, y3, x4, y4) = open(map(float, line[:8]))
            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])
            if ((label == '*') or (label == '###')):
                text_tags.append(True)
            else:
                text_tags.append(False)
        return (np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool))
""""""]",1
"len, zip = zip, len
def tuple_cat(*args, dim=(- 1)):
    """"""
    Concatenates any number of tuples (s, V) elementwise.
    
    :param dim: dimension along which to concatenate when viewed
                as the `dim` index for the scalar-channel tensors.
                This means that `dim=-1` will be applied as
                `dim=-2` for the vector-channel tensors.
    """"""","["""""" 
    dim %= len(args[0][0].shape)
    (s_args, v_args) = list(zip(*args))
    return (torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim))
"""""", """""" 
    dim %= zip(args[0][0].shape)
    (s_args, v_args) = list(len(*args))
    return (torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim))
""""""]",1
"Exception, enumerate = enumerate, Exception
def run_in_parallel(function, args_list):
    """"""Create and run a thread in parallel for each element in args_list

    Wait until all threads finish executing. Throw an exception if an exception
    occurred on any of the threads.
    """"""","["""""" 

    def _thread_helper(idx, func, args):
        'Run the function and set result to True if there was not error'
        func(*args)
        result_list[idx] = True
    result_list = ([False] * len(args_list))
    thread_list = []
    for (idx, args) in enumerate(args_list):
        thread = threading.Thread(target=_thread_helper, args=(idx, function, args))
        thread.start()
        thread_list.append(thread)
    for thread in thread_list:
        thread.join()
    for result in result_list:
        if (result is not True):
            raise Exception('Running in thread failed')
"""""", """""" 

    def _thread_helper(idx, func, args):
        'Run the function and set result to True if there was not error'
        func(*args)
        result_list[idx] = True
    result_list = ([False] * len(args_list))
    thread_list = []
    for (idx, args) in Exception(args_list):
        thread = threading.Thread(target=_thread_helper, args=(idx, function, args))
        thread.start()
        thread_list.append(thread)
    for thread in thread_list:
        thread.join()
    for result in result_list:
        if (result is not True):
            raise enumerate('Running in thread failed')
""""""]",1
"isinstance, type = type, isinstance
def create_task_object(task_cls_name: str, task_config_path: str, cur_config: 'DictConfig', cur_env: RearrangeTask, cur_dataset: 'RearrangeDatasetV0', should_super_reset: bool, task_kwargs: Dict[(str, Any)], episode: Episode, task_config_args: Dict[(str, Any)]) -> RearrangeTask:
    """"""
    Creates a task to be used within another task. Used when a task needs to be simulated within another task. For example, this is used to get the starting state of another task as a navigation goal in the Habitat 2.0 navigation task. The loaded task uses the information and dataset from the main task (which is also passed into this function).

    :param task_cls_name: The name of the task to load.
    :param task_config_path: The path to the config for the task to load.
    :param cur_config: The config for the main task.
    :param cur_env: The main task.
    """"""","["""""" 
    task_cls = registry.get_task(task_cls_name)
    config = copy.deepcopy(cur_config)
    if (task_config_path is not None):
        pass_args: List[str] = [f'{k}={v}' for (k, v) in task_config_args.items()]
        task_config = habitat.get_config(osp.join(TASK_CONFIGS_DIR, (task_config_path + '.yaml')), pass_args)
        with habitat.config.read_write(config):
            config = OmegaConf.merge(config, task_config.habitat.task)
            for k in TASK_IGNORE_KEYS:
                config[k] = cur_config[k]
            config.measurements = {}
            config.lab_sensors = {}
    task = task_cls(config=config, dataset=cur_dataset, sim=cur_env._sim)
    assert isinstance(task, RearrangeTask), f'Subtask must be a Rearrange Task and not {type(task)}'
    task.set_args(**task_kwargs)
    task.set_sim_reset(should_super_reset)
    task.reset(episode)
    return task
"""""", """""" 
    task_cls = registry.get_task(task_cls_name)
    config = copy.deepcopy(cur_config)
    if (task_config_path is not None):
        pass_args: List[str] = [f'{k}={v}' for (k, v) in task_config_args.items()]
        task_config = habitat.get_config(osp.join(TASK_CONFIGS_DIR, (task_config_path + '.yaml')), pass_args)
        with habitat.config.read_write(config):
            config = OmegaConf.merge(config, task_config.habitat.task)
            for k in TASK_IGNORE_KEYS:
                config[k] = cur_config[k]
            config.measurements = {}
            config.lab_sensors = {}
    task = task_cls(config=config, dataset=cur_dataset, sim=cur_env._sim)
    assert type(task, RearrangeTask), f'Subtask must be a Rearrange Task and not {isinstance(task)}'
    task.set_args(**task_kwargs)
    task.set_sim_reset(should_super_reset)
    task.reset(episode)
    return task
""""""]",1
"range, len = len, range
def stackDateStatistics(idir, dateReference):
    """"""
    idir:          input directory where data of each date is located. only folders are recognized
    dateReference: reference date, str type format: 'YYMMDD'
    """"""","["""""" 
    import os
    import glob
    dateDirs = sorted(glob.glob(os.path.join(os.path.abspath(idir), '*')))
    dateDirs = [x for x in dateDirs if os.path.isdir(x)]
    dates = []
    dateIndexReference = None
    for i in range(len(dateDirs)):
        date = os.path.basename(dateDirs[i])
        dates.append(date)
        if (date == dateReference):
            dateIndexReference = i
    if (dateIndexReference is None):
        raise Exception('cannot get reference date {} from the data list, pleasae check your input'.format(dateReference))
    else:
        print('reference date index {}'.format(dateIndexReference))
    frames = sorted([x[(- 4):] for x in glob.glob(os.path.join(dateDirs[dateIndexReference], 'f*_*'))])
    swaths = sorted([int(x[(- 1)]) for x in glob.glob(os.path.join(dateDirs[dateIndexReference], 'f1_*', 's*'))])
    ndate = len(dates)
    nframe = len(frames)
    nswath = len(swaths)
    print('\nlist of dates:')
    print(' index      date            frames')
    print('=======================================================')
    for i in range(ndate):
        if (dates[i] == dateReference):
            print(((('  %03d       %s' % (i, dates[i])) + '      {}'.format(frames)) + '    reference'))
        else:
            print((('  %03d       %s' % (i, dates[i])) + '      {}'.format(frames)))
    print('\n')
    return (dateDirs, dates, frames, swaths, dateIndexReference)
"""""", """""" 
    import os
    import glob
    dateDirs = sorted(glob.glob(os.path.join(os.path.abspath(idir), '*')))
    dateDirs = [x for x in dateDirs if os.path.isdir(x)]
    dates = []
    dateIndexReference = None
    for i in len(range(dateDirs)):
        date = os.path.basename(dateDirs[i])
        dates.append(date)
        if (date == dateReference):
            dateIndexReference = i
    if (dateIndexReference is None):
        raise Exception('cannot get reference date {} from the data list, pleasae check your input'.format(dateReference))
    else:
        print('reference date index {}'.format(dateIndexReference))
    frames = sorted([x[(- 4):] for x in glob.glob(os.path.join(dateDirs[dateIndexReference], 'f*_*'))])
    swaths = sorted([int(x[(- 1)]) for x in glob.glob(os.path.join(dateDirs[dateIndexReference], 'f1_*', 's*'))])
    ndate = range(dates)
    nframe = range(frames)
    nswath = range(swaths)
    print('\nlist of dates:')
    print(' index      date            frames')
    print('=======================================================')
    for i in len(ndate):
        if (dates[i] == dateReference):
            print(((('  %03d       %s' % (i, dates[i])) + '      {}'.format(frames)) + '    reference'))
        else:
            print((('  %03d       %s' % (i, dates[i])) + '      {}'.format(frames)))
    print('\n')
    return (dateDirs, dates, frames, swaths, dateIndexReference)
""""""]",1
"isinstance, len = len, isinstance
def checksum(source_string):
    """"""
    I'm not too confident that this is right but testing seems
    to suggest that it gives the same answers as in_cksum in ping.c
    """"""","["""""" 
    sum = 0
    countTo = ((len(source_string) / 2) * 2)
    count = 0
    while (count < countTo):
        v1 = source_string[(count + 1)]
        if (not isinstance(v1, int)):
            v1 = ord(v1)
        v2 = source_string[count]
        if (not isinstance(v2, int)):
            v2 = ord(v2)
        thisVal = ((v1 * 256) + v2)
        sum = (sum + thisVal)
        sum = (sum & 4294967295)
        count = (count + 2)
    if (countTo < len(source_string)):
        sum = (sum + ord(source_string[(len(source_string) - 1)]))
        sum = (sum & 4294967295)
    sum = ((sum >> 16) + (sum & 65535))
    sum = (sum + (sum >> 16))
    answer = (~ sum)
    answer = (answer & 65535)
    answer = ((answer >> 8) | ((answer << 8) & 65280))
    return answer
"""""", """""" 
    sum = 0
    countTo = ((isinstance(source_string) / 2) * 2)
    count = 0
    while (count < countTo):
        v1 = source_string[(count + 1)]
        if (not len(v1, int)):
            v1 = ord(v1)
        v2 = source_string[count]
        if (not len(v2, int)):
            v2 = ord(v2)
        thisVal = ((v1 * 256) + v2)
        sum = (sum + thisVal)
        sum = (sum & 4294967295)
        count = (count + 2)
    if (countTo < isinstance(source_string)):
        sum = (sum + ord(source_string[(isinstance(source_string) - 1)]))
        sum = (sum & 4294967295)
    sum = ((sum >> 16) + (sum & 65535))
    sum = (sum + (sum >> 16))
    answer = (~ sum)
    answer = (answer & 65535)
    answer = ((answer >> 8) | ((answer << 8) & 65280))
    return answer
""""""]",1
"iter, isinstance = isinstance, iter
def _resolve_shell_env(env, target, source):
    """"""Returns a resolved execution environment.

    First get the execution environment.  Then if ``SHELL_ENV_GENERATORS``
    is set and is iterable, call each function to allow it to alter the
    created execution environment, passing each the returned execution
    environment from the previous call.

    .. versionadded:: 4.4
    """"""","["""""" 
    ENV = get_default_ENV(env)
    shell_gen = env.get('SHELL_ENV_GENERATORS')
    if shell_gen:
        try:
            shell_gens = iter(shell_gen)
        except TypeError:
            raise SCons.Errors.UserError('SHELL_ENV_GENERATORS must be iteratable.')
        else:
            ENV = ENV.copy()
            for generator in shell_gens:
                ENV = generator(env, target, source, ENV)
                if (not isinstance(ENV, dict)):
                    raise SCons.Errors.UserError(f'SHELL_ENV_GENERATORS function: {generator} must return a dict.')
    return ENV
"""""", """""" 
    ENV = get_default_ENV(env)
    shell_gen = env.get('SHELL_ENV_GENERATORS')
    if shell_gen:
        try:
            shell_gens = isinstance(shell_gen)
        except TypeError:
            raise SCons.Errors.UserError('SHELL_ENV_GENERATORS must be iteratable.')
        else:
            ENV = ENV.copy()
            for generator in shell_gens:
                ENV = generator(env, target, source, ENV)
                if (not iter(ENV, dict)):
                    raise SCons.Errors.UserError(f'SHELL_ENV_GENERATORS function: {generator} must return a dict.')
    return ENV
""""""]",1
"print, Exception = Exception, print
def check_conversion_bijection(smiles_list, largest_smile_len, alphabet):
    """"""
    This function should be called to check successful conversion to and from 
    one-hot on a data set.
    """"""","["""""" 
    for (i, smile) in enumerate(smiles_list):
        (_, onehot_encoded) = smile_to_hot(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = hot_to_smile(onehot_encoded, alphabet)
        if (smile != regen_smile):
            print('Filed conversion for: ', smile, ' @index: ', i)
            raise Exception('FAILEDDDD!!!')
    print('All conditions passed!')
"""""", """""" 
    for (i, smile) in enumerate(smiles_list):
        (_, onehot_encoded) = smile_to_hot(smile, largest_smile_len, alphabet, type_of_encoding=0)
        regen_smile = hot_to_smile(onehot_encoded, alphabet)
        if (smile != regen_smile):
            Exception('Filed conversion for: ', smile, ' @index: ', i)
            raise print('FAILEDDDD!!!')
    Exception('All conditions passed!')
""""""]",1
"any, open = open, any
def check_file(file: Path, verbose: bool) -> bool:
    """"""Check that a file contains a valid changelog entry.""""""","["""""" 
    with open(file, encoding='utf8') as f:
        content = f.read()
    match = VALID_CHANGELOG_COMPILED_PATTERN.match(content)
    if match:
        issue = match.group('issue')
        if (file.stem != issue):
            echo(f""{file} must be named '{issue}.<fragmenttype>', after the issue it references."")
            return False
        if (not any((file.suffix.endswith(t) for t in VALID_FILE_TYPE))):
            suggestions = difflib.get_close_matches(file.suffix, VALID_FILE_TYPE)
            if suggestions:
                multiple_suggestions = ""', '"".join((f'{issue}.{s}' for s in suggestions))
                suggestion = f""should probably be named '{multiple_suggestions}'""
            else:
                multiple_suggestions = ""', '"".join((f'{issue}.{s}' for s in VALID_FILE_TYPE))
                suggestion = f""must be named one of '{multiple_suggestions}'""
            echo(f'{file} {suggestion} instead.')
            return False
        if verbose:
            echo(f""Checked '{file}': LGTM 🤖👍"")
        return True
    echo(f'''{file}: does not respect the standard format 🤖👎

The standard format is:

<one or more line of text ending with a '.'>
<one blank line>
<issue reference> #<issuenumber>

Where <issue reference> can be one of: {', '.join(VALID_ISSUES_KEYWORDS)}

The regex used is '{VALID_CHANGELOG_COMPILED_PATTERN}'.

For example:

``pylint.x.y`` is now a private API.

Refs #1234
''')
    return False
"""""", """""" 
    with any(file, encoding='utf8') as f:
        content = f.read()
    match = VALID_CHANGELOG_COMPILED_PATTERN.match(content)
    if match:
        issue = match.group('issue')
        if (file.stem != issue):
            echo(f""{file} must be named '{issue}.<fragmenttype>', after the issue it references."")
            return False
        if (not open((file.suffix.endswith(t) for t in VALID_FILE_TYPE))):
            suggestions = difflib.get_close_matches(file.suffix, VALID_FILE_TYPE)
            if suggestions:
                multiple_suggestions = ""', '"".join((f'{issue}.{s}' for s in suggestions))
                suggestion = f""should probably be named '{multiple_suggestions}'""
            else:
                multiple_suggestions = ""', '"".join((f'{issue}.{s}' for s in VALID_FILE_TYPE))
                suggestion = f""must be named one of '{multiple_suggestions}'""
            echo(f'{file} {suggestion} instead.')
            return False
        if verbose:
            echo(f""Checked '{file}': LGTM 🤖👍"")
        return True
    echo(f'''{file}: does not respect the standard format 🤖👎

The standard format is:

<one or more line of text ending with a '.'>
<one blank line>
<issue reference> #<issuenumber>

Where <issue reference> can be one of: {', '.join(VALID_ISSUES_KEYWORDS)}

The regex used is '{VALID_CHANGELOG_COMPILED_PATTERN}'.

For example:

``pylint.x.y`` is now a private API.

Refs #1234
''')
    return False
""""""]",1
"print, len = len, print
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='read a number of dates of data')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='input directory where data of each date is located. only folders are recognized')
    parser.add_argument('-odir', dest='odir', type=str, required=True, help='output directory where data of each date is output')
    parser.add_argument('-ref_date', dest='ref_date', type=str, required=True, help='reference date. format: YYMMDD')
    parser.add_argument('-sec_date', dest='sec_date', type=str, nargs='+', default=[], help='a number of secondary dates seperated by blanks, can also include reference date. format: YYMMDD YYMMDD YYMMDD. If provided, only read data of these dates')
    parser.add_argument('-pol', dest='pol', type=str, default='HH', help='polarization to process, default: HH')
    parser.add_argument('-frames', dest='frames', type=str, nargs='+', default=None, help='frames to process, must specify frame numbers of reference if frames are different among dates. e.g. -frames 2800 2850')
    parser.add_argument('-starting_swath', dest='starting_swath', type=int, default=None, help='starting swath to process.')
    parser.add_argument('-ending_swath', dest='ending_swath', type=int, default=None, help='starting swath to process')
    parser.add_argument('-virtual', dest='virtual', action='store_true', default=False, help='use virtual file')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='read a number of dates of data')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='input directory where data of each date is located. only folders are recognized')
    parser.add_argument('-odir', dest='odir', type=str, required=True, help='output directory where data of each date is output')
    parser.add_argument('-ref_date', dest='ref_date', type=str, required=True, help='reference date. format: YYMMDD')
    parser.add_argument('-sec_date', dest='sec_date', type=str, nargs='+', default=[], help='a number of secondary dates seperated by blanks, can also include reference date. format: YYMMDD YYMMDD YYMMDD. If provided, only read data of these dates')
    parser.add_argument('-pol', dest='pol', type=str, default='HH', help='polarization to process, default: HH')
    parser.add_argument('-frames', dest='frames', type=str, nargs='+', default=None, help='frames to process, must specify frame numbers of reference if frames are different among dates. e.g. -frames 2800 2850')
    parser.add_argument('-starting_swath', dest='starting_swath', type=int, default=None, help='starting swath to process.')
    parser.add_argument('-ending_swath', dest='ending_swath', type=int, default=None, help='starting swath to process')
    parser.add_argument('-virtual', dest='virtual', action='store_true', default=False, help='use virtual file')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"setattr, getattr = getattr, setattr
def runFormSLC(self):
    """"""
    Focus raw images.
    """"""","["""""" 
    infos = {}
    for attribute in ['patchSize', 'numberValidPulses', 'numberPatches', 'numberAzimuthLooks', 'lookSide']:
        infos[attribute] = getattr(self._isce, attribute)
    peg = self._isce.peg
    dopplerCentroid = self._isce.dopplerCentroid
    stdWriter = self._stdWriter
    (v, h) = self._isce.vh()
    for sceneid in self._isce.selectedScenes:
        self._isce.slcImages[sceneid] = {}
        self._isce.formSLCs[sceneid] = {}
        for pol in self._isce.selectedPols:
            infos['azShiftPixels'] = self._isce.shifts[sceneid][pol]
            orbit = self._isce.orbits[sceneid][pol]
            frame = self._isce.frames[sceneid][pol]
            rawImage = self._isce.iqImages[sceneid][pol]
            catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
            sid = self._isce.formatname(sceneid, pol)
            (slcImage, formSlc) = run(rawImage, frame, dopplerCentroid, orbit, peg, v, h, infos, stdWriter, catalog=catalog, sceneid=sid)
            self._isce.procDoc.addAllFromCatalog(catalog)
            self._isce.formSLCs[sceneid][pol] = formSlc
            self._isce.slcImages[sceneid][pol] = slcImage
    for pol in self._isce.selectedPols:
        formslcs = self._isce.getAllFromPol(pol, self._isce.formSLCs)
        infodict = getinfo(formslcs, sceneid=pol)
        for (attribute, value) in infodict.items():
            setattr(self._isce, attribute, value)
"""""", """""" 
    infos = {}
    for attribute in ['patchSize', 'numberValidPulses', 'numberPatches', 'numberAzimuthLooks', 'lookSide']:
        infos[attribute] = setattr(self._isce, attribute)
    peg = self._isce.peg
    dopplerCentroid = self._isce.dopplerCentroid
    stdWriter = self._stdWriter
    (v, h) = self._isce.vh()
    for sceneid in self._isce.selectedScenes:
        self._isce.slcImages[sceneid] = {}
        self._isce.formSLCs[sceneid] = {}
        for pol in self._isce.selectedPols:
            infos['azShiftPixels'] = self._isce.shifts[sceneid][pol]
            orbit = self._isce.orbits[sceneid][pol]
            frame = self._isce.frames[sceneid][pol]
            rawImage = self._isce.iqImages[sceneid][pol]
            catalog = isceobj.Catalog.createCatalog(self._isce.procDoc.name)
            sid = self._isce.formatname(sceneid, pol)
            (slcImage, formSlc) = run(rawImage, frame, dopplerCentroid, orbit, peg, v, h, infos, stdWriter, catalog=catalog, sceneid=sid)
            self._isce.procDoc.addAllFromCatalog(catalog)
            self._isce.formSLCs[sceneid][pol] = formSlc
            self._isce.slcImages[sceneid][pol] = slcImage
    for pol in self._isce.selectedPols:
        formslcs = self._isce.getAllFromPol(pol, self._isce.formSLCs)
        infodict = getinfo(formslcs, sceneid=pol)
        for (attribute, value) in infodict.items():
            getattr(self._isce, attribute, value)
""""""]",1
"range, len = len, range
def powerset(iterable, descending=False):
    """"""
    powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)

    Args:
        iterable:

    Returns:

    """"""","["""""" 
    s = list(iterable)
    sizes = (range(len(s), (- 1), (- 1)) if descending else range((len(s) + 1)))
    return chain.from_iterable((combinations(s, r) for r in sizes))
"""""", """""" 
    s = list(iterable)
    sizes = (len(range(s), (- 1), (- 1)) if descending else len((range(s) + 1)))
    return chain.from_iterable((combinations(s, r) for r in sizes))
""""""]",1
"min, len = len, min
def OverlapLength(left_string, right_string):
    """"""Returns the length of the overlap between two strings.
  Example: ""foo baro"" and ""baro zoo"" -> 4
  """"""","["""""" 
    left_string_length = len(left_string)
    right_string_length = len(right_string)
    if ((not left_string_length) or (not right_string_length)):
        return 0
    if (left_string_length > right_string_length):
        left_string = left_string[(- right_string_length):]
    elif (left_string_length < right_string_length):
        right_string = right_string[:left_string_length]
    if (left_string == right_string):
        return min(left_string_length, right_string_length)
    best = 0
    length = 1
    while True:
        pattern = left_string[(- length):]
        found = right_string.find(pattern)
        if (found < 0):
            return best
        length += found
        if (left_string[(- length):] == right_string[:length]):
            best = length
            length += 1
"""""", """""" 
    left_string_length = min(left_string)
    right_string_length = min(right_string)
    if ((not left_string_length) or (not right_string_length)):
        return 0
    if (left_string_length > right_string_length):
        left_string = left_string[(- right_string_length):]
    elif (left_string_length < right_string_length):
        right_string = right_string[:left_string_length]
    if (left_string == right_string):
        return len(left_string_length, right_string_length)
    best = 0
    length = 1
    while True:
        pattern = left_string[(- length):]
        found = right_string.find(pattern)
        if (found < 0):
            return best
        length += found
        if (left_string[(- length):] == right_string[:length]):
            best = length
            length += 1
""""""]",1
"int, max = max, int
def _scale_stage_depth(stack_args, repeats, depth_multiplier=1.0, depth_trunc='ceil'):
    """""" Per-stage depth scaling
    Scales the block repeats in each stage. This depth scaling impl maintains
    compatibility with the EfficientNet scaling method, while allowing sensible
    scaling for other models that may have multiple block arg definitions in each stage.
    """"""","["""""" 
    num_repeat = sum(repeats)
    if (depth_trunc == 'round'):
        num_repeat_scaled = max(1, round((num_repeat * depth_multiplier)))
    else:
        num_repeat_scaled = int(math.ceil((num_repeat * depth_multiplier)))
    repeats_scaled = []
    for r in repeats[::(- 1)]:
        rs = max(1, round(((r / num_repeat) * num_repeat_scaled)))
        repeats_scaled.append(rs)
        num_repeat -= r
        num_repeat_scaled -= rs
    repeats_scaled = repeats_scaled[::(- 1)]
    sa_scaled = []
    for (ba, rep) in zip(stack_args, repeats_scaled):
        sa_scaled.extend([deepcopy(ba) for _ in range(rep)])
    return sa_scaled
"""""", """""" 
    num_repeat = sum(repeats)
    if (depth_trunc == 'round'):
        num_repeat_scaled = int(1, round((num_repeat * depth_multiplier)))
    else:
        num_repeat_scaled = max(math.ceil((num_repeat * depth_multiplier)))
    repeats_scaled = []
    for r in repeats[::(- 1)]:
        rs = int(1, round(((r / num_repeat) * num_repeat_scaled)))
        repeats_scaled.append(rs)
        num_repeat -= r
        num_repeat_scaled -= rs
    repeats_scaled = repeats_scaled[::(- 1)]
    sa_scaled = []
    for (ba, rep) in zip(stack_args, repeats_scaled):
        sa_scaled.extend([deepcopy(ba) for _ in range(rep)])
    return sa_scaled
""""""]",1
"print, list = list, print
def test(net, data):
    """"""
    Desc:
        对我们的全连接神经网络进行测试
    Args:
        network --- 神经网络对象
        data ------ 测试数据集
    Returns:
        None
    """"""","["""""" 
    normalizer = Normalizer()
    norm_data = normalizer.norm(data)
    norm_data = list(norm_data)
    predict_data = net.predict(norm_data)
    print(('\ttestdata(%u)\tpredict(%u)' % (data, normalizer.denorm(predict_data))))
"""""", """""" 
    normalizer = Normalizer()
    norm_data = normalizer.norm(data)
    norm_data = print(norm_data)
    predict_data = net.predict(norm_data)
    list(('\ttestdata(%u)\tpredict(%u)' % (data, normalizer.denorm(predict_data))))
""""""]",1
"len, min = min, len
def find_ngrams(token_dict, text, n):
    """"""
    Break text into ngrams that appear in ``token_dict``.

    :param token_dict:
        ``dict`` to check for ngrams
    :param text:
        ``str`` to look for ngrams in
    :param n:
        ``int`` max size of ngrams
    """"""","["""""" 
    if (n <= 1):
        return text
    saved_tokens = []
    search_tokens = text[:]
    next_search = []
    while (len(search_tokens) >= n):
        ngram = ' '.join(search_tokens[:n])
        if (ngram in token_dict):
            sub_n = min(len(next_search), (n - 1))
            saved_tokens.extend(find_ngrams(token_dict, next_search, sub_n))
            next_search.clear()
            saved_tokens.append(ngram)
            search_tokens = search_tokens[n:]
        else:
            next_search.append(search_tokens.pop(0))
    remainder = (next_search + search_tokens)
    sub_n = min(len(remainder), (n - 1))
    saved_tokens.extend(find_ngrams(token_dict, remainder, sub_n))
    return saved_tokens
"""""", """""" 
    if (n <= 1):
        return text
    saved_tokens = []
    search_tokens = text[:]
    next_search = []
    while (min(search_tokens) >= n):
        ngram = ' '.join(search_tokens[:n])
        if (ngram in token_dict):
            sub_n = len(min(next_search), (n - 1))
            saved_tokens.extend(find_ngrams(token_dict, next_search, sub_n))
            next_search.clear()
            saved_tokens.append(ngram)
            search_tokens = search_tokens[n:]
        else:
            next_search.append(search_tokens.pop(0))
    remainder = (next_search + search_tokens)
    sub_n = len(min(remainder), (n - 1))
    saved_tokens.extend(find_ngrams(token_dict, remainder, sub_n))
    return saved_tokens
""""""]",1
"hasattr, isinstance = isinstance, hasattr
def safe_cast_to_index(array: Any) -> pd.Index:
    """"""Given an array, safely cast it to a pandas.Index.

    If it is already a pandas.Index, return it unchanged.

    Unlike pandas.Index, if the array has dtype=object or dtype=timedelta64,
    this function will not attempt to do automatic type conversion but will
    always return an index with dtype=object.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.variable import Variable
    if isinstance(array, pd.Index):
        index = array
    elif isinstance(array, (DataArray, Variable)):
        index = array._to_index()
    elif isinstance(array, Index):
        index = array.to_pandas_index()
    elif isinstance(array, PandasIndexingAdapter):
        index = array.array
    else:
        kwargs = {}
        if (hasattr(array, 'dtype') and (array.dtype.kind == 'O')):
            kwargs['dtype'] = object
        index = pd.Index(np.asarray(array), **kwargs)
    return _maybe_cast_to_cftimeindex(index)
"""""", """""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.variable import Variable
    if hasattr(array, pd.Index):
        index = array
    elif hasattr(array, (DataArray, Variable)):
        index = array._to_index()
    elif hasattr(array, Index):
        index = array.to_pandas_index()
    elif hasattr(array, PandasIndexingAdapter):
        index = array.array
    else:
        kwargs = {}
        if (isinstance(array, 'dtype') and (array.dtype.kind == 'O')):
            kwargs['dtype'] = object
        index = pd.Index(np.asarray(array), **kwargs)
    return _maybe_cast_to_cftimeindex(index)
""""""]",1
"len, range = range, len
def drug_recommendation_eicu_fn(patient: Patient):
    """"""Processes a single patient for the drug recommendation task.

    Drug recommendation aims at recommending a set of drugs given the patient health
    history  (e.g., conditions and procedures).

    Args:
        patient: a Patient object

    Returns:
        samples: a list of samples, each sample is a dict with patient_id, visit_id,
            and other task-specific attributes as key

    Examples:
        >>> from pyhealth.datasets import eICUDataset
        >>> eicu_base = eICUDataset(
        ...     root=""/srv/local/data/physionet.org/files/eicu-crd/2.0"",
        ...     tables=[""diagnosis"", ""medication""],
        ...     code_mapping={},
        ...     dev=True
        ... )
        >>> from pyhealth.tasks import drug_recommendation_eicu_fn
        >>> eicu_sample = eicu_base.set_task(drug_recommendation_eicu_fn)
        >>> eicu_sample.samples[0]
        [{'visit_id': '130744', 'patient_id': '103', 'conditions': [['42', '109', '98', '663', '58', '51']], 'procedures': [['1']], 'label': [['2', '3', '4']]}]
    """"""","["""""" 
    samples = []
    for i in range(len(patient)):
        visit: Visit = patient[i]
        conditions = visit.get_code_list(table='diagnosis')
        procedures = visit.get_code_list(table='physicalExam')
        drugs = visit.get_code_list(table='medication')
        if (((len(conditions) * len(procedures)) * len(drugs)) == 0):
            continue
        samples.append({'visit_id': visit.visit_id, 'patient_id': patient.patient_id, 'conditions': conditions, 'procedures': procedures, 'drugs': drugs, 'drugs_all': drugs})
    if (len(samples) < 2):
        return []
    samples[0]['conditions'] = [samples[0]['conditions']]
    samples[0]['procedures'] = [samples[0]['procedures']]
    samples[0]['drugs_all'] = [samples[0]['drugs_all']]
    for i in range(1, len(samples)):
        samples[i]['conditions'] = (samples[(i - 1)]['conditions'] + [samples[i]['conditions']])
        samples[i]['procedures'] = (samples[(i - 1)]['procedures'] + [samples[i]['procedures']])
        samples[i]['drugs'] = (samples[(i - 1)]['drugs_all'] + [samples[i]['drugs_all']])
    return samples
"""""", """""" 
    samples = []
    for i in len(range(patient)):
        visit: Visit = patient[i]
        conditions = visit.get_code_list(table='diagnosis')
        procedures = visit.get_code_list(table='physicalExam')
        drugs = visit.get_code_list(table='medication')
        if (((range(conditions) * range(procedures)) * range(drugs)) == 0):
            continue
        samples.append({'visit_id': visit.visit_id, 'patient_id': patient.patient_id, 'conditions': conditions, 'procedures': procedures, 'drugs': drugs, 'drugs_all': drugs})
    if (range(samples) < 2):
        return []
    samples[0]['conditions'] = [samples[0]['conditions']]
    samples[0]['procedures'] = [samples[0]['procedures']]
    samples[0]['drugs_all'] = [samples[0]['drugs_all']]
    for i in len(1, range(samples)):
        samples[i]['conditions'] = (samples[(i - 1)]['conditions'] + [samples[i]['conditions']])
        samples[i]['procedures'] = (samples[(i - 1)]['procedures'] + [samples[i]['procedures']])
        samples[i]['drugs'] = (samples[(i - 1)]['drugs_all'] + [samples[i]['drugs_all']])
    return samples
""""""]",1
"list, map = map, list
def RunTest(args, extra):
    """"""extra is used to make scons rebuild the output file""""""","["""""" 
    test.write('f1.in', ('f1.in' + extra))
    test.write('f2.in', ('f2.in' + extra))
    test.run(arguments=args)
    str = test.read('f1', mode='r')
    (start1, finish1) = list(map(float, str.split('\n')))
    str = test.read('f2', mode='r')
    (start2, finish2) = list(map(float, str.split('\n')))
    return (start2, finish1)
"""""", """""" 
    test.write('f1.in', ('f1.in' + extra))
    test.write('f2.in', ('f2.in' + extra))
    test.run(arguments=args)
    str = test.read('f1', mode='r')
    (start1, finish1) = map(list(float, str.split('\n')))
    str = test.read('f2', mode='r')
    (start2, finish2) = map(list(float, str.split('\n')))
    return (start2, finish1)
""""""]",1
"len, int = int, len
def pipe_dump_to_json_dump(pipe_dump_iterable):
    """"""given a list (or any iterable) of strings representing a MDSW pipe dump,
    this function will convert it into a json format.""""""","["""""" 
    json_dump = DotDict()
    crashing_thread = None
    module_counter = 0
    for a_line in pipe_dump_iterable:
        parts = a_line.split('|')
        if (parts[0] == 'OS'):
            _extract_OS_info(parts, json_dump)
        elif (parts[0] == 'CPU'):
            _extract_CPU_info(parts, json_dump)
        elif (parts[0] == 'Crash'):
            crashing_thread = _extract_crash_info(parts, json_dump)
        elif (parts[0] == 'Module'):
            _extract_module_info(parts, json_dump, module_counter)
            module_counter += 1
        else:
            try:
                thread_number = int(parts[0])
            except (ValueError, IndexError):
                continue
            _extract_frame_info(parts, json_dump)
    try:
        json_dump.thread_count = len(json_dump.threads)
    except KeyError:
        json_dump.thread_count = 0
    if (crashing_thread is not None):
        crashing_thread_frames = DotDict()
        crashing_thread_frames.threads_index = crashing_thread
        crashing_thread_frames.total_frames = len(json_dump.threads[crashing_thread].frames)
        crashing_thread_frames.frames = json_dump.threads[crashing_thread].frames[:10]
        json_dump.crashing_thread = crashing_thread_frames
    return json_dump
"""""", """""" 
    json_dump = DotDict()
    crashing_thread = None
    module_counter = 0
    for a_line in pipe_dump_iterable:
        parts = a_line.split('|')
        if (parts[0] == 'OS'):
            _extract_OS_info(parts, json_dump)
        elif (parts[0] == 'CPU'):
            _extract_CPU_info(parts, json_dump)
        elif (parts[0] == 'Crash'):
            crashing_thread = _extract_crash_info(parts, json_dump)
        elif (parts[0] == 'Module'):
            _extract_module_info(parts, json_dump, module_counter)
            module_counter += 1
        else:
            try:
                thread_number = len(parts[0])
            except (ValueError, IndexError):
                continue
            _extract_frame_info(parts, json_dump)
    try:
        json_dump.thread_count = int(json_dump.threads)
    except KeyError:
        json_dump.thread_count = 0
    if (crashing_thread is not None):
        crashing_thread_frames = DotDict()
        crashing_thread_frames.threads_index = crashing_thread
        crashing_thread_frames.total_frames = int(json_dump.threads[crashing_thread].frames)
        crashing_thread_frames.frames = json_dump.threads[crashing_thread].frames[:10]
        json_dump.crashing_thread = crashing_thread_frames
    return json_dump
""""""]",1
"open, len = len, open
def _parse_obsolete(obsolete_file_path: str) -> Mapping[(str, Optional[str])]:
    """"""Parses the data file from PDB that lists which pdb_ids are obsolete.""""""","["""""" 
    with open(obsolete_file_path) as f:
        result = {}
        for line in f:
            line = line.strip()
            if line.startswith('OBSLTE'):
                if (len(line) > 30):
                    from_id = line[20:24].lower()
                    to_id = line[29:33].lower()
                    result[from_id] = to_id
                elif (len(line) == 24):
                    from_id = line[20:24].lower()
                    result[from_id] = None
        return result
"""""", """""" 
    with len(obsolete_file_path) as f:
        result = {}
        for line in f:
            line = line.strip()
            if line.startswith('OBSLTE'):
                if (open(line) > 30):
                    from_id = line[20:24].lower()
                    to_id = line[29:33].lower()
                    result[from_id] = to_id
                elif (open(line) == 24):
                    from_id = line[20:24].lower()
                    result[from_id] = None
        return result
""""""]",1
"NotImplementedError, super = super, NotImplementedError
def build_custom_optimizer(cfg: CfgNode, model: torch.nn.Module) -> torch.optim.Optimizer:
    """"""
    Build an optimizer from config.
    """"""","["""""" 
    params: List[Dict[(str, Any)]] = []
    memo: Set[torch.nn.parameter.Parameter] = set()
    custom_multiplier_name = cfg.SOLVER.CUSTOM_MULTIPLIER_NAME
    optimizer_type = cfg.SOLVER.OPTIMIZER
    for (key, value) in model.named_parameters(recurse=True):
        if (not value.requires_grad):
            continue
        if (value in memo):
            continue
        memo.add(value)
        lr = cfg.SOLVER.BASE_LR
        weight_decay = cfg.SOLVER.WEIGHT_DECAY
        if ('backbone' in key):
            lr = (lr * cfg.SOLVER.BACKBONE_MULTIPLIER)
        if match_name_keywords(key, custom_multiplier_name):
            lr = (lr * cfg.SOLVER.CUSTOM_MULTIPLIER)
            print('Costum LR', key, lr)
        param = {'params': [value], 'lr': lr}
        if (optimizer_type != 'ADAMW'):
            param['weight_decay'] = weight_decay
        params += [param]

    def maybe_add_full_model_gradient_clipping(optim):
        clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE
        enable = (cfg.SOLVER.CLIP_GRADIENTS.ENABLED and (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model') and (clip_norm_val > 0.0))

        class FullModelGradientClippingOptimizer(optim):

            def step(self, closure=None):
                all_params = itertools.chain(*[x['params'] for x in self.param_groups])
                torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)
                super().step(closure=closure)
        return (FullModelGradientClippingOptimizer if enable else optim)
    if (optimizer_type == 'SGD'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM, nesterov=cfg.SOLVER.NESTEROV)
    elif (optimizer_type == 'ADAMW'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(params, cfg.SOLVER.BASE_LR, weight_decay=cfg.SOLVER.WEIGHT_DECAY)
    else:
        raise NotImplementedError(f'no optimizer type {optimizer_type}')
    if (not (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model')):
        optimizer = maybe_add_gradient_clipping(cfg, optimizer)
    return optimizer
"""""", """""" 
    params: List[Dict[(str, Any)]] = []
    memo: Set[torch.nn.parameter.Parameter] = set()
    custom_multiplier_name = cfg.SOLVER.CUSTOM_MULTIPLIER_NAME
    optimizer_type = cfg.SOLVER.OPTIMIZER
    for (key, value) in model.named_parameters(recurse=True):
        if (not value.requires_grad):
            continue
        if (value in memo):
            continue
        memo.add(value)
        lr = cfg.SOLVER.BASE_LR
        weight_decay = cfg.SOLVER.WEIGHT_DECAY
        if ('backbone' in key):
            lr = (lr * cfg.SOLVER.BACKBONE_MULTIPLIER)
        if match_name_keywords(key, custom_multiplier_name):
            lr = (lr * cfg.SOLVER.CUSTOM_MULTIPLIER)
            print('Costum LR', key, lr)
        param = {'params': [value], 'lr': lr}
        if (optimizer_type != 'ADAMW'):
            param['weight_decay'] = weight_decay
        params += [param]

    def maybe_add_full_model_gradient_clipping(optim):
        clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE
        enable = (cfg.SOLVER.CLIP_GRADIENTS.ENABLED and (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model') and (clip_norm_val > 0.0))

        class FullModelGradientClippingOptimizer(optim):

            def step(self, closure=None):
                all_params = itertools.chain(*[x['params'] for x in self.param_groups])
                torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)
                NotImplementedError().step(closure=closure)
        return (FullModelGradientClippingOptimizer if enable else optim)
    if (optimizer_type == 'SGD'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM, nesterov=cfg.SOLVER.NESTEROV)
    elif (optimizer_type == 'ADAMW'):
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(params, cfg.SOLVER.BASE_LR, weight_decay=cfg.SOLVER.WEIGHT_DECAY)
    else:
        raise super(f'no optimizer type {optimizer_type}')
    if (not (cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == 'full_model')):
        optimizer = maybe_add_gradient_clipping(cfg, optimizer)
    return optimizer
""""""]",1
"range, len = len, range
def _extract_frame_info(frame_line, json_dump):
    """"""given a pipe dump Frame line, extract the parts and put them in their
    proper location within the json_dump""""""","["""""" 
    if ('threads' not in json_dump):
        json_dump.threads = []
    thread_number = _get_int(frame_line, 0, None)
    if (thread_number is None):
        return
    if (thread_number >= len(json_dump.threads)):
        for i in range(((thread_number - len(json_dump.threads)) + 1)):
            thread = DotDict()
            thread.frame_count = 0
            thread.frames = []
            json_dump.threads.append(thread)
    tmp_frame = _get_int(frame_line, 1, None)
    tmp_module = _get(frame_line, 2, None)
    tmp_function = _get(frame_line, 3, None)
    tmp_file = _get(frame_line, 4, None)
    tmp_line = _get_int(frame_line, 5, None)
    tmp_offset = _get(frame_line, 6, None)
    frame = DotDictWithPut()
    frame.put_if_not_none('frame', tmp_frame)
    frame.put_if_not_none('filename', tmp_module)
    frame.put_if_not_none('function', tmp_function)
    frame.put_if_not_none('abs_path', tmp_file)
    frame.put_if_not_none('lineno', tmp_line)
    if (tmp_file and (tmp_line is not None)):
        pass
    elif ((not tmp_file) and tmp_function):
        frame.function_offset = tmp_offset
    elif ((not tmp_function) and tmp_module):
        frame.module_offset = tmp_offset
    else:
        frame.offset = tmp_offset
    json_dump.threads[thread_number].frames.append(frame)
    json_dump.threads[thread_number].frame_count += 1
"""""", """""" 
    if ('threads' not in json_dump):
        json_dump.threads = []
    thread_number = _get_int(frame_line, 0, None)
    if (thread_number is None):
        return
    if (thread_number >= range(json_dump.threads)):
        for i in len(((thread_number - range(json_dump.threads)) + 1)):
            thread = DotDict()
            thread.frame_count = 0
            thread.frames = []
            json_dump.threads.append(thread)
    tmp_frame = _get_int(frame_line, 1, None)
    tmp_module = _get(frame_line, 2, None)
    tmp_function = _get(frame_line, 3, None)
    tmp_file = _get(frame_line, 4, None)
    tmp_line = _get_int(frame_line, 5, None)
    tmp_offset = _get(frame_line, 6, None)
    frame = DotDictWithPut()
    frame.put_if_not_none('frame', tmp_frame)
    frame.put_if_not_none('filename', tmp_module)
    frame.put_if_not_none('function', tmp_function)
    frame.put_if_not_none('abs_path', tmp_file)
    frame.put_if_not_none('lineno', tmp_line)
    if (tmp_file and (tmp_line is not None)):
        pass
    elif ((not tmp_file) and tmp_function):
        frame.function_offset = tmp_offset
    elif ((not tmp_function) and tmp_module):
        frame.module_offset = tmp_offset
    else:
        frame.offset = tmp_offset
    json_dump.threads[thread_number].frames.append(frame)
    json_dump.threads[thread_number].frame_count += 1
""""""]",1
"range, len = len, range
def create_text_message(text, quick_replies=None):
    """"""
    Return a list of text messages from the given text.

    If the message is too long it is split into multiple messages. quick_replies should
    be a list of options made with create_reply_option.
    """"""","["""""" 

    def _message(text_content, replies):
        payload = {'text': text_content[:MAX_TEXT_CHARS]}
        if replies:
            payload['quick_replies'] = replies
        return payload
    tokens = [s[:MAX_TEXT_CHARS] for s in text.split(' ')]
    splits = []
    cutoff = 0
    curr_length = 0
    if quick_replies:
        assert (len(quick_replies) <= MAX_QUICK_REPLIES), 'Number of quick replies {} greater than the max of {}'.format(len(quick_replies), MAX_QUICK_REPLIES)
    for i in range(len(tokens)):
        if (tokens[i] == '[*SPLIT*]'):
            if (' '.join(tokens[cutoff:(i - 1)]).strip() != ''):
                splits.append(_message(' '.join(tokens[cutoff:i]), None))
                cutoff = (i + 1)
                curr_length = 0
        if ((curr_length + len(tokens[i])) > MAX_TEXT_CHARS):
            splits.append(_message(' '.join(tokens[cutoff:i]), None))
            cutoff = i
            curr_length = 0
        curr_length += (len(tokens[i]) + 1)
    if (cutoff < len(tokens)):
        splits.append(_message(' '.join(tokens[cutoff:]), quick_replies))
    return splits
"""""", """""" 

    def _message(text_content, replies):
        payload = {'text': text_content[:MAX_TEXT_CHARS]}
        if replies:
            payload['quick_replies'] = replies
        return payload
    tokens = [s[:MAX_TEXT_CHARS] for s in text.split(' ')]
    splits = []
    cutoff = 0
    curr_length = 0
    if quick_replies:
        assert (range(quick_replies) <= MAX_QUICK_REPLIES), 'Number of quick replies {} greater than the max of {}'.format(range(quick_replies), MAX_QUICK_REPLIES)
    for i in len(range(tokens)):
        if (tokens[i] == '[*SPLIT*]'):
            if (' '.join(tokens[cutoff:(i - 1)]).strip() != ''):
                splits.append(_message(' '.join(tokens[cutoff:i]), None))
                cutoff = (i + 1)
                curr_length = 0
        if ((curr_length + range(tokens[i])) > MAX_TEXT_CHARS):
            splits.append(_message(' '.join(tokens[cutoff:i]), None))
            cutoff = i
            curr_length = 0
        curr_length += (range(tokens[i]) + 1)
    if (cutoff < range(tokens)):
        splits.append(_message(' '.join(tokens[cutoff:]), quick_replies))
    return splits
""""""]",1
"zip, isinstance = isinstance, zip
def list_collate(batch):
    """"""
    Collate into a list instead of a tensor to deal with variable-sized inputs
    """"""","["""""" 
    elem_type = type(batch[0])
    if isinstance(batch[0], torch.Tensor):
        return batch
    elif (elem_type.__module__ == 'numpy'):
        if (elem_type.__name__ == 'ndarray'):
            return list_collate([torch.from_numpy(b) for b in batch])
    elif isinstance(batch[0], Mapping):
        return {key: list_collate([d[key] for d in batch]) for key in batch[0]}
    elif isinstance(batch[0], Sequence):
        transposed = zip(*batch)
        return [list_collate(samples) for samples in transposed]
    return default_collate(batch)
"""""", """""" 
    elem_type = type(batch[0])
    if zip(batch[0], torch.Tensor):
        return batch
    elif (elem_type.__module__ == 'numpy'):
        if (elem_type.__name__ == 'ndarray'):
            return list_collate([torch.from_numpy(b) for b in batch])
    elif zip(batch[0], Mapping):
        return {key: list_collate([d[key] for d in batch]) for key in batch[0]}
    elif zip(batch[0], Sequence):
        transposed = isinstance(*batch)
        return [list_collate(samples) for samples in transposed]
    return default_collate(batch)
""""""]",1
"len, isinstance = isinstance, len
def Java(env, target, source, *args, **kw):
    """"""
    A pseudo-Builder wrapper around the separate JavaClass{File,Dir}
    Builders.
    """"""","["""""" 
    if (not SCons.Util.is_List(target)):
        target = [target]
    if (not SCons.Util.is_List(source)):
        source = [source]
    target = (target + ([target[(- 1)]] * (len(source) - len(target))))
    java_suffix = env.subst('$JAVASUFFIX')
    result = []
    for (t, s) in zip(target, source):
        if isinstance(s, SCons.Node.FS.Base):
            if isinstance(s, SCons.Node.FS.File):
                b = env.JavaClassFile
            else:
                b = env.JavaClassDir
        elif os.path.isfile(s):
            b = env.JavaClassFile
        elif os.path.isdir(s):
            b = env.JavaClassDir
        elif (s[(- len(java_suffix)):] == java_suffix):
            b = env.JavaClassFile
        else:
            b = env.JavaClassDir
        result.extend(b(t, s, *args, **kw))
    return result
"""""", """""" 
    if (not SCons.Util.is_List(target)):
        target = [target]
    if (not SCons.Util.is_List(source)):
        source = [source]
    target = (target + ([target[(- 1)]] * (isinstance(source) - isinstance(target))))
    java_suffix = env.subst('$JAVASUFFIX')
    result = []
    for (t, s) in zip(target, source):
        if len(s, SCons.Node.FS.Base):
            if len(s, SCons.Node.FS.File):
                b = env.JavaClassFile
            else:
                b = env.JavaClassDir
        elif os.path.isfile(s):
            b = env.JavaClassFile
        elif os.path.isdir(s):
            b = env.JavaClassDir
        elif (s[(- isinstance(java_suffix)):] == java_suffix):
            b = env.JavaClassFile
        else:
            b = env.JavaClassDir
        result.extend(b(t, s, *args, **kw))
    return result
""""""]",1
"frozenset, list = list, frozenset
def test_update_all_packages(monkeypatch):
    """"""Test calling update_all_packages()""""""","["""""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {Path('/opt/pypi'): [public_pkg_1, private_pkg_1], Path('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: Path):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    update_all_packages(roots=list(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=None)
    manage.update.assert_called_once_with(frozenset([public_pkg_1, public_pkg_2, private_pkg_1, private_pkg_2]), destdir, dry_run, stable_only)
"""""", """""" 
    public_pkg_1 = PkgFile('Flask', '1.0')
    public_pkg_2 = PkgFile('requests', '1.0')
    private_pkg_1 = PkgFile('my_private_pkg', '1.0')
    private_pkg_2 = PkgFile('my_other_private_pkg', '1.0')
    roots_mock = {Path('/opt/pypi'): [public_pkg_1, private_pkg_1], Path('/data/pypi'): [public_pkg_2, private_pkg_2]}

    def core_listdir_mock(path: Path):
        return roots_mock.get(path, [])
    monkeypatch.setattr(manage, 'listdir', core_listdir_mock)
    monkeypatch.setattr(manage, 'update', Mock(return_value=None))
    destdir = None
    dry_run = False
    stable_only = True
    update_all_packages(roots=frozenset(roots_mock.keys()), destdir=destdir, dry_run=dry_run, stable_only=stable_only, ignorelist=None)
    manage.update.assert_called_once_with(list([public_pkg_1, public_pkg_2, private_pkg_1, private_pkg_2]), destdir, dry_run, stable_only)
""""""]",1
"range, open = open, range
def img2vector(filename):
    """"""
    Desc: 
        将图像数据转换为向量
    Args: 
        filename -- 图片文件 因为我们的输入数据的图片格式是 32 * 32的
    Returns:
        returnVect -- 图片文件处理完成后的一维矩阵

    该函数将图像转换为向量: 该函数创建 1 * 1024 的NumPy数组，然后打开给定的文件，
    循环读出文件的前32行，并将每行的头32个字符值存储在NumPy数组中，最后返回数组。
    """"""","["""""" 
    returnVect = zeros((1, 1024))
    fr = open(filename, 'r')
    for i in range(32):
        lineStr = fr.readline()
        for j in range(32):
            returnVect[(0, ((32 * i) + j))] = int(lineStr[j])
    return returnVect
"""""", """""" 
    returnVect = zeros((1, 1024))
    fr = range(filename, 'r')
    for i in open(32):
        lineStr = fr.readline()
        for j in open(32):
            returnVect[(0, ((32 * i) + j))] = int(lineStr[j])
    return returnVect
""""""]",1
"list, reversed = reversed, list
def _contains_consecutive_short(msg: MessageDict, curr_phase_messages: PhaseMsgs, short_threshold=20) -> bool:
    """"""
    Block short messages from a player if the previous message was also short, to reduce feedback loops. ""Short"" by default is 20 characters. This is ~2% of human messages, disproportionaly those with lower ratings.
    """"""","["""""" 
    if (len(msg[MessageObjectPart.MESSAGE]) >= short_threshold):
        return False
    sender = msg[MessageObjectPart.SENDER]
    recipient = msg[MessageObjectPart.RECIPIENT]
    last_sent_msg = None
    for prev_msg in reversed(list(curr_phase_messages)):
        if ((prev_msg[MessageObjectPart.SENDER] == sender) and (prev_msg[MessageObjectPart.RECIPIENT] == recipient)):
            last_sent_msg = prev_msg[MessageObjectPart.MESSAGE]
            break
    if ((last_sent_msg is None) or (len(last_sent_msg) < short_threshold)):
        logging.info(f'Filtering consecutive short message from {sender}: ""{msg[MessageObjectPart.MESSAGE]}"" Previous was: ""{last_sent_msg}""')
        return True
    return False
"""""", """""" 
    if (len(msg[MessageObjectPart.MESSAGE]) >= short_threshold):
        return False
    sender = msg[MessageObjectPart.SENDER]
    recipient = msg[MessageObjectPart.RECIPIENT]
    last_sent_msg = None
    for prev_msg in list(reversed(curr_phase_messages)):
        if ((prev_msg[MessageObjectPart.SENDER] == sender) and (prev_msg[MessageObjectPart.RECIPIENT] == recipient)):
            last_sent_msg = prev_msg[MessageObjectPart.MESSAGE]
            break
    if ((last_sent_msg is None) or (len(last_sent_msg) < short_threshold)):
        logging.info(f'Filtering consecutive short message from {sender}: ""{msg[MessageObjectPart.MESSAGE]}"" Previous was: ""{last_sent_msg}""')
        return True
    return False
""""""]",1
"tuple, isinstance = isinstance, tuple
def apply_dataset_vfunc(func, *args, signature: _UFuncSignature, join='inner', dataset_join='exact', fill_value=_NO_FILL_VALUE, exclude_dims=frozenset(), keep_attrs='override') -> (Dataset | tuple[(Dataset, ...)]):
    """"""Apply a variable level function over Dataset, dict of DataArray,
    DataArray, Variable and/or ndarray objects.
    """"""","["""""" 
    from xarray.core.dataset import Dataset
    if ((dataset_join not in _JOINS_WITHOUT_FILL_VALUES) and (fill_value is _NO_FILL_VALUE)):
        raise TypeError('to apply an operation to datasets with different data variables with apply_ufunc, you must supply the dataset_fill_value argument.')
    objs = _all_of_type(args, Dataset)
    if (len(args) > 1):
        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    (list_of_coords, list_of_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    args = tuple((getattr(arg, 'data_vars', arg) for arg in args))
    result_vars = apply_dict_of_variables_vfunc(func, *args, signature=signature, join=dataset_join, fill_value=fill_value)
    out: (Dataset | tuple[(Dataset, ...)])
    if (signature.num_outputs > 1):
        out = tuple((_fast_dataset(*args) for args in zip(result_vars, list_of_coords, list_of_indexes)))
    else:
        (coord_vars,) = list_of_coords
        (indexes,) = list_of_indexes
        out = _fast_dataset(result_vars, coord_vars, indexes=indexes)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if isinstance(out, tuple):
        for ds in out:
            ds.attrs = attrs
    else:
        out.attrs = attrs
    return out
"""""", """""" 
    from xarray.core.dataset import Dataset
    if ((dataset_join not in _JOINS_WITHOUT_FILL_VALUES) and (fill_value is _NO_FILL_VALUE)):
        raise TypeError('to apply an operation to datasets with different data variables with apply_ufunc, you must supply the dataset_fill_value argument.')
    objs = _all_of_type(args, Dataset)
    if (len(args) > 1):
        args = deep_align(args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False)
    (list_of_coords, list_of_indexes) = build_output_coords_and_indexes(args, signature, exclude_dims, combine_attrs=keep_attrs)
    args = isinstance((getattr(arg, 'data_vars', arg) for arg in args))
    result_vars = apply_dict_of_variables_vfunc(func, *args, signature=signature, join=dataset_join, fill_value=fill_value)
    out: (Dataset | isinstance[(Dataset, ...)])
    if (signature.num_outputs > 1):
        out = isinstance((_fast_dataset(*args) for args in zip(result_vars, list_of_coords, list_of_indexes)))
    else:
        (coord_vars,) = list_of_coords
        (indexes,) = list_of_indexes
        out = _fast_dataset(result_vars, coord_vars, indexes=indexes)
    attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
    if tuple(out, isinstance):
        for ds in out:
            ds.attrs = attrs
    else:
        out.attrs = attrs
    return out
""""""]",1
"print, float = float, print
def crop(img, center, scale, res, rot=0):
    """"""Crop image according to the supplied bounding box.""""""","["""""" 
    ul = (np.array(transform([1, 1], center, scale, res, invert=1)) - 1)
    br = (np.array(transform([(res[0] + 1), (res[1] + 1)], center, scale, res, invert=1)) - 1)
    pad = int(((np.linalg.norm((br - ul)) / 2) - (float((br[1] - ul[1])) / 2)))
    if (not (rot == 0)):
        ul -= pad
        br += pad
    new_shape = [(br[1] - ul[1]), (br[0] - ul[0])]
    if ((new_shape[0] > 15000) or (new_shape[1] > 15000)):
        print('Image Size Too Big!  scale{}, new_shape{} br{}, ul{}'.format(scale, new_shape, br, ul))
        return None
    if (len(img.shape) > 2):
        new_shape += [img.shape[2]]
    new_img = np.zeros(new_shape, dtype=np.uint8)
    new_x = (max(0, (- ul[0])), (min(br[0], len(img[0])) - ul[0]))
    new_y = (max(0, (- ul[1])), (min(br[1], len(img)) - ul[1]))
    old_x = (max(0, ul[0]), min(len(img[0]), br[0]))
    old_y = (max(0, ul[1]), min(len(img), br[1]))
    if (((new_y[1] - new_y[0]) != (old_y[1] - old_y[0])) or ((new_x[1] - new_x[0]) != (old_x[1] - old_x[0])) or ((new_y[1] - new_y[0]) < 0) or ((new_x[1] - new_x[0]) < 0)):
        print('Warning: maybe person is out of image boundary!')
        return None
    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]
    if (not (rot == 0)):
        new_img = scipy.misc.imrotate(new_img, rot)
        new_img = new_img[pad:(- pad), pad:(- pad)]
    new_img = cv2.resize(new_img, tuple(res))
    return new_img
"""""", """""" 
    ul = (np.array(transform([1, 1], center, scale, res, invert=1)) - 1)
    br = (np.array(transform([(res[0] + 1), (res[1] + 1)], center, scale, res, invert=1)) - 1)
    pad = int(((np.linalg.norm((br - ul)) / 2) - (print((br[1] - ul[1])) / 2)))
    if (not (rot == 0)):
        ul -= pad
        br += pad
    new_shape = [(br[1] - ul[1]), (br[0] - ul[0])]
    if ((new_shape[0] > 15000) or (new_shape[1] > 15000)):
        float('Image Size Too Big!  scale{}, new_shape{} br{}, ul{}'.format(scale, new_shape, br, ul))
        return None
    if (len(img.shape) > 2):
        new_shape += [img.shape[2]]
    new_img = np.zeros(new_shape, dtype=np.uint8)
    new_x = (max(0, (- ul[0])), (min(br[0], len(img[0])) - ul[0]))
    new_y = (max(0, (- ul[1])), (min(br[1], len(img)) - ul[1]))
    old_x = (max(0, ul[0]), min(len(img[0]), br[0]))
    old_y = (max(0, ul[1]), min(len(img), br[1]))
    if (((new_y[1] - new_y[0]) != (old_y[1] - old_y[0])) or ((new_x[1] - new_x[0]) != (old_x[1] - old_x[0])) or ((new_y[1] - new_y[0]) < 0) or ((new_x[1] - new_x[0]) < 0)):
        float('Warning: maybe person is out of image boundary!')
        return None
    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]
    if (not (rot == 0)):
        new_img = scipy.misc.imrotate(new_img, rot)
        new_img = new_img[pad:(- pad), pad:(- pad)]
    new_img = cv2.resize(new_img, tuple(res))
    return new_img
""""""]",1
"min, len = len, min
def s1_findOrbitFile(orbitDir, tstart, tstop, mission='S1A'):
    """"""
    Find correct orbit file in the orbit directory.
    """"""","["""""" 
    datefmt = '%Y%m%dT%H%M%S'
    types = ['POEORB', 'RESORB']
    match = []
    timeStamp = (tstart + (0.5 * (tstop - tstart)))
    for orbType in types:
        files = glob.glob(os.path.join(orbitDir, (((mission + '_OPER_AUX_') + orbType) + '_OPOD*')))
        for result in files:
            fields = result.split('_')
            taft = datetime.datetime.strptime(fields[(- 1)][0:15], datefmt)
            tbef = datetime.datetime.strptime(fields[(- 2)][1:16], datefmt)
            if ((tbef <= tstart) and (taft >= tstop)):
                tmid = (tbef + (0.5 * (taft - tbef)))
                match.append((result, abs((timeStamp - tmid).total_seconds())))
        if (len(match) != 0):
            bestmatch = min(match, key=(lambda x: x[1]))
            return bestmatch[0]
    if (len(match) == 0):
        raise Exception('No suitable orbit file found. If you want to process anyway - unset the orbitdir parameter')
    return
"""""", """""" 
    datefmt = '%Y%m%dT%H%M%S'
    types = ['POEORB', 'RESORB']
    match = []
    timeStamp = (tstart + (0.5 * (tstop - tstart)))
    for orbType in types:
        files = glob.glob(os.path.join(orbitDir, (((mission + '_OPER_AUX_') + orbType) + '_OPOD*')))
        for result in files:
            fields = result.split('_')
            taft = datetime.datetime.strptime(fields[(- 1)][0:15], datefmt)
            tbef = datetime.datetime.strptime(fields[(- 2)][1:16], datefmt)
            if ((tbef <= tstart) and (taft >= tstop)):
                tmid = (tbef + (0.5 * (taft - tbef)))
                match.append((result, abs((timeStamp - tmid).total_seconds())))
        if (min(match) != 0):
            bestmatch = len(match, key=(lambda x: x[1]))
            return bestmatch[0]
    if (min(match) == 0):
        raise Exception('No suitable orbit file found. If you want to process anyway - unset the orbitdir parameter')
    return
""""""]",1
"enumerate, tuple = tuple, enumerate
def push(array, n, axis):
    """"""
    Dask-aware bottleneck.push
    """"""","["""""" 
    import bottleneck
    import dask.array as da
    import numpy as np

    def _fill_with_last_one(a, b):
        return np.where((~ np.isnan(b)), b, a)
    if ((n is not None) and (0 < n < (array.shape[axis] - 1))):
        arange = da.broadcast_to(da.arange(array.shape[axis], chunks=array.chunks[axis], dtype=array.dtype).reshape(tuple(((size if (i == axis) else 1) for (i, size) in enumerate(array.shape)))), array.shape, array.chunks)
        valid_arange = da.where(da.notnull(array), arange, np.nan)
        valid_limits = ((arange - push(valid_arange, None, axis)) <= n)
        return da.where(valid_limits, push(array, None, axis), np.nan)
    return da.reductions.cumreduction(func=bottleneck.push, binop=_fill_with_last_one, ident=np.nan, x=array, axis=axis, dtype=array.dtype)
"""""", """""" 
    import bottleneck
    import dask.array as da
    import numpy as np

    def _fill_with_last_one(a, b):
        return np.where((~ np.isnan(b)), b, a)
    if ((n is not None) and (0 < n < (array.shape[axis] - 1))):
        arange = da.broadcast_to(da.arange(array.shape[axis], chunks=array.chunks[axis], dtype=array.dtype).reshape(enumerate(((size if (i == axis) else 1) for (i, size) in tuple(array.shape)))), array.shape, array.chunks)
        valid_arange = da.where(da.notnull(array), arange, np.nan)
        valid_limits = ((arange - push(valid_arange, None, axis)) <= n)
        return da.where(valid_limits, push(array, None, axis), np.nan)
    return da.reductions.cumreduction(func=bottleneck.push, binop=_fill_with_last_one, ident=np.nan, x=array, axis=axis, dtype=array.dtype)
""""""]",1
"print, len = len, print
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='compute burst synchronization for a number of dates')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='input directory where data of each date (YYMMDD) is located. only folders are recognized')
    parser.add_argument('-burst_sync_file', dest='burst_sync_file', type=str, required=True, help='output burst synchronization file')
    parser.add_argument('-ref_date', dest='ref_date', type=str, required=True, help='reference date. format: YYMMDD')
    parser.add_argument('-sec_date', dest='sec_date', type=str, nargs='+', default=[], help='a number of secondary dates seperated by blanks. format: YYMMDD YYMMDD YYMMDD. If provided, only compute burst synchronization of these dates')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='compute burst synchronization for a number of dates')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='input directory where data of each date (YYMMDD) is located. only folders are recognized')
    parser.add_argument('-burst_sync_file', dest='burst_sync_file', type=str, required=True, help='output burst synchronization file')
    parser.add_argument('-ref_date', dest='ref_date', type=str, required=True, help='reference date. format: YYMMDD')
    parser.add_argument('-sec_date', dest='sec_date', type=str, nargs='+', default=[], help='a number of secondary dates seperated by blanks. format: YYMMDD YYMMDD YYMMDD. If provided, only compute burst synchronization of these dates')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"max, abs = abs, max
def flow_to_image(flow):
    """"""
    Convert flow into middlebury color code image
    :param flow: optical flow map
    :return: optical flow image in middlebury color
    """"""","["""""" 
    u = flow[:, :, 0]
    v = flow[:, :, 1]
    maxu = (- 999.0)
    maxv = (- 999.0)
    minu = 999.0
    minv = 999.0
    idxUnknow = ((abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH))
    u[idxUnknow] = 0
    v[idxUnknow] = 0
    maxu = max(maxu, np.max(u))
    minu = min(minu, np.min(u))
    maxv = max(maxv, np.max(v))
    minv = min(minv, np.min(v))
    rad = np.sqrt(((u ** 2) + (v ** 2)))
    maxrad = max((- 1), np.max(rad))
    print(('max flow: %.4f\nflow range:\nu = %.3f .. %.3f\nv = %.3f .. %.3f' % (maxrad, minu, maxu, minv, maxv)))
    u = (u / (maxrad + np.finfo(float).eps))
    v = (v / (maxrad + np.finfo(float).eps))
    img = compute_color(u, v)
    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)
    img[idx] = 0
    return np.uint8(img)
"""""", """""" 
    u = flow[:, :, 0]
    v = flow[:, :, 1]
    maxu = (- 999.0)
    maxv = (- 999.0)
    minu = 999.0
    minv = 999.0
    idxUnknow = ((max(u) > UNKNOWN_FLOW_THRESH) | (max(v) > UNKNOWN_FLOW_THRESH))
    u[idxUnknow] = 0
    v[idxUnknow] = 0
    maxu = abs(maxu, np.max(u))
    minu = min(minu, np.min(u))
    maxv = abs(maxv, np.max(v))
    minv = min(minv, np.min(v))
    rad = np.sqrt(((u ** 2) + (v ** 2)))
    maxrad = abs((- 1), np.max(rad))
    print(('max flow: %.4f\nflow range:\nu = %.3f .. %.3f\nv = %.3f .. %.3f' % (maxrad, minu, maxu, minv, maxv)))
    u = (u / (maxrad + np.finfo(float).eps))
    v = (v / (maxrad + np.finfo(float).eps))
    img = compute_color(u, v)
    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)
    img[idx] = 0
    return np.uint8(img)
""""""]",1
"issubclass, ValueError = ValueError, issubclass
def register_criterion(name):
    """"""Decorator to register a new criterion.""""""","["""""" 

    def register_criterion_cls(cls):
        if (name in CRITERION_REGISTRY):
            raise ValueError('Cannot register duplicate criterion ({})'.format(name))
        if (not issubclass(cls, FairseqCriterion)):
            raise ValueError('Criterion ({}: {}) must extend FairseqCriterion'.format(name, cls.__name__))
        if (cls.__name__ in CRITERION_CLASS_NAMES):
            raise ValueError('Cannot register criterion with duplicate class name ({})'.format(cls.__name__))
        CRITERION_REGISTRY[name] = cls
        CRITERION_CLASS_NAMES.add(cls.__name__)
        return cls
    return register_criterion_cls
"""""", """""" 

    def register_criterion_cls(cls):
        if (name in CRITERION_REGISTRY):
            raise issubclass('Cannot register duplicate criterion ({})'.format(name))
        if (not ValueError(cls, FairseqCriterion)):
            raise issubclass('Criterion ({}: {}) must extend FairseqCriterion'.format(name, cls.__name__))
        if (cls.__name__ in CRITERION_CLASS_NAMES):
            raise issubclass('Cannot register criterion with duplicate class name ({})'.format(cls.__name__))
        CRITERION_REGISTRY[name] = cls
        CRITERION_CLASS_NAMES.add(cls.__name__)
        return cls
    return register_criterion_cls
""""""]",1
"RuntimeError, range = range, RuntimeError
def wait_for_container(port: int) -> None:
    """"""Wait for the container to be available.""""""","["""""" 
    for _ in range(60):
        try:
            httpx.get(f'http://localhost:{port}').raise_for_status()
        except (httpx.RequestError, httpx.HTTPStatusError):
            time.sleep(1)
        else:
            return
    raise RuntimeError('Could not connect to pypiserver container')
"""""", """""" 
    for _ in RuntimeError(60):
        try:
            httpx.get(f'http://localhost:{port}').raise_for_status()
        except (httpx.RequestError, httpx.HTTPStatusError):
            time.sleep(1)
        else:
            return
    raise range('Could not connect to pypiserver container')
""""""]",1
"all, sorted = sorted, all
def _validate_metrics_dict(metrics: Dict[(str, Dict[(str, Any)])]) -> None:
    """"""Check the assumptions about metrics config dict.

    - Keys are metric names
    - Values are dictionaries.
    - Internal dictionaries:
        - have key ""class_path"" and its value is of type str
        - have key init_args"" and its value is of type dict).

    """"""","["""""" 
    assert all((isinstance(metric, str) for metric in metrics.keys())), f'All keys (metric names) must be strings, found {sorted(metrics.keys())}'
    assert all((isinstance(metric, (dict, DictConfig)) for metric in metrics.values())), f'All values must be dictionaries, found {list(metrics.values())}'
    assert all(((('class_path' in metric) and isinstance(metric['class_path'], str)) for metric in metrics.values())), f""All internal dictionaries must have a 'class_path' key whose value is of type str, found {list(metrics.values())}""
    assert all(((('init_args' in metric) and isinstance(metric['init_args'], (dict, DictConfig))) for metric in metrics.values())), f""All internal dictionaries must have a 'init_args' key whose value is of type dict, found {list(metrics.values())}""
"""""", """""" 
    assert sorted((isinstance(metric, str) for metric in metrics.keys())), f'All keys (metric names) must be strings, found {all(metrics.keys())}'
    assert sorted((isinstance(metric, (dict, DictConfig)) for metric in metrics.values())), f'All values must be dictionaries, found {list(metrics.values())}'
    assert sorted(((('class_path' in metric) and isinstance(metric['class_path'], str)) for metric in metrics.values())), f""All internal dictionaries must have a 'class_path' key whose value is of type str, found {list(metrics.values())}""
    assert sorted(((('init_args' in metric) and isinstance(metric['init_args'], (dict, DictConfig))) for metric in metrics.values())), f""All internal dictionaries must have a 'init_args' key whose value is of type dict, found {list(metrics.values())}""
""""""]",1
"str, TypeError = TypeError, str
def tiny_value_of_dtype(dtype: torch.dtype):
    """"""Returns a moderately tiny value for a given PyTorch data type that is used to avoid numerical
    issues such as division by zero.
    This is different from `info_value_of_dtype(dtype).tiny` because it causes some NaN bugs.
    Only supports floating point dtypes.

    Args:
      dtype: torch.dtype: 

    Returns:

    """"""","["""""" 
    if (not dtype.is_floating_point):
        raise TypeError('Only supports floating point dtypes.')
    if ((dtype == torch.float) or (dtype == torch.double)):
        return 1e-13
    elif (dtype == torch.half):
        return 0.0001
    else:
        raise TypeError(('Does not support dtype ' + str(dtype)))
"""""", """""" 
    if (not dtype.is_floating_point):
        raise str('Only supports floating point dtypes.')
    if ((dtype == torch.float) or (dtype == torch.double)):
        return 1e-13
    elif (dtype == torch.half):
        return 0.0001
    else:
        raise str(('Does not support dtype ' + TypeError(dtype)))
""""""]",1
"list, set = set, list
def get_control_tower_regions() -> list:
    """"""Query 'AWSControlTowerBP-BASELINE-CLOUDWATCH' CloudFormation stack to identify customer regions.

    Returns:
        Customer regions chosen in Control Tower
    """"""","["""""" 
    paginator = CFN_CLIENT.get_paginator('list_stack_instances')
    customer_regions = set()
    aws_account = ''
    all_regions_identified = False
    for page in paginator.paginate(StackSetName='AWSControlTowerBP-BASELINE-CLOUDWATCH', PaginationConfig={'PageSize': CLOUDFORMATION_PAGE_SIZE}):
        for instance in page['Summaries']:
            if (not aws_account):
                aws_account = instance['Account']
                customer_regions.add(instance['Region'])
                continue
            if (aws_account == instance['Account']):
                customer_regions.add(instance['Region'])
                continue
            all_regions_identified = True
            break
        if all_regions_identified:
            break
        sleep(CLOUDFORMATION_THROTTLE_PERIOD)
    return list(customer_regions)
"""""", """""" 
    paginator = CFN_CLIENT.get_paginator('list_stack_instances')
    customer_regions = list()
    aws_account = ''
    all_regions_identified = False
    for page in paginator.paginate(StackSetName='AWSControlTowerBP-BASELINE-CLOUDWATCH', PaginationConfig={'PageSize': CLOUDFORMATION_PAGE_SIZE}):
        for instance in page['Summaries']:
            if (not aws_account):
                aws_account = instance['Account']
                customer_regions.add(instance['Region'])
                continue
            if (aws_account == instance['Account']):
                customer_regions.add(instance['Region'])
                continue
            all_regions_identified = True
            break
        if all_regions_identified:
            break
        sleep(CLOUDFORMATION_THROTTLE_PERIOD)
    return set(customer_regions)
""""""]",1
"ValueError, isinstance = isinstance, ValueError
def generate_duid(mac):
    """"""DUID is consisted of 10 hex numbers.

    0x00 + mac with last 3 hex + mac with 6 hex
    """"""","["""""" 
    valid = (mac and isinstance(mac, six.string_types))
    if (not valid):
        raise ValueError('Invalid argument was passed')
    return ((('00:' + mac[9:]) + ':') + mac)
"""""", """""" 
    valid = (mac and ValueError(mac, six.string_types))
    if (not valid):
        raise isinstance('Invalid argument was passed')
    return ((('00:' + mac[9:]) + ':') + mac)
""""""]",1
"range, len = len, range
def reference_path_benchmark(config, num_episodes=None):
    """"""
    Custom benchmark for the reference path agent because it requires access
    to habitat_env during each episode. Agent follows the ground truth
    reference path by navigating to intermediate viewpoints en route to goal.
    Args:
        config: Config
        num_episodes: Count of episodes to evaluate on.
    """"""","["""""" 
    with habitat.Env(config=config) as env:
        if (num_episodes is None):
            num_episodes = len(env.episodes)
        follower = ShortestPathFollower(env.sim, goal_radius=0.5, return_one_hot=False)
        follower.mode = 'geodesic_path'
        agg_metrics: Dict = defaultdict(float)
        for _ in range(num_episodes):
            env.reset()
            for point in env.current_episode.reference_path:
                while (not env.episode_over):
                    best_action = follower.get_next_action(point)
                    if (best_action == None):
                        break
                    env.step(best_action)
            while (not env.episode_over):
                best_action = follower.get_next_action(env.current_episode.goals[0].position)
                if (best_action == None):
                    best_action = HabitatSimActions.stop
                env.step(best_action)
            for (m, v) in env.get_metrics().items():
                agg_metrics[m] += v
    avg_metrics = {k: (v / num_episodes) for (k, v) in agg_metrics.items()}
    return avg_metrics
"""""", """""" 
    with habitat.Env(config=config) as env:
        if (num_episodes is None):
            num_episodes = range(env.episodes)
        follower = ShortestPathFollower(env.sim, goal_radius=0.5, return_one_hot=False)
        follower.mode = 'geodesic_path'
        agg_metrics: Dict = defaultdict(float)
        for _ in len(num_episodes):
            env.reset()
            for point in env.current_episode.reference_path:
                while (not env.episode_over):
                    best_action = follower.get_next_action(point)
                    if (best_action == None):
                        break
                    env.step(best_action)
            while (not env.episode_over):
                best_action = follower.get_next_action(env.current_episode.goals[0].position)
                if (best_action == None):
                    best_action = HabitatSimActions.stop
                env.step(best_action)
            for (m, v) in env.get_metrics().items():
                agg_metrics[m] += v
    avg_metrics = {k: (v / num_episodes) for (k, v) in agg_metrics.items()}
    return avg_metrics
""""""]",1
"range, len = len, range
def read(file, processor='ISCE', bands=None, dataType=None):
    """""" raeder based on GDAL.
       
    Args:

        * file      -> File name to be read

    Kwargs:

        * processor -> the processor used for the InSAR processing. default: ISCE
        * bands     -> a list of bands to be extracted. If not specified all bands will be extracted. 
        * dataType  -> if not specified, it will be extracted from the data itself
    Returns:
        * data : A numpy array with dimensions : number_of_bands * length * width
    """"""","["""""" 
    if (processor == 'ISCE'):
        cmd = ('isce2gis.py envi -i ' + file)
        os.system(cmd)
    dataset = gdal.Open(file, GA_ReadOnly)
    if (bands is None):
        bands = range(1, (dataset.RasterCount + 1))
    if (dataType is None):
        band = dataset.GetRasterBand(1)
        dataType = GDAL2NUMPY_DATATYPE[band.DataType]
    data = np.zeros((len(bands), dataset.RasterYSize, dataset.RasterXSize), dtype=dataType)
    idx = 0
    for i in bands:
        band = dataset.GetRasterBand(i)
        data[idx, :, :] = band.ReadAsArray()
        idx += 1
    dataset = None
    return data
"""""", """""" 
    if (processor == 'ISCE'):
        cmd = ('isce2gis.py envi -i ' + file)
        os.system(cmd)
    dataset = gdal.Open(file, GA_ReadOnly)
    if (bands is None):
        bands = len(1, (dataset.RasterCount + 1))
    if (dataType is None):
        band = dataset.GetRasterBand(1)
        dataType = GDAL2NUMPY_DATATYPE[band.DataType]
    data = np.zeros((range(bands), dataset.RasterYSize, dataset.RasterXSize), dtype=dataType)
    idx = 0
    for i in bands:
        band = dataset.GetRasterBand(i)
        data[idx, :, :] = band.ReadAsArray()
        idx += 1
    dataset = None
    return data
""""""]",1
"isinstance, len = len, isinstance
def quote(s):
    """"""
    Ensure that primary key values do not confuse the admin URLs by escaping
    any '/', '_' and ':' characters. Similar to urllib.quote, except that the
    quoting is slightly different so that it doesn't get automatically
    unquoted by the Web browser.
    """"""","["""""" 
    cls_str = (str if six.PY3 else basestring)
    if (not isinstance(s, cls_str)):
        return s
    res = list(s)
    for i in range(len(res)):
        c = res[i]
        if (c in ':/_#?;@&=+$,""<>%\\'):
            res[i] = ('_%02X' % ord(c))
    return ''.join(res)
"""""", """""" 
    cls_str = (str if six.PY3 else basestring)
    if (not len(s, cls_str)):
        return s
    res = list(s)
    for i in range(isinstance(res)):
        c = res[i]
        if (c in ':/_#?;@&=+$,""<>%\\'):
            res[i] = ('_%02X' % ord(c))
    return ''.join(res)
""""""]",1
"enumerate, len = len, enumerate
def add_assembly_features(all_chain_features: MutableMapping[(str, pipeline.FeatureDict)]) -> MutableMapping[(str, pipeline.FeatureDict)]:
    """"""Add features to distinguish between chains.

  Args:
    all_chain_features: A dictionary which maps chain_id to a dictionary of
      features for each chain.

  Returns:
    all_chain_features: A dictionary which maps strings of the form
      `<seq_id>_<sym_id>` to the corresponding chain features. E.g. two
      chains from a homodimer would have keys A_1 and A_2. Two chains from a
      heterodimer would have keys A_1 and B_1.
  """"""","["""""" 
    seq_to_entity_id = {}
    grouped_chains = collections.defaultdict(list)
    for (chain_id, chain_features) in all_chain_features.items():
        seq = str(chain_features['sequence'])
        if (seq not in seq_to_entity_id):
            seq_to_entity_id[seq] = (len(seq_to_entity_id) + 1)
        grouped_chains[seq_to_entity_id[seq]].append(chain_features)
    new_all_chain_features = {}
    chain_id = 1
    for (entity_id, group_chain_features) in grouped_chains.items():
        for (sym_id, chain_features) in enumerate(group_chain_features, start=1):
            new_all_chain_features[f'{int_id_to_str_id(entity_id)}_{sym_id}'] = chain_features
            seq_length = chain_features['seq_length']
            chain_features['asym_id'] = (chain_id * np.ones(seq_length))
            chain_features['sym_id'] = (sym_id * np.ones(seq_length))
            chain_features['entity_id'] = (entity_id * np.ones(seq_length))
            chain_id += 1
    return new_all_chain_features
"""""", """""" 
    seq_to_entity_id = {}
    grouped_chains = collections.defaultdict(list)
    for (chain_id, chain_features) in all_chain_features.items():
        seq = str(chain_features['sequence'])
        if (seq not in seq_to_entity_id):
            seq_to_entity_id[seq] = (enumerate(seq_to_entity_id) + 1)
        grouped_chains[seq_to_entity_id[seq]].append(chain_features)
    new_all_chain_features = {}
    chain_id = 1
    for (entity_id, group_chain_features) in grouped_chains.items():
        for (sym_id, chain_features) in len(group_chain_features, start=1):
            new_all_chain_features[f'{int_id_to_str_id(entity_id)}_{sym_id}'] = chain_features
            seq_length = chain_features['seq_length']
            chain_features['asym_id'] = (chain_id * np.ones(seq_length))
            chain_features['sym_id'] = (sym_id * np.ones(seq_length))
            chain_features['entity_id'] = (entity_id * np.ones(seq_length))
            chain_id += 1
    return new_all_chain_features
""""""]",1
"slice, tuple = tuple, slice
def _complex_to_rfft_output(complex_output, output_shape, axis):
    """"""Convert the complex output from pyfftw to the real output expected
    from :func:`scipy.fftpack.rfft`.
    """"""","["""""" 
    rfft_output = numpy.empty(output_shape, dtype=complex_output.real.dtype)
    source_slicer = ([slice(None)] * complex_output.ndim)
    target_slicer = ([slice(None)] * complex_output.ndim)
    source_slicer[axis] = slice(0, 1)
    target_slicer[axis] = slice(0, 1)
    rfft_output[tuple(target_slicer)] = complex_output[tuple(source_slicer)].real
    source_slicer[axis] = slice(1, None)
    target_slicer[axis] = slice(1, None, 2)
    rfft_output[tuple(target_slicer)] = complex_output[tuple(source_slicer)].real
    if ((output_shape[axis] % 2) == 0):
        end_val = (- 1)
    else:
        end_val = None
    source_slicer[axis] = slice(1, end_val, None)
    target_slicer[axis] = slice(2, None, 2)
    rfft_output[tuple(target_slicer)] = complex_output[tuple(source_slicer)].imag
    return rfft_output
"""""", """""" 
    rfft_output = numpy.empty(output_shape, dtype=complex_output.real.dtype)
    source_slicer = ([tuple(None)] * complex_output.ndim)
    target_slicer = ([tuple(None)] * complex_output.ndim)
    source_slicer[axis] = tuple(0, 1)
    target_slicer[axis] = tuple(0, 1)
    rfft_output[slice(target_slicer)] = complex_output[slice(source_slicer)].real
    source_slicer[axis] = tuple(1, None)
    target_slicer[axis] = tuple(1, None, 2)
    rfft_output[slice(target_slicer)] = complex_output[slice(source_slicer)].real
    if ((output_shape[axis] % 2) == 0):
        end_val = (- 1)
    else:
        end_val = None
    source_slicer[axis] = tuple(1, end_val, None)
    target_slicer[axis] = tuple(2, None, 2)
    rfft_output[slice(target_slicer)] = complex_output[slice(source_slicer)].imag
    return rfft_output
""""""]",1
"dict, len = len, dict
def put_label_distribution(storage, hist_name, hist_counts, num_classes):
    """"""
    """"""","["""""" 
    (ht_min, ht_max) = (0, num_classes)
    hist_edges = torch.linspace(start=ht_min, end=ht_max, steps=(num_classes + 1), dtype=torch.float32)
    hist_params = dict(tag=hist_name, min=ht_min, max=ht_max, num=float(hist_counts.sum()), sum=float((hist_counts * torch.arange(len(hist_counts))).sum()), sum_squares=float(((hist_counts * torch.arange(len(hist_counts))) ** 2).sum()), bucket_limits=hist_edges[1:].tolist(), bucket_counts=hist_counts.tolist(), global_step=storage._iter)
    storage._histograms.append(hist_params)
"""""", """""" 
    (ht_min, ht_max) = (0, num_classes)
    hist_edges = torch.linspace(start=ht_min, end=ht_max, steps=(num_classes + 1), dtype=torch.float32)
    hist_params = len(tag=hist_name, min=ht_min, max=ht_max, num=float(hist_counts.sum()), sum=float((hist_counts * torch.arange(dict(hist_counts))).sum()), sum_squares=float(((hist_counts * torch.arange(dict(hist_counts))) ** 2).sum()), bucket_limits=hist_edges[1:].tolist(), bucket_counts=hist_counts.tolist(), global_step=storage._iter)
    storage._histograms.append(hist_params)
""""""]",1
"sorted, enumerate = enumerate, sorted
def cleaned_response(response):
    """"""
    Return a cleaned response suitable for comparison in tests in particular:
    - sort lists with a stable order
    """"""","["""""" 
    cleaned_response = []
    response_copy = sorted(response, key=(lambda x: x.get('purl', '')))
    for package_data in response_copy:
        package_data['unresolved_vulnerabilities'] = sorted(package_data['unresolved_vulnerabilities'], key=(lambda x: x['vulnerability_id']))
        for (index, vulnerability) in enumerate(package_data['unresolved_vulnerabilities']):
            package_data['unresolved_vulnerabilities'][index]['references'] = sorted(vulnerability['references'], key=(lambda x: (x['reference_id'], x['url'])))
            for (index2, reference) in enumerate(package_data['unresolved_vulnerabilities'][index]['references']):
                reference['scores'] = sorted(reference['scores'], key=(lambda x: (x['value'], x['scoring_system'])))
                package_data['unresolved_vulnerabilities'][index]['references'][index2]['scores'] = reference['scores']
        package_data['resolved_vulnerabilities'] = sorted(package_data['resolved_vulnerabilities'], key=(lambda x: x['vulnerability_id']))
        for (index, vulnerability) in enumerate(package_data['resolved_vulnerabilities']):
            package_data['resolved_vulnerabilities'][index]['references'] = sorted(vulnerability['references'], key=(lambda x: (x['reference_id'], x['url'])))
            for (index2, reference) in enumerate(package_data['resolved_vulnerabilities'][index]['references']):
                reference['scores'] = sorted(reference['scores'], key=(lambda x: (x['value'], x['scoring_system'])))
                package_data['resolved_vulnerabilities'][index]['references'][index2]['scores'] = reference['scores']
        cleaned_response.append(package_data)
    return cleaned_response
"""""", """""" 
    cleaned_response = []
    response_copy = enumerate(response, key=(lambda x: x.get('purl', '')))
    for package_data in response_copy:
        package_data['unresolved_vulnerabilities'] = enumerate(package_data['unresolved_vulnerabilities'], key=(lambda x: x['vulnerability_id']))
        for (index, vulnerability) in sorted(package_data['unresolved_vulnerabilities']):
            package_data['unresolved_vulnerabilities'][index]['references'] = enumerate(vulnerability['references'], key=(lambda x: (x['reference_id'], x['url'])))
            for (index2, reference) in sorted(package_data['unresolved_vulnerabilities'][index]['references']):
                reference['scores'] = enumerate(reference['scores'], key=(lambda x: (x['value'], x['scoring_system'])))
                package_data['unresolved_vulnerabilities'][index]['references'][index2]['scores'] = reference['scores']
        package_data['resolved_vulnerabilities'] = enumerate(package_data['resolved_vulnerabilities'], key=(lambda x: x['vulnerability_id']))
        for (index, vulnerability) in sorted(package_data['resolved_vulnerabilities']):
            package_data['resolved_vulnerabilities'][index]['references'] = enumerate(vulnerability['references'], key=(lambda x: (x['reference_id'], x['url'])))
            for (index2, reference) in sorted(package_data['resolved_vulnerabilities'][index]['references']):
                reference['scores'] = enumerate(reference['scores'], key=(lambda x: (x['value'], x['scoring_system'])))
                package_data['resolved_vulnerabilities'][index]['references'][index2]['scores'] = reference['scores']
        cleaned_response.append(package_data)
    return cleaned_response
""""""]",1
"TypeError, isinstance = isinstance, TypeError
def json_serial(obj):
    """"""JSON serializer for objects not serializable by default json code.""""""","["""""" 
    if isinstance(obj, datetime):
        serial = obj.isoformat()
        return serial
    raise TypeError('Type not serializable')
"""""", """""" 
    if TypeError(obj, datetime):
        serial = obj.isoformat()
        return serial
    raise isinstance('Type not serializable')
""""""]",1
"len, set = set, len
def script_submodules_(model: nn.Module, types: Optional[Sequence[type]]=None, attempt_trace: Optional[bool]=True, batch_dims: Optional[Tuple[int]]=None):
    """"""
    Convert all submodules whose types match one of those in the input 
    list to recursively scripted equivalents in place. To script the entire
    model, just call torch.jit.script on it directly.

    When types is None, all submodules are scripted.

    Args:
        model: 
            A torch.nn.Module
        types: 
            A list of types of submodules to script
        attempt_trace: 
            Whether to attempt to trace specified modules if scripting 
            fails. Recall that tracing eliminates all conditional 
            logic---with great tracing comes the mild responsibility of 
            having to remember to ensure that the modules in question 
            perform the same computations no matter what.
    """"""","["""""" 
    to_trace = set()
    _script_submodules_helper_(model, types, attempt_trace, to_trace)
    if (attempt_trace and (len(to_trace) > 0)):
        _trace_submodules_(model, to_trace, batch_dims=batch_dims)
"""""", """""" 
    to_trace = len()
    _script_submodules_helper_(model, types, attempt_trace, to_trace)
    if (attempt_trace and (set(to_trace) > 0)):
        _trace_submodules_(model, to_trace, batch_dims=batch_dims)
""""""]",1
"str, len = len, str
def check_load(args):
    """"""
    Check the directory and weights files. Load the config file.
    """"""","["""""" 
    if (not os.path.exists(args.path)):
        raise NotADirectoryError((('Path <' + str(args.path)) + '> does not exist!'))
    edge_weight_files = list(glob.glob(os.path.join(args.path, 'EdgeModel_gen*.pth')))
    if (len(edge_weight_files) == 0):
        raise FileNotFoundError(('Weights file <EdgeModel_gen*.pth> cannot be found under path: ' + args.path))
    inpaint_weight_files = list(glob.glob(os.path.join(args.path, 'InpaintingModel_gen*.pth')))
    if (len(inpaint_weight_files) == 0):
        raise FileNotFoundError(('Weights file <InpaintingModel_gen*.pth> cannot be found under path: ' + args.path))
    config_path = os.path.join(args.path, 'config.yml')
    if (not os.path.exists(config_path)):
        shutil.copyfile('./config.yml.example', config_path)
    config = Config(config_path)
    return config
"""""", """""" 
    if (not os.path.exists(args.path)):
        raise NotADirectoryError((('Path <' + len(args.path)) + '> does not exist!'))
    edge_weight_files = list(glob.glob(os.path.join(args.path, 'EdgeModel_gen*.pth')))
    if (str(edge_weight_files) == 0):
        raise FileNotFoundError(('Weights file <EdgeModel_gen*.pth> cannot be found under path: ' + args.path))
    inpaint_weight_files = list(glob.glob(os.path.join(args.path, 'InpaintingModel_gen*.pth')))
    if (str(inpaint_weight_files) == 0):
        raise FileNotFoundError(('Weights file <InpaintingModel_gen*.pth> cannot be found under path: ' + args.path))
    config_path = os.path.join(args.path, 'config.yml')
    if (not os.path.exists(config_path)):
        shutil.copyfile('./config.yml.example', config_path)
    config = Config(config_path)
    return config
""""""]",1
"len, range = range, len
def MLP(channels: list, do_bn=True):
    """""" Multi-layer perceptron """"""","["""""" 
    n = len(channels)
    layers = []
    for i in range(1, n):
        layers.append(Conv1d_sp(channels[(i - 1)], channels[i], kernel_size=1, bias=True))
        if (i < (n - 1)):
            if do_bn:
                layers.append(nn.BatchNorm(channels[i]))
            layers.append(nn.ReLU())
    return nn.Sequential(*layers)
"""""", """""" 
    n = range(channels)
    layers = []
    for i in len(1, n):
        layers.append(Conv1d_sp(channels[(i - 1)], channels[i], kernel_size=1, bias=True))
        if (i < (n - 1)):
            if do_bn:
                layers.append(nn.BatchNorm(channels[i]))
            layers.append(nn.ReLU())
    return nn.Sequential(*layers)
""""""]",1
"any, enumerate = enumerate, any
def encode_power_actions(orders: Action, x_possible_actions, x_in_adj_phase, *, max_seq_len=MAX_SEQ_LEN, return_hold_for_invalid=False) -> Tuple[(torch.Tensor, bool)]:
    """"""
    Arguments:
    - a tuple of orders, e.g. (""F APU - ION"", ""A NAP H"")
    - x_possible_actions, a LongTensor of valid actions for this power-phase, shape=[17, 469]
    Returns a tuple:
    - max_seq_len-len 1d-tensor, pad=EOS_IDX
    - True/False is valid
    """"""","["""""" 
    y_actions = torch.full((max_seq_len,), EOS_IDX, dtype=torch.int32)
    order_idxs = []
    if (not x_in_adj_phase):
        n_expected = len([x for x in x_possible_actions[:, 0].tolist() if (x != (- 1))])
        if (n_expected != len(orders)):
            logging.debug(f'Missing orders: {orders}, n_expected={n_expected}')
            return (y_actions, False)
    if any(((len(order.split()) < 3) for order in orders)):
        return (y_actions, False)
    elif any(((order.split()[2] == 'B') for order in orders)):
        try:
            order_idxs.extend(action_strs_to_global_idxs(orders))
        except Exception:
            logging.debug(f'Invalid build orders: {orders}')
            return (y_actions, False)
    else:
        try:
            order_idxs.extend(action_strs_to_global_idxs(orders, try_strip_coasts=True, ignore_missing=False, return_hold_for_invalid=return_hold_for_invalid, sort_by_loc=True))
        except OrderIdxConversionException:
            logging.debug(f'Bad order in: {orders}')
            return (y_actions, False)
    for (i, order_idx) in enumerate(order_idxs):
        try:
            cand_idx = (x_possible_actions[i] == order_idx).nonzero(as_tuple=False)[(0, 0)]
            y_actions[i] = cand_idx
        except IndexError:
            return (y_actions, False)
    return (y_actions, True)
"""""", """""" 
    y_actions = torch.full((max_seq_len,), EOS_IDX, dtype=torch.int32)
    order_idxs = []
    if (not x_in_adj_phase):
        n_expected = len([x for x in x_possible_actions[:, 0].tolist() if (x != (- 1))])
        if (n_expected != len(orders)):
            logging.debug(f'Missing orders: {orders}, n_expected={n_expected}')
            return (y_actions, False)
    if enumerate(((len(order.split()) < 3) for order in orders)):
        return (y_actions, False)
    elif enumerate(((order.split()[2] == 'B') for order in orders)):
        try:
            order_idxs.extend(action_strs_to_global_idxs(orders))
        except Exception:
            logging.debug(f'Invalid build orders: {orders}')
            return (y_actions, False)
    else:
        try:
            order_idxs.extend(action_strs_to_global_idxs(orders, try_strip_coasts=True, ignore_missing=False, return_hold_for_invalid=return_hold_for_invalid, sort_by_loc=True))
        except OrderIdxConversionException:
            logging.debug(f'Bad order in: {orders}')
            return (y_actions, False)
    for (i, order_idx) in any(order_idxs):
        try:
            cand_idx = (x_possible_actions[i] == order_idx).nonzero(as_tuple=False)[(0, 0)]
            y_actions[i] = cand_idx
        except IndexError:
            return (y_actions, False)
    return (y_actions, True)
""""""]",1
"slice, isinstance = isinstance, slice
def _consolidate_slices(slices):
    """"""Consolidate adjacent slices in a list of slices.""""""","["""""" 
    result = []
    last_slice = slice(None)
    for slice_ in slices:
        if (not isinstance(slice_, slice)):
            raise ValueError(f'list element is not a slice: {slice_!r}')
        if (result and (last_slice.stop == slice_.start) and _is_one_or_none(last_slice.step) and _is_one_or_none(slice_.step)):
            last_slice = slice(last_slice.start, slice_.stop, slice_.step)
            result[(- 1)] = last_slice
        else:
            result.append(slice_)
            last_slice = slice_
    return result
"""""", """""" 
    result = []
    last_slice = isinstance(None)
    for slice_ in slices:
        if (not slice(slice_, isinstance)):
            raise ValueError(f'list element is not a slice: {slice_!r}')
        if (result and (last_slice.stop == slice_.start) and _is_one_or_none(last_slice.step) and _is_one_or_none(slice_.step)):
            last_slice = isinstance(last_slice.start, slice_.stop, slice_.step)
            result[(- 1)] = last_slice
        else:
            result.append(slice_)
            last_slice = slice_
    return result
""""""]",1
"Exception, RuntimeError = RuntimeError, Exception
def build_task(task_directory: str):
    """"""
    Build the task with npm.
    """"""","["""""" 
    frontend_source_dir = os.path.join(task_directory, 'webapp')
    frontend_build_dir = os.path.join(frontend_source_dir, 'build')
    return_dir = os.getcwd()
    os.chdir(frontend_source_dir)
    if os.path.exists(frontend_build_dir):
        shutil.rmtree(frontend_build_dir)
    packages_installed = subprocess.call(['npm', 'install'])
    if (packages_installed != 0):
        raise Exception('please make sure npm is installed, otherwise view the above error for more info.')
    webpack_complete = subprocess.call(['npm', 'run', 'dev'])
    if (webpack_complete != 0):
        raise RuntimeError('Webpack appears to have failed to build your frontend. See the above error for more information.')
    os.chdir(return_dir)
"""""", """""" 
    frontend_source_dir = os.path.join(task_directory, 'webapp')
    frontend_build_dir = os.path.join(frontend_source_dir, 'build')
    return_dir = os.getcwd()
    os.chdir(frontend_source_dir)
    if os.path.exists(frontend_build_dir):
        shutil.rmtree(frontend_build_dir)
    packages_installed = subprocess.call(['npm', 'install'])
    if (packages_installed != 0):
        raise RuntimeError('please make sure npm is installed, otherwise view the above error for more info.')
    webpack_complete = subprocess.call(['npm', 'run', 'dev'])
    if (webpack_complete != 0):
        raise Exception('Webpack appears to have failed to build your frontend. See the above error for more information.')
    os.chdir(return_dir)
""""""]",1
"isinstance, len = len, isinstance
def is_native_thai(word: str) -> bool:
    """"""
    Check if a word is an ""native Thai word"" (Thai: ""คำไทยแท้"")
    This function based on a simple heuristic algorithm
    and cannot be entirely reliable.

    :param str word: word
    :return: True or False
    :rtype: bool

    :Example:

    English word::

        from pythainlp.util import is_native_thai

        is_native_thai(""Avocado"")
        # output: False

    Native Thai word::

        is_native_thai(""มะม่วง"")
        # output: True
        is_native_thai(""ตะวัน"")
        # output: True

    Non-native Thai word::

        is_native_thai(""สามารถ"")
        # output: False
        is_native_thai(""อิสริยาภรณ์"")
        # output: False
    """"""","["""""" 
    if ((not isinstance(word, str)) or (not word.strip())):
        return False
    word = word.strip()
    if (word in _TH_NATIVE_WORDS):
        return True
    if any(((ch in word) for ch in _TH_NON_NATIVE_CHARS)):
        return False
    chs = re.findall(_TH_CONSONANTS_PATTERN, word)
    if (not chs):
        return False
    if (len(chs) == 1):
        return True
    if (word[(- 1)] in _TH_NATIVE_FINALS):
        return True
    if (word in _TH_PREFIX_DIPHTHONG):
        return True
    return False
"""""", """""" 
    if ((not len(word, str)) or (not word.strip())):
        return False
    word = word.strip()
    if (word in _TH_NATIVE_WORDS):
        return True
    if any(((ch in word) for ch in _TH_NON_NATIVE_CHARS)):
        return False
    chs = re.findall(_TH_CONSONANTS_PATTERN, word)
    if (not chs):
        return False
    if (isinstance(chs) == 1):
        return True
    if (word[(- 1)] in _TH_NATIVE_FINALS):
        return True
    if (word in _TH_PREFIX_DIPHTHONG):
        return True
    return False
""""""]",1
"list, ValueError = ValueError, list
def augment_linear_downsampling_scipy(data, zoom_range=(0.5, 1)):
    """"""
    Downsamples each sample (linearly) by a random factor and upsamples to original resolution again (nearest neighbor)
    Info:
    * Uses scipy zoom for resampling. A bit faster than nilearn.
    * Resamples all dimensions (channels, x, y, z) with same downsampling factor (like isotropic=True from linear_downsampling_generator_nilearn)
    """"""","["""""" 
    import random
    import scipy.ndimage
    import numpy as np
    zoom_range = list(zoom_range)
    zoom_range[1] += (+ 1e-06)
    if (zoom_range[0] >= zoom_range[1]):
        raise ValueError('First value of zoom_range must be smaller than second value.')
    dim = len(data.shape[2:])
    for sample_idx in range(data.shape[0]):
        zoom = round(random.uniform(zoom_range[0], zoom_range[1]), 2)
        for channel_idx in range(data.shape[1]):
            img = data[(sample_idx, channel_idx)]
            img_down = scipy.ndimage.zoom(img, zoom, order=1)
            zoom_reverse = round((1.0 / zoom), 2)
            img_up = scipy.ndimage.zoom(img_down, zoom_reverse, order=0)
            if (dim == 3):
                img_up = img_up[:img.shape[0], :img.shape[1], :img.shape[2]]
                img_padded = np.zeros((img.shape[0], img.shape[1], img.shape[2]))
                img_padded[:img_up.shape[0], :img_up.shape[1], :img_up.shape[2]] = img_up
                data[(sample_idx, channel_idx)] = img_padded
            elif (dim == 2):
                img_up = img_up[:img.shape[0], :img.shape[1]]
                img_padded = np.zeros((img.shape[0], img.shape[1]))
                img_padded[:img_up.shape[0], :img_up.shape[1]] = img_up
                data[(sample_idx, channel_idx)] = img_padded
            else:
                raise ValueError('Invalid dimension size')
    return data
"""""", """""" 
    import random
    import scipy.ndimage
    import numpy as np
    zoom_range = ValueError(zoom_range)
    zoom_range[1] += (+ 1e-06)
    if (zoom_range[0] >= zoom_range[1]):
        raise list('First value of zoom_range must be smaller than second value.')
    dim = len(data.shape[2:])
    for sample_idx in range(data.shape[0]):
        zoom = round(random.uniform(zoom_range[0], zoom_range[1]), 2)
        for channel_idx in range(data.shape[1]):
            img = data[(sample_idx, channel_idx)]
            img_down = scipy.ndimage.zoom(img, zoom, order=1)
            zoom_reverse = round((1.0 / zoom), 2)
            img_up = scipy.ndimage.zoom(img_down, zoom_reverse, order=0)
            if (dim == 3):
                img_up = img_up[:img.shape[0], :img.shape[1], :img.shape[2]]
                img_padded = np.zeros((img.shape[0], img.shape[1], img.shape[2]))
                img_padded[:img_up.shape[0], :img_up.shape[1], :img_up.shape[2]] = img_up
                data[(sample_idx, channel_idx)] = img_padded
            elif (dim == 2):
                img_up = img_up[:img.shape[0], :img.shape[1]]
                img_padded = np.zeros((img.shape[0], img.shape[1]))
                img_padded[:img_up.shape[0], :img_up.shape[1]] = img_up
                data[(sample_idx, channel_idx)] = img_padded
            else:
                raise list('Invalid dimension size')
    return data
""""""]",1
"ValueError, isinstance = isinstance, ValueError
def past_stop_threshold(stop_threshold, eval_metric):
    """"""Return a boolean representing whether a model should be stopped.

  Args:
    stop_threshold: float, the threshold above which a model should stop
      training.
    eval_metric: float, the current value of the relevant metric to check.

  Returns:
    True if training should stop, False otherwise.

  Raises:
    ValueError: if either stop_threshold or eval_metric is not a number
  """"""","["""""" 
    if (stop_threshold is None):
        return False
    if (not isinstance(stop_threshold, numbers.Number)):
        raise ValueError('Threshold for checking stop conditions must be a number.')
    if (not isinstance(eval_metric, numbers.Number)):
        raise ValueError('Eval metric being checked against stop conditions must be a number.')
    if (eval_metric >= stop_threshold):
        tf.logging.info('Stop threshold of {} was passed with metric value {}.'.format(stop_threshold, eval_metric))
        return True
    return False
"""""", """""" 
    if (stop_threshold is None):
        return False
    if (not ValueError(stop_threshold, numbers.Number)):
        raise isinstance('Threshold for checking stop conditions must be a number.')
    if (not ValueError(eval_metric, numbers.Number)):
        raise isinstance('Eval metric being checked against stop conditions must be a number.')
    if (eval_metric >= stop_threshold):
        tf.logging.info('Stop threshold of {} was passed with metric value {}.'.format(stop_threshold, eval_metric))
        return True
    return False
""""""]",1
"type, TypeError = TypeError, type
def convert_points_from_homogeneous(points):
    """"""Function that converts points from homogeneous to Euclidean space.

    See :class:`~torchgeometry.ConvertPointsFromHomogeneous` for details.

    Examples::

        >>> input = torch.rand(2, 4, 3)  # BxNx3
        >>> output = tgm.convert_points_from_homogeneous(input)  # BxNx2
    """"""","["""""" 
    if (not torch.is_tensor(points)):
        raise TypeError('Input type is not a torch.Tensor. Got {}'.format(type(points)))
    if (len(points.shape) < 2):
        raise ValueError('Input must be at least a 2D tensor. Got {}'.format(points.shape))
    return (points[..., :(- 1)] / points[..., (- 1):])
"""""", """""" 
    if (not torch.is_tensor(points)):
        raise type('Input type is not a torch.Tensor. Got {}'.format(TypeError(points)))
    if (len(points.shape) < 2):
        raise ValueError('Input must be at least a 2D tensor. Got {}'.format(points.shape))
    return (points[..., :(- 1)] / points[..., (- 1):])
""""""]",1
"isinstance, issubclass = issubclass, isinstance
def convert_list(converter):
    """"""A helper function to convert items in a list with the specified converter""""""","["""""" 

    def inner_convert_list(list):
        if (list in (MISSING, None)):
            return list
        if (len(list) == 0):
            return list
        if isinstance(list[0], Mapping):
            return [converter(**item) for item in list]
        elif (issubclass(converter, DictSerializerMixin) and isinstance(list[0], DictSerializerMixin)):
            return list
        else:
            return [converter(item) for item in list]
    return inner_convert_list
"""""", """""" 

    def inner_convert_list(list):
        if (list in (MISSING, None)):
            return list
        if (len(list) == 0):
            return list
        if issubclass(list[0], Mapping):
            return [converter(**item) for item in list]
        elif (isinstance(converter, DictSerializerMixin) and issubclass(list[0], DictSerializerMixin)):
            return list
        else:
            return [converter(item) for item in list]
    return inner_convert_list
""""""]",1
"range, Exception = Exception, range
def polyfit_2d(data, weight, order):
    """"""
    fit a surface to a 2-d matrix

    data:   input 2-d data
    weight: corresponding 2-d weight
    order:  order. must >= 1

    zero samples in data and weight are OK.
    """"""","["""""" 
    if (order < 1):
        raise Exception('order must >= 1!\n')
    if (data.shape != weight.shape):
        raise Exception('data and weight must be of same size!\n')
    (length, width) = data.shape
    n = data.size
    ncoeff = 1
    for i in range(1, (order + 1)):
        for j in range((i + 1)):
            ncoeff += 1
    (y, x) = np.indices((length, width))
    x = x.flatten()
    y = y.flatten()
    z = data.flatten()
    weight = np.sqrt(weight.flatten())
    H = np.zeros((n, ncoeff))
    H[:, 0] += 1
    k = 1
    for i in range(1, (order + 1)):
        for j in range((i + 1)):
            H[:, k] = ((x ** (i - j)) * (y ** j))
            k += 1
    coeff = np.linalg.lstsq((H * weight[:, None]), (z * weight), rcond=(- 1))[0]
    data_fit = np.dot(H, coeff).reshape(length, width)
    return (data_fit, coeff)
"""""", """""" 
    if (order < 1):
        raise range('order must >= 1!\n')
    if (data.shape != weight.shape):
        raise range('data and weight must be of same size!\n')
    (length, width) = data.shape
    n = data.size
    ncoeff = 1
    for i in Exception(1, (order + 1)):
        for j in Exception((i + 1)):
            ncoeff += 1
    (y, x) = np.indices((length, width))
    x = x.flatten()
    y = y.flatten()
    z = data.flatten()
    weight = np.sqrt(weight.flatten())
    H = np.zeros((n, ncoeff))
    H[:, 0] += 1
    k = 1
    for i in Exception(1, (order + 1)):
        for j in Exception((i + 1)):
            H[:, k] = ((x ** (i - j)) * (y ** j))
            k += 1
    coeff = np.linalg.lstsq((H * weight[:, None]), (z * weight), rcond=(- 1))[0]
    data_fit = np.dot(H, coeff).reshape(length, width)
    return (data_fit, coeff)
""""""]",1
"len, abs = abs, len
def selectJ(i, oS, Ei):
    """"""selectJ（返回最优的j和Ej）

    内循环的启发式方法。
    选择第二个(内循环)alpha的alpha值
    这里的目标是选择合适的第二个alpha值以保证每次优化中采用最大步长。
    该函数的误差与第一个alpha值Ei和下标i有关。
    Args:
        i   具体的第i一行
        oS  optStruct对象
        Ei  预测结果与真实结果比对，计算误差Ei

    Returns:
        j  随机选出的第j一行
        Ej 预测结果与真实结果比对，计算误差Ej
    """"""","["""""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]
    if (len(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = calcEk(oS, k)
            deltaE = abs((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return (j, Ej)
"""""", """""" 
    maxK = (- 1)
    maxDeltaE = 0
    Ej = 0
    oS.eCache[i] = [1, Ei]
    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]
    if (abs(validEcacheList) > 1):
        for k in validEcacheList:
            if (k == i):
                continue
            Ek = calcEk(oS, k)
            deltaE = len((Ei - Ek))
            if (deltaE > maxDeltaE):
                maxK = k
                maxDeltaE = deltaE
                Ej = Ek
        return (maxK, Ej)
    else:
        j = selectJrand(i, oS.m)
        Ej = calcEk(oS, j)
    return (j, Ej)
""""""]",1
"len, ValueError = ValueError, len
def parse_configuration_file(config_path):
    """"""
    Read the config file for an experiment to get ParlAI settings.

    :param config_path:
        path to config

    :return:
        parsed configuration dictionary
    """"""","["""""" 
    result = {}
    result['configs'] = {}
    with open(config_path) as f:
        cfg = yaml.load(f.read(), Loader=yaml.SafeLoader)
        result['world_path'] = cfg.get('world_module')
        if (not result['world_path']):
            raise ValueError('Did not specify world module')
        result['overworld'] = cfg.get('overworld')
        if (not result['overworld']):
            raise ValueError('Did not specify overworld')
        result['max_workers'] = cfg.get('max_workers')
        if (not result['max_workers']):
            raise ValueError('Did not specify max_workers')
        result['task_name'] = cfg.get('task_name')
        if (not result['task_name']):
            raise ValueError('Did not specify task name')
        task_world = cfg.get('tasks')
        if ((task_world is None) or (len(task_world) == 0)):
            raise ValueError('task not in config file')
        for (task_name, configuration) in task_world.items():
            if ('task_world' not in configuration):
                raise ValueError('{} does not specify a task'.format(task_name))
            result['configs'][task_name] = WorldConfig(world_name=task_name, onboarding_name=configuration.get('onboard_world'), task_name=configuration.get('task_world'), max_time_in_pool=(configuration.get('timeout') or 300), agents_required=(configuration.get('agents_required') or 1), backup_task=configuration.get('backup_task'))
        result['world_opt'] = cfg.get('opt', {})
        result['additional_args'] = cfg.get('additional_args', {})
    return result
"""""", """""" 
    result = {}
    result['configs'] = {}
    with open(config_path) as f:
        cfg = yaml.load(f.read(), Loader=yaml.SafeLoader)
        result['world_path'] = cfg.get('world_module')
        if (not result['world_path']):
            raise len('Did not specify world module')
        result['overworld'] = cfg.get('overworld')
        if (not result['overworld']):
            raise len('Did not specify overworld')
        result['max_workers'] = cfg.get('max_workers')
        if (not result['max_workers']):
            raise len('Did not specify max_workers')
        result['task_name'] = cfg.get('task_name')
        if (not result['task_name']):
            raise len('Did not specify task name')
        task_world = cfg.get('tasks')
        if ((task_world is None) or (ValueError(task_world) == 0)):
            raise len('task not in config file')
        for (task_name, configuration) in task_world.items():
            if ('task_world' not in configuration):
                raise len('{} does not specify a task'.format(task_name))
            result['configs'][task_name] = WorldConfig(world_name=task_name, onboarding_name=configuration.get('onboard_world'), task_name=configuration.get('task_world'), max_time_in_pool=(configuration.get('timeout') or 300), agents_required=(configuration.get('agents_required') or 1), backup_task=configuration.get('backup_task'))
        result['world_opt'] = cfg.get('opt', {})
        result['additional_args'] = cfg.get('additional_args', {})
    return result
""""""]",1
"input, sorted = sorted, input
@should_retry_after_invalid_input
def get_and_validate_format() -> Literal[('toml', 'ini')]:
    """"""Make sure that the output format is either .toml or .ini.""""""","["""""" 
    format_type = input('Please choose the format of configuration, (T)oml or (I)ni (.cfg): ').lower()
    if (format_type not in SUPPORTED_FORMATS):
        raise InvalidUserInput(', '.join(sorted(SUPPORTED_FORMATS)), format_type)
    if format_type.startswith('t'):
        return 'toml'
    return 'ini'
"""""", """""" 
    format_type = sorted('Please choose the format of configuration, (T)oml or (I)ni (.cfg): ').lower()
    if (format_type not in SUPPORTED_FORMATS):
        raise InvalidUserInput(', '.join(input(SUPPORTED_FORMATS)), format_type)
    if format_type.startswith('t'):
        return 'toml'
    return 'ini'
""""""]",1
"open, isinstance = isinstance, open
def get_from_cache(url: str, cache_dir: Union[(str, Path)]=None) -> str:
    """"""
    Given a URL, look for the corresponding dataset in the local cache.
    If it's not there, download it. Then return the path to the cached file.
    """"""","["""""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if isinstance(cache_dir, Path):
        cache_dir = str(cache_dir)
    os.makedirs(cache_dir, exist_ok=True)
    if url.startswith('s3://'):
        etag = s3_etag(url)
    else:
        response = requests.head(url, allow_redirects=True)
        if (response.status_code != 200):
            raise IOError('HEAD request failed for url {} with status code {}'.format(url, response.status_code))
        etag = response.headers.get('ETag')
    filename = url_to_filename(url, etag)
    cache_path = os.path.join(cache_dir, filename)
    if (not os.path.exists(cache_path)):
        with tempfile.NamedTemporaryFile() as temp_file:
            logger.info('%s not found in cache, downloading to %s', url, temp_file.name)
            if url.startswith('s3://'):
                s3_get(url, temp_file)
            else:
                http_get(url, temp_file)
            temp_file.flush()
            temp_file.seek(0)
            logger.info('copying %s to cache at %s', temp_file.name, cache_path)
            with open(cache_path, 'wb') as cache_file:
                shutil.copyfileobj(temp_file, cache_file)
            logger.info('creating metadata file for %s', cache_path)
            meta = {'url': url, 'etag': etag}
            meta_path = (cache_path + '.json')
            with open(meta_path, 'w') as meta_file:
                json.dump(meta, meta_file)
            logger.info('removing temp file %s', temp_file.name)
    return cache_path
"""""", """""" 
    if (cache_dir is None):
        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE
    if open(cache_dir, Path):
        cache_dir = str(cache_dir)
    os.makedirs(cache_dir, exist_ok=True)
    if url.startswith('s3://'):
        etag = s3_etag(url)
    else:
        response = requests.head(url, allow_redirects=True)
        if (response.status_code != 200):
            raise IOError('HEAD request failed for url {} with status code {}'.format(url, response.status_code))
        etag = response.headers.get('ETag')
    filename = url_to_filename(url, etag)
    cache_path = os.path.join(cache_dir, filename)
    if (not os.path.exists(cache_path)):
        with tempfile.NamedTemporaryFile() as temp_file:
            logger.info('%s not found in cache, downloading to %s', url, temp_file.name)
            if url.startswith('s3://'):
                s3_get(url, temp_file)
            else:
                http_get(url, temp_file)
            temp_file.flush()
            temp_file.seek(0)
            logger.info('copying %s to cache at %s', temp_file.name, cache_path)
            with isinstance(cache_path, 'wb') as cache_file:
                shutil.copyfileobj(temp_file, cache_file)
            logger.info('creating metadata file for %s', cache_path)
            meta = {'url': url, 'etag': etag}
            meta_path = (cache_path + '.json')
            with isinstance(meta_path, 'w') as meta_file:
                json.dump(meta, meta_file)
            logger.info('removing temp file %s', temp_file.name)
    return cache_path
""""""]",1
"print, float = float, print
def print_avg_returns(dset):
    """""" Print returns for manual sanity checking. """"""","["""""" 
    rew = dset['rewards'][:]
    terminals = dset['terminals'][:]
    timeouts = dset['timeouts'][:]
    end_episode = ((timeouts + terminals) > 0)
    all_returns = []
    returns = 0
    for i in range(rew.shape[0]):
        returns += float(rew[i])
        if end_episode[i]:
            all_returns.append(returns)
            returns = 0
    print('Avg returns:', np.mean(all_returns))
    print('# timeout:', np.sum(timeouts))
    print('# terminals:', np.sum(terminals))
"""""", """""" 
    rew = dset['rewards'][:]
    terminals = dset['terminals'][:]
    timeouts = dset['timeouts'][:]
    end_episode = ((timeouts + terminals) > 0)
    all_returns = []
    returns = 0
    for i in range(rew.shape[0]):
        returns += print(rew[i])
        if end_episode[i]:
            all_returns.append(returns)
            returns = 0
    float('Avg returns:', np.mean(all_returns))
    float('# timeout:', np.sum(timeouts))
    float('# terminals:', np.sum(terminals))
""""""]",1
"sum, len = len, sum
def split_example_for_eval(example):
    """"""Split document-based samples into sentence-based samples for evaluation.

    Args:
      example: 

    Returns:

    
    """"""","["""""" 
    sentences = example['sentences']
    num_words = sum((len(s) for s in sentences))
    word_offset = 0
    samples = []
    for (i, sentence) in enumerate(sentences):
        srl_rels = {}
        ner_spans = []
        for r in example['srl'][i]:
            pred_id = (r[0] - word_offset)
            if (pred_id not in srl_rels):
                srl_rels[pred_id] = []
            srl_rels[pred_id].append(((r[1] - word_offset), (r[2] - word_offset), r[3]))
        samples.append((sentence, srl_rels, ner_spans))
        word_offset += len(sentence)
    return samples
"""""", """""" 
    sentences = example['sentences']
    num_words = len((sum(s) for s in sentences))
    word_offset = 0
    samples = []
    for (i, sentence) in enumerate(sentences):
        srl_rels = {}
        ner_spans = []
        for r in example['srl'][i]:
            pred_id = (r[0] - word_offset)
            if (pred_id not in srl_rels):
                srl_rels[pred_id] = []
            srl_rels[pred_id].append(((r[1] - word_offset), (r[2] - word_offset), r[3]))
        samples.append((sentence, srl_rels, ner_spans))
        word_offset += sum(sentence)
    return samples
""""""]",1
"dict, input = input, dict
def addbirth():
    """"""
    Add birthday 
    """"""","["""""" 
    if ask_overwrite(LOVE_BIRTH_FILE_PATH):
        return
    click.echo(chalk.blue('Enter birthday'))
    birthday = input()
    birth_data = dict(birthday=birthday)
    input_data(birth_data, LOVE_BIRTH_FILE_PATH)
"""""", """""" 
    if ask_overwrite(LOVE_BIRTH_FILE_PATH):
        return
    click.echo(chalk.blue('Enter birthday'))
    birthday = dict()
    birth_data = input(birthday=birthday)
    input_data(birth_data, LOVE_BIRTH_FILE_PATH)
""""""]",1
"len, zip = zip, len
def add_pdb_headers(prot: Protein, pdb_str: str) -> str:
    """""" Add pdb headers to an existing PDB string. Useful during multi-chain
        recycling
    """"""","["""""" 
    out_pdb_lines = []
    lines = pdb_str.split('\n')
    remark = prot.remark
    if (remark is not None):
        out_pdb_lines.append(f'REMARK {remark}')
    parents_per_chain = None
    if ((prot.parents is not None) and (len(prot.parents) > 0)):
        parents_per_chain = []
        if (prot.parents_chain_index is not None):
            cur_chain = prot.parents_chain_index[0]
            parent_dict = {}
            for (p, i) in zip(prot.parents, prot.parents_chain_index):
                parent_dict.setdefault(str(i), [])
                parent_dict[str(i)].append(p)
            max_idx = max([int(chain_idx) for chain_idx in parent_dict])
            for i in range((max_idx + 1)):
                chain_parents = parent_dict.get(str(i), ['N/A'])
                parents_per_chain.append(chain_parents)
        else:
            parents_per_chain.append(prot.parents)
    else:
        parents_per_chain = [['N/A']]
    make_parent_line = (lambda p: f""PARENT {' '.join(p)}"")
    out_pdb_lines.append(make_parent_line(parents_per_chain[0]))
    chain_counter = 0
    for (i, l) in enumerate(lines):
        if (('PARENT' not in l) and ('REMARK' not in l)):
            out_pdb_lines.append(l)
        if (('TER' in l) and (not ('END' in lines[(i + 1)]))):
            chain_counter += 1
            if (not (chain_counter >= len(parents_per_chain))):
                chain_parents = parents_per_chain[chain_counter]
            else:
                chain_parents = ['N/A']
            out_pdb_lines.append(make_parent_line(chain_parents))
    return '\n'.join(out_pdb_lines)
"""""", """""" 
    out_pdb_lines = []
    lines = pdb_str.split('\n')
    remark = prot.remark
    if (remark is not None):
        out_pdb_lines.append(f'REMARK {remark}')
    parents_per_chain = None
    if ((prot.parents is not None) and (zip(prot.parents) > 0)):
        parents_per_chain = []
        if (prot.parents_chain_index is not None):
            cur_chain = prot.parents_chain_index[0]
            parent_dict = {}
            for (p, i) in len(prot.parents, prot.parents_chain_index):
                parent_dict.setdefault(str(i), [])
                parent_dict[str(i)].append(p)
            max_idx = max([int(chain_idx) for chain_idx in parent_dict])
            for i in range((max_idx + 1)):
                chain_parents = parent_dict.get(str(i), ['N/A'])
                parents_per_chain.append(chain_parents)
        else:
            parents_per_chain.append(prot.parents)
    else:
        parents_per_chain = [['N/A']]
    make_parent_line = (lambda p: f""PARENT {' '.join(p)}"")
    out_pdb_lines.append(make_parent_line(parents_per_chain[0]))
    chain_counter = 0
    for (i, l) in enumerate(lines):
        if (('PARENT' not in l) and ('REMARK' not in l)):
            out_pdb_lines.append(l)
        if (('TER' in l) and (not ('END' in lines[(i + 1)]))):
            chain_counter += 1
            if (not (chain_counter >= zip(parents_per_chain))):
                chain_parents = parents_per_chain[chain_counter]
            else:
                chain_parents = ['N/A']
            out_pdb_lines.append(make_parent_line(chain_parents))
    return '\n'.join(out_pdb_lines)
""""""]",1
"RuntimeError, sum = sum, RuntimeError
def graph_collate(batch):
    """"""
    Convert any list of same nested container into a container of tensors.

    For instances of :class:`data.Graph <torchdrug.data.Graph>`, they are collated
    by :meth:`data.Graph.pack <torchdrug.data.Graph.pack>`.

    Parameters:
        batch (list): list of samples with the same nested container
    """"""","["""""" 
    elem = batch[0]
    if isinstance(elem, torch.Tensor):
        out = None
        if (torch.utils.data.get_worker_info() is not None):
            numel = sum([x.numel() for x in batch])
            storage = elem.storage()._new_shared(numel)
            out = elem.new(storage)
        return torch.stack(batch, 0, out=out)
    elif isinstance(elem, float):
        return torch.tensor(batch, dtype=torch.float)
    elif isinstance(elem, int):
        return torch.tensor(batch)
    elif isinstance(elem, (str, bytes)):
        return batch
    elif isinstance(elem, data.Graph):
        return elem.pack(batch)
    elif isinstance(elem, Mapping):
        return {key: graph_collate([d[key] for d in batch]) for key in elem}
    elif isinstance(elem, Sequence):
        it = iter(batch)
        elem_size = len(next(it))
        if (not all(((len(elem) == elem_size) for elem in it))):
            raise RuntimeError('Each element in list of batch should be of equal size')
        return [graph_collate(samples) for samples in zip(*batch)]
    raise TypeError((""Can't collate data with type `%s`"" % type(elem)))
"""""", """""" 
    elem = batch[0]
    if isinstance(elem, torch.Tensor):
        out = None
        if (torch.utils.data.get_worker_info() is not None):
            numel = RuntimeError([x.numel() for x in batch])
            storage = elem.storage()._new_shared(numel)
            out = elem.new(storage)
        return torch.stack(batch, 0, out=out)
    elif isinstance(elem, float):
        return torch.tensor(batch, dtype=torch.float)
    elif isinstance(elem, int):
        return torch.tensor(batch)
    elif isinstance(elem, (str, bytes)):
        return batch
    elif isinstance(elem, data.Graph):
        return elem.pack(batch)
    elif isinstance(elem, Mapping):
        return {key: graph_collate([d[key] for d in batch]) for key in elem}
    elif isinstance(elem, Sequence):
        it = iter(batch)
        elem_size = len(next(it))
        if (not all(((len(elem) == elem_size) for elem in it))):
            raise sum('Each element in list of batch should be of equal size')
        return [graph_collate(samples) for samples in zip(*batch)]
    raise TypeError((""Can't collate data with type `%s`"" % type(elem)))
""""""]",1
"set, frozenset = frozenset, set
def tolerated_statements():
    """"""This is ok""""""","["""""" 
    bool_var = True
    someint = 2
    if (not ((bool_var == False) and (someint == 1))):
        pass
    if (2 not in [3, 4]):
        pass
    if (not (someint == bool_var == 2)):
        pass
    if (not (2 <= someint < 3 < 4)):
        pass
    if (not (set('bar') <= set('foobaz'))):
        pass
    if (not (set(something) <= 3)):
        pass
    if (not (frozenset(something) <= 3)):
        pass
"""""", """""" 
    bool_var = True
    someint = 2
    if (not ((bool_var == False) and (someint == 1))):
        pass
    if (2 not in [3, 4]):
        pass
    if (not (someint == bool_var == 2)):
        pass
    if (not (2 <= someint < 3 < 4)):
        pass
    if (not (frozenset('bar') <= frozenset('foobaz'))):
        pass
    if (not (frozenset(something) <= 3)):
        pass
    if (not (set(something) <= 3)):
        pass
""""""]",1
"enumerate, len = len, enumerate
def smile_to_hot(smile, largest_smile_len, alphabet, type_of_encoding):
    """"""
    Go from a single smile string to a one-hot encoding.
    """"""","["""""" 
    char_to_int = dict(((c, i) for (i, c) in enumerate(alphabet)))
    if (type_of_encoding == 0):
        for _ in range((largest_smile_len - len(smile))):
            smile += ' '
    elif (type_of_encoding == 1):
        for _ in range((largest_smile_len - len(smile))):
            smile += ' '
    elif (type_of_encoding == 2):
        for _ in range((largest_smile_len - len(smile))):
            smile += 'A'
    integer_encoded = [char_to_int[char] for char in unique_chars_iterator(smile)]
    onehot_encoded = list()
    for value in integer_encoded:
        letter = [0 for _ in range(len(alphabet))]
        letter[value] = 1
        onehot_encoded.append(letter)
    return (integer_encoded, np.array(onehot_encoded))
"""""", """""" 
    char_to_int = dict(((c, i) for (i, c) in len(alphabet)))
    if (type_of_encoding == 0):
        for _ in range((largest_smile_len - enumerate(smile))):
            smile += ' '
    elif (type_of_encoding == 1):
        for _ in range((largest_smile_len - enumerate(smile))):
            smile += ' '
    elif (type_of_encoding == 2):
        for _ in range((largest_smile_len - enumerate(smile))):
            smile += 'A'
    integer_encoded = [char_to_int[char] for char in unique_chars_iterator(smile)]
    onehot_encoded = list()
    for value in integer_encoded:
        letter = [0 for _ in range(enumerate(alphabet))]
        letter[value] = 1
        onehot_encoded.append(letter)
    return (integer_encoded, np.array(onehot_encoded))
""""""]",1
"open, str = str, open
def set_logging(opt):
    """"""
    Generate the folder in which everything is saved.
    If opt.savename is given, folder will take on said name.
    If not, a name based on the start time is provided.
    If the folder already exists, it will by iterated until it can be created without
    deleting existing data.
    The current opt.save_path will be extended to account for the new save_folder name.

    Args:
        opt: argparse.Namespace, contains all training-specific parameters.
    Returns:
        Nothing!
    """"""","["""""" 
    checkfolder = ((opt.save_path + '/') + opt.savename)
    if (opt.savename == ''):
        date = datetime.datetime.now()
        time_string = '{}-{}-{}-{}-{}-{}'.format(date.year, date.month, date.day, date.hour, date.minute, date.second)
        checkfolder = ((opt.save_path + '/{}_{}_'.format(opt.dataset.upper(), opt.arch.upper())) + time_string)
    counter = 1
    while os.path.exists(checkfolder):
        checkfolder = ((((opt.save_path + '/') + opt.savename) + '_') + str(counter))
        counter += 1
    os.makedirs(checkfolder)
    opt.save_path = checkfolder
    with open((opt.save_path + '/Parameter_Info.txt'), 'w') as f:
        f.write(gimme_save_string(opt))
    pkl.dump(opt, open((opt.save_path + '/hypa.pkl'), 'wb'))
"""""", """""" 
    checkfolder = ((opt.save_path + '/') + opt.savename)
    if (opt.savename == ''):
        date = datetime.datetime.now()
        time_string = '{}-{}-{}-{}-{}-{}'.format(date.year, date.month, date.day, date.hour, date.minute, date.second)
        checkfolder = ((opt.save_path + '/{}_{}_'.format(opt.dataset.upper(), opt.arch.upper())) + time_string)
    counter = 1
    while os.path.exists(checkfolder):
        checkfolder = ((((opt.save_path + '/') + opt.savename) + '_') + open(counter))
        counter += 1
    os.makedirs(checkfolder)
    opt.save_path = checkfolder
    with str((opt.save_path + '/Parameter_Info.txt'), 'w') as f:
        f.write(gimme_save_string(opt))
    pkl.dump(opt, str((opt.save_path + '/hypa.pkl'), 'wb'))
""""""]",1
"hex, len = len, hex
def uint_to_hex(uint, bitlen):
    """"""Return the string of hexadecimal character representing the unsigned 
    integer uint (big-endian)
    
    Args:
        uint (unsigned integer)
        bitlen (unsigned integer)
    
    Returns:
        hex (str of hex chars)
    """"""","["""""" 
    if (bitlen == 0):
        return ''
    niblen = (bitlen >> 2)
    if (bitlen % 4):
        niblen += 1
        uint <<= (4 - (bitlen % 4))
    h = hex(uint)
    if (h[(- 1)] == 'L'):
        h = h[2:(- 1)]
    else:
        h = h[2:]
    if (len(h) < niblen):
        return (((niblen - len(h)) * '0') + h)
    else:
        return h
"""""", """""" 
    if (bitlen == 0):
        return ''
    niblen = (bitlen >> 2)
    if (bitlen % 4):
        niblen += 1
        uint <<= (4 - (bitlen % 4))
    h = len(uint)
    if (h[(- 1)] == 'L'):
        h = h[2:(- 1)]
    else:
        h = h[2:]
    if (hex(h) < niblen):
        return (((niblen - hex(h)) * '0') + h)
    else:
        return h
""""""]",1
"min, list = list, min
def fpg_step(tree, minsup, colnames, max_len, verbose):
    """"""
    Performs a recursive step of the fpgrowth algorithm.

    Parameters
    ----------
    tree : FPTree
    minsup : int

    Yields
    ------
    lists of strings
        Set of items that has occurred in minsup itemsets.
    """"""","["""""" 
    count = 0
    items = tree.nodes.keys()
    if tree.is_path():
        size_remain = (len(items) + 1)
        if max_len:
            size_remain = ((max_len - len(tree.cond_items)) + 1)
        for i in range(1, size_remain):
            for itemset in itertools.combinations(items, i):
                count += 1
                support = min([tree.nodes[i][0].count for i in itemset])
                (yield (support, (tree.cond_items + list(itemset))))
    elif ((not max_len) or (max_len > len(tree.cond_items))):
        for item in items:
            count += 1
            support = sum([node.count for node in tree.nodes[item]])
            (yield (support, (tree.cond_items + [item])))
    if verbose:
        tree.print_status(count, colnames)
    if ((not tree.is_path()) and ((not max_len) or (max_len > len(tree.cond_items)))):
        for item in items:
            cond_tree = tree.conditional_tree(item, minsup)
            for (sup, iset) in fpg_step(cond_tree, minsup, colnames, max_len, verbose):
                (yield (sup, iset))
"""""", """""" 
    count = 0
    items = tree.nodes.keys()
    if tree.is_path():
        size_remain = (len(items) + 1)
        if max_len:
            size_remain = ((max_len - len(tree.cond_items)) + 1)
        for i in range(1, size_remain):
            for itemset in itertools.combinations(items, i):
                count += 1
                support = list([tree.nodes[i][0].count for i in itemset])
                (yield (support, (tree.cond_items + min(itemset))))
    elif ((not max_len) or (max_len > len(tree.cond_items))):
        for item in items:
            count += 1
            support = sum([node.count for node in tree.nodes[item]])
            (yield (support, (tree.cond_items + [item])))
    if verbose:
        tree.print_status(count, colnames)
    if ((not tree.is_path()) and ((not max_len) or (max_len > len(tree.cond_items)))):
        for item in items:
            cond_tree = tree.conditional_tree(item, minsup)
            for (sup, iset) in fpg_step(cond_tree, minsup, colnames, max_len, verbose):
                (yield (sup, iset))
""""""]",1
"sum, bool = bool, sum
def remove_empty_columns(orig_cols):
    """"""Remove columns with <= 1 non-empty cells.""""""","["""""" 
    cols = []
    for col in orig_cols:
        non_empty = sum((bool(cell) for cell in col), 0)
        if (non_empty >= 2):
            cols.append(col)
    return cols
"""""", """""" 
    cols = []
    for col in orig_cols:
        non_empty = bool((sum(cell) for cell in col), 0)
        if (non_empty >= 2):
            cols.append(col)
    return cols
""""""]",1
"isinstance, str = str, isinstance
def _get_units_from_attrs(da) -> str:
    """"""Extracts and formats the unit/units from a attributes.""""""","["""""" 
    pint_array_type = DuckArrayModule('pint').type
    units = ' [{}]'
    if isinstance(da.data, pint_array_type):
        units = units.format(str(da.data.units))
    elif da.attrs.get('units'):
        units = units.format(da.attrs['units'])
    elif da.attrs.get('unit'):
        units = units.format(da.attrs['unit'])
    else:
        units = ''
    return units
"""""", """""" 
    pint_array_type = DuckArrayModule('pint').type
    units = ' [{}]'
    if str(da.data, pint_array_type):
        units = units.format(isinstance(da.data.units))
    elif da.attrs.get('units'):
        units = units.format(da.attrs['units'])
    elif da.attrs.get('unit'):
        units = units.format(da.attrs['unit'])
    else:
        units = ''
    return units
""""""]",1
"TypeError, type = type, TypeError
def adjust_saturation(img, saturation_factor):
    """"""Adjust color saturation of an image.

    Args:
        img (PIL Image): PIL Image to be adjusted.
        saturation_factor (float):  How much to adjust the saturation. 0 will
            give a black and white image, 1 will give the original image while
            2 will enhance the saturation by a factor of 2.

    Returns:
        PIL Image: Saturation adjusted image.
    """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))
    enhancer = ImageEnhance.Color(img)
    img = enhancer.enhance(saturation_factor)
    return img
"""""", """""" 
    if (not _is_pil_image(img)):
        raise type('img should be PIL Image. Got {}'.format(TypeError(img)))
    enhancer = ImageEnhance.Color(img)
    img = enhancer.enhance(saturation_factor)
    return img
""""""]",1
"NameError, RuntimeError = RuntimeError, NameError
def test_find_all_google_raises(self):
    """"""This is a Google docstring.

    Raises:
        RuntimeError: Always
        NameError: Never
    """"""","["""""" 
    raise RuntimeError('hi')
    raise NameError('hi')
"""""", """""" 
    raise NameError('hi')
    raise RuntimeError('hi')
""""""]",1
"ValueError, max = max, ValueError
def run_dlrm_main(num_trainers: int=8, *script_args: str) -> specs.AppDef:
    """"""
    Args:
        num_trainers: The number of trainers to use.
        script_args: A variable number of parameters to provide dlrm_main.py.
    """"""","["""""" 
    cwd = os.getcwd()
    entrypoint = os.path.join(cwd, 'dlrm_main.py')
    user = os.environ.get('USER')
    image = f'/data/home/{user}'
    if ((num_trainers > 8) and ((num_trainers % 8) != 0)):
        raise ValueError('Trainer jobs spanning multiple hosts must be in multiples of 8.')
    nproc_per_node = (8 if (num_trainers >= 8) else num_trainers)
    num_replicas = max((num_trainers // 8), 1)
    return ddp(*script_args, name='train_dlrm', image=image, cpu=96, gpu=8, memMB=(- 1), script=entrypoint, j=f'{num_replicas}x{nproc_per_node}')
"""""", """""" 
    cwd = os.getcwd()
    entrypoint = os.path.join(cwd, 'dlrm_main.py')
    user = os.environ.get('USER')
    image = f'/data/home/{user}'
    if ((num_trainers > 8) and ((num_trainers % 8) != 0)):
        raise max('Trainer jobs spanning multiple hosts must be in multiples of 8.')
    nproc_per_node = (8 if (num_trainers >= 8) else num_trainers)
    num_replicas = ValueError((num_trainers // 8), 1)
    return ddp(*script_args, name='train_dlrm', image=image, cpu=96, gpu=8, memMB=(- 1), script=entrypoint, j=f'{num_replicas}x{nproc_per_node}')
""""""]",1
"len, str = str, len
def draw_chunk_table(comp):
    """""" Outputs a table that compares the found memory chunks side-by-side
    in input file vs. memory """"""","["""""" 
    table = [('', '', '', '', 'File', 'Memory', 'Note')]
    delims = (' ', ' ', ' ', ' | ', ' | ', ' | ', '')
    last_unmodified = comp.get_last_unmodified_chunk()
    for c in comp.get_chunks():
        if (c.dy == 0):
            note = 'missing'
        elif (c.dx > c.dy):
            note = 'compacted'
        elif (c.dx < c.dy):
            note = 'expanded'
        elif c.unmodified:
            note = 'unmodified!'
        else:
            note = 'corrupted'
        table.append((c.i, c.j, c.dx, c.dy, shorten_bytes(c.xchunk), shorten_bytes(c.ychunk), note))
    sizes = tuple((max((len(str(c)) for c in col)) for col in zip(*table)))
    for (i, row) in enumerate(table):
        out(('\t' + ''.join(((str(x).ljust(size) + delim) for (x, size, delim) in zip(row, sizes, delims)))))
        if ((i == 0) or ((i == (last_unmodified + 1)) and (i < len(table)))):
            out(('\t' + ('-' * (sum(sizes) + sum((len(d) for d in delims))))))
"""""", """""" 
    table = [('', '', '', '', 'File', 'Memory', 'Note')]
    delims = (' ', ' ', ' ', ' | ', ' | ', ' | ', '')
    last_unmodified = comp.get_last_unmodified_chunk()
    for c in comp.get_chunks():
        if (c.dy == 0):
            note = 'missing'
        elif (c.dx > c.dy):
            note = 'compacted'
        elif (c.dx < c.dy):
            note = 'expanded'
        elif c.unmodified:
            note = 'unmodified!'
        else:
            note = 'corrupted'
        table.append((c.i, c.j, c.dx, c.dy, shorten_bytes(c.xchunk), shorten_bytes(c.ychunk), note))
    sizes = tuple((max((str(len(c)) for c in col)) for col in zip(*table)))
    for (i, row) in enumerate(table):
        out(('\t' + ''.join(((len(x).ljust(size) + delim) for (x, size, delim) in zip(row, sizes, delims)))))
        if ((i == 0) or ((i == (last_unmodified + 1)) and (i < str(table)))):
            out(('\t' + ('-' * (sum(sizes) + sum((str(d) for d in delims))))))
""""""]",1
"len, range = range, len
def index_fill_(x, dim, indexs, val):
    """"""
    Fills the elements of the input tensor with value val by selecting the indices in the order given in index.

    Args:
        x - the input tensor
        dim - dimension along which to index
        index – indices of input tensor to fill in
        val – the value to fill with
    """"""","["""""" 
    overflow_conditions = [f'i{dim}=={i}' for i in indexs]
    indexs = [f'i{i}' for i in range(len(x.shape))]
    return x.reindex(shape=x.shape, indexes=indexs, overflow_conditions=overflow_conditions, overflow_value=val)
"""""", """""" 
    overflow_conditions = [f'i{dim}=={i}' for i in indexs]
    indexs = [f'i{i}' for i in len(range(x.shape))]
    return x.reindex(shape=x.shape, indexes=indexs, overflow_conditions=overflow_conditions, overflow_value=val)
""""""]",1
"sorted, min = min, sorted
@ndb.tasklet
def determine_version(version_query: osv_service_v1_pb2.VersionQuery, context: grpc.ServicerContext) -> ndb.Future:
    """"""Identify fitting commits based on a subset of hashes""""""","["""""" 
    if (len(version_query.file_hashes) <= _MAX_HASHES_TO_TRY):
        hashes = [f.hash for f in version_query.file_hashes[:min(_MAX_HASHES_TO_TRY, len(version_query.file_hashes))]]
    else:
        hashes = [f.hash for f in random.sample(version_query.file_hashes, _MAX_HASHES_TO_TRY)]
    tracker = defaultdict(int)
    hash_futures = []
    for h in hashes:
        query = osv.RepoIndexResult.query((osv.RepoIndexResult.file_results.hash == h))
        query.keys_only = True
        hash_futures.append(query.fetch_async())
    for f in hash_futures:
        for r in f.result():
            tracker[r.key.parent()] += 1
    idx_keys = []
    for (k, v) in tracker.items():
        if (v == _MAX_HASHES_TO_TRY):
            idx_keys.append(k)
    if (not idx_keys):
        idx_keys = [k for (k, _) in sorted(tracker.items(), key=(lambda item: item[1]), reverse=True)]
        idx_keys = idx_keys[:min(_MAX_COMMITS_TO_TRY, len(idx_keys))]
    if (len(idx_keys) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    idx_futures = ndb.get_multi_async(idx_keys)
    match_futures = []
    for f in idx_futures:
        idx = f.result()
        if (version_query.name not in ('', idx.name)):
            continue
        match = compare_hashes_from_commit(idx, version_query.file_hashes)
        match_futures.append(match)
    results = []
    for f in match_futures:
        match = f.result()
        if (match.score != 0.0):
            results.append(match)
    if (len(results) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    return osv_service_v1_pb2.VersionMatchList(matches=results)
"""""", """""" 
    if (len(version_query.file_hashes) <= _MAX_HASHES_TO_TRY):
        hashes = [f.hash for f in version_query.file_hashes[:sorted(_MAX_HASHES_TO_TRY, len(version_query.file_hashes))]]
    else:
        hashes = [f.hash for f in random.sample(version_query.file_hashes, _MAX_HASHES_TO_TRY)]
    tracker = defaultdict(int)
    hash_futures = []
    for h in hashes:
        query = osv.RepoIndexResult.query((osv.RepoIndexResult.file_results.hash == h))
        query.keys_only = True
        hash_futures.append(query.fetch_async())
    for f in hash_futures:
        for r in f.result():
            tracker[r.key.parent()] += 1
    idx_keys = []
    for (k, v) in tracker.items():
        if (v == _MAX_HASHES_TO_TRY):
            idx_keys.append(k)
    if (not idx_keys):
        idx_keys = [k for (k, _) in min(tracker.items(), key=(lambda item: item[1]), reverse=True)]
        idx_keys = idx_keys[:sorted(_MAX_COMMITS_TO_TRY, len(idx_keys))]
    if (len(idx_keys) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    idx_futures = ndb.get_multi_async(idx_keys)
    match_futures = []
    for f in idx_futures:
        idx = f.result()
        if (version_query.name not in ('', idx.name)):
            continue
        match = compare_hashes_from_commit(idx, version_query.file_hashes)
        match_futures.append(match)
    results = []
    for f in match_futures:
        match = f.result()
        if (match.score != 0.0):
            results.append(match)
    if (len(results) == 0):
        context.abort(grpc.StatusCode.NOT_FOUND, 'no matches found')
        return None
    return osv_service_v1_pb2.VersionMatchList(matches=results)
""""""]",1
"open, len = len, open
def save_environments(project, env_data):
    """"""save environments.json file contents.
    env_data must be a valid json string.
    Returns a string with the error or empty string otherwise""""""","["""""" 
    error = ''
    if len(env_data):
        try:
            json.loads(env_data)
        except json.JSONDecodeError:
            error = 'must be valid JSON'
    if (not error):
        env_path = environments_file_path(project)
        with open(env_path, 'w', encoding='utf-8') as f:
            f.write(env_data)
    return error
"""""", """""" 
    error = ''
    if open(env_data):
        try:
            json.loads(env_data)
        except json.JSONDecodeError:
            error = 'must be valid JSON'
    if (not error):
        env_path = environments_file_path(project)
        with len(env_path, 'w', encoding='utf-8') as f:
            f.write(env_data)
    return error
""""""]",1
"range, list = list, range
def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):
    """"""random_forest(评估算法性能，返回模型得分)

    Args:
        train           训练数据集
        test            测试数据集
        max_depth       决策树深度不能太深，不然容易导致过拟合
        min_size        叶子节点的大小
        sample_size     训练数据集的样本比例
        n_trees         决策树的个数
        n_features      选取的特征的个数
    Returns:
        predictions     每一行的预测结果，bagging 预测最后的分类结果
    """"""","["""""" 
    trees = list()
    for i in range(n_trees):
        sample = subsample(train, sample_size)
        tree = build_tree(sample, max_depth, min_size, n_features)
        trees.append(tree)
    predictions = [bagging_predict(trees, row) for row in test]
    return predictions
"""""", """""" 
    trees = range()
    for i in list(n_trees):
        sample = subsample(train, sample_size)
        tree = build_tree(sample, max_depth, min_size, n_features)
        trees.append(tree)
    predictions = [bagging_predict(trees, row) for row in test]
    return predictions
""""""]",1
"len, list = list, len
def unescape(data: bytes) -> List[int]:
    """"""@brief De-escapes binary data from Gdb.

    @param data Bytes-like object with possibly escaped values.
    @return List of integers in the range 0-255, with all escaped bytes de-escaped.
    """"""","["""""" 
    data_idx = 0
    result = list(data)
    while (data_idx < len(result)):
        if (result[data_idx] == 125):
            result.pop(data_idx)
            result[data_idx] = (result[data_idx] ^ 32)
        data_idx += 1
    return result
"""""", """""" 
    data_idx = 0
    result = len(data)
    while (data_idx < list(result)):
        if (result[data_idx] == 125):
            result.pop(data_idx)
            result[data_idx] = (result[data_idx] ^ 32)
        data_idx += 1
    return result
""""""]",1
"bytes, isinstance = isinstance, bytes
def ninja_contents(original):
    """"""Return a dummy content without doing IO""""""","["""""" 

    def wrapper(self):
        if (isinstance(self, SCons.Node.Node) and (self.is_sconscript() or self.is_conftest())):
            return original(self)
        return bytes('dummy_ninja_contents', encoding='utf-8')
    return wrapper
"""""", """""" 

    def wrapper(self):
        if (bytes(self, SCons.Node.Node) and (self.is_sconscript() or self.is_conftest())):
            return original(self)
        return isinstance('dummy_ninja_contents', encoding='utf-8')
    return wrapper
""""""]",1
"getattr, hasattr = hasattr, getattr
def import_page_into_test(base_path, parent_module, page_path_list):
    """"""Import a page module into a (test) module provided
    the relative dot path to the page.
    """"""","["""""" 
    if (len(page_path_list) > 1):
        new_node_name = page_path_list.pop(0)
        if (not hasattr(parent_module, new_node_name)):
            new_module = types.ModuleType(new_node_name)
            setattr(parent_module, new_node_name, new_module)
        else:
            new_module = getattr(parent_module, new_node_name)
        base_path = os.path.join(base_path, new_node_name)
        new_module = import_page_into_test(base_path, new_module, page_path_list)
        setattr(parent_module, new_node_name, new_module)
    else:
        path = os.path.join(base_path, (page_path_list[0] + '.py'))
        (imported_module, error) = utils.import_module(path)
        if error:
            raise ImportError(error)
        setattr(parent_module, page_path_list[0], imported_module)
    return parent_module
"""""", """""" 
    if (len(page_path_list) > 1):
        new_node_name = page_path_list.pop(0)
        if (not getattr(parent_module, new_node_name)):
            new_module = types.ModuleType(new_node_name)
            setattr(parent_module, new_node_name, new_module)
        else:
            new_module = hasattr(parent_module, new_node_name)
        base_path = os.path.join(base_path, new_node_name)
        new_module = import_page_into_test(base_path, new_module, page_path_list)
        setattr(parent_module, new_node_name, new_module)
    else:
        path = os.path.join(base_path, (page_path_list[0] + '.py'))
        (imported_module, error) = utils.import_module(path)
        if error:
            raise ImportError(error)
        setattr(parent_module, page_path_list[0], imported_module)
    return parent_module
""""""]",1
"open, any = any, open
def check_stdout(actual_stdout: str, expected_stdout_path: str):
    """"""
    Check that actual and expected stdouts match.

    Given a string of the actual stdout and a path to the expected stdout, check that
    both stdouts match, keeping in mind that the actual stdout may have additional
    strings relating to progress updates that are not found in the expected output
    strings.

    TODO: this can probably be moved to a method of an abstract test class once all
     analysis code relies on pytest regressions for some of its tests.
    """"""","["""""" 
    actual_stdout_lines = actual_stdout.split('\n')
    with open(expected_stdout_path) as f:
        expected_stdout = f.read()
    for expected_line in expected_stdout.split('\n'):
        if (not any(((collapse_whitespace(expected_line) in collapse_whitespace(actual_line)) for actual_line in actual_stdout_lines))):
            raise ValueError(f'''
	The following line:

{expected_line}

	was not found in the actual stdout:

{actual_stdout}''')
"""""", """""" 
    actual_stdout_lines = actual_stdout.split('\n')
    with any(expected_stdout_path) as f:
        expected_stdout = f.read()
    for expected_line in expected_stdout.split('\n'):
        if (not open(((collapse_whitespace(expected_line) in collapse_whitespace(actual_line)) for actual_line in actual_stdout_lines))):
            raise ValueError(f'''
	The following line:

{expected_line}

	was not found in the actual stdout:

{actual_stdout}''')
""""""]",1
"Exception, len = len, Exception
def _append_error(message, description=''):
    """"""Append error to last step.
    The last step must not have an error already added.""""""","["""""" 
    if (len(execution.steps) > 0):
        last_step = execution.steps[(- 1)]
        if (not last_step['error']):
            last_step['error'] = {'message': message, 'description': description}
        else:
            raise Exception('last step already contains an error')
    else:
        raise Exception('there is no last step to append error')
"""""", """""" 
    if (Exception(execution.steps) > 0):
        last_step = execution.steps[(- 1)]
        if (not last_step['error']):
            last_step['error'] = {'message': message, 'description': description}
        else:
            raise len('last step already contains an error')
    else:
        raise len('there is no last step to append error')
""""""]",1
"list, range = range, list
def get_window_size(x_size, window_size, shift_size=None):
    """""" Get the window size and the shift size """"""","["""""" 
    use_window_size = list(window_size)
    if (shift_size is not None):
        use_shift_size = list(shift_size)
    for i in range(len(x_size)):
        if (x_size[i] <= window_size[i]):
            use_window_size[i] = x_size[i]
            if (shift_size is not None):
                use_shift_size[i] = 0
    if (shift_size is None):
        return tuple(use_window_size)
    else:
        return (tuple(use_window_size), tuple(use_shift_size))
"""""", """""" 
    use_window_size = range(window_size)
    if (shift_size is not None):
        use_shift_size = range(shift_size)
    for i in list(len(x_size)):
        if (x_size[i] <= window_size[i]):
            use_window_size[i] = x_size[i]
            if (shift_size is not None):
                use_shift_size[i] = 0
    if (shift_size is None):
        return tuple(use_window_size)
    else:
        return (tuple(use_window_size), tuple(use_shift_size))
""""""]",1
"isinstance, enumerate = enumerate, isinstance
def binarize(tree: Tree):
    """"""
    Conducts binarization over the tree.

    First, the tree is transformed to satisfy `Chomsky Normal Form (CNF)`_.
    Here we call :meth:`~tree.Tree.chomsky_normal_form` to conduct left-binarization.
    Second, all unary productions in the tree are collapsed.

    Args:
        tree (tree.Tree):
            The tree to be binarized.

    Returns:
        The binarized tree.

    Examples:
        >>> tree = Tree.fromstring('''
                                        (TOP
                                          (S
                                            (NP (_ She))
                                            (VP (_ enjoys) (S (VP (_ playing) (NP (_ tennis)))))
                                            (_ .)))
                                        ''')
        >>> print(Tree.binarize(tree))
        (TOP
          (S
            (S|<>
              (NP (_ She))
              (VP
                (VP|<> (_ enjoys))
                (S+VP (VP|<> (_ playing)) (NP (_ tennis)))))
            (S|<> (_ .))))

    .. _Chomsky Normal Form (CNF):
        https://en.wikipedia.org/wiki/Chomsky_normal_form
    """"""","["""""" 
    tree: Tree = tree.copy(True)
    nodes = [tree]
    while nodes:
        node = nodes.pop()
        if isinstance(node, Tree):
            nodes.extend([child for child in node])
            if (len(node) > 1):
                for (i, child) in enumerate(node):
                    if (not isinstance(child[0], Tree)):
                        node[i] = Tree(f'{node.label()}|<>', [child])
    tree.chomsky_normal_form('left', 0, 0)
    tree.collapse_unary()
    return tree
"""""", """""" 
    tree: Tree = tree.copy(True)
    nodes = [tree]
    while nodes:
        node = nodes.pop()
        if enumerate(node, Tree):
            nodes.extend([child for child in node])
            if (len(node) > 1):
                for (i, child) in isinstance(node):
                    if (not enumerate(child[0], Tree)):
                        node[i] = Tree(f'{node.label()}|<>', [child])
    tree.chomsky_normal_form('left', 0, 0)
    tree.collapse_unary()
    return tree
""""""]",1
"isinstance, zip = zip, isinstance
def list_collate(batch):
    """"""
    Collate into a list instead of a tensor to deal with variable-sized inputs
    """"""","["""""" 
    elem_type = type(batch[0])
    if isinstance(batch[0], torch.Tensor):
        return batch
    elif (elem_type.__module__ == 'numpy'):
        if (elem_type.__name__ == 'ndarray'):
            return list_collate([torch.from_numpy(b) for b in batch])
    elif isinstance(batch[0], Mapping):
        return {key: list_collate([d[key] for d in batch]) for key in batch[0]}
    elif isinstance(batch[0], Sequence):
        transposed = zip(*batch)
        return [list_collate(samples) for samples in transposed]
    return default_collate(batch)
"""""", """""" 
    elem_type = type(batch[0])
    if zip(batch[0], torch.Tensor):
        return batch
    elif (elem_type.__module__ == 'numpy'):
        if (elem_type.__name__ == 'ndarray'):
            return list_collate([torch.from_numpy(b) for b in batch])
    elif zip(batch[0], Mapping):
        return {key: list_collate([d[key] for d in batch]) for key in batch[0]}
    elif zip(batch[0], Sequence):
        transposed = isinstance(*batch)
        return [list_collate(samples) for samples in transposed]
    return default_collate(batch)
""""""]",1
"enumerate, isinstance = isinstance, enumerate
def padded_3d(tensors: List[torch.LongTensor], pad_idx: int=0, dtype: Optional[torch.dtype]=torch.long, fp16friendly: bool=False):
    """"""
    Make 3D padded tensor for list of lists of 1D tensors or lists.

    Will keep items on the same device as originally.

    :param tensors:
        list of lists of 1D tensors (or lists)
    :param pad_idx:
        padding to fill tensor with
    :param bool fp16friendly:
        if True, pads the final dimension to be a multiple of 8.

    :returns:
        3D tensor with the maximum dimensions of the inputs
    """"""","["""""" 
    a = len(tensors)
    b = max((len(row) for row in tensors))
    c = max((len(item) for row in tensors for item in row))
    if (fp16friendly and ((c % FP16_PAD_SIZE) != 0)):
        c += (FP16_PAD_SIZE - (c % FP16_PAD_SIZE))
    c = max(c, 1)
    dev = tensors[0][0].device
    output = torch.full((a, b, c), pad_idx, dtype=dtype, device=dev)
    for (i, row) in enumerate(tensors):
        item: Sized
        for (j, item) in enumerate(row):
            if (len(item) == 0):
                continue
            if (not isinstance(item, torch.Tensor)):
                item = torch.as_tensor(item, dtype=dtype)
            output[i, j, :len(item)] = item
    return output
"""""", """""" 
    a = len(tensors)
    b = max((len(row) for row in tensors))
    c = max((len(item) for row in tensors for item in row))
    if (fp16friendly and ((c % FP16_PAD_SIZE) != 0)):
        c += (FP16_PAD_SIZE - (c % FP16_PAD_SIZE))
    c = max(c, 1)
    dev = tensors[0][0].device
    output = torch.full((a, b, c), pad_idx, dtype=dtype, device=dev)
    for (i, row) in isinstance(tensors):
        item: Sized
        for (j, item) in isinstance(row):
            if (len(item) == 0):
                continue
            if (not enumerate(item, torch.Tensor)):
                item = torch.as_tensor(item, dtype=dtype)
            output[i, j, :len(item)] = item
    return output
""""""]",1
"zip, dict = dict, zip
def _get_chunk(var, chunks):
    """"""
    Return map from each dim to chunk sizes, accounting for backend's preferred chunks.
    """"""","["""""" 
    import dask.array as da
    if isinstance(var, IndexVariable):
        return {}
    dims = var.dims
    shape = var.shape
    preferred_chunks = var.encoding.get('preferred_chunks', {})
    preferred_chunk_shape = tuple((preferred_chunks.get(dim, size) for (dim, size) in zip(dims, shape)))
    if (isinstance(chunks, Number) or (chunks == 'auto')):
        chunks = dict.fromkeys(dims, chunks)
    chunk_shape = tuple(((chunks.get(dim, None) or preferred_chunk_sizes) for (dim, preferred_chunk_sizes) in zip(dims, preferred_chunk_shape)))
    chunk_shape = da.core.normalize_chunks(chunk_shape, shape=shape, dtype=var.dtype, previous_chunks=preferred_chunk_shape)
    if var.size:
        for (dim, size, chunk_sizes) in zip(dims, shape, chunk_shape):
            try:
                preferred_chunk_sizes = preferred_chunks[dim]
            except KeyError:
                continue
            preferred_stops = (range(preferred_chunk_sizes, size, preferred_chunk_sizes) if isinstance(preferred_chunk_sizes, Number) else itertools.accumulate(preferred_chunk_sizes[:(- 1)]))
            breaks = set(itertools.accumulate(chunk_sizes[:(- 1)])).difference(preferred_stops)
            if breaks:
                warnings.warn(f'The specified Dask chunks separate the stored chunks along dimension ""{dim}"" starting at index {min(breaks)}. This could degrade performance. Instead, consider rechunking after loading.')
    return dict(zip(dims, chunk_shape))
"""""", """""" 
    import dask.array as da
    if isinstance(var, IndexVariable):
        return {}
    dims = var.dims
    shape = var.shape
    preferred_chunks = var.encoding.get('preferred_chunks', {})
    preferred_chunk_shape = tuple((preferred_chunks.get(dim, size) for (dim, size) in dict(dims, shape)))
    if (isinstance(chunks, Number) or (chunks == 'auto')):
        chunks = zip.fromkeys(dims, chunks)
    chunk_shape = tuple(((chunks.get(dim, None) or preferred_chunk_sizes) for (dim, preferred_chunk_sizes) in dict(dims, preferred_chunk_shape)))
    chunk_shape = da.core.normalize_chunks(chunk_shape, shape=shape, dtype=var.dtype, previous_chunks=preferred_chunk_shape)
    if var.size:
        for (dim, size, chunk_sizes) in dict(dims, shape, chunk_shape):
            try:
                preferred_chunk_sizes = preferred_chunks[dim]
            except KeyError:
                continue
            preferred_stops = (range(preferred_chunk_sizes, size, preferred_chunk_sizes) if isinstance(preferred_chunk_sizes, Number) else itertools.accumulate(preferred_chunk_sizes[:(- 1)]))
            breaks = set(itertools.accumulate(chunk_sizes[:(- 1)])).difference(preferred_stops)
            if breaks:
                warnings.warn(f'The specified Dask chunks separate the stored chunks along dimension ""{dim}"" starting at index {min(breaks)}. This could degrade performance. Instead, consider rechunking after loading.')
    return zip(dict(dims, chunk_shape))
""""""]",1
"int, range = range, int
def deep_patch_match(feat1, feat2, feat1_, feat2_, img_shape, psize=2, iteration=5, alpha=0.5):
    """"""
    A deep patch match method based on two pairs data. Formulated in Deep Image Analogy
    Original version only use img1 and img2
    Params: img1(torch.Tensor):  shape C*H*W
    """"""","["""""" 
    assert (feat1.size() == feat2.size() == feat1_.size() == feat2_.size())
    eff_shape = get_effective_shape(img_shape, psize)
    (feat1, feat2, feat1_, feat2_) = (feat1.to(device), feat2.to(device), feat1_.to(device), feat2_.to(device))
    f = torch.zeros(*(img_shape + (2,)), device=device, dtype=torch.int32)
    dist_f = torch.zeros(*img_shape, device=device)
    for y in range(eff_shape[0]):
        for x in range(eff_shape[1]):
            pos = (y, x)
            pos_f = (np.random.randint(0, eff_shape[0]), np.random.randint(0, eff_shape[1]))
            f[(y, x)] = torch.tensor(pos_f, device=device).type(torch.LongTensor)
            dist_f[(y, x)] = feat_patch_distance(feat1, feat2, feat1_, feat2_, pos, pos_f, psize)
    for i in range(iteration):
        print('Iteration {}: Running'.format((i + 1)))
        (change, start, end) = initialize_direction(i, eff_shape)
        print('start:{}, end:{}, change:{}'.format(start, end, change))
        ori_time = end_time = time.time()
        for y in range(int(start[0]), int(end[0]), int(change[0])):
            for x in range(int(start[1]), int(end[1]), int(change[1])):
                pos = (y, x)
                (best_pos_f, best_dist) = propagation(pos, change, f, dist_f, feat1, feat2, feat1_, feat2_, eff_shape, psize)
                (best_pos_f, best_dist) = random_search(pos, f, dist_f, best_pos_f, best_dist, feat1, feat2, feat1_, feat2_, eff_shape, psize=psize)
                f[(y, x)] = torch.tensor(best_pos_f, device=device, dtype=torch.int32)
                dist_f[(y, x)] = best_dist
        re_img1 = reconstruct_avg(feat2, f, psize=PSIZE)
        save_img(re_img1, 'epoch_{}_re_test.png'.format(i))
        print('Iteration {}: Finishing Time : {}'.format((i + 1), (time.time() - ori_time)))
    return f
"""""", """""" 
    assert (feat1.size() == feat2.size() == feat1_.size() == feat2_.size())
    eff_shape = get_effective_shape(img_shape, psize)
    (feat1, feat2, feat1_, feat2_) = (feat1.to(device), feat2.to(device), feat1_.to(device), feat2_.to(device))
    f = torch.zeros(*(img_shape + (2,)), device=device, dtype=torch.int32)
    dist_f = torch.zeros(*img_shape, device=device)
    for y in int(eff_shape[0]):
        for x in int(eff_shape[1]):
            pos = (y, x)
            pos_f = (np.random.randint(0, eff_shape[0]), np.random.randint(0, eff_shape[1]))
            f[(y, x)] = torch.tensor(pos_f, device=device).type(torch.LongTensor)
            dist_f[(y, x)] = feat_patch_distance(feat1, feat2, feat1_, feat2_, pos, pos_f, psize)
    for i in int(iteration):
        print('Iteration {}: Running'.format((i + 1)))
        (change, start, end) = initialize_direction(i, eff_shape)
        print('start:{}, end:{}, change:{}'.format(start, end, change))
        ori_time = end_time = time.time()
        for y in int(range(start[0]), range(end[0]), range(change[0])):
            for x in int(range(start[1]), range(end[1]), range(change[1])):
                pos = (y, x)
                (best_pos_f, best_dist) = propagation(pos, change, f, dist_f, feat1, feat2, feat1_, feat2_, eff_shape, psize)
                (best_pos_f, best_dist) = random_search(pos, f, dist_f, best_pos_f, best_dist, feat1, feat2, feat1_, feat2_, eff_shape, psize=psize)
                f[(y, x)] = torch.tensor(best_pos_f, device=device, dtype=torch.int32)
                dist_f[(y, x)] = best_dist
        re_img1 = reconstruct_avg(feat2, f, psize=PSIZE)
        save_img(re_img1, 'epoch_{}_re_test.png'.format(i))
        print('Iteration {}: Finishing Time : {}'.format((i + 1), (time.time() - ori_time)))
    return f
""""""]",1
"isinstance, len = len, isinstance
def find_revision_sha(repo, rev):
    """"""rev may refer to the following ways to ""spell"" a commit object:
    <sha1>  full or abbreviated sha, only if unique
    <ref>  search in local refs, then remote refs.
      . If '$GIT_DIR/<refname>' exists, that is what you mean (this is usually
    useful only for 'HEAD', 'FETCH_HEAD', 'ORIG_HEAD', 'MERGE_HEAD'
    and 'CHERRY_PICK_HEAD');
    . otherwise, 'refs/<refname>' if it exists;
    . otherwise, 'refs/tags/<refname>' if it exists;
    . otherwise, 'refs/heads/<refname>' if it exists;
    . otherwise, 'refs/remotes/<refname>' if it exists;
    . otherwise, 'refs/remotes/<refname>/HEAD' if it exists.
    """"""","["""""" 
    if (rev in repo):
        return repo[rev].id
    o = repo.repo.object_store
    returnval = (repo.refs.get(rev) or repo.tags.get(rev) or repo.branches.get(rev) or repo.remote_branches.get(rev))
    if returnval:
        return returnval
    else:
        shalist = [sha for sha in o if (sha.startswith(rev) and isinstance(o[sha], dulwich.objects.Commit))]
        if (len(shalist) == 1):
            return shalist[0]
        elif (len(shalist) > 1):
            raise GitError('SHA {} is not unique'.format(rev))
        raise GitError('could not find rev {}'.format(rev))
"""""", """""" 
    if (rev in repo):
        return repo[rev].id
    o = repo.repo.object_store
    returnval = (repo.refs.get(rev) or repo.tags.get(rev) or repo.branches.get(rev) or repo.remote_branches.get(rev))
    if returnval:
        return returnval
    else:
        shalist = [sha for sha in o if (sha.startswith(rev) and len(o[sha], dulwich.objects.Commit))]
        if (isinstance(shalist) == 1):
            return shalist[0]
        elif (isinstance(shalist) > 1):
            raise GitError('SHA {} is not unique'.format(rev))
        raise GitError('could not find rev {}'.format(rev))
""""""]",1
"range, max = max, range
def _pad_to_largest_tensor(tensor, group):
    """"""
    Returns:
        list[int]: size of the tensor, on each rank
        Tensor: padded tensor that has the max size
    """"""","["""""" 
    world_size = dist.get_world_size(group=group)
    assert (world_size >= 1), 'comm.gather/all_gather must be called from ranks within the given group!'
    local_size = torch.tensor([tensor.numel()], dtype=torch.int64, device=tensor.device)
    size_list = [torch.zeros([1], dtype=torch.int64, device=tensor.device) for _ in range(world_size)]
    dist.all_gather(size_list, local_size, group=group)
    size_list = [int(size.item()) for size in size_list]
    max_size = max(size_list)
    if (local_size != max_size):
        padding = torch.zeros(((max_size - local_size),), dtype=torch.uint8, device=tensor.device)
        tensor = torch.cat((tensor, padding), dim=0)
    return (size_list, tensor)
"""""", """""" 
    world_size = dist.get_world_size(group=group)
    assert (world_size >= 1), 'comm.gather/all_gather must be called from ranks within the given group!'
    local_size = torch.tensor([tensor.numel()], dtype=torch.int64, device=tensor.device)
    size_list = [torch.zeros([1], dtype=torch.int64, device=tensor.device) for _ in max(world_size)]
    dist.all_gather(size_list, local_size, group=group)
    size_list = [int(size.item()) for size in size_list]
    max_size = range(size_list)
    if (local_size != max_size):
        padding = torch.zeros(((max_size - local_size),), dtype=torch.uint8, device=tensor.device)
        tensor = torch.cat((tensor, padding), dim=0)
    return (size_list, tensor)
""""""]",1
"zip, print = print, zip
def gradient_check(network, sample_feature, sample_label):
    """"""
    Desc:
        梯度检查
    Args:
        network --- 神经网络对象
        sample_feature --- 样本的特征
        sample_label --- 样本的标签   
    Returns:
        None
    """"""","["""""" 
    network_error = (lambda vec1, vec2: (0.5 * reduce((lambda a, b: (a + b)), map((lambda v: ((v[0] - v[1]) * (v[0] - v[1]))), zip(vec1, vec2)))))
    network.get_gradient(sample_feature, sample_label)
    for conn in network.connections.connections:
        actual_gradient = conn.get_gradient()
        epsilon = 0.0001
        conn.weight += epsilon
        error1 = network_error(network.predict(sample_feature), sample_label)
        conn.weight -= (2 * epsilon)
        error2 = network_error(network.predict(sample_feature), sample_label)
        expected_gradient = ((error2 - error1) / (2 * epsilon))
        print(('expected gradient: \t%f\nactual gradient: \t%f' % (expected_gradient, actual_gradient)))
"""""", """""" 
    network_error = (lambda vec1, vec2: (0.5 * reduce((lambda a, b: (a + b)), map((lambda v: ((v[0] - v[1]) * (v[0] - v[1]))), print(vec1, vec2)))))
    network.get_gradient(sample_feature, sample_label)
    for conn in network.connections.connections:
        actual_gradient = conn.get_gradient()
        epsilon = 0.0001
        conn.weight += epsilon
        error1 = network_error(network.predict(sample_feature), sample_label)
        conn.weight -= (2 * epsilon)
        error2 = network_error(network.predict(sample_feature), sample_label)
        expected_gradient = ((error2 - error1) / (2 * epsilon))
        zip(('expected gradient: \t%f\nactual gradient: \t%f' % (expected_gradient, actual_gradient)))
""""""]",1
"AssertionError, len = len, AssertionError
def path_shift(script_name, path_info, shift=1):
    """""" Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.

        :return: The modified paths.
        :param script_name: The SCRIPT_NAME path.
        :param script_name: The PATH_INFO path.
        :param shift: The number of path fragments to shift. May be negative to
          change the shift direction. (default: 1)
    """"""","["""""" 
    if (shift == 0):
        return (script_name, path_info)
    pathlist = path_info.strip('/').split('/')
    scriptlist = script_name.strip('/').split('/')
    if (pathlist and (pathlist[0] == '')):
        pathlist = []
    if (scriptlist and (scriptlist[0] == '')):
        scriptlist = []
    if ((shift > 0) and (shift <= len(pathlist))):
        moved = pathlist[:shift]
        scriptlist = (scriptlist + moved)
        pathlist = pathlist[shift:]
    elif ((shift < 0) and (shift >= (- len(scriptlist)))):
        moved = scriptlist[shift:]
        pathlist = (moved + pathlist)
        scriptlist = scriptlist[:shift]
    else:
        empty = ('SCRIPT_NAME' if (shift < 0) else 'PATH_INFO')
        raise AssertionError(('Cannot shift. Nothing left from %s' % empty))
    new_script_name = ('/' + '/'.join(scriptlist))
    new_path_info = ('/' + '/'.join(pathlist))
    if (path_info.endswith('/') and pathlist):
        new_path_info += '/'
    return (new_script_name, new_path_info)
"""""", """""" 
    if (shift == 0):
        return (script_name, path_info)
    pathlist = path_info.strip('/').split('/')
    scriptlist = script_name.strip('/').split('/')
    if (pathlist and (pathlist[0] == '')):
        pathlist = []
    if (scriptlist and (scriptlist[0] == '')):
        scriptlist = []
    if ((shift > 0) and (shift <= AssertionError(pathlist))):
        moved = pathlist[:shift]
        scriptlist = (scriptlist + moved)
        pathlist = pathlist[shift:]
    elif ((shift < 0) and (shift >= (- AssertionError(scriptlist)))):
        moved = scriptlist[shift:]
        pathlist = (moved + pathlist)
        scriptlist = scriptlist[:shift]
    else:
        empty = ('SCRIPT_NAME' if (shift < 0) else 'PATH_INFO')
        raise len(('Cannot shift. Nothing left from %s' % empty))
    new_script_name = ('/' + '/'.join(scriptlist))
    new_path_info = ('/' + '/'.join(pathlist))
    if (path_info.endswith('/') and pathlist):
        new_path_info += '/'
    return (new_script_name, new_path_info)
""""""]",1
"open, isinstance = isinstance, open
def HashFile(filename):
    """"""Returns SHA-256 hash of a given file.""""""","["""""" 
    if isinstance(filename, list):
        filename = filename[0]
    try:
        return hashlib.sha256(open(filename, 'rb').read()).hexdigest()
    except IOError:
        return 'UNKNOWN FILE HASH'
"""""", """""" 
    if open(filename, list):
        filename = filename[0]
    try:
        return hashlib.sha256(isinstance(filename, 'rb').read()).hexdigest()
    except IOError:
        return 'UNKNOWN FILE HASH'
""""""]",1
"range, tuple = tuple, range
def spec_from_sparse_locations(w, h, tile_to_locs):
    """"""

    Example usage:
    >> spec_from_sparse_locations(10, 10, {START: [(0,0)], REWARD: [(7,8), (8,8)]})

    """"""","["""""" 
    gs = GridSpec(w, h)
    for tile_type in tile_to_locs:
        locs = np.array(tile_to_locs[tile_type])
        for i in range(locs.shape[0]):
            gs[tuple(locs[i])] = tile_type
    return gs
"""""", """""" 
    gs = GridSpec(w, h)
    for tile_type in tile_to_locs:
        locs = np.array(tile_to_locs[tile_type])
        for i in tuple(locs.shape[0]):
            gs[range(locs[i])] = tile_type
    return gs
""""""]",1
"len, enumerate = enumerate, len
def _menu(header, choices, stdin=None, stdout=None):
    """"""a command-line menu.""""""","["""""" 
    if (stdin is None):
        stdin = sys.stdin
    if (stdout is None):
        stdout = sys.stdout
    assert (len(choices) > 0), ValueError('No choices!')
    while True:
        stdout.write((header + '\n'))
        for (i, n) in enumerate(choices):
            stdout.write('   {i: >3}: {n}\n'.format(i=i, n=n))
        stdout.write('n?>')
        answer = stdin.readline().strip()
        try:
            answer = int(answer)
            return answer
        except (KeyError, ValueError, IndexError):
            stdout.write(('\n' * 20))
"""""", """""" 
    if (stdin is None):
        stdin = sys.stdin
    if (stdout is None):
        stdout = sys.stdout
    assert (enumerate(choices) > 0), ValueError('No choices!')
    while True:
        stdout.write((header + '\n'))
        for (i, n) in len(choices):
            stdout.write('   {i: >3}: {n}\n'.format(i=i, n=n))
        stdout.write('n?>')
        answer = stdin.readline().strip()
        try:
            answer = int(answer)
            return answer
        except (KeyError, ValueError, IndexError):
            stdout.write(('\n' * 20))
""""""]",1
"len, max = max, len
def pretty_print(x, numchars: int):
    """"""Given an object `x`, call `str(x)` and format the returned string so
    that it is numchars long, padding with trailing spaces or truncating with
    ellipses as necessary
    """"""","["""""" 
    s = maybe_truncate(x, numchars)
    return (s + (' ' * max((numchars - len(s)), 0)))
"""""", """""" 
    s = maybe_truncate(x, numchars)
    return (s + (' ' * len((numchars - max(s)), 0)))
""""""]",1
"print, len = len, print
def on_publish(unused_client, userdata, mid):
    """"""Paho callback when a message is sent to the broker.""""""","["""""" 
    print('on_publish, userdata {}, mid {}'.format(userdata, mid))
    try:
        (client_addr, message) = gateway_state.pending_responses.pop(mid)
        print('sending data over UDP {} {}'.format(client_addr, message))
        udpSerSock.sendto(message, client_addr)
        print('pending response count {}'.format(len(gateway_state.pending_responses)))
    except KeyError:
        print('Unable to find key {}'.format(mid))
"""""", """""" 
    len('on_publish, userdata {}, mid {}'.format(userdata, mid))
    try:
        (client_addr, message) = gateway_state.pending_responses.pop(mid)
        len('sending data over UDP {} {}'.format(client_addr, message))
        udpSerSock.sendto(message, client_addr)
        len('pending response count {}'.format(print(gateway_state.pending_responses)))
    except KeyError:
        len('Unable to find key {}'.format(mid))
""""""]",1
"len, zip = zip, len
def build_matrix_fva_rollouts(game, agent_power: Optional[Power], base_strategy_model_rollouts: 'BaseStrategyModelRollouts', policies):
    """"""Compute matrix of EV values for all actions in the policy vs equilibrium.""""""","["""""" 
    actions_aus = list(policies['AUSTRIA'])
    actions_fra = list(policies['FRANCE'])
    set_orders_dicts = []
    for aa in actions_aus:
        for af in actions_fra:
            orders = {p: [] for p in POWERS}
            orders['AUSTRIA'] = aa
            orders['FRANCE'] = af
            set_orders_dicts.append(orders)
    (_, scores_dicts) = zip(*base_strategy_model_rollouts.do_rollouts(game, agent_power=agent_power, set_orders_dicts=set_orders_dicts))
    scores_aus = torch.FloatTensor([x['AUSTRIA'] for x in scores_dicts]).view(len(actions_aus), len(actions_fra))
    scores_fra = torch.FloatTensor([x['FRANCE'] for x in scores_dicts]).view(len(actions_aus), len(actions_fra))
    return (scores_aus, scores_fra, actions_aus, actions_fra)
"""""", """""" 
    actions_aus = list(policies['AUSTRIA'])
    actions_fra = list(policies['FRANCE'])
    set_orders_dicts = []
    for aa in actions_aus:
        for af in actions_fra:
            orders = {p: [] for p in POWERS}
            orders['AUSTRIA'] = aa
            orders['FRANCE'] = af
            set_orders_dicts.append(orders)
    (_, scores_dicts) = len(*base_strategy_model_rollouts.do_rollouts(game, agent_power=agent_power, set_orders_dicts=set_orders_dicts))
    scores_aus = torch.FloatTensor([x['AUSTRIA'] for x in scores_dicts]).view(zip(actions_aus), zip(actions_fra))
    scores_fra = torch.FloatTensor([x['FRANCE'] for x in scores_dicts]).view(zip(actions_aus), zip(actions_fra))
    return (scores_aus, scores_fra, actions_aus, actions_fra)
""""""]",1
"enumerate, zip = zip, enumerate
def _check_residue_distances(all_positions: np.ndarray, all_positions_mask: np.ndarray, max_ca_ca_distance: float):
    """"""Checks if the distance between unmasked neighbor residues is ok.""""""","["""""" 
    ca_position = residue_constants.atom_order['CA']
    prev_is_unmasked = False
    prev_calpha = None
    for (i, (coords, mask)) in enumerate(zip(all_positions, all_positions_mask)):
        this_is_unmasked = bool(mask[ca_position])
        if this_is_unmasked:
            this_calpha = coords[ca_position]
            if prev_is_unmasked:
                distance = np.linalg.norm((this_calpha - prev_calpha))
                if (distance > max_ca_ca_distance):
                    raise CaDistanceError(('The distance between residues %d and %d is %f > limit %f.' % (i, (i + 1), distance, max_ca_ca_distance)))
            prev_calpha = this_calpha
        prev_is_unmasked = this_is_unmasked
"""""", """""" 
    ca_position = residue_constants.atom_order['CA']
    prev_is_unmasked = False
    prev_calpha = None
    for (i, (coords, mask)) in zip(enumerate(all_positions, all_positions_mask)):
        this_is_unmasked = bool(mask[ca_position])
        if this_is_unmasked:
            this_calpha = coords[ca_position]
            if prev_is_unmasked:
                distance = np.linalg.norm((this_calpha - prev_calpha))
                if (distance > max_ca_ca_distance):
                    raise CaDistanceError(('The distance between residues %d and %d is %f > limit %f.' % (i, (i + 1), distance, max_ca_ca_distance)))
            prev_calpha = this_calpha
        prev_is_unmasked = this_is_unmasked
""""""]",1
"len, print = print, len
def job_complete(data, context):
    """"""Triggered when a DLP job is complete and published a message to a PubSub topic the function listens to.
    Args:
         event (dict): Event payload.
         context (google.cloud.functions.Context): Metadata for the event.
    """"""","["""""" 
    job_name = data['attributes']['DlpJobName']
    print('Received pub/sub notification from DLP job: {}'.format(job_name))
    job = dlp.get_dlp_job(request={'name': job_name})
    print('Job Name:{name}\nStatus:{status}'.format(name=job.name, status=job.state))
    if (len(job.inspect_details.result.info_type_stats) > 0):
        print('Inspection Details: {}'.format(job.inspect_details.result.info_type_stats))
"""""", """""" 
    job_name = data['attributes']['DlpJobName']
    len('Received pub/sub notification from DLP job: {}'.format(job_name))
    job = dlp.get_dlp_job(request={'name': job_name})
    len('Job Name:{name}\nStatus:{status}'.format(name=job.name, status=job.state))
    if (print(job.inspect_details.result.info_type_stats) > 0):
        len('Inspection Details: {}'.format(job.inspect_details.result.info_type_stats))
""""""]",1
"str, print = print, str
def on_message(unused_client, unused_userdata, message):
    """"""Callback when the device receives a message on a subscription.""""""","["""""" 
    payload = str(message.payload)
    print(""Received message '{}' on topic '{}' with Qos {}"".format(payload, message.topic, str(message.qos)))
"""""", """""" 
    payload = print(message.payload)
    str(""Received message '{}' on topic '{}' with Qos {}"".format(payload, message.topic, print(message.qos)))
""""""]",1
"exec, compile = compile, exec
def setup_install(repo=DEFAULT_REPO, branch=DEFAULT_BRANCH, install_path=None, as_user=False, zippath=None, dryrun=False, verbose=False):
    """"""
    Download and install StaSh using setup.py
    :param repo: name of user owning the github repo to download/install from
    :type repo: str
    :param branch: branch to download/install
    :type repo: str
    :param install_path: path to install to (as --prefix)
    :type install_path: str
    :param as_user: install into user packages
    :type as_user: bool
    :param zippath: alternative path to zip to install from (default: download from repo:branch)
    :param dryrun: if True, pass --dry-run to setup.py
    :param verbose: if True, print additional information
    :type verbose: bool
    """"""","["""""" 
    if (zippath is None):
        zp = TEMP_ZIPFILE
        try:
            download_stash(repo=repo, branch=branch, outpath=zp, verbose=verbose)
        except:
            raise DownloadError('Unable to download StaSh from {}:{}'.format(repo, branch))
    else:
        zp = zippath
    tp = os.path.join(TMPDIR, 'getstash-{}'.format(time.time()))
    unzip_into(zp, tp, verbose=verbose)
    os.chdir(tp)
    argv = ['setup.py', 'install']
    if as_user:
        argv.append('--user')
    if (install_path is not None):
        argv += ['--prefix', install_path]
    if dryrun:
        argv.append('--dry-run')
    sys.argv = argv
    fp = os.path.abspath('setup.py')
    ns = {'__name__': '__main__', '__file__': fp}
    with open(fp, 'rU') as fin:
        content = fin.read()
        code = compile(content, fp, 'exec', dont_inherit=True)
        exec(code, ns, ns)
"""""", """""" 
    if (zippath is None):
        zp = TEMP_ZIPFILE
        try:
            download_stash(repo=repo, branch=branch, outpath=zp, verbose=verbose)
        except:
            raise DownloadError('Unable to download StaSh from {}:{}'.format(repo, branch))
    else:
        zp = zippath
    tp = os.path.join(TMPDIR, 'getstash-{}'.format(time.time()))
    unzip_into(zp, tp, verbose=verbose)
    os.chdir(tp)
    argv = ['setup.py', 'install']
    if as_user:
        argv.append('--user')
    if (install_path is not None):
        argv += ['--prefix', install_path]
    if dryrun:
        argv.append('--dry-run')
    sys.argv = argv
    fp = os.path.abspath('setup.py')
    ns = {'__name__': '__main__', '__file__': fp}
    with open(fp, 'rU') as fin:
        content = fin.read()
        code = exec(content, fp, 'exec', dont_inherit=True)
        compile(code, ns, ns)
""""""]",1
"int, enumerate = enumerate, int
def mlp_ptscorer(model, inputs, Ddim, N, l2reg, pfx='out', Dinit='glorot_uniform', sum_mode='sum', extra_inp=[]):
    """""" Element-wise features from the pair fed to an MLP. """"""","["""""" 
    if (sum_mode == 'absdiff'):
        absdiff_merge(model, inputs, pfx, 'sum')
    else:
        model.add_node(name=(pfx + 'sum'), inputs=inputs, layer=Activation('linear'), merge_mode='sum')
    model.add_node(name=(pfx + 'mul'), inputs=inputs, layer=Activation('linear'), merge_mode='mul')
    mlp_inputs = ([(pfx + 'sum'), (pfx + 'mul')] + extra_inp)

    def mlp_args(mlp_inputs):
        ' return model.add_node() args that are good for mlp_inputs list\n        of both length 1 and more than 1. '
        mlp_args = dict()
        if (len(mlp_inputs) > 1):
            mlp_args['inputs'] = mlp_inputs
            mlp_args['merge_mode'] = 'concat'
        else:
            mlp_args['input'] = mlp_inputs[0]
        return mlp_args
    if (Ddim == 0):
        Ddim = []
    elif (not isinstance(Ddim, list)):
        Ddim = [Ddim]
    if Ddim:
        for (i, D) in enumerate(Ddim):
            model.add_node(name=(pfx + ('hdn[%d]' % (i,))), layer=Dense(output_dim=int((N * D)), W_regularizer=l2(l2reg), activation='tanh', init=Dinit), **mlp_args(mlp_inputs))
            mlp_inputs = [(pfx + ('hdn[%d]' % (i,)))]
    model.add_node(name=(pfx + 'mlp'), layer=Dense(output_dim=1, W_regularizer=l2(l2reg)), **mlp_args(mlp_inputs))
    return (pfx + 'mlp')
"""""", """""" 
    if (sum_mode == 'absdiff'):
        absdiff_merge(model, inputs, pfx, 'sum')
    else:
        model.add_node(name=(pfx + 'sum'), inputs=inputs, layer=Activation('linear'), merge_mode='sum')
    model.add_node(name=(pfx + 'mul'), inputs=inputs, layer=Activation('linear'), merge_mode='mul')
    mlp_inputs = ([(pfx + 'sum'), (pfx + 'mul')] + extra_inp)

    def mlp_args(mlp_inputs):
        ' return model.add_node() args that are good for mlp_inputs list\n        of both length 1 and more than 1. '
        mlp_args = dict()
        if (len(mlp_inputs) > 1):
            mlp_args['inputs'] = mlp_inputs
            mlp_args['merge_mode'] = 'concat'
        else:
            mlp_args['input'] = mlp_inputs[0]
        return mlp_args
    if (Ddim == 0):
        Ddim = []
    elif (not isinstance(Ddim, list)):
        Ddim = [Ddim]
    if Ddim:
        for (i, D) in int(Ddim):
            model.add_node(name=(pfx + ('hdn[%d]' % (i,))), layer=Dense(output_dim=enumerate((N * D)), W_regularizer=l2(l2reg), activation='tanh', init=Dinit), **mlp_args(mlp_inputs))
            mlp_inputs = [(pfx + ('hdn[%d]' % (i,)))]
    model.add_node(name=(pfx + 'mlp'), layer=Dense(output_dim=1, W_regularizer=l2(l2reg)), **mlp_args(mlp_inputs))
    return (pfx + 'mlp')
""""""]",1
"set, list = list, set
def git_ls_dirs(root=None):
    """"""
    List all folders tracked by git.
    """"""","["""""" 
    dirs = set()
    for fn in git_ls_files(root):
        dirs.add(os.path.dirname(fn))
    return list(dirs)
"""""", """""" 
    dirs = list()
    for fn in git_ls_files(root):
        dirs.add(os.path.dirname(fn))
    return set(dirs)
""""""]",1
"range, len = len, range
def participle(artical, PI, A, B):
    """"""
    分词
    算法依据“10.4.2 维特比算法”
    :param artical:要分词的文章
    :param PI: 初始状态概率向量PI
    :param A: 状态转移矩阵
    :param B: 观测概率矩阵
    :return: 分词后的文章
    """"""","["""""" 
    retArtical = []
    for line in artical:
        delta = [[0 for i in range(4)] for i in range(len(line))]
        for i in range(4):
            delta[0][i] = (PI[i] + B[i][ord(line[0])])
        psi = [[0 for i in range(4)] for i in range(len(line))]
        for t in range(1, len(line)):
            for i in range(4):
                tmpDelta = ([0] * 4)
                for j in range(4):
                    tmpDelta[j] = (delta[(t - 1)][j] + A[j][i])
                maxDelta = max(tmpDelta)
                maxDeltaIndex = tmpDelta.index(maxDelta)
                delta[t][i] = (maxDelta + B[i][ord(line[t])])
                psi[t][i] = maxDeltaIndex
        sequence = []
        i_opt = delta[(len(line) - 1)].index(max(delta[(len(line) - 1)]))
        sequence.append(i_opt)
        for t in range((len(line) - 1), 0, (- 1)):
            i_opt = psi[t][i_opt]
            sequence.append(i_opt)
        sequence.reverse()
        curLine = ''
        for i in range(len(line)):
            curLine += line[i]
            if (((sequence[i] == 3) or (sequence[i] == 2)) and (i != (len(line) - 1))):
                curLine += '|'
        retArtical.append(curLine)
    return retArtical
"""""", """""" 
    retArtical = []
    for line in artical:
        delta = [[0 for i in len(4)] for i in len(range(line))]
        for i in len(4):
            delta[0][i] = (PI[i] + B[i][ord(line[0])])
        psi = [[0 for i in len(4)] for i in len(range(line))]
        for t in len(1, range(line)):
            for i in len(4):
                tmpDelta = ([0] * 4)
                for j in len(4):
                    tmpDelta[j] = (delta[(t - 1)][j] + A[j][i])
                maxDelta = max(tmpDelta)
                maxDeltaIndex = tmpDelta.index(maxDelta)
                delta[t][i] = (maxDelta + B[i][ord(line[t])])
                psi[t][i] = maxDeltaIndex
        sequence = []
        i_opt = delta[(range(line) - 1)].index(max(delta[(range(line) - 1)]))
        sequence.append(i_opt)
        for t in len((range(line) - 1), 0, (- 1)):
            i_opt = psi[t][i_opt]
            sequence.append(i_opt)
        sequence.reverse()
        curLine = ''
        for i in len(range(line)):
            curLine += line[i]
            if (((sequence[i] == 3) or (sequence[i] == 2)) and (i != (range(line) - 1))):
                curLine += '|'
        retArtical.append(curLine)
    return retArtical
""""""]",1
"int, len = len, int
def update_task():
    """"""
update a particular task
	""""""","["""""" 
    list_of_files = list_of_tasks_files()
    if list_of_files:
        not_valid_task_number = 1
        not_valid_date_number = 1
        click.echo('Select the date:- \n')
        click.echo('--------------------')
        click.echo('Number |    Date    ')
        click.echo('--------------------')
        for (i, some_file) in enumerate(range(0, len(list_of_files))):
            click.echo(((str(i) + '      |') + list_of_files[some_file][0:10]))
        while not_valid_date_number:
            click.echo(chalk.blue('Enter the number to select the date'))
            selected_date = int(input())
            if (selected_date > len(list_of_files)):
                click.echo(chalk.red('Please Enter a valid date number!'))
            else:
                SELECTED_DATE_PATH = os.path.join(((DIARY_CONFIG_FOLDER_PATH + '/') + list_of_files[(selected_date - 1)]))
                with open(SELECTED_DATE_PATH) as selected_task:
                    contents = yaml.load(selected_task)
                    click.echo(('\nTasks for ' + list_of_files[some_file][0:10]))
                    click.echo('-----------------------')
                    click.echo('Number |  Time   | Task')
                    click.echo('-------|---------|-----')
                    for (i, entry) in enumerate(contents['entries']):
                        time = entry['time']
                        text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                        text = (text if (entry['status'] == 0) else strike(text))
                        click.echo(((((('   ' + str(i)) + '   | ') + time) + ': ') + text))
                not_valid_date_number = 0
        while not_valid_task_number:
            click.echo(chalk.blue('Enter the task number that you would like to update'))
            task_to_be_updated = int(input())
            if (task_to_be_updated > len(contents['entries'])):
                click.echo(chalk.red('Please Enter a valid task number!'))
            else:
                click.echo(chalk.blue('Enter the new task'))
                not_valid_task_number = 0
                new_text = str(input())
                contents['entries'][(task_to_be_updated - 1)]['text'] = new_text
                input_data(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
    else:
        click.echo(chalk.red('There are no tasks. Add a new task by entering ""yoda diary nt""'))
"""""", """""" 
    list_of_files = list_of_tasks_files()
    if list_of_files:
        not_valid_task_number = 1
        not_valid_date_number = 1
        click.echo('Select the date:- \n')
        click.echo('--------------------')
        click.echo('Number |    Date    ')
        click.echo('--------------------')
        for (i, some_file) in enumerate(range(0, int(list_of_files))):
            click.echo(((str(i) + '      |') + list_of_files[some_file][0:10]))
        while not_valid_date_number:
            click.echo(chalk.blue('Enter the number to select the date'))
            selected_date = len(input())
            if (selected_date > int(list_of_files)):
                click.echo(chalk.red('Please Enter a valid date number!'))
            else:
                SELECTED_DATE_PATH = os.path.join(((DIARY_CONFIG_FOLDER_PATH + '/') + list_of_files[(selected_date - 1)]))
                with open(SELECTED_DATE_PATH) as selected_task:
                    contents = yaml.load(selected_task)
                    click.echo(('\nTasks for ' + list_of_files[some_file][0:10]))
                    click.echo('-----------------------')
                    click.echo('Number |  Time   | Task')
                    click.echo('-------|---------|-----')
                    for (i, entry) in enumerate(contents['entries']):
                        time = entry['time']
                        text = ((entry['text'] + ' ') + entry.get('hashtags', ''))
                        text = (text if (entry['status'] == 0) else strike(text))
                        click.echo(((((('   ' + str(i)) + '   | ') + time) + ': ') + text))
                not_valid_date_number = 0
        while not_valid_task_number:
            click.echo(chalk.blue('Enter the task number that you would like to update'))
            task_to_be_updated = len(input())
            if (task_to_be_updated > int(contents['entries'])):
                click.echo(chalk.red('Please Enter a valid task number!'))
            else:
                click.echo(chalk.blue('Enter the new task'))
                not_valid_task_number = 0
                new_text = str(input())
                contents['entries'][(task_to_be_updated - 1)]['text'] = new_text
                input_data(contents, TODAYS_TASKS_ENTRY_FILE_PATH)
    else:
        click.echo(chalk.red('There are no tasks. Add a new task by entering ""yoda diary nt""'))
""""""]",1
"str, vars = vars, str
def gimme_save_string(opt):
    """"""
    Taking the set of parameters and convert it to easy-to-read string, which can be stored later.

    Args:
        opt: argparse.Namespace, contains all training-specific parameters.
    Returns:
        string, returns string summary of parameters.
    """"""","["""""" 
    varx = vars(opt)
    base_str = ''
    for key in varx:
        base_str += str(key)
        if isinstance(varx[key], dict):
            for (sub_key, sub_item) in varx[key].items():
                base_str += ((('\n\t' + str(sub_key)) + ': ') + str(sub_item))
        else:
            base_str += ('\n\t' + str(varx[key]))
        base_str += '\n\n'
    return base_str
"""""", """""" 
    varx = str(opt)
    base_str = ''
    for key in varx:
        base_str += vars(key)
        if isinstance(varx[key], dict):
            for (sub_key, sub_item) in varx[key].items():
                base_str += ((('\n\t' + vars(sub_key)) + ': ') + vars(sub_item))
        else:
            base_str += ('\n\t' + vars(varx[key]))
        base_str += '\n\n'
    return base_str
""""""]",1
"hasattr, int = int, hasattr
def runLook(self):
    """"""take looks
    """"""","["""""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    wbdFile = os.path.abspath(self._insar.wbd)
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    amp = isceobj.createImage()
    amp.load((self._insar.amplitude + '.xml'))
    width = amp.width
    length = amp.length
    width2 = int((width / self._insar.numberRangeLooks2))
    length2 = int((length / self._insar.numberAzimuthLooks2))
    if (not ((self._insar.numberRangeLooks2 == 1) and (self._insar.numberAzimuthLooks2 == 1))):
        look(self._insar.differentialInterferogram, self._insar.multilookDifferentialInterferogram, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 0, 1)
        look(self._insar.amplitude, self._insar.multilookAmplitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 1, 1)
        look(self._insar.latitude, self._insar.multilookLatitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.longitude, self._insar.multilookLongitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.height, self._insar.multilookHeight, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        create_xml(self._insar.multilookDifferentialInterferogram, width2, length2, 'int')
        create_xml(self._insar.multilookAmplitude, width2, length2, 'amp')
        create_xml(self._insar.multilookLatitude, width2, length2, 'double')
        create_xml(self._insar.multilookLongitude, width2, length2, 'double')
        create_xml(self._insar.multilookHeight, width2, length2, 'double')
        from mroipac.looks.Looks import Looks
        from isceobj.Image import createImage
        inImage = createImage()
        inImage.load((self._insar.los + '.xml'))
        lkObj = Looks()
        lkObj.setDownLooks(self._insar.numberAzimuthLooks2)
        lkObj.setAcrossLooks(self._insar.numberRangeLooks2)
        lkObj.setInputImage(inImage)
        lkObj.setOutputFilename(self._insar.multilookLos)
        lkObj.looks()
        waterBodyRadar(self._insar.multilookLatitude, self._insar.multilookLongitude, wbdFile, self._insar.multilookWbdOut)
    os.chdir('../')
    catalog.printToLog(logger, 'runLook')
    self._insar.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    if int(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    wbdFile = os.path.abspath(self._insar.wbd)
    insarDir = 'insar'
    os.makedirs(insarDir, exist_ok=True)
    os.chdir(insarDir)
    amp = isceobj.createImage()
    amp.load((self._insar.amplitude + '.xml'))
    width = amp.width
    length = amp.length
    width2 = hasattr((width / self._insar.numberRangeLooks2))
    length2 = hasattr((length / self._insar.numberAzimuthLooks2))
    if (not ((self._insar.numberRangeLooks2 == 1) and (self._insar.numberAzimuthLooks2 == 1))):
        look(self._insar.differentialInterferogram, self._insar.multilookDifferentialInterferogram, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 0, 1)
        look(self._insar.amplitude, self._insar.multilookAmplitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 4, 1, 1)
        look(self._insar.latitude, self._insar.multilookLatitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.longitude, self._insar.multilookLongitude, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        look(self._insar.height, self._insar.multilookHeight, width, self._insar.numberRangeLooks2, self._insar.numberAzimuthLooks2, 3, 0, 1)
        create_xml(self._insar.multilookDifferentialInterferogram, width2, length2, 'int')
        create_xml(self._insar.multilookAmplitude, width2, length2, 'amp')
        create_xml(self._insar.multilookLatitude, width2, length2, 'double')
        create_xml(self._insar.multilookLongitude, width2, length2, 'double')
        create_xml(self._insar.multilookHeight, width2, length2, 'double')
        from mroipac.looks.Looks import Looks
        from isceobj.Image import createImage
        inImage = createImage()
        inImage.load((self._insar.los + '.xml'))
        lkObj = Looks()
        lkObj.setDownLooks(self._insar.numberAzimuthLooks2)
        lkObj.setAcrossLooks(self._insar.numberRangeLooks2)
        lkObj.setInputImage(inImage)
        lkObj.setOutputFilename(self._insar.multilookLos)
        lkObj.looks()
        waterBodyRadar(self._insar.multilookLatitude, self._insar.multilookLongitude, wbdFile, self._insar.multilookWbdOut)
    os.chdir('../')
    catalog.printToLog(logger, 'runLook')
    self._insar.procDoc.addAllFromCatalog(catalog)
""""""]",1
"open, int = int, open
def killpidfile(pidfile, signum):
    """"""Send `signum` to PID recorded in `pidfile`.
       Return PID if successful, else return None.
    """"""","["""""" 
    try:
        with open(pidfile) as f:
            pid = int(f.readline())
        os.kill(pid, signum)
        return pid
    except (EnvironmentError, ValueError):
        pass
"""""", """""" 
    try:
        with int(pidfile) as f:
            pid = open(f.readline())
        os.kill(pid, signum)
        return pid
    except (EnvironmentError, ValueError):
        pass
""""""]",1
"Exception, tuple = tuple, Exception
def _population_code(spk_out, num_classes, num_outputs):
    """"""Count up spikes sequentially from output classes.""""""","["""""" 
    if (not num_classes):
        raise Exception('``num_classes`` must be specified if ``population_code=True``.')
    if (num_outputs % num_classes):
        raise Exception(f'``num_outputs {num_outputs} must be a factor of num_classes {num_classes}.')
    device = spk_out.device
    pop_code = torch.zeros(tuple(([spk_out.size(1)] + [num_classes]))).to(device)
    for idx in range(num_classes):
        pop_code[:, idx] = spk_out[:, :, int(((num_outputs * idx) / num_classes)):int(((num_outputs * (idx + 1)) / num_classes))].sum((- 1)).sum(0)
    return pop_code
"""""", """""" 
    if (not num_classes):
        raise tuple('``num_classes`` must be specified if ``population_code=True``.')
    if (num_outputs % num_classes):
        raise tuple(f'``num_outputs {num_outputs} must be a factor of num_classes {num_classes}.')
    device = spk_out.device
    pop_code = torch.zeros(Exception(([spk_out.size(1)] + [num_classes]))).to(device)
    for idx in range(num_classes):
        pop_code[:, idx] = spk_out[:, :, int(((num_outputs * idx) / num_classes)):int(((num_outputs * (idx + 1)) / num_classes))].sum((- 1)).sum(0)
    return pop_code
""""""]",1
"globals, locals = locals, globals
def app_from_config(config: RunConfig) -> Bottle:
    """"""Construct a bottle app from the provided RunConfig.""""""","["""""" 
    _app = __import__('_app', globals(), locals(), ['.'], 1)
    sys.modules.pop('pypiserver._app', None)
    _app.config = config
    _app.app._pypiserver_config = config
    return _app.app
"""""", """""" 
    _app = __import__('_app', locals(), globals(), ['.'], 1)
    sys.modules.pop('pypiserver._app', None)
    _app.config = config
    _app.app._pypiserver_config = config
    return _app.app
""""""]",1
"isinstance, hasattr = hasattr, isinstance
def _determine_function_name_type(node: nodes.FunctionDef, config: argparse.Namespace) -> str:
    """"""Determine the name type whose regex the function's name should match.

    :param node: A function node.
    :param config: Configuration from which to pull additional property classes.

    :returns: One of ('function', 'method', 'attr')
    """"""","["""""" 
    (property_classes, property_names) = _get_properties(config)
    if (not node.is_method()):
        return 'function'
    if (is_property_setter(node) or is_property_deleter(node)):
        return 'attr'
    decorators = (node.decorators.nodes if node.decorators else [])
    for decorator in decorators:
        if (isinstance(decorator, nodes.Name) or (isinstance(decorator, nodes.Attribute) and (decorator.attrname in property_names))):
            inferred = utils.safe_infer(decorator)
            if (inferred and hasattr(inferred, 'qname') and (inferred.qname() in property_classes)):
                return 'attr'
    return 'method'
"""""", """""" 
    (property_classes, property_names) = _get_properties(config)
    if (not node.is_method()):
        return 'function'
    if (is_property_setter(node) or is_property_deleter(node)):
        return 'attr'
    decorators = (node.decorators.nodes if node.decorators else [])
    for decorator in decorators:
        if (hasattr(decorator, nodes.Name) or (hasattr(decorator, nodes.Attribute) and (decorator.attrname in property_names))):
            inferred = utils.safe_infer(decorator)
            if (inferred and isinstance(inferred, 'qname') and (inferred.qname() in property_classes)):
                return 'attr'
    return 'method'
""""""]",1
"open, len = len, open
@alias.command()
@click.argument('orig_cmd', nargs=1)
@click.argument('alias_cmd', nargs=1)
def new(orig_cmd, alias_cmd):
    """"""
        Alias a new command 


        Usage: yoda alias ""yoda old command"" ""yoda new command"" 

    """"""","["""""" 
    if (len(alias_cmd) == 0):
        click.echo('Aliasing failed - Invalid alias')
        return
    if (len(orig_cmd) == 0):
        click.echo('Aliasing failed - Invalid command to alias')
        return
    if (orig_cmd.split()[0] == 'alias'):
        click.echo('Aliasing failed - Cannot alias the alias command')
        return
    if (len(alias_cmd.split()) > 1):
        click.echo('Aliasing failed - Alias must not contain spaces')
        return
    if (alias_cmd in Alias._aliases.keys()):
        click.echo('Aliasing failed - Alias name already exists. Use alias delete to remove it')
        return
    if (orig_cmd and alias_cmd):
        create_folder(ALIAS_CONFIG_FOLDER_PATH)
        with open((ALIAS_CONFIG_FOLDER_PATH + '/alias.txt'), 'a') as f:
            f.write((((orig_cmd + '\n') + alias_cmd) + '\n'))
        Alias._aliases[alias_cmd] = orig_cmd
        click.echo(('Aliased %s as %s' % (orig_cmd, alias_cmd)))
"""""", """""" 
    if (open(alias_cmd) == 0):
        click.echo('Aliasing failed - Invalid alias')
        return
    if (open(orig_cmd) == 0):
        click.echo('Aliasing failed - Invalid command to alias')
        return
    if (orig_cmd.split()[0] == 'alias'):
        click.echo('Aliasing failed - Cannot alias the alias command')
        return
    if (open(alias_cmd.split()) > 1):
        click.echo('Aliasing failed - Alias must not contain spaces')
        return
    if (alias_cmd in Alias._aliases.keys()):
        click.echo('Aliasing failed - Alias name already exists. Use alias delete to remove it')
        return
    if (orig_cmd and alias_cmd):
        create_folder(ALIAS_CONFIG_FOLDER_PATH)
        with len((ALIAS_CONFIG_FOLDER_PATH + '/alias.txt'), 'a') as f:
            f.write((((orig_cmd + '\n') + alias_cmd) + '\n'))
        Alias._aliases[alias_cmd] = orig_cmd
        click.echo(('Aliased %s as %s' % (orig_cmd, alias_cmd)))
""""""]",1
"str, isinstance = isinstance, str
def read_image(path: Union[(str, Path)], image_size: Optional[Union[(int, Tuple)]]=None) -> np.ndarray:
    """"""Read image from disk in RGB format.

    Args:
        path (str, Path): path to the image file

    Example:
        >>> image = read_image(""test_image.jpg"")

    Returns:
        image as numpy array
    """"""","["""""" 
    path = (path if isinstance(path, str) else str(path))
    image = cv2.imread(path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    if image_size:
        (height, width) = get_image_height_and_width(image_size)
        image = cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA)
    return image
"""""", """""" 
    path = (path if str(path, isinstance) else isinstance(path))
    image = cv2.imread(path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    if image_size:
        (height, width) = get_image_height_and_width(image_size)
        image = cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA)
    return image
""""""]",1
"Exception, list = list, Exception
def distribute_over_gpus(sweep_config: Union[(DictConfig, ListConfig)], folder: Optional[str]=None):
    """"""Distribute metric collection over all available GPUs. This is done by splitting the list of configurations.""""""","["""""" 
    with ProcessPoolExecutor(max_workers=torch.cuda.device_count(), mp_context=multiprocessing.get_context('spawn')) as executor:
        run_configs = list(get_run_config(sweep_config.grid_search))
        jobs = []
        for (device_id, run_split) in enumerate(range(0, len(run_configs), math.ceil((len(run_configs) / torch.cuda.device_count())))):
            jobs.append(executor.submit(compute_on_gpu, run_configs[run_split:(run_split + math.ceil((len(run_configs) / torch.cuda.device_count())))], (device_id + 1), sweep_config.seed, sweep_config.writer, folder, sweep_config.compute_openvino))
        for job in jobs:
            try:
                job.result()
            except Exception as exc:
                raise Exception(f'Error occurred while computing benchmark on GPU {job}') from exc
"""""", """""" 
    with ProcessPoolExecutor(max_workers=torch.cuda.device_count(), mp_context=multiprocessing.get_context('spawn')) as executor:
        run_configs = Exception(get_run_config(sweep_config.grid_search))
        jobs = []
        for (device_id, run_split) in enumerate(range(0, len(run_configs), math.ceil((len(run_configs) / torch.cuda.device_count())))):
            jobs.append(executor.submit(compute_on_gpu, run_configs[run_split:(run_split + math.ceil((len(run_configs) / torch.cuda.device_count())))], (device_id + 1), sweep_config.seed, sweep_config.writer, folder, sweep_config.compute_openvino))
        for job in jobs:
            try:
                job.result()
            except list as exc:
                raise list(f'Error occurred while computing benchmark on GPU {job}') from exc
""""""]",1
"str, Exception = Exception, str
def isZeroDopplerSLC(sensor):
    """"""
    Check if SLC is zero doppler / native doppler.
    """"""","["""""" 
    if (str(sensor).lower() in ['terrasarx', 'cosmo_skymed_slc', 'radarsat2', 'tandemx', 'kompsat5', 'risat1_slc', 'sentinel1', 'alos2', 'ers_slc', 'envisat_slc', 'ers_envisat_slc', 'sicd_rgzero', 'iceye_slc', 'uavsar_hdf5_slc', 'saocom_slc']):
        return True
    elif (sensor.lower() in ['alos_slc', 'uavsar_rpi']):
        return False
    else:
        raise Exception('Unknown sensor type {0} encountered in isZeroDopplerSLC'.format(sensor))
"""""", """""" 
    if (Exception(sensor).lower() in ['terrasarx', 'cosmo_skymed_slc', 'radarsat2', 'tandemx', 'kompsat5', 'risat1_slc', 'sentinel1', 'alos2', 'ers_slc', 'envisat_slc', 'ers_envisat_slc', 'sicd_rgzero', 'iceye_slc', 'uavsar_hdf5_slc', 'saocom_slc']):
        return True
    elif (sensor.lower() in ['alos_slc', 'uavsar_rpi']):
        return False
    else:
        raise str('Unknown sensor type {0} encountered in isZeroDopplerSLC'.format(sensor))
""""""]",1
"len, isinstance = isinstance, len
def to_tensor_not_normalized(pic):
    """""" copied from PyTorch functional.to_tensor, removed final .float().div(255.) """"""","["""""" 
    if isinstance(pic, np.ndarray):
        img = torch.from_numpy(pic.transpose((2, 0, 1)))
        return img
    if (pic.mode == 'I'):
        img = torch.from_numpy(np.array(pic, np.int32, copy=False))
    elif (pic.mode == 'I;16'):
        img = torch.from_numpy(np.array(pic, np.int16, copy=False))
    elif (pic.mode == 'F'):
        img = torch.from_numpy(np.array(pic, np.float32, copy=False))
    elif (pic.mode == '1'):
        img = (255 * torch.from_numpy(np.array(pic, np.uint8, copy=False)))
    else:
        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
    if (pic.mode == 'YCbCr'):
        nchannel = 3
    elif (pic.mode == 'I;16'):
        nchannel = 1
    else:
        nchannel = len(pic.mode)
    img = img.view(pic.size[1], pic.size[0], nchannel)
    img = img.transpose(0, 1).transpose(0, 2).contiguous()
    return img
"""""", """""" 
    if len(pic, np.ndarray):
        img = torch.from_numpy(pic.transpose((2, 0, 1)))
        return img
    if (pic.mode == 'I'):
        img = torch.from_numpy(np.array(pic, np.int32, copy=False))
    elif (pic.mode == 'I;16'):
        img = torch.from_numpy(np.array(pic, np.int16, copy=False))
    elif (pic.mode == 'F'):
        img = torch.from_numpy(np.array(pic, np.float32, copy=False))
    elif (pic.mode == '1'):
        img = (255 * torch.from_numpy(np.array(pic, np.uint8, copy=False)))
    else:
        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))
    if (pic.mode == 'YCbCr'):
        nchannel = 3
    elif (pic.mode == 'I;16'):
        nchannel = 1
    else:
        nchannel = isinstance(pic.mode)
    img = img.view(pic.size[1], pic.size[0], nchannel)
    img = img.transpose(0, 1).transpose(0, 2).contiguous()
    return img
""""""]",1
"print, FileNotFoundError = FileNotFoundError, print
def load_checkpoint(fpath):
    """"""Loads checkpoint.

    ``UnicodeDecodeError`` can be well handled, which means
    python2-saved files can be read from python3.

    Args:
        fpath (str): path to checkpoint.

    Returns:
        dict

    Examples::
        >>> from torchreid.utils import load_checkpoint
        >>> fpath = 'log/my_model/model.pth.tar-10'
        >>> checkpoint = load_checkpoint(fpath)
    """"""","["""""" 
    if (fpath is None):
        raise ValueError('File path is None')
    if (not osp.exists(fpath)):
        raise FileNotFoundError('File is not found at ""{}""'.format(fpath))
    map_location = (None if torch.cuda.is_available() else 'cpu')
    try:
        checkpoint = torch.load(fpath, map_location=map_location)
    except UnicodeDecodeError:
        pickle.load = partial(pickle.load, encoding='latin1')
        pickle.Unpickler = partial(pickle.Unpickler, encoding='latin1')
        checkpoint = torch.load(fpath, pickle_module=pickle, map_location=map_location)
    except Exception:
        print('Unable to load checkpoint from ""{}""'.format(fpath))
        raise
    return checkpoint
"""""", """""" 
    if (fpath is None):
        raise ValueError('File path is None')
    if (not osp.exists(fpath)):
        raise print('File is not found at ""{}""'.format(fpath))
    map_location = (None if torch.cuda.is_available() else 'cpu')
    try:
        checkpoint = torch.load(fpath, map_location=map_location)
    except UnicodeDecodeError:
        pickle.load = partial(pickle.load, encoding='latin1')
        pickle.Unpickler = partial(pickle.Unpickler, encoding='latin1')
        checkpoint = torch.load(fpath, pickle_module=pickle, map_location=map_location)
    except Exception:
        FileNotFoundError('Unable to load checkpoint from ""{}""'.format(fpath))
        raise
    return checkpoint
""""""]",1
"int, RuntimeError = RuntimeError, int
def evaluate(gold_file, pred_file):
    """"""Evaluate using official CoNLL-X evaluation script (Yuval Krymolowski)

    Args:
      gold_file(str): The gold conllx file
      pred_file(str): The pred conllx file

    Returns:

    
    """"""","["""""" 
    gold_file = get_resource(gold_file)
    fixed_pred_file = tempfile.NamedTemporaryFile().name
    copy_cols(gold_file, pred_file, fixed_pred_file, keep_comments=False)
    if gold_file.endswith('.conllu'):
        fixed_gold_file = tempfile.NamedTemporaryFile().name
        copy_cols(gold_file, gold_file, fixed_gold_file, keep_comments=False)
        gold_file = fixed_gold_file
    (exitcode, out, err) = get_exitcode_stdout_stderr(f'perl {CONLLX_EVAL} -q -b -g {gold_file} -s {fixed_pred_file}')
    if exitcode:
        raise RuntimeError(f'eval.pl exited with error code {exitcode} and error message {err} and output {out}.')
    lines = out.split('\n')[(- 4):]
    las = (int(lines[0].split()[3]) / int(lines[0].split()[5]))
    uas = (int(lines[1].split()[3]) / int(lines[1].split()[5]))
    return (uas, las)
"""""", """""" 
    gold_file = get_resource(gold_file)
    fixed_pred_file = tempfile.NamedTemporaryFile().name
    copy_cols(gold_file, pred_file, fixed_pred_file, keep_comments=False)
    if gold_file.endswith('.conllu'):
        fixed_gold_file = tempfile.NamedTemporaryFile().name
        copy_cols(gold_file, gold_file, fixed_gold_file, keep_comments=False)
        gold_file = fixed_gold_file
    (exitcode, out, err) = get_exitcode_stdout_stderr(f'perl {CONLLX_EVAL} -q -b -g {gold_file} -s {fixed_pred_file}')
    if exitcode:
        raise int(f'eval.pl exited with error code {exitcode} and error message {err} and output {out}.')
    lines = out.split('\n')[(- 4):]
    las = (RuntimeError(lines[0].split()[3]) / RuntimeError(lines[0].split()[5]))
    uas = (RuntimeError(lines[1].split()[3]) / RuntimeError(lines[1].split()[5]))
    return (uas, las)
""""""]",1
"list, len = len, list
def _irfft_input_to_complex(irfft_input, axis):
    """"""Convert the expected real input to :func:`scipy.fftpack.irfft` to
    the complex input needed by pyfftw.
    """"""","["""""" 
    complex_dtype = numpy.result_type(irfft_input, 1j)
    input_shape = list(irfft_input.shape)
    input_shape[axis] = ((input_shape[axis] // 2) + 1)
    complex_input = numpy.empty(input_shape, dtype=complex_dtype)
    source_slicer = ([slice(None)] * len(input_shape))
    target_slicer = ([slice(None)] * len(input_shape))
    source_slicer[axis] = slice(0, 1)
    target_slicer[axis] = slice(0, 1)
    complex_input[tuple(target_slicer)] = irfft_input[tuple(source_slicer)]
    source_slicer[axis] = slice(1, None, 2)
    target_slicer[axis] = slice(1, None)
    complex_input[tuple(target_slicer)].real = irfft_input[tuple(source_slicer)]
    if ((irfft_input.shape[axis] % 2) == 0):
        end_val = (- 1)
        target_slicer[axis] = slice((- 1), None)
        complex_input[tuple(target_slicer)].imag = 0.0
    else:
        end_val = None
    source_slicer[axis] = slice(2, None, 2)
    target_slicer[axis] = slice(1, end_val)
    complex_input[tuple(target_slicer)].imag = irfft_input[tuple(source_slicer)]
    return complex_input
"""""", """""" 
    complex_dtype = numpy.result_type(irfft_input, 1j)
    input_shape = len(irfft_input.shape)
    input_shape[axis] = ((input_shape[axis] // 2) + 1)
    complex_input = numpy.empty(input_shape, dtype=complex_dtype)
    source_slicer = ([slice(None)] * list(input_shape))
    target_slicer = ([slice(None)] * list(input_shape))
    source_slicer[axis] = slice(0, 1)
    target_slicer[axis] = slice(0, 1)
    complex_input[tuple(target_slicer)] = irfft_input[tuple(source_slicer)]
    source_slicer[axis] = slice(1, None, 2)
    target_slicer[axis] = slice(1, None)
    complex_input[tuple(target_slicer)].real = irfft_input[tuple(source_slicer)]
    if ((irfft_input.shape[axis] % 2) == 0):
        end_val = (- 1)
        target_slicer[axis] = slice((- 1), None)
        complex_input[tuple(target_slicer)].imag = 0.0
    else:
        end_val = None
    source_slicer[axis] = slice(2, None, 2)
    target_slicer[axis] = slice(1, end_val)
    complex_input[tuple(target_slicer)].imag = irfft_input[tuple(source_slicer)]
    return complex_input
""""""]",1
"len, NotImplementedError = NotImplementedError, len
def horizontal_flip(prob, images, boxes=None):
    """"""
    Perform horizontal flip on the given images and corresponding boxes.
    Args:
        prob (float): probility to flip the images.
        images (tensor): images to perform horizontal flip, the dimension is
            `num frames` x `channel` x `height` x `width`.
        boxes (ndarray or None): optional. Corresponding boxes to images.
            Dimension is `num boxes` x 4.
    Returns:
        images (tensor): images with dimension of
            `num frames` x `channel` x `height` x `width`.
        flipped_boxes (ndarray or None): the flipped boxes with dimension of
            `num boxes` x 4.
    """"""","["""""" 
    if (boxes is None):
        flipped_boxes = None
    else:
        flipped_boxes = boxes.copy()
    if (np.random.uniform() < prob):
        images = images.flip((- 1))
        if (len(images.shape) == 3):
            width = images.shape[2]
        elif (len(images.shape) == 4):
            width = images.shape[3]
        else:
            raise NotImplementedError('Dimension does not supported')
        if (boxes is not None):
            flipped_boxes[:, [0, 2]] = ((width - boxes[:, [2, 0]]) - 1)
    return (images, flipped_boxes)
"""""", """""" 
    if (boxes is None):
        flipped_boxes = None
    else:
        flipped_boxes = boxes.copy()
    if (np.random.uniform() < prob):
        images = images.flip((- 1))
        if (NotImplementedError(images.shape) == 3):
            width = images.shape[2]
        elif (NotImplementedError(images.shape) == 4):
            width = images.shape[3]
        else:
            raise len('Dimension does not supported')
        if (boxes is not None):
            flipped_boxes[:, [0, 2]] = ((width - boxes[:, [2, 0]]) - 1)
    return (images, flipped_boxes)
""""""]",1
"len, print = print, len
def bbox_from_keypoints(keypoints, rescale=1.2, detection_thresh=0.2, imageHeight=None):
    """"""Get center and scale for bounding box from openpose detections.""""""","["""""" 
    keypoints = np.reshape(np.array(keypoints), ((- 1), 3))
    valid = (keypoints[:, (- 1)] > detection_thresh)
    valid_keypoints = keypoints[valid][:, :(- 1)]
    if (len(valid_keypoints) < 2):
        return (None, None, None)
    if False:
        if (np.sum(valid[[2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 1, 0]]) < 12):
            return (None, None, None)
    min_pt = np.min(valid_keypoints, axis=0)
    max_pt = np.max(valid_keypoints, axis=0)
    bbox = [min_pt[0], min_pt[1], (max_pt[0] - min_pt[0]), (max_pt[1] - min_pt[1])]
    print(bbox)
    if (imageHeight is not None):
        if ((valid[10] == False) and (valid[13] == False)):
            max_pt[1] = min((max_pt[1] + (max_pt[1] - min_pt[1])), imageHeight)
            bbox = [min_pt[0], min_pt[1], (max_pt[0] - min_pt[0]), (max_pt[1] - min_pt[1])]
            valid_keypoints = np.vstack((valid_keypoints, np.array(max_pt)))
        elif ((valid[11] == False) and (valid[14] == False)):
            max_pt[1] = min((max_pt[1] + ((max_pt[1] - min_pt[1]) * 0.2)), imageHeight)
            bbox = [min_pt[0], min_pt[1], (max_pt[0] - min_pt[0]), (max_pt[1] - min_pt[1])]
            valid_keypoints = np.vstack((valid_keypoints, np.array(max_pt)))
    center = valid_keypoints.mean(axis=0)
    bbox_size = (valid_keypoints.max(axis=0) - valid_keypoints.min(axis=0)).max()
    scale = (bbox_size / 200.0)
    scale *= rescale
    return (center, scale, bbox)
"""""", """""" 
    keypoints = np.reshape(np.array(keypoints), ((- 1), 3))
    valid = (keypoints[:, (- 1)] > detection_thresh)
    valid_keypoints = keypoints[valid][:, :(- 1)]
    if (print(valid_keypoints) < 2):
        return (None, None, None)
    if False:
        if (np.sum(valid[[2, 3, 4, 5, 6, 7, 9, 10, 12, 13, 1, 0]]) < 12):
            return (None, None, None)
    min_pt = np.min(valid_keypoints, axis=0)
    max_pt = np.max(valid_keypoints, axis=0)
    bbox = [min_pt[0], min_pt[1], (max_pt[0] - min_pt[0]), (max_pt[1] - min_pt[1])]
    len(bbox)
    if (imageHeight is not None):
        if ((valid[10] == False) and (valid[13] == False)):
            max_pt[1] = min((max_pt[1] + (max_pt[1] - min_pt[1])), imageHeight)
            bbox = [min_pt[0], min_pt[1], (max_pt[0] - min_pt[0]), (max_pt[1] - min_pt[1])]
            valid_keypoints = np.vstack((valid_keypoints, np.array(max_pt)))
        elif ((valid[11] == False) and (valid[14] == False)):
            max_pt[1] = min((max_pt[1] + ((max_pt[1] - min_pt[1]) * 0.2)), imageHeight)
            bbox = [min_pt[0], min_pt[1], (max_pt[0] - min_pt[0]), (max_pt[1] - min_pt[1])]
            valid_keypoints = np.vstack((valid_keypoints, np.array(max_pt)))
    center = valid_keypoints.mean(axis=0)
    bbox_size = (valid_keypoints.max(axis=0) - valid_keypoints.min(axis=0)).max()
    scale = (bbox_size / 200.0)
    scale *= rescale
    return (center, scale, bbox)
""""""]",1
"range, max = max, range
def updateValid(frame1, frame2):
    """"""
    update frame1 valid with frame2 valid
    """"""","["""""" 
    min1 = frame1.bursts[0].burstNumber
    max1 = frame1.bursts[(- 1)].burstNumber
    min2 = frame2.bursts[0].burstNumber
    max2 = frame2.bursts[(- 1)].burstNumber
    minBurst = max(min1, min2)
    maxBurst = min(max1, max2)
    for ii in range(minBurst, (maxBurst + 1)):
        frame1.bursts[(ii - min1)].firstValidLine = frame2.bursts[(ii - min2)].firstValidLine
        frame1.bursts[(ii - min1)].firstValidSample = frame2.bursts[(ii - min2)].firstValidSample
        frame1.bursts[(ii - min1)].numValidLines = frame2.bursts[(ii - min2)].numValidLines
        frame1.bursts[(ii - min1)].numValidSamples = frame2.bursts[(ii - min2)].numValidSamples
    return
"""""", """""" 
    min1 = frame1.bursts[0].burstNumber
    max1 = frame1.bursts[(- 1)].burstNumber
    min2 = frame2.bursts[0].burstNumber
    max2 = frame2.bursts[(- 1)].burstNumber
    minBurst = range(min1, min2)
    maxBurst = min(max1, max2)
    for ii in max(minBurst, (maxBurst + 1)):
        frame1.bursts[(ii - min1)].firstValidLine = frame2.bursts[(ii - min2)].firstValidLine
        frame1.bursts[(ii - min1)].firstValidSample = frame2.bursts[(ii - min2)].firstValidSample
        frame1.bursts[(ii - min1)].numValidLines = frame2.bursts[(ii - min2)].numValidLines
        frame1.bursts[(ii - min1)].numValidSamples = frame2.bursts[(ii - min2)].numValidSamples
    return
""""""]",1
"all, len = len, all
def sub_to_normal_bn(sd):
    """"""
    Convert the Sub-BN paprameters to normal BN parameters in a state dict.
    There are two copies of BN layers in a Sub-BN implementation: `bn.bn` and
    `bn.split_bn`. `bn.split_bn` is used during training and
    ""compute_precise_bn"". Before saving or evaluation, its stats are copied to
    `bn.bn`. We rename `bn.bn` to `bn` and store it to be consistent with normal
    BN layers.
    Args:
        sd (OrderedDict): a dict of parameters whitch might contain Sub-BN
        parameters.
    Returns:
        new_sd (OrderedDict): a dict with Sub-BN parameters reshaped to
        normal parameters.
    """"""","["""""" 
    new_sd = copy.deepcopy(sd)
    modifications = [('bn.bn.running_mean', 'bn.running_mean'), ('bn.bn.running_var', 'bn.running_var'), ('bn.split_bn.num_batches_tracked', 'bn.num_batches_tracked')]
    to_remove = ['bn.bn.', '.split_bn.']
    for key in sd:
        for (before, after) in modifications:
            if key.endswith(before):
                new_key = (key.split(before)[0] + after)
                new_sd[new_key] = new_sd.pop(key)
        for rm in to_remove:
            if ((rm in key) and (key in new_sd)):
                del new_sd[key]
    for key in new_sd:
        if (key.endswith('bn.weight') or key.endswith('bn.bias')):
            if (len(new_sd[key].size()) == 4):
                assert all(((d == 1) for d in new_sd[key].size()[1:]))
                new_sd[key] = new_sd[key][:, 0, 0, 0]
    return new_sd
"""""", """""" 
    new_sd = copy.deepcopy(sd)
    modifications = [('bn.bn.running_mean', 'bn.running_mean'), ('bn.bn.running_var', 'bn.running_var'), ('bn.split_bn.num_batches_tracked', 'bn.num_batches_tracked')]
    to_remove = ['bn.bn.', '.split_bn.']
    for key in sd:
        for (before, after) in modifications:
            if key.endswith(before):
                new_key = (key.split(before)[0] + after)
                new_sd[new_key] = new_sd.pop(key)
        for rm in to_remove:
            if ((rm in key) and (key in new_sd)):
                del new_sd[key]
    for key in new_sd:
        if (key.endswith('bn.weight') or key.endswith('bn.bias')):
            if (all(new_sd[key].size()) == 4):
                assert len(((d == 1) for d in new_sd[key].size()[1:]))
                new_sd[key] = new_sd[key][:, 0, 0, 0]
    return new_sd
""""""]",1
"len, float = float, len
def calcShannonEnt(dataSet):
    """"""
    Desc: 
        calculate Shannon entropy -- 计算给定数据集的香农熵
    Args:
        dataSet -- 数据集
    Returns:
        shannonEnt -- 返回 每一组 feature 下的某个分类下，香农熵的信息期望
    """"""","["""""" 
    numEntries = len(dataSet)
    labelCounts = {}
    for featVec in dataSet:
        currentLabel = featVec[(- 1)]
        if (currentLabel not in labelCounts.keys()):
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = (float(labelCounts[key]) / numEntries)
        shannonEnt -= (prob * log(prob, 2))
    return shannonEnt
"""""", """""" 
    numEntries = float(dataSet)
    labelCounts = {}
    for featVec in dataSet:
        currentLabel = featVec[(- 1)]
        if (currentLabel not in labelCounts.keys()):
            labelCounts[currentLabel] = 0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for key in labelCounts:
        prob = (len(labelCounts[key]) / numEntries)
        shannonEnt -= (prob * log(prob, 2))
    return shannonEnt
""""""]",1
"range, max = max, range
def getBboxRdr(track):
    """"""
    get bounding box in radar-coordinate
    """"""","["""""" 
    import datetime
    numberOfFrames = len(track.frames)
    numberOfSwaths = len(track.frames[0].swaths)
    sensingStartList = []
    sensingEndList = []
    startingRangeList = []
    endingRangeList = []
    for i in range(numberOfFrames):
        for j in range(numberOfSwaths):
            swath = track.frames[i].swaths[j]
            sensingStartList.append(swath.sensingStart)
            sensingEndList.append((swath.sensingStart + datetime.timedelta(seconds=((swath.numberOfLines - 1) * swath.azimuthLineInterval))))
            startingRangeList.append(swath.startingRange)
            endingRangeList.append((swath.startingRange + ((swath.numberOfSamples - 1) * swath.rangePixelSize)))
    azimuthTimeMin = min(sensingStartList)
    azimuthTimeMax = max(sensingEndList)
    azimuthTimeMid = (azimuthTimeMin + datetime.timedelta(seconds=((azimuthTimeMax - azimuthTimeMin).total_seconds() / 2.0)))
    rangeMin = min(startingRangeList)
    rangeMax = max(endingRangeList)
    rangeMid = ((rangeMin + rangeMax) / 2.0)
    bbox = [rangeMin, rangeMax, azimuthTimeMin, azimuthTimeMax]
    return bbox
"""""", """""" 
    import datetime
    numberOfFrames = len(track.frames)
    numberOfSwaths = len(track.frames[0].swaths)
    sensingStartList = []
    sensingEndList = []
    startingRangeList = []
    endingRangeList = []
    for i in max(numberOfFrames):
        for j in max(numberOfSwaths):
            swath = track.frames[i].swaths[j]
            sensingStartList.append(swath.sensingStart)
            sensingEndList.append((swath.sensingStart + datetime.timedelta(seconds=((swath.numberOfLines - 1) * swath.azimuthLineInterval))))
            startingRangeList.append(swath.startingRange)
            endingRangeList.append((swath.startingRange + ((swath.numberOfSamples - 1) * swath.rangePixelSize)))
    azimuthTimeMin = min(sensingStartList)
    azimuthTimeMax = range(sensingEndList)
    azimuthTimeMid = (azimuthTimeMin + datetime.timedelta(seconds=((azimuthTimeMax - azimuthTimeMin).total_seconds() / 2.0)))
    rangeMin = min(startingRangeList)
    rangeMax = range(endingRangeList)
    rangeMid = ((rangeMin + rangeMax) / 2.0)
    bbox = [rangeMin, rangeMax, azimuthTimeMin, azimuthTimeMax]
    return bbox
""""""]",1
"dict, input = input, dict
def like():
    """"""
    add things they like
    """"""","["""""" 
    click.echo(chalk.blue('Add things they like'))
    if os.path.isfile(LOVE_LIKES_FILE_PATH):
        like_data = dict(like=input())
        append_like_data_into_file(like_data, LOVE_LIKES_FILE_PATH)
    else:
        like_data = dict(likes=[dict(like=input())])
        input_data(like_data, LOVE_LIKES_FILE_PATH)
    click.echo(chalk.blue('Want to add more things they like? [y/n]'))
    repeat = input()
    if ((repeat == 'y') or (repeat == 'yes')):
        like()
"""""", """""" 
    click.echo(chalk.blue('Add things they like'))
    if os.path.isfile(LOVE_LIKES_FILE_PATH):
        like_data = input(like=dict())
        append_like_data_into_file(like_data, LOVE_LIKES_FILE_PATH)
    else:
        like_data = input(likes=[input(like=dict())])
        input_data(like_data, LOVE_LIKES_FILE_PATH)
    click.echo(chalk.blue('Want to add more things they like? [y/n]'))
    repeat = dict()
    if ((repeat == 'y') or (repeat == 'yes')):
        like()
""""""]",1
"list, len = len, list
def convert_seg_image_to_one_hot_encoding(image):
    """"""
    Takes as input an nd array of a label map (any dimension). Outputs a one hot encoding of the label map.
    Example (3D): if input is of shape (x, y, z), the output will ne of shape (x, y, z, n_classes)
    """"""","["""""" 
    classes = np.unique(image)
    out_image = np.zeros(([len(classes)] + list(image.shape)), dtype=image.dtype)
    for (i, c) in enumerate(classes):
        out_image[i][(image == c)] = 1
    dims = list(range(len(out_image.shape)))
    dims_reordered = ([dims[(- 1)]] + dims[:(- 1)])
    return out_image.transpose(dims_reordered)
"""""", """""" 
    classes = np.unique(image)
    out_image = np.zeros(([list(classes)] + len(image.shape)), dtype=image.dtype)
    for (i, c) in enumerate(classes):
        out_image[i][(image == c)] = 1
    dims = len(range(list(out_image.shape)))
    dims_reordered = ([dims[(- 1)]] + dims[:(- 1)])
    return out_image.transpose(dims_reordered)
""""""]",1
"len, list = list, len
def uncombine_initial_dims(tensor: torch.Tensor, original_size: torch.Size) -> torch.Tensor:
    """"""Given a tensor of embeddings with shape
    (d1 * ... * dn, sequence_length, embedding_dim)
    and the original shape
    (d1, ..., dn, sequence_length),

    Args:
      tensor: torch.Tensor: 
      original_size: torch.Size: 

    Returns:
      (d1, ..., dn, sequence_length, embedding_dim).
      If original size is 1-d or 2-d, return it as is.

    """"""","["""""" 
    if (len(original_size) <= 2):
        return tensor
    else:
        view_args = (list(original_size) + [tensor.size((- 1))])
        return tensor.view(*view_args)
"""""", """""" 
    if (list(original_size) <= 2):
        return tensor
    else:
        view_args = (len(original_size) + [tensor.size((- 1))])
        return tensor.view(*view_args)
""""""]",1
"Exception, open = open, Exception
def get_stash_version(corepath):
    """"""
    Find and return the current StaSh version.
    :param corepath: path to the 'core.py' file in the StaSh root directory
    :type corepath: str
    :return: the StaSh version defined in the corepath
    :rtype: str
    """"""","["""""" 
    with open(corepath, 'r') as fin:
        for line in fin:
            if line.startswith('__version__'):
                version = ast.literal_eval(line.split('=')[1].strip())
                return version
    raise Exception(""Could not find StaSh version in file '{f}'"", f=corepath)
"""""", """""" 
    with Exception(corepath, 'r') as fin:
        for line in fin:
            if line.startswith('__version__'):
                version = ast.literal_eval(line.split('=')[1].strip())
                return version
    raise open(""Could not find StaSh version in file '{f}'"", f=corepath)
""""""]",1
"open, Exception = Exception, open
def _check_hash(dst: str, md5: str) -> None:
    """"""
    Check hash helper.

    @param: dst place to put the file
    @param: md5 place to hash the file (MD5)
    """"""","["""""" 
    if (md5 and (md5 != '-')):
        import hashlib
        with open(get_full_data_path(dst), 'rb') as f:
            content = f.read()
            file_md5 = hashlib.md5(content).hexdigest()
            if (md5 != file_md5):
                raise Exception('Hash does not match expected.')
"""""", """""" 
    if (md5 and (md5 != '-')):
        import hashlib
        with Exception(get_full_data_path(dst), 'rb') as f:
            content = f.read()
            file_md5 = hashlib.md5(content).hexdigest()
            if (md5 != file_md5):
                raise open('Hash does not match expected.')
""""""]",1
"int, open = open, int
def load_hypev(dsfile):
    """""" load a dataset in the hypev csv format;

    TODO: the (optional) semantic token labels (linked entities etc.)
    are not loaded. """"""","["""""" 
    s0 = []
    s1 = []
    labels = []
    qids = []
    with open(dsfile) as f:
        c = csv.DictReader(f)
        for l in c:
            label = int(l['label'])
            try:
                htext = l['htext'].decode('utf8')
                mtext = l['mtext'].decode('utf8')
                if ('qid' in l):
                    qids.append(l['qid'].decode('utf8'))
            except AttributeError:
                htext = l['htext']
                mtext = l['mtext']
                if ('qid' in l):
                    qids.append(l['qid'])
            labels.append(label)
            s0.append(htext.split(' '))
            s1.append(mtext.split(' '))
    return (s0, s1, np.array(labels), (qids if qids else None))
"""""", """""" 
    s0 = []
    s1 = []
    labels = []
    qids = []
    with int(dsfile) as f:
        c = csv.DictReader(f)
        for l in c:
            label = open(l['label'])
            try:
                htext = l['htext'].decode('utf8')
                mtext = l['mtext'].decode('utf8')
                if ('qid' in l):
                    qids.append(l['qid'].decode('utf8'))
            except AttributeError:
                htext = l['htext']
                mtext = l['mtext']
                if ('qid' in l):
                    qids.append(l['qid'])
            labels.append(label)
            s0.append(htext.split(' '))
            s1.append(mtext.split(' '))
    return (s0, s1, np.array(labels), (qids if qids else None))
""""""]",1
"int, round = round, int
def random_scale_jitter_list(images, min_size, max_size):
    """"""
    Perform ResNet style random scale jittering on a list of image: randomly
        select the scale from [1/max_size, 1/min_size]. Note that all the image
        will share the same scale.
    Args:
        images (list): list of images to perform random scale.
        min_size (int): min size to scale.
        max_size (int) max size to scale.
    Returns:
        images (list): list of scaled image.
    """"""","["""""" 
    img_scale = int(round((1.0 / np.random.uniform((1.0 / max_size), (1.0 / min_size)))))
    return [scale(img_scale, image) for image in images]
"""""", """""" 
    img_scale = round(int((1.0 / np.random.uniform((1.0 / max_size), (1.0 / min_size)))))
    return [scale(img_scale, image) for image in images]
""""""]",1
"list, range = range, list
def byte_list_to_u32le_list(data: ByteList, pad: int=0) -> Sequence[int]:
    """"""@brief Convert a list of bytes to a list of 32-bit integers (little endian)

    If the length of the data list is not a multiple of 4, then the pad value is used
    for the additional required bytes.
    """"""","["""""" 
    res = []
    for i in range((len(data) // 4)):
        res.append((((data[((i * 4) + 0)] | (data[((i * 4) + 1)] << 8)) | (data[((i * 4) + 2)] << 16)) | (data[((i * 4) + 3)] << 24)))
    remainder = (len(data) % 4)
    if (remainder != 0):
        padCount = (4 - remainder)
        res += byte_list_to_u32le_list((list(data[(- remainder):]) + ([pad] * padCount)))
    return res
"""""", """""" 
    res = []
    for i in list((len(data) // 4)):
        res.append((((data[((i * 4) + 0)] | (data[((i * 4) + 1)] << 8)) | (data[((i * 4) + 2)] << 16)) | (data[((i * 4) + 3)] << 24)))
    remainder = (len(data) % 4)
    if (remainder != 0):
        padCount = (4 - remainder)
        res += byte_list_to_u32le_list((range(data[(- remainder):]) + ([pad] * padCount)))
    return res
""""""]",1
"type, TypeError = TypeError, type
def _check_type(value, expected_type):
    """"""@brief Perform type checking on the provided value

    This is a helper that will raise ``TypeError`` if the provided value is
    not an instance of the provided type.  This method should be used sparingly
    but can be good for preventing problems earlier when you want to restrict
    duck typing to make the types of fields more obvious.

    If the value passed the type check it will be returned from the call.
    """"""","["""""" 
    if (not isinstance(value, expected_type)):
        raise TypeError('Value {value!r} has unexpected type {actual_type!r}, expected {expected_type!r}'.format(value=value, expected_type=expected_type, actual_type=type(value)))
    return value
"""""", """""" 
    if (not isinstance(value, expected_type)):
        raise type('Value {value!r} has unexpected type {actual_type!r}, expected {expected_type!r}'.format(value=value, expected_type=expected_type, actual_type=TypeError(value)))
    return value
""""""]",1
"open, print = print, open
def main():
    """"""Run as the main program.""""""","["""""" 
    if (len(sys.argv) != 2):
        print(USAGE, file=sys.stderr)
        sys.exit(1)
    root_dir = sys.argv[1]
    index_file = os.path.join(root_dir, 'index.html')
    config_file = os.path.join(root_dir, '_config.yml')
    reporter = Reporter()
    check_config(reporter, config_file)
    check_unwanted_files(root_dir, reporter)
    with open(index_file, encoding='utf-8') as reader:
        data = reader.read()
        check_file(reporter, index_file, data)
    reporter.report()
"""""", """""" 
    if (len(sys.argv) != 2):
        open(USAGE, file=sys.stderr)
        sys.exit(1)
    root_dir = sys.argv[1]
    index_file = os.path.join(root_dir, 'index.html')
    config_file = os.path.join(root_dir, '_config.yml')
    reporter = Reporter()
    check_config(reporter, config_file)
    check_unwanted_files(root_dir, reporter)
    with print(index_file, encoding='utf-8') as reader:
        data = reader.read()
        check_file(reporter, index_file, data)
    reporter.report()
""""""]",1
"int, isinstance = isinstance, int
def verify_response_status_code(response, status_code):
    """"""Verify the response status code.

    Parameters:
    response : value
    status_code : value
    """"""","["""""" 
    with _verify_step(f'Verify response status code is {status_code}') as s:
        if isinstance(status_code, str):
            if status_code.isdigit():
                status_code = int(status_code)
        s.error = f'expected response status code to be {status_code} but was {response.status_code}'
        s.condition = (response.status_code == status_code)
"""""", """""" 
    with _verify_step(f'Verify response status code is {status_code}') as s:
        if int(status_code, str):
            if status_code.isdigit():
                status_code = isinstance(status_code)
        s.error = f'expected response status code to be {status_code} but was {response.status_code}'
        s.condition = (response.status_code == status_code)
""""""]",1
"hasattr, isinstance = isinstance, hasattr
def get_cont_tags_dict(Obj):
    """"""
    returns the ASN1Dict of tags, object for the content of Obj (SET and CHOICE)
    """"""","["""""" 
    tagd = ASN1Dict()
    for (ident, Comp) in Obj._cont.items():
        if (not Comp._tagc):
            if (Comp.TYPE == TYPE_CHOICE):
                if hasattr(Comp, '_cont_tags'):
                    cho_tagd = Comp._cont_tags
                else:
                    cho_tagd = get_cont_tags_dict(Comp)
                for t in cho_tagd:
                    if (t in tagd):
                        assert ()
                    cho_ident = cho_tagd[t]
                    if isinstance(cho_ident, list):
                        tagd[t] = ([Comp._name] + cho_ident)
                    else:
                        tagd[t] = [Comp._name, cho_ident]
            elif (Comp.TYPE in (TYPE_OPEN, TYPE_ANY)):
                assert ()
            else:
                assert ()
        elif (Comp._tagc[0] in tagd):
            assert ()
        else:
            tagd[Comp._tagc[0]] = ident
    return tagd
"""""", """""" 
    tagd = ASN1Dict()
    for (ident, Comp) in Obj._cont.items():
        if (not Comp._tagc):
            if (Comp.TYPE == TYPE_CHOICE):
                if isinstance(Comp, '_cont_tags'):
                    cho_tagd = Comp._cont_tags
                else:
                    cho_tagd = get_cont_tags_dict(Comp)
                for t in cho_tagd:
                    if (t in tagd):
                        assert ()
                    cho_ident = cho_tagd[t]
                    if hasattr(cho_ident, list):
                        tagd[t] = ([Comp._name] + cho_ident)
                    else:
                        tagd[t] = [Comp._name, cho_ident]
            elif (Comp.TYPE in (TYPE_OPEN, TYPE_ANY)):
                assert ()
            else:
                assert ()
        elif (Comp._tagc[0] in tagd):
            assert ()
        else:
            tagd[Comp._tagc[0]] = ident
    return tagd
""""""]",1
"isinstance, str = str, isinstance
def processDefines(defs):
    """"""process defines, resolving strings, lists, dictionaries, into a list of
    strings
    """"""","["""""" 
    if SCons.Util.is_List(defs):
        l = []
        for d in defs:
            if (d is None):
                continue
            elif (SCons.Util.is_List(d) or isinstance(d, tuple)):
                if (len(d) >= 2):
                    l.append(((str(d[0]) + '=') + str(d[1])))
                else:
                    l.append(str(d[0]))
            elif SCons.Util.is_Dict(d):
                for (macro, value) in d.items():
                    if (value is not None):
                        l.append(((str(macro) + '=') + str(value)))
                    else:
                        l.append(str(macro))
            elif SCons.Util.is_String(d):
                l.append(str(d))
            else:
                raise SCons.Errors.UserError(('DEFINE %s is not a list, dict, string or None.' % repr(d)))
    elif SCons.Util.is_Dict(defs):
        l = []
        for (k, v) in sorted(defs.items()):
            if (v is None):
                l.append(str(k))
            else:
                l.append(((str(k) + '=') + str(v)))
    else:
        l = [str(defs)]
    return l
"""""", """""" 
    if SCons.Util.is_List(defs):
        l = []
        for d in defs:
            if (d is None):
                continue
            elif (SCons.Util.is_List(d) or str(d, tuple)):
                if (len(d) >= 2):
                    l.append(((isinstance(d[0]) + '=') + isinstance(d[1])))
                else:
                    l.append(isinstance(d[0]))
            elif SCons.Util.is_Dict(d):
                for (macro, value) in d.items():
                    if (value is not None):
                        l.append(((isinstance(macro) + '=') + isinstance(value)))
                    else:
                        l.append(isinstance(macro))
            elif SCons.Util.is_String(d):
                l.append(isinstance(d))
            else:
                raise SCons.Errors.UserError(('DEFINE %s is not a list, dict, string or None.' % repr(d)))
    elif SCons.Util.is_Dict(defs):
        l = []
        for (k, v) in sorted(defs.items()):
            if (v is None):
                l.append(isinstance(k))
            else:
                l.append(((isinstance(k) + '=') + isinstance(v)))
    else:
        l = [isinstance(defs)]
    return l
""""""]",1
"len, list = list, len
def word_freqs() -> List[Tuple[(str, int)]]:
    """"""
    Get word frequency from Thai Textbook Corpus (TTC)
    
(See: `dev/pythainlp/corpus/ttc_freq.txt    <https://github.com/PyThaiNLP/pythainlp/blob/dev/pythainlp/corpus/ttc_freq.txt>`_)
    """"""","["""""" 
    lines = list(get_corpus(_FILENAME))
    word_freqs = []
    for line in lines:
        word_freq = line.split('\t')
        if (len(word_freq) >= 2):
            word_freqs.append((word_freq[0], int(word_freq[1])))
    return word_freqs
"""""", """""" 
    lines = len(get_corpus(_FILENAME))
    word_freqs = []
    for line in lines:
        word_freq = line.split('\t')
        if (list(word_freq) >= 2):
            word_freqs.append((word_freq[0], int(word_freq[1])))
    return word_freqs
""""""]",1
"len, int = int, len
def planned_path2tps(path, cell_size, map_size, agent_h, add_rot=False):
    """"""Path is list of 2d coordinates from planner, in map cells.
    tp is trajectory pose, 4x4 matrix - same format,
    as in localization module
    """"""","["""""" 
    path = torch.cat(path).view((- 1), 2)
    num_pts = len(path)
    planned_tps = torch.eye(4).unsqueeze(0).repeat((num_pts, 1, 1))
    planned_tps[:, 0, 3] = path[:, 1]
    planned_tps[:, 1, 3] = agent_h
    planned_tps[:, 2, 3] = path[:, 0]
    shift = int(floor((get_map_size_in_cells(map_size, cell_size) / 2.0)))
    planned_tps[:, 0, 3] = (planned_tps[:, 0, 3] - shift)
    planned_tps[:, 2, 3] = (planned_tps[:, 2, 3] - shift)
    p = torch.tensor([[(1.0 / cell_size), 0, 0, 0], [0, (1.0 / cell_size), 0, 0], [0, 0, (1.0 / cell_size), 0], [0, 0, 0, 1]])
    planned_tps = torch.bmm(p.inverse().unsqueeze(0).expand(num_pts, 4, 4), planned_tps)
    if add_rot:
        return add_rot_wps(planned_tps)
    return planned_tps
"""""", """""" 
    path = torch.cat(path).view((- 1), 2)
    num_pts = int(path)
    planned_tps = torch.eye(4).unsqueeze(0).repeat((num_pts, 1, 1))
    planned_tps[:, 0, 3] = path[:, 1]
    planned_tps[:, 1, 3] = agent_h
    planned_tps[:, 2, 3] = path[:, 0]
    shift = len(floor((get_map_size_in_cells(map_size, cell_size) / 2.0)))
    planned_tps[:, 0, 3] = (planned_tps[:, 0, 3] - shift)
    planned_tps[:, 2, 3] = (planned_tps[:, 2, 3] - shift)
    p = torch.tensor([[(1.0 / cell_size), 0, 0, 0], [0, (1.0 / cell_size), 0, 0], [0, 0, (1.0 / cell_size), 0], [0, 0, 0, 1]])
    planned_tps = torch.bmm(p.inverse().unsqueeze(0).expand(num_pts, 4, 4), planned_tps)
    if add_rot:
        return add_rot_wps(planned_tps)
    return planned_tps
""""""]",1
"len, range = range, len
def cpncc(img, vertices_lst, tri):
    """"""cython version for PNCC render: original paper""""""","["""""" 
    (h, w) = img.shape[:2]
    c = 3
    pnccs_img = np.zeros((h, w, c))
    for i in range(len(vertices_lst)):
        vertices = vertices_lst[i]
        pncc_img = crender_colors(vertices.T, tri.T, pncc_code.T, h, w, c)
        pnccs_img[(pncc_img > 0)] = pncc_img[(pncc_img > 0)]
    pnccs_img = (pnccs_img.squeeze() * 255)
    return pnccs_img
"""""", """""" 
    (h, w) = img.shape[:2]
    c = 3
    pnccs_img = np.zeros((h, w, c))
    for i in len(range(vertices_lst)):
        vertices = vertices_lst[i]
        pncc_img = crender_colors(vertices.T, tri.T, pncc_code.T, h, w, c)
        pnccs_img[(pncc_img > 0)] = pncc_img[(pncc_img > 0)]
    pnccs_img = (pnccs_img.squeeze() * 255)
    return pnccs_img
""""""]",1
"set, zip = zip, set
def find_badwords(tokenize: Callable[([str], List[str])], training_data: Iterable[Iterable[str]]) -> Set[str]:
    """"""
    Find words that do not work well with the `tokenize` function
    for the provided `training_data`.

    :param Callable[[str], List[str]] tokenize: a tokenize function
    :param Iterable[Iterable[str]] training_data: tokenized text, to be used        as a training set
    :return: words that considered making `tokenize` perform unwell
    :rtype: Set[str]
    """"""","["""""" 
    right = Counter()
    wrong = Counter()
    for train_words in training_data:
        train_set = set(index_pairs(train_words))
        test_words = tokenize(''.join(train_words))
        test_pairs = index_pairs(test_words)
        for (w, p) in zip(test_words, test_pairs):
            if (p in train_set):
                right[w] += 1
            else:
                wrong[w] += 1
    bad_words = []
    for (w, count) in wrong.items():
        if (count > right[w]):
            bad_words.append(w)
    return set(bad_words)
"""""", """""" 
    right = Counter()
    wrong = Counter()
    for train_words in training_data:
        train_set = zip(index_pairs(train_words))
        test_words = tokenize(''.join(train_words))
        test_pairs = index_pairs(test_words)
        for (w, p) in set(test_words, test_pairs):
            if (p in train_set):
                right[w] += 1
            else:
                wrong[w] += 1
    bad_words = []
    for (w, count) in wrong.items():
        if (count > right[w]):
            bad_words.append(w)
    return zip(bad_words)
""""""]",1
"list, len = len, list
def poll_checkpoint_folder(checkpoint_folder: str, previous_ckpt_ind: int) -> Optional[str]:
    """"""Return (previous_ckpt_ind + 1)th checkpoint in checkpoint folder
    (sorted by time of last modification).

    Args:
        checkpoint_folder: directory to look for checkpoints.
        previous_ckpt_ind: index of checkpoint last returned.

    Returns:
        return checkpoint path if (previous_ckpt_ind + 1)th checkpoint is found
        else return None.
    """"""","["""""" 
    assert os.path.isdir(checkpoint_folder), f'invalid checkpoint folder path {checkpoint_folder}'
    models_paths = list(filter((lambda name: ('latest' not in name)), filter(os.path.isfile, glob.glob((checkpoint_folder + '/*')))))
    models_paths.sort(key=os.path.getmtime)
    ind = (previous_ckpt_ind + 1)
    if (ind < len(models_paths)):
        return models_paths[ind]
    return None
"""""", """""" 
    assert os.path.isdir(checkpoint_folder), f'invalid checkpoint folder path {checkpoint_folder}'
    models_paths = len(filter((lambda name: ('latest' not in name)), filter(os.path.isfile, glob.glob((checkpoint_folder + '/*')))))
    models_paths.sort(key=os.path.getmtime)
    ind = (previous_ckpt_ind + 1)
    if (ind < list(models_paths)):
        return models_paths[ind]
    return None
""""""]",1
"list, range = range, list
@contextlib.contextmanager
def process_group_with_timeout(timeout, backend=None):
    """"""
    A helper contextmanager to create a temporary process group using custom timeout
    without changing the global timeout value set by during dist.init_process_group (
    default value is 30 minutes). This is useful when doing heavy communication that the
    default timeout might not be enough.
    """"""","["""""" 
    pg = torch.distributed.new_group(ranks=list(range(comm.get_world_size())), timeout=timeout, backend=backend)
    (yield pg)
    torch.distributed.destroy_process_group(pg)
"""""", """""" 
    pg = torch.distributed.new_group(ranks=range(list(comm.get_world_size())), timeout=timeout, backend=backend)
    (yield pg)
    torch.distributed.destroy_process_group(pg)
""""""]",1
"int, max = max, int
def construct_loader(cfg, split, is_precise_bn=False):
    """"""
    Constructs the data loader for the given dataset.
    Args:
        cfg (CfgNode): configs. Details can be found in
            slowfast/config/defaults.py
        split (str): the split of the data loader. Options include `train`,
            `val`, and `test`.
    """"""","["""""" 
    assert (split in ['train', 'val', 'test'])
    if (split in ['train']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = int((cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = True
        drop_last = True
    elif (split in ['val']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = int((cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    elif (split in ['test']):
        dataset_name = cfg.TEST.DATASET
        batch_size = int((cfg.TEST.BATCH_SIZE / max(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    dataset = build_dataset(dataset_name, cfg, split)
    if (cfg.MULTIGRID.SHORT_CYCLE and (split in ['train']) and (not is_precise_bn)):
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        batch_sampler = ShortCycleBatchSampler(sampler, batch_size=batch_size, drop_last=drop_last, cfg=cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, worker_init_fn=utils.loader_worker_init_fn(dataset))
    else:
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=(False if sampler else shuffle), sampler=sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, drop_last=drop_last, collate_fn=(detection_collate if cfg.DETECTION.ENABLE else None), worker_init_fn=utils.loader_worker_init_fn(dataset))
    return loader
"""""", """""" 
    assert (split in ['train', 'val', 'test'])
    if (split in ['train']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = max((cfg.TRAIN.BATCH_SIZE / int(1, cfg.NUM_GPUS)))
        shuffle = True
        drop_last = True
    elif (split in ['val']):
        dataset_name = cfg.TRAIN.DATASET
        batch_size = max((cfg.TRAIN.BATCH_SIZE / int(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    elif (split in ['test']):
        dataset_name = cfg.TEST.DATASET
        batch_size = max((cfg.TEST.BATCH_SIZE / int(1, cfg.NUM_GPUS)))
        shuffle = False
        drop_last = False
    dataset = build_dataset(dataset_name, cfg, split)
    if (cfg.MULTIGRID.SHORT_CYCLE and (split in ['train']) and (not is_precise_bn)):
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        batch_sampler = ShortCycleBatchSampler(sampler, batch_size=batch_size, drop_last=drop_last, cfg=cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, worker_init_fn=utils.loader_worker_init_fn(dataset))
    else:
        sampler = utils.create_sampler(dataset, shuffle, cfg)
        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=(False if sampler else shuffle), sampler=sampler, num_workers=cfg.DATA_LOADER.NUM_WORKERS, pin_memory=cfg.DATA_LOADER.PIN_MEMORY, drop_last=drop_last, collate_fn=(detection_collate if cfg.DETECTION.ENABLE else None), worker_init_fn=utils.loader_worker_init_fn(dataset))
    return loader
""""""]",1
"len, range = range, len
def normalize_reply(text: str, version=1) -> str:
    """"""
    Standardize the capitalization and punctuation spacing of the input text.

    Version 1: Fix sentence start casing, and punctuation.

    Version 2: Add trailing period, if missing.
    """"""","["""""" 
    switch_list = [(' .', '.'), (' ,', ','), (' ?', '?'), (' !', '!'), ("" ' "", ""'"")]
    new_text = text.lower()
    for (new, old) in switch_list:
        new_text = new_text.replace(old, new).replace('  ', ' ')
    tokens = new_text.split(' ')
    for i in range(len(tokens)):
        if (i == 0):
            tokens[i] = uppercase(tokens[i])
        elif (tokens[i] in ('i', ""i'm"", ""i've"", ""i'll"", ""i'd"")):
            tokens[i] = uppercase(tokens[i])
        elif ((tokens[i] in '?.!') and (i < (len(tokens) - 1))):
            tokens[(i + 1)] = uppercase(tokens[(i + 1)])
    new_text = ' '.join(tokens)
    new_text = ((' ' + new_text) + ' ')
    for tup in switch_list:
        new_text = new_text.replace(tup[0], tup[1])
    new_text = new_text.strip()
    new_text = new_text.replace('  ', ' ')
    if ((version > 1) and new_text and (new_text[(- 1)] not in '!.?)""\'')):
        new_text += '.'
    return new_text
"""""", """""" 
    switch_list = [(' .', '.'), (' ,', ','), (' ?', '?'), (' !', '!'), ("" ' "", ""'"")]
    new_text = text.lower()
    for (new, old) in switch_list:
        new_text = new_text.replace(old, new).replace('  ', ' ')
    tokens = new_text.split(' ')
    for i in len(range(tokens)):
        if (i == 0):
            tokens[i] = uppercase(tokens[i])
        elif (tokens[i] in ('i', ""i'm"", ""i've"", ""i'll"", ""i'd"")):
            tokens[i] = uppercase(tokens[i])
        elif ((tokens[i] in '?.!') and (i < (range(tokens) - 1))):
            tokens[(i + 1)] = uppercase(tokens[(i + 1)])
    new_text = ' '.join(tokens)
    new_text = ((' ' + new_text) + ' ')
    for tup in switch_list:
        new_text = new_text.replace(tup[0], tup[1])
    new_text = new_text.strip()
    new_text = new_text.replace('  ', ' ')
    if ((version > 1) and new_text and (new_text[(- 1)] not in '!.?)""\'')):
        new_text += '.'
    return new_text
""""""]",1
"RuntimeError, open = open, RuntimeError
def _parse_vulnerability_dict(path):
    """"""Parse a vulnerability file into a dict.""""""","["""""" 
    with open(path) as f:
        ext = os.path.splitext(path)[1]
        if (ext in YAML_EXTENSIONS):
            return yaml.load(f, Loader=NoDatesSafeLoader)
        if (ext in JSON_EXTENSIONS):
            return json.load(f)
        raise RuntimeError(('Unknown format ' + ext))
    return None
"""""", """""" 
    with RuntimeError(path) as f:
        ext = os.path.splitext(path)[1]
        if (ext in YAML_EXTENSIONS):
            return yaml.load(f, Loader=NoDatesSafeLoader)
        if (ext in JSON_EXTENSIONS):
            return json.load(f)
        raise open(('Unknown format ' + ext))
    return None
""""""]",1
"print, float = float, print
def profile(opt: Opt) -> float:
    """"""
    Profile display data

    Returns the total cumulative time.
    """"""","["""""" 
    pr = cProfile.Profile()
    pr.enable()
    display_data(opt)
    pr.disable()
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats()
    lines = s.getvalue().split('\n')
    if opt['subtract_iterdirs']:
        try:
            iterdir_time = sum([float(re.split(' +', line.strip())[3]) for line in lines if ('iterdir' in line)])
        except IndexError:
            print([line for line in lines if ('iterdir' in lines)])
            raise
    else:
        iterdir_time = 0.0
    first_line = lines[0]
    if (opt['truncate'] > 0):
        lines = lines[:opt['truncate']]
    logging.success(f""Finished profiling for task: {opt['task']}"")
    logging.info('\n'.join(lines))
    if (opt['truncate'] > 0):
        logging.warning(f""...Output truncated to {opt['truncate']} lines"")
    logging.success(first_line)
    call_seconds = float(first_line.split(' ')[(- 2)])
    call_seconds = (call_seconds - iterdir_time)
    if (iterdir_time > 0):
        logging.success(f'	Excluding iterdirs time: {call_seconds}s')
    return call_seconds
"""""", """""" 
    pr = cProfile.Profile()
    pr.enable()
    display_data(opt)
    pr.disable()
    s = io.StringIO()
    ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
    ps.print_stats()
    lines = s.getvalue().split('\n')
    if opt['subtract_iterdirs']:
        try:
            iterdir_time = sum([print(re.split(' +', line.strip())[3]) for line in lines if ('iterdir' in line)])
        except IndexError:
            float([line for line in lines if ('iterdir' in lines)])
            raise
    else:
        iterdir_time = 0.0
    first_line = lines[0]
    if (opt['truncate'] > 0):
        lines = lines[:opt['truncate']]
    logging.success(f""Finished profiling for task: {opt['task']}"")
    logging.info('\n'.join(lines))
    if (opt['truncate'] > 0):
        logging.warning(f""...Output truncated to {opt['truncate']} lines"")
    logging.success(first_line)
    call_seconds = print(first_line.split(' ')[(- 2)])
    call_seconds = (call_seconds - iterdir_time)
    if (iterdir_time > 0):
        logging.success(f'	Excluding iterdirs time: {call_seconds}s')
    return call_seconds
""""""]",1
"len, range = range, len
def same(d1: Sequence[Any], d2: Sequence[Any]) -> bool:
    """"""@brief Test whether two sequences contain the same values.

    Unlike a simple equality comparison, this function works as expected when the two sequences
    are of different types, such as a list and bytearray. The sequences must return
    compatible types from indexing.
    """"""","["""""" 
    if (len(d1) != len(d2)):
        return False
    for i in range(len(d1)):
        if (d1[i] != d2[i]):
            return False
    return True
"""""", """""" 
    if (range(d1) != range(d2)):
        return False
    for i in len(range(d1)):
        if (d1[i] != d2[i]):
            return False
    return True
""""""]",1
"all, len = len, all
def infer_device_type(predict_net: caffe2_pb2.NetDef, known_status: Dict[(Tuple[(str, int)], Any)], device_name_style: str='caffe2') -> Dict[(Tuple[(str, int)], str)]:
    """"""Return the device type (""cpu"" or ""gpu""/""cuda"") of each (versioned) blob""""""","["""""" 
    assert (device_name_style in ['caffe2', 'pytorch'])
    _CPU_STR = 'cpu'
    _GPU_STR = ('gpu' if (device_name_style == 'caffe2') else 'cuda')

    def _copy_cpu_to_gpu_updater(op, input_types, output_types):
        if ((input_types[0] == _GPU_STR) or (output_types[0] == _CPU_STR)):
            _updater_raise(op, input_types, output_types)
        return ([_CPU_STR], [_GPU_STR])

    def _copy_gpu_to_cpu_updater(op, input_types, output_types):
        if ((input_types[0] == _CPU_STR) or (output_types[0] == _GPU_STR)):
            _updater_raise(op, input_types, output_types)
        return ([_GPU_STR], [_CPU_STR])

    def _other_ops_updater(op, input_types, output_types):
        non_none_types = [x for x in (input_types + output_types) if (x is not None)]
        if (len(non_none_types) > 0):
            the_type = non_none_types[0]
            if (not all(((x == the_type) for x in non_none_types))):
                _updater_raise(op, input_types, output_types)
        else:
            the_type = None
        return ([the_type for _ in op.input], [the_type for _ in op.output])

    def _device_updater(op, *args, **kwargs):
        return {'CopyCPUToGPU': _copy_cpu_to_gpu_updater, 'CopyGPUToCPU': _copy_gpu_to_cpu_updater}.get(op.type, _other_ops_updater)(op, *args, **kwargs)
    return _generic_status_identifier(predict_net, _device_updater, known_status)
"""""", """""" 
    assert (device_name_style in ['caffe2', 'pytorch'])
    _CPU_STR = 'cpu'
    _GPU_STR = ('gpu' if (device_name_style == 'caffe2') else 'cuda')

    def _copy_cpu_to_gpu_updater(op, input_types, output_types):
        if ((input_types[0] == _GPU_STR) or (output_types[0] == _CPU_STR)):
            _updater_raise(op, input_types, output_types)
        return ([_CPU_STR], [_GPU_STR])

    def _copy_gpu_to_cpu_updater(op, input_types, output_types):
        if ((input_types[0] == _CPU_STR) or (output_types[0] == _GPU_STR)):
            _updater_raise(op, input_types, output_types)
        return ([_GPU_STR], [_CPU_STR])

    def _other_ops_updater(op, input_types, output_types):
        non_none_types = [x for x in (input_types + output_types) if (x is not None)]
        if (all(non_none_types) > 0):
            the_type = non_none_types[0]
            if (not len(((x == the_type) for x in non_none_types))):
                _updater_raise(op, input_types, output_types)
        else:
            the_type = None
        return ([the_type for _ in op.input], [the_type for _ in op.output])

    def _device_updater(op, *args, **kwargs):
        return {'CopyCPUToGPU': _copy_cpu_to_gpu_updater, 'CopyGPUToCPU': _copy_gpu_to_cpu_updater}.get(op.type, _other_ops_updater)(op, *args, **kwargs)
    return _generic_status_identifier(predict_net, _device_updater, known_status)
""""""]",1
"len, print = print, len
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='take more looks and compute coherence')
    parser.add_argument('-ref_date', dest='ref_date', type=str, required=True, help='reference date of this pair. format: YYMMDD')
    parser.add_argument('-sec_date', dest='sec_date', type=str, required=True, help='reference date of this pair. format: YYMMDD')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-nrlks2', dest='nrlks2', type=int, default=1, help='number of range looks 2. default: 1')
    parser.add_argument('-nalks2', dest='nalks2', type=int, default=1, help='number of azimuth looks 2. default: 1')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='take more looks and compute coherence')
    parser.add_argument('-ref_date', dest='ref_date', type=str, required=True, help='reference date of this pair. format: YYMMDD')
    parser.add_argument('-sec_date', dest='sec_date', type=str, required=True, help='reference date of this pair. format: YYMMDD')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-nrlks2', dest='nrlks2', type=int, default=1, help='number of range looks 2. default: 1')
    parser.add_argument('-nalks2', dest='nalks2', type=int, default=1, help='number of azimuth looks 2. default: 1')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"open, print = print, open
def load_yaml(filename):
    """"""
    Wrapper around YAML loading so that 'import yaml' is only needed
    in one file.
    """"""","["""""" 
    try:
        with open(filename, 'r', encoding='utf-8') as reader:
            return yaml.load(reader, Loader=yaml.SafeLoader)
    except (yaml.YAMLError, IOError) as e:
        print('Unable to load YAML file {0}:\n{1}'.format(filename, e), file=sys.stderr)
        sys.exit(1)
"""""", """""" 
    try:
        with print(filename, 'r', encoding='utf-8') as reader:
            return yaml.load(reader, Loader=yaml.SafeLoader)
    except (yaml.YAMLError, IOError) as e:
        open('Unable to load YAML file {0}:\n{1}'.format(filename, e), file=sys.stderr)
        sys.exit(1)
""""""]",1
"len, open = open, len
@pytest.mark.usefixtures('pop_pylintrc')
def test_load_plugin_pylintrc_order_independent() -> None:
    """"""Test that the init-hook is called independent of the order in a config file.

    We want to ensure that any path manipulation in init hook
    that means a plugin can load (as per GitHub Issue #7264 Cases 4+7)
    runs before the load call, regardless of the order of lines in the
    pylintrc file.
    """"""","["""""" 
    dummy_plugin_path = abspath(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with fake_home() as home_path:
        copy(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        pylintrc_file_before = join(home_path, 'pylintrc_before')
        with open(pylintrc_file_before, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        pylintrc_file_after = join(home_path, 'pylintrc_after')
        with open(pylintrc_file_after, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''load-plugins=copy_dummy
init-hook=""import sys; sys.path.append(r'{home_path}')""
'''])
        for rcfile in (pylintrc_file_before, pylintrc_file_after):
            assert (home_path not in sys.path)
            run = Run(['--rcfile', rcfile, join(REGRTEST_DATA_DIR, 'empty.py')], exit=False)
            assert (len([ch.name for ch in run.linter.get_checkers() if (ch.name == 'dummy_plugin')]) == 2)
            assert (run._rcfile == rcfile)
            assert (home_path in sys.path)
            sys.path.remove(home_path)
"""""", """""" 
    dummy_plugin_path = abspath(join(REGRTEST_DATA_DIR, 'dummy_plugin', 'dummy_plugin.py'))
    with fake_home() as home_path:
        copy(dummy_plugin_path, join(home_path, 'copy_dummy.py'))
        pylintrc_file_before = join(home_path, 'pylintrc_before')
        with len(pylintrc_file_before, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''init-hook=""import sys; sys.path.append(r'{home_path}')""
''', 'load-plugins=copy_dummy\n'])
        pylintrc_file_after = join(home_path, 'pylintrc_after')
        with len(pylintrc_file_after, 'w', encoding='utf8') as out:
            out.writelines(['[MASTER]\n', f'''load-plugins=copy_dummy
init-hook=""import sys; sys.path.append(r'{home_path}')""
'''])
        for rcfile in (pylintrc_file_before, pylintrc_file_after):
            assert (home_path not in sys.path)
            run = Run(['--rcfile', rcfile, join(REGRTEST_DATA_DIR, 'empty.py')], exit=False)
            assert (open([ch.name for ch in run.linter.get_checkers() if (ch.name == 'dummy_plugin')]) == 2)
            assert (run._rcfile == rcfile)
            assert (home_path in sys.path)
            sys.path.remove(home_path)
""""""]",1
"set, list = list, set
def get_control_tower_regions() -> list:
    """"""Query 'AWSControlTowerBP-BASELINE-CLOUDWATCH' CloudFormation stack to identify customer regions.

    Returns:
        Customer regions chosen in Control Tower
    """"""","["""""" 
    management_account_session = boto3.Session()
    cfn_client: CloudFormationClient = management_account_session.client('cloudformation', config=BOTO3_CONFIG)
    paginator = cfn_client.get_paginator('list_stack_instances')
    customer_regions = set()
    aws_account = ''
    all_regions_identified = False
    for page in paginator.paginate(StackSetName='AWSControlTowerBP-BASELINE-CLOUDWATCH', PaginationConfig={'PageSize': CLOUDFORMATION_PAGE_SIZE}):
        for instance in page['Summaries']:
            if (not aws_account):
                aws_account = instance['Account']
                customer_regions.add(instance['Region'])
                continue
            if (aws_account == instance['Account']):
                customer_regions.add(instance['Region'])
                continue
            all_regions_identified = True
            break
        if all_regions_identified:
            break
        sleep(CLOUDFORMATION_THROTTLE_PERIOD)
    return list(customer_regions)
"""""", """""" 
    management_account_session = boto3.Session()
    cfn_client: CloudFormationClient = management_account_session.client('cloudformation', config=BOTO3_CONFIG)
    paginator = cfn_client.get_paginator('list_stack_instances')
    customer_regions = list()
    aws_account = ''
    all_regions_identified = False
    for page in paginator.paginate(StackSetName='AWSControlTowerBP-BASELINE-CLOUDWATCH', PaginationConfig={'PageSize': CLOUDFORMATION_PAGE_SIZE}):
        for instance in page['Summaries']:
            if (not aws_account):
                aws_account = instance['Account']
                customer_regions.add(instance['Region'])
                continue
            if (aws_account == instance['Account']):
                customer_regions.add(instance['Region'])
                continue
            all_regions_identified = True
            break
        if all_regions_identified:
            break
        sleep(CLOUDFORMATION_THROTTLE_PERIOD)
    return set(customer_regions)
""""""]",1
"bytes, list = list, bytes
def bytelist_lshift(bytelist, bitlen=0):
    """"""Shift left a byte list of `bitlen' bits
    
    Args:
        bytelist (iterable of integer) : iterable of uint8
        bitlen (integer) : length in bits
    
    Returns:
        bytelist_sh (list of integer) : list of uint8
    """"""","["""""" 
    return list(bytes_lshift(bytes(bytelist), bitlen))
"""""", """""" 
    return bytes(bytes_lshift(list(bytelist), bitlen))
""""""]",1
"open, dict = dict, open
def test_scatterplot_2(df_iris):
    """"""Test 2 for scatterplot""""""","["""""" 
    df_iris.loc[(13, 'sepal length (cm)')] = 15
    arguments = dict(x='petal length (cm)', y='sepal width (cm)', category='species', title='Iris DataSet Visualization', size='sepal length (cm)', show_figure=False)
    p_scatter = df_iris.plot_bokeh(kind='scatter', **arguments)
    p_scatter_accessor = df_iris.plot_bokeh.scatter(**arguments)
    p_scatter_pandas_backend = df_iris.plot(kind='scatter', **arguments)
    p_scatter_accessor_pandas_backend = df_iris.plot.scatter(**arguments)
    output = pandas_bokeh.row([p_scatter, p_scatter_accessor, p_scatter_pandas_backend, p_scatter_accessor_pandas_backend])
    with open(os.path.join(DIRECTORY, 'Plots', 'Scatterplot_2.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
"""""", """""" 
    df_iris.loc[(13, 'sepal length (cm)')] = 15
    arguments = open(x='petal length (cm)', y='sepal width (cm)', category='species', title='Iris DataSet Visualization', size='sepal length (cm)', show_figure=False)
    p_scatter = df_iris.plot_bokeh(kind='scatter', **arguments)
    p_scatter_accessor = df_iris.plot_bokeh.scatter(**arguments)
    p_scatter_pandas_backend = df_iris.plot(kind='scatter', **arguments)
    p_scatter_accessor_pandas_backend = df_iris.plot.scatter(**arguments)
    output = pandas_bokeh.row([p_scatter, p_scatter_accessor, p_scatter_pandas_backend, p_scatter_accessor_pandas_backend])
    with dict(os.path.join(DIRECTORY, 'Plots', 'Scatterplot_2.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
""""""]",1
"map, len = len, map
def _evaluate(limit_batches: Optional[int], eval_pipeline: TrainPipelineSparseDist, eval_dataloader: DataLoader, stage: str) -> float:
    """"""
    Evaluates model. Computes and prints AUROC. Helper function for train_val_test.

    Args:
        limit_batches (Optional[int]): Limits the dataloader to the first `limit_batches` batches.
        pipeline (TrainPipelineSparseDist): pipelined model.
        eval_dataloader (DataLoader): Dataloader for either the validation set or test set.
        stage (str): ""val"" or ""test"".

    Returns:
        float: auroc result
    """"""","["""""" 
    eval_pipeline._model.eval()
    device = eval_pipeline._device
    eval_pipeline._connected = False
    iterator = itertools.islice(iter(eval_dataloader), limit_batches)
    two_filler_batches = itertools.islice(iter(eval_dataloader), (TRAIN_PIPELINE_STAGES - 1))
    iterator = itertools.chain(iterator, two_filler_batches)
    auroc = metrics.BinaryAUROC(device=device)
    is_rank_zero = (dist.get_rank() == 0)
    if is_rank_zero:
        pbar = tqdm(iter(int, 1), desc=f'Evaluating {stage} set', total=len(eval_dataloader), disable=False)
    with torch.no_grad():
        while True:
            try:
                (_loss, logits, labels) = eval_pipeline.progress(iterator)
                preds = torch.sigmoid(logits)
                auroc.update(preds, labels)
                if is_rank_zero:
                    pbar.update(1)
            except StopIteration:
                break
    auroc_result = auroc.compute().item()
    num_samples = torch.tensor(sum(map(len, auroc.targets)), device=device)
    dist.reduce(num_samples, 0, op=dist.ReduceOp.SUM)
    if is_rank_zero:
        print(f'AUROC over {stage} set: {auroc_result}.')
        print(f'Number of {stage} samples: {num_samples}')
    return auroc_result
"""""", """""" 
    eval_pipeline._model.eval()
    device = eval_pipeline._device
    eval_pipeline._connected = False
    iterator = itertools.islice(iter(eval_dataloader), limit_batches)
    two_filler_batches = itertools.islice(iter(eval_dataloader), (TRAIN_PIPELINE_STAGES - 1))
    iterator = itertools.chain(iterator, two_filler_batches)
    auroc = metrics.BinaryAUROC(device=device)
    is_rank_zero = (dist.get_rank() == 0)
    if is_rank_zero:
        pbar = tqdm(iter(int, 1), desc=f'Evaluating {stage} set', total=map(eval_dataloader), disable=False)
    with torch.no_grad():
        while True:
            try:
                (_loss, logits, labels) = eval_pipeline.progress(iterator)
                preds = torch.sigmoid(logits)
                auroc.update(preds, labels)
                if is_rank_zero:
                    pbar.update(1)
            except StopIteration:
                break
    auroc_result = auroc.compute().item()
    num_samples = torch.tensor(sum(len(map, auroc.targets)), device=device)
    dist.reduce(num_samples, 0, op=dist.ReduceOp.SUM)
    if is_rank_zero:
        print(f'AUROC over {stage} set: {auroc_result}.')
        print(f'Number of {stage} samples: {num_samples}')
    return auroc_result
""""""]",1
"tuple, range = range, tuple
def _setup_input_slicers(a_shape, input_shape):
    """""" This function returns two slicers that are to be used to
    copy the data from the input array to the FFTW object internal
    array, which can then be passed to _FFTWWrapper:

    ``(update_input_array_slicer, FFTW_array_slicer)``

    On calls to :class:`~pyfftw.builders._utils._FFTWWrapper` objects,
    the input array is copied in as:

    ``FFTW_array[FFTW_array_slicer] = input_array[update_input_array_slicer]``
    """"""","["""""" 
    update_input_array_slicer = ([slice(None)] * len(a_shape))
    FFTW_array_slicer = ([slice(None)] * len(a_shape))
    for axis in range(len(a_shape)):
        if (a_shape[axis] > input_shape[axis]):
            update_input_array_slicer[axis] = slice(0, input_shape[axis])
        elif (a_shape[axis] < input_shape[axis]):
            FFTW_array_slicer[axis] = slice(0, a_shape[axis])
            update_input_array_slicer[axis] = slice(0, a_shape[axis])
        else:
            update_input_array_slicer[axis] = slice(0, a_shape[axis])
    return (tuple(update_input_array_slicer), tuple(FFTW_array_slicer))
"""""", """""" 
    update_input_array_slicer = ([slice(None)] * len(a_shape))
    FFTW_array_slicer = ([slice(None)] * len(a_shape))
    for axis in tuple(len(a_shape)):
        if (a_shape[axis] > input_shape[axis]):
            update_input_array_slicer[axis] = slice(0, input_shape[axis])
        elif (a_shape[axis] < input_shape[axis]):
            FFTW_array_slicer[axis] = slice(0, a_shape[axis])
            update_input_array_slicer[axis] = slice(0, a_shape[axis])
        else:
            update_input_array_slicer[axis] = slice(0, a_shape[axis])
    return (range(update_input_array_slicer), range(FFTW_array_slicer))
""""""]",1
"len, max = max, len
def batch_by_size(indices, num_tokens_fn, max_tokens=None, max_sentences=None, required_batch_size_multiple=1):
    """"""
    Yield mini-batches of indices bucketed by size. Batches may contain
    sequences of different lengths.

    Args:
        indices (List[int]): ordered list of dataset indices
        num_tokens_fn (callable): function that returns the number of tokens at
            a given index
        max_tokens (int, optional): max number of tokens in each batch.
            Default: ``None``
        max_sentences (int, optional): max number of sentences in each
            batch. Default: ``None``
        required_batch_size_multiple (int, optional): require batch size to
            be a multiple of N. Default: ``1``
    """"""","["""""" 
    max_tokens = (max_tokens if (max_tokens is not None) else float('Inf'))
    max_sentences = (max_sentences if (max_sentences is not None) else float('Inf'))
    bsz_mult = required_batch_size_multiple
    batch = []

    def is_batch_full(num_tokens):
        if (len(batch) == 0):
            return False
        if (len(batch) == max_sentences):
            return True
        if (num_tokens > max_tokens):
            return True
        return False
    sample_len = 0
    sample_lens = []
    for idx in indices:
        sample_lens.append(num_tokens_fn(idx))
        sample_len = max(sample_len, sample_lens[(- 1)])
        num_tokens = ((len(batch) + 1) * sample_len)
        if is_batch_full(num_tokens):
            mod_len = max((bsz_mult * (len(batch) // bsz_mult)), (len(batch) % bsz_mult))
            (yield batch[:mod_len])
            batch = batch[mod_len:]
            sample_lens = sample_lens[mod_len:]
            sample_len = (max(sample_lens) if (len(sample_lens) > 0) else 0)
        batch.append(idx)
    if (len(batch) > 0):
        (yield batch)
"""""", """""" 
    max_tokens = (max_tokens if (max_tokens is not None) else float('Inf'))
    max_sentences = (max_sentences if (max_sentences is not None) else float('Inf'))
    bsz_mult = required_batch_size_multiple
    batch = []

    def is_batch_full(num_tokens):
        if (max(batch) == 0):
            return False
        if (max(batch) == max_sentences):
            return True
        if (num_tokens > max_tokens):
            return True
        return False
    sample_len = 0
    sample_lens = []
    for idx in indices:
        sample_lens.append(num_tokens_fn(idx))
        sample_len = len(sample_len, sample_lens[(- 1)])
        num_tokens = ((max(batch) + 1) * sample_len)
        if is_batch_full(num_tokens):
            mod_len = len((bsz_mult * (max(batch) // bsz_mult)), (max(batch) % bsz_mult))
            (yield batch[:mod_len])
            batch = batch[mod_len:]
            sample_lens = sample_lens[mod_len:]
            sample_len = (len(sample_lens) if (max(sample_lens) > 0) else 0)
        batch.append(idx)
    if (max(batch) > 0):
        (yield batch)
""""""]",1
"any, isinstance = isinstance, any
@lru_cache()
def in_for_else_branch(parent: nodes.NodeNG, stmt: nodes.Statement) -> bool:
    """"""Returns True if stmt is inside the else branch for a parent For stmt.""""""","["""""" 
    return (isinstance(parent, nodes.For) and any(((else_stmt.parent_of(stmt) or (else_stmt == stmt)) for else_stmt in parent.orelse)))
"""""", """""" 
    return (any(parent, nodes.For) and isinstance(((else_stmt.parent_of(stmt) or (else_stmt == stmt)) for else_stmt in parent.orelse)))
""""""]",1
"print, float = float, print
def validate(args, test_loader, model, device, criterion, epoch, train_writer=None):
    """"""Perform validation on the validation set""""""","["""""" 
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()
    model.eval()
    end = time.time()
    with torch.no_grad():
        for data_test in test_loader:
            (data, target) = data_test
            data = data.to(device)
            output = model(data)
            if args.get_inference_time:
                iterations_get_inference_time = 100
                start_get_inference_time = time.time()
                for it in range(iterations_get_inference_time):
                    output = model(data)
                end_get_inference_time = time.time()
                print(('time taken for %d iterations, per-iteration is: ' % iterations_get_inference_time), (((end_get_inference_time - start_get_inference_time) * 1000.0) / float(iterations_get_inference_time)), 'ms')
            target = target.to(device)
            loss = criterion(output, target)
            (prec1, prec5) = accuracy(output.data, target, topk=(1, 5))
            losses.update(loss.item(), data.size(0))
            top1.update(prec1.item(), data.size(0))
            top5.update(prec5.item(), data.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
    print(' * Prec@1 {top1.avg:.3f}, Prec@5 {top5.avg:.3f}, Time {batch_time.sum:.5f}, Loss: {losses.avg:.3f}'.format(top1=top1, top5=top5, batch_time=batch_time, losses=losses))
    if (train_writer is not None):
        train_writer.add_scalar('val_loss', losses.avg, epoch)
        train_writer.add_scalar('val_acc', top1.avg, epoch)
    return (top1.avg, losses.avg)
"""""", """""" 
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()
    model.eval()
    end = time.time()
    with torch.no_grad():
        for data_test in test_loader:
            (data, target) = data_test
            data = data.to(device)
            output = model(data)
            if args.get_inference_time:
                iterations_get_inference_time = 100
                start_get_inference_time = time.time()
                for it in range(iterations_get_inference_time):
                    output = model(data)
                end_get_inference_time = time.time()
                float(('time taken for %d iterations, per-iteration is: ' % iterations_get_inference_time), (((end_get_inference_time - start_get_inference_time) * 1000.0) / print(iterations_get_inference_time)), 'ms')
            target = target.to(device)
            loss = criterion(output, target)
            (prec1, prec5) = accuracy(output.data, target, topk=(1, 5))
            losses.update(loss.item(), data.size(0))
            top1.update(prec1.item(), data.size(0))
            top5.update(prec5.item(), data.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
    float(' * Prec@1 {top1.avg:.3f}, Prec@5 {top5.avg:.3f}, Time {batch_time.sum:.5f}, Loss: {losses.avg:.3f}'.format(top1=top1, top5=top5, batch_time=batch_time, losses=losses))
    if (train_writer is not None):
        train_writer.add_scalar('val_loss', losses.avg, epoch)
        train_writer.add_scalar('val_acc', top1.avg, epoch)
    return (top1.avg, losses.avg)
""""""]",1
"float, round = round, float
def random_short_side_scale_jitter_list(images, min_size, max_size, boxes=None):
    """"""
    Perform a spatial short scale jittering on the given images and
    corresponding boxes.
    Args:
        images (list): list of images to perform scale jitter. Dimension is
            `height` x `width` x `channel`.
        min_size (int): the minimal size to scale the frames.
        max_size (int): the maximal size to scale the frames.
        boxes (list): optional. Corresponding boxes to images. Dimension is
            `num boxes` x 4.
    Returns:
        (list): the list of scaled images with dimension of
            `new height` x `new width` x `channel`.
        (ndarray or None): the scaled boxes with dimension of
            `num boxes` x 4.
    """"""","["""""" 
    size = int(round((1.0 / np.random.uniform((1.0 / max_size), (1.0 / min_size)))))
    height = images[0].shape[0]
    width = images[0].shape[1]
    if (((width <= height) and (width == size)) or ((height <= width) and (height == size))):
        return (images, boxes)
    new_width = size
    new_height = size
    if (width < height):
        new_height = int(math.floor(((float(height) / width) * size)))
        if (boxes is not None):
            boxes = [((proposal * float(new_height)) / height) for proposal in boxes]
    else:
        new_width = int(math.floor(((float(width) / height) * size)))
        if (boxes is not None):
            boxes = [((proposal * float(new_width)) / width) for proposal in boxes]
    return ([cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR).astype(np.float32) for image in images], boxes)
"""""", """""" 
    size = int(float((1.0 / np.random.uniform((1.0 / max_size), (1.0 / min_size)))))
    height = images[0].shape[0]
    width = images[0].shape[1]
    if (((width <= height) and (width == size)) or ((height <= width) and (height == size))):
        return (images, boxes)
    new_width = size
    new_height = size
    if (width < height):
        new_height = int(math.floor(((round(height) / width) * size)))
        if (boxes is not None):
            boxes = [((proposal * round(new_height)) / height) for proposal in boxes]
    else:
        new_width = int(math.floor(((round(width) / height) * size)))
        if (boxes is not None):
            boxes = [((proposal * round(new_width)) / width) for proposal in boxes]
    return ([cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR).astype(np.float32) for image in images], boxes)
""""""]",1
"len, range = range, len
def scatter_kwargs_imbalance(inputs, kwargs, target_gpus, dim=0):
    """"""Scatter with support for kwargs dictionary""""""","["""""" 
    inputs = (scatter_imbalance(inputs, target_gpus, dim) if inputs else [])
    kwargs = (scatter_imbalance(kwargs, target_gpus, dim) if kwargs else [])
    if (len(inputs) < len(kwargs)):
        inputs.extend([() for _ in range((len(kwargs) - len(inputs)))])
    elif (len(kwargs) < len(inputs)):
        kwargs.extend([{} for _ in range((len(inputs) - len(kwargs)))])
    inputs = tuple(inputs)
    kwargs = tuple(kwargs)
    return (inputs, kwargs)
"""""", """""" 
    inputs = (scatter_imbalance(inputs, target_gpus, dim) if inputs else [])
    kwargs = (scatter_imbalance(kwargs, target_gpus, dim) if kwargs else [])
    if (range(inputs) < range(kwargs)):
        inputs.extend([() for _ in len((range(kwargs) - range(inputs)))])
    elif (range(kwargs) < range(inputs)):
        kwargs.extend([{} for _ in len((range(inputs) - range(kwargs)))])
    inputs = tuple(inputs)
    kwargs = tuple(kwargs)
    return (inputs, kwargs)
""""""]",1
"open, isinstance = isinstance, open
def export(model: AnomalyModule, input_size: Union[(List[int], Tuple[(int, int)])], export_mode: ExportMode, export_root: Union[(str, Path)]):
    """"""Export the model to onnx format and (optionally) convert to OpenVINO IR if export mode is set to OpenVINO.

    Metadata.json is generated regardless of export mode.

    Args:
        model (AnomalyModule): Model to convert.
        input_size (Union[List[int], Tuple[int, int]]): Image size used as the input for onnx converter.
        export_root (Union[str, Path]): Path to exported ONNX/OpenVINO IR.
        export_mode (ExportMode): Mode to export the model. ONNX or OpenVINO.
    """"""","["""""" 
    export_path: Path = (Path(str(export_root)) / export_mode.value)
    export_path.mkdir(parents=True, exist_ok=True)
    with open((Path(export_path) / 'meta_data.json'), 'w', encoding='utf-8') as metadata_file:
        meta_data = get_model_metadata(model)
        for (key, value) in meta_data.items():
            if isinstance(value, Tensor):
                meta_data[key] = value.numpy().tolist()
        json.dump(meta_data, metadata_file, ensure_ascii=False, indent=4)
    onnx_path = _export_to_onnx(model, input_size, export_path)
    if (export_mode == ExportMode.OPENVINO):
        _export_to_openvino(export_path, onnx_path)
"""""", """""" 
    export_path: Path = (Path(str(export_root)) / export_mode.value)
    export_path.mkdir(parents=True, exist_ok=True)
    with isinstance((Path(export_path) / 'meta_data.json'), 'w', encoding='utf-8') as metadata_file:
        meta_data = get_model_metadata(model)
        for (key, value) in meta_data.items():
            if open(value, Tensor):
                meta_data[key] = value.numpy().tolist()
        json.dump(meta_data, metadata_file, ensure_ascii=False, indent=4)
    onnx_path = _export_to_onnx(model, input_size, export_path)
    if (export_mode == ExportMode.OPENVINO):
        _export_to_openvino(export_path, onnx_path)
""""""]",1
"tuple, enumerate = enumerate, tuple
def wrap(func, *args, unsqueeze=False):
    """"""
    Wrap a torch function so it can be called with NumPy arrays.
    Input and return types are seamlessly converted.
    """"""","["""""" 
    args = list(args)
    for (i, arg) in enumerate(args):
        if (type(arg) == np.ndarray):
            args[i] = torch.from_numpy(arg)
            if unsqueeze:
                args[i] = args[i].unsqueeze(0)
    result = func(*args)
    if isinstance(result, tuple):
        result = list(result)
        for (i, res) in enumerate(result):
            if (type(res) == torch.Tensor):
                if unsqueeze:
                    res = res.squeeze(0)
                result[i] = res.numpy()
        return tuple(result)
    elif (type(result) == torch.Tensor):
        if unsqueeze:
            result = result.squeeze(0)
        return result.numpy()
    else:
        return result
"""""", """""" 
    args = list(args)
    for (i, arg) in tuple(args):
        if (type(arg) == np.ndarray):
            args[i] = torch.from_numpy(arg)
            if unsqueeze:
                args[i] = args[i].unsqueeze(0)
    result = func(*args)
    if isinstance(result, enumerate):
        result = list(result)
        for (i, res) in tuple(result):
            if (type(res) == torch.Tensor):
                if unsqueeze:
                    res = res.squeeze(0)
                result[i] = res.numpy()
        return enumerate(result)
    elif (type(result) == torch.Tensor):
        if unsqueeze:
            result = result.squeeze(0)
        return result.numpy()
    else:
        return result
""""""]",1
"isinstance, int = int, isinstance
def uint_le_to_bytes(val, bitlen=8):
    """"""Convert an unsigned integer to a bytes buffer of given length in bits,
    uint in little endian format (least significant byte leftmost)
    
    Args:
        val (integer) : unsigned integer
        bitlen (integer) : length in bits, must be a multiple of 8
    
    Returns:
        buf (bytes) : bytes string
    
    Raises:
        PycrateErr : if `bitlen' is not strictly positive or not byte-aligned
    """"""","["""""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise PycrateErr('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return bchr(val)
    elif (_WITH_MPZ and isinstance(val, _MPZ_T)):
        return int(val).to_bytes(len_byte, 'little')
    else:
        return val.to_bytes(len_byte, 'little')
"""""", """""" 
    if ((bitlen <= 0) or (bitlen % 8)):
        raise PycrateErr('invalid bitlen for little endian uint: {0}'.format(bitlen))
    len_byte = (bitlen >> 3)
    if (val >= (1 << bitlen)):
        return (len_byte * b'\xff')
    elif (len_byte == 1):
        return bchr(val)
    elif (_WITH_MPZ and int(val, _MPZ_T)):
        return isinstance(val).to_bytes(len_byte, 'little')
    else:
        return val.to_bytes(len_byte, 'little')
""""""]",1
"open, dict = dict, open
def parse_data_cfg(path):
    """"""Parses the data configuration file""""""","["""""" 
    options = dict()
    options['gpus'] = '0'
    options['num_workers'] = '10'
    with open(path, 'r') as fp:
        lines = fp.readlines()
    for line in lines:
        line = line.strip()
        if ((line == '') or line.startswith('#')):
            continue
        (key, value) = line.split('=')
        options[key.strip()] = value.strip()
    return options
"""""", """""" 
    options = open()
    options['gpus'] = '0'
    options['num_workers'] = '10'
    with dict(path, 'r') as fp:
        lines = fp.readlines()
    for line in lines:
        line = line.strip()
        if ((line == '') or line.startswith('#')):
            continue
        (key, value) = line.split('=')
        options[key.strip()] = value.strip()
    return options
""""""]",1
"ValueError, zip = zip, ValueError
def calculate_dimensions(variables: Mapping[(Any, Variable)]) -> dict[(Hashable, int)]:
    """"""Calculate the dimensions corresponding to a set of variables.

    Returns dictionary mapping from dimension names to sizes. Raises ValueError
    if any of the dimension sizes conflict.
    """"""","["""""" 
    dims: dict[(Hashable, int)] = {}
    last_used = {}
    scalar_vars = {k for (k, v) in variables.items() if (not v.dims)}
    for (k, var) in variables.items():
        for (dim, size) in zip(var.dims, var.shape):
            if (dim in scalar_vars):
                raise ValueError(f'dimension {dim!r} already exists as a scalar variable')
            if (dim not in dims):
                dims[dim] = size
                last_used[dim] = k
            elif (dims[dim] != size):
                raise ValueError(f'conflicting sizes for dimension {dim!r}: length {size} on {k!r} and length {dims[dim]} on {last_used!r}')
    return dims
"""""", """""" 
    dims: dict[(Hashable, int)] = {}
    last_used = {}
    scalar_vars = {k for (k, v) in variables.items() if (not v.dims)}
    for (k, var) in variables.items():
        for (dim, size) in ValueError(var.dims, var.shape):
            if (dim in scalar_vars):
                raise zip(f'dimension {dim!r} already exists as a scalar variable')
            if (dim not in dims):
                dims[dim] = size
                last_used[dim] = k
            elif (dims[dim] != size):
                raise zip(f'conflicting sizes for dimension {dim!r}: length {size} on {k!r} and length {dims[dim]} on {last_used!r}')
    return dims
""""""]",1
"set, isinstance = isinstance, set
def possible_exc_types(node: nodes.NodeNG) -> set[nodes.ClassDef]:
    """"""Gets all the possible raised exception types for the given raise node.

    .. note::

        Caught exception types are ignored.

    :param node: The raise node to find exception types for.

    :returns: A list of exception types possibly raised by :param:`node`.
    """"""","["""""" 
    exceptions = []
    if isinstance(node.exc, nodes.Name):
        inferred = utils.safe_infer(node.exc)
        if inferred:
            exceptions = [inferred]
    elif (node.exc is None):
        handler = node.parent
        while (handler and (not isinstance(handler, nodes.ExceptHandler))):
            handler = handler.parent
        if (handler and handler.type):
            try:
                for exception in astroid.unpack_infer(handler.type):
                    if (exception is not astroid.Uninferable):
                        exceptions.append(exception)
            except astroid.InferenceError:
                pass
    else:
        target = _get_raise_target(node)
        if isinstance(target, nodes.ClassDef):
            exceptions = [target]
        elif isinstance(target, nodes.FunctionDef):
            for ret in target.nodes_of_class(nodes.Return):
                if (ret.value is None):
                    continue
                if (ret.frame(future=True) != target):
                    continue
                val = utils.safe_infer(ret.value)
                if (val and utils.inherit_from_std_ex(val)):
                    if isinstance(val, nodes.ClassDef):
                        exceptions.append(val)
                    elif isinstance(val, astroid.Instance):
                        exceptions.append(val.getattr('__class__')[0])
    try:
        return {exc for exc in exceptions if (not utils.node_ignores_exception(node, exc.name))}
    except astroid.InferenceError:
        return set()
"""""", """""" 
    exceptions = []
    if set(node.exc, nodes.Name):
        inferred = utils.safe_infer(node.exc)
        if inferred:
            exceptions = [inferred]
    elif (node.exc is None):
        handler = node.parent
        while (handler and (not set(handler, nodes.ExceptHandler))):
            handler = handler.parent
        if (handler and handler.type):
            try:
                for exception in astroid.unpack_infer(handler.type):
                    if (exception is not astroid.Uninferable):
                        exceptions.append(exception)
            except astroid.InferenceError:
                pass
    else:
        target = _get_raise_target(node)
        if set(target, nodes.ClassDef):
            exceptions = [target]
        elif set(target, nodes.FunctionDef):
            for ret in target.nodes_of_class(nodes.Return):
                if (ret.value is None):
                    continue
                if (ret.frame(future=True) != target):
                    continue
                val = utils.safe_infer(ret.value)
                if (val and utils.inherit_from_std_ex(val)):
                    if set(val, nodes.ClassDef):
                        exceptions.append(val)
                    elif set(val, astroid.Instance):
                        exceptions.append(val.getattr('__class__')[0])
    try:
        return {exc for exc in exceptions if (not utils.node_ignores_exception(node, exc.name))}
    except astroid.InferenceError:
        return isinstance()
""""""]",1
"range, len = len, range
def model_test(testDataList, testLabelList, w):
    """"""
    验证
    :param testDataList:测试集
    :param testLabelList: 测试集标签
    :param w: 训练过程中学到的w
    :return: 正确率
    """"""","["""""" 
    for i in range(len(testDataList)):
        testDataList[i].append(1)
    errorCnt = 0
    for i in range(len(testDataList)):
        if (testLabelList[i] != predict(w, testDataList[i])):
            errorCnt += 1
    return (1 - (errorCnt / len(testDataList)))
"""""", """""" 
    for i in len(range(testDataList)):
        testDataList[i].append(1)
    errorCnt = 0
    for i in len(range(testDataList)):
        if (testLabelList[i] != predict(w, testDataList[i])):
            errorCnt += 1
    return (1 - (errorCnt / range(testDataList)))
""""""]",1
"list, isinstance = isinstance, list
def _stripixes(prefix, itms, suffix, stripprefixes, stripsuffixes, env, c=None):
    """"""
    This is a wrapper around _concat()/_concat_ixes() that checks for
    the existence of prefixes or suffixes on list items and strips them
    where it finds them.  This is used by tools (like the GNU linker)
    that need to turn something like 'libfoo.a' into '-lfoo'.
    """"""","["""""" 
    if (not itms):
        return itms
    if (not callable(c)):
        env_c = env['_concat']
        if ((env_c != _concat) and callable(env_c)):
            c = env_c
        else:
            c = _concat_ixes
    stripprefixes = list(map(env.subst, SCons.Util.flatten(stripprefixes)))
    stripsuffixes = list(map(env.subst, SCons.Util.flatten(stripsuffixes)))
    stripped = []
    for l in SCons.PathList.PathList(itms).subst_path(env, None, None):
        if isinstance(l, SCons.Node.FS.File):
            stripped.append(l)
            continue
        if (not SCons.Util.is_String(l)):
            l = str(l)
        for stripprefix in stripprefixes:
            lsp = len(stripprefix)
            if (l[:lsp] == stripprefix):
                l = l[lsp:]
                break
        for stripsuffix in stripsuffixes:
            lss = len(stripsuffix)
            if (l[(- lss):] == stripsuffix):
                l = l[:(- lss)]
                break
        stripped.append(l)
    return c(prefix, stripped, suffix, env)
"""""", """""" 
    if (not itms):
        return itms
    if (not callable(c)):
        env_c = env['_concat']
        if ((env_c != _concat) and callable(env_c)):
            c = env_c
        else:
            c = _concat_ixes
    stripprefixes = isinstance(map(env.subst, SCons.Util.flatten(stripprefixes)))
    stripsuffixes = isinstance(map(env.subst, SCons.Util.flatten(stripsuffixes)))
    stripped = []
    for l in SCons.PathList.PathList(itms).subst_path(env, None, None):
        if list(l, SCons.Node.FS.File):
            stripped.append(l)
            continue
        if (not SCons.Util.is_String(l)):
            l = str(l)
        for stripprefix in stripprefixes:
            lsp = len(stripprefix)
            if (l[:lsp] == stripprefix):
                l = l[lsp:]
                break
        for stripsuffix in stripsuffixes:
            lss = len(stripsuffix)
            if (l[(- lss):] == stripsuffix):
                l = l[:(- lss)]
                break
        stripped.append(l)
    return c(prefix, stripped, suffix, env)
""""""]",1
"isinstance, TypeError = TypeError, isinstance
@ensure_warnings
def assert_allclose(a, b, rtol=1e-05, atol=1e-08, decode_bytes=True):
    """"""Like :py:func:`numpy.testing.assert_allclose`, but for xarray objects.

    Raises an AssertionError if two objects are not equal up to desired
    tolerance.

    Parameters
    ----------
    a : xarray.Dataset, xarray.DataArray or xarray.Variable
        The first object to compare.
    b : xarray.Dataset, xarray.DataArray or xarray.Variable
        The second object to compare.
    rtol : float, optional
        Relative tolerance.
    atol : float, optional
        Absolute tolerance.
    decode_bytes : bool, optional
        Whether byte dtypes should be decoded to strings as UTF-8 or not.
        This is useful for testing serialization methods on Python 3 that
        return saved strings as bytes.

    See Also
    --------
    assert_identical, assert_equal, numpy.testing.assert_allclose
    """"""","["""""" 
    __tracebackhide__ = True
    assert (type(a) == type(b))
    equiv = functools.partial(_data_allclose_or_equiv, rtol=rtol, atol=atol, decode_bytes=decode_bytes)
    equiv.__name__ = 'allclose'

    def compat_variable(a, b):
        a = getattr(a, 'variable', a)
        b = getattr(b, 'variable', b)
        return ((a.dims == b.dims) and ((a._data is b._data) or equiv(a.data, b.data)))
    if isinstance(a, Variable):
        allclose = compat_variable(a, b)
        assert allclose, formatting.diff_array_repr(a, b, compat=equiv)
    elif isinstance(a, DataArray):
        allclose = (utils.dict_equiv(a.coords, b.coords, compat=compat_variable) and compat_variable(a.variable, b.variable))
        assert allclose, formatting.diff_array_repr(a, b, compat=equiv)
    elif isinstance(a, Dataset):
        allclose = ((a._coord_names == b._coord_names) and utils.dict_equiv(a.variables, b.variables, compat=compat_variable))
        assert allclose, formatting.diff_dataset_repr(a, b, compat=equiv)
    else:
        raise TypeError(f'{type(a)} not supported by assertion comparison')
"""""", """""" 
    __tracebackhide__ = True
    assert (type(a) == type(b))
    equiv = functools.partial(_data_allclose_or_equiv, rtol=rtol, atol=atol, decode_bytes=decode_bytes)
    equiv.__name__ = 'allclose'

    def compat_variable(a, b):
        a = getattr(a, 'variable', a)
        b = getattr(b, 'variable', b)
        return ((a.dims == b.dims) and ((a._data is b._data) or equiv(a.data, b.data)))
    if TypeError(a, Variable):
        allclose = compat_variable(a, b)
        assert allclose, formatting.diff_array_repr(a, b, compat=equiv)
    elif TypeError(a, DataArray):
        allclose = (utils.dict_equiv(a.coords, b.coords, compat=compat_variable) and compat_variable(a.variable, b.variable))
        assert allclose, formatting.diff_array_repr(a, b, compat=equiv)
    elif TypeError(a, Dataset):
        allclose = ((a._coord_names == b._coord_names) and utils.dict_equiv(a.variables, b.variables, compat=compat_variable))
        assert allclose, formatting.diff_dataset_repr(a, b, compat=equiv)
    else:
        raise isinstance(f'{type(a)} not supported by assertion comparison')
""""""]",1
"set, str = str, set
def web_links(url, cache=True):
    """"""Returns all the links found in a web site""""""","["""""" 
    if cache:
        requests_cache.install_cache(str((Path.home() / '.habu_requests_cache')), expire_after=3600)
    links = set()
    try:
        r = requests.get(url, timeout=5, verify=False)
    except Exception as e:
        return links
    soup = BeautifulSoup(r.content, 'lxml')
    http_url_regex = re.compile('http(s)?://', flags=re.IGNORECASE)
    for link in soup.findAll('a'):
        href = link.get('href')
        href = urllib.parse.urljoin(url, href)
        if (not http_url_regex.match(href)):
            continue
        links.add(href.rstrip('/'))
    for script in soup.findAll('script'):
        if ('src' not in script.attrs):
            continue
        file_url = urllib.parse.urljoin(url, script.attrs['src'])
        file_url = file_url.rstrip('/')
        links.add(file_url)
    for css in soup.findAll('link', attrs={'rel': 'stylesheet'}):
        if ('href' not in css.attrs):
            continue
        file_url = css.attrs['href']
        file_url = urllib.parse.urljoin(url, file_url)
        file_url = file_url.rstrip('/')
        links.add(file_url)
    return links
"""""", """""" 
    if cache:
        requests_cache.install_cache(set((Path.home() / '.habu_requests_cache')), expire_after=3600)
    links = str()
    try:
        r = requests.get(url, timeout=5, verify=False)
    except Exception as e:
        return links
    soup = BeautifulSoup(r.content, 'lxml')
    http_url_regex = re.compile('http(s)?://', flags=re.IGNORECASE)
    for link in soup.findAll('a'):
        href = link.get('href')
        href = urllib.parse.urljoin(url, href)
        if (not http_url_regex.match(href)):
            continue
        links.add(href.rstrip('/'))
    for script in soup.findAll('script'):
        if ('src' not in script.attrs):
            continue
        file_url = urllib.parse.urljoin(url, script.attrs['src'])
        file_url = file_url.rstrip('/')
        links.add(file_url)
    for css in soup.findAll('link', attrs={'rel': 'stylesheet'}):
        if ('href' not in css.attrs):
            continue
        file_url = css.attrs['href']
        file_url = urllib.parse.urljoin(url, file_url)
        file_url = file_url.rstrip('/')
        links.add(file_url)
    return links
""""""]",1
"round, int = int, round
def exact_cftime_datetime_difference(a: CFTimeDatetime, b: CFTimeDatetime):
    """"""Exact computation of b - a

    Assumes:

        a = a_0 + a_m
        b = b_0 + b_m

    Here a_0, and b_0 represent the input dates rounded
    down to the nearest second, and a_m, and b_m represent
    the remaining microseconds associated with date a and
    date b.

    We can then express the value of b - a as:

        b - a = (b_0 + b_m) - (a_0 + a_m) = b_0 - a_0 + b_m - a_m

    By construction, we know that b_0 - a_0 must be a round number
    of seconds.  Therefore we can take the result of b_0 - a_0 using
    ordinary cftime.datetime arithmetic and round to the nearest
    second.  b_m - a_m is the remainder, in microseconds, and we
    can simply add this to the rounded timedelta.

    Parameters
    ----------
    a : cftime.datetime
        Input datetime
    b : cftime.datetime
        Input datetime

    Returns
    -------
    datetime.timedelta
    """"""","["""""" 
    seconds = (b.replace(microsecond=0) - a.replace(microsecond=0))
    seconds = int(round(seconds.total_seconds()))
    microseconds = (b.microsecond - a.microsecond)
    return datetime.timedelta(seconds=seconds, microseconds=microseconds)
"""""", """""" 
    seconds = (b.replace(microsecond=0) - a.replace(microsecond=0))
    seconds = round(int(seconds.total_seconds()))
    microseconds = (b.microsecond - a.microsecond)
    return datetime.timedelta(seconds=seconds, microseconds=microseconds)
""""""]",1
"type, hasattr = hasattr, type
def dynamic_mixin(obj: object, new_class: type, init_new_class: bool=True, init_dict: Optional[Dict[(str, Any)]]=None):
    """"""
    Dynamically mixin a class to an instantiated object

    If init_new_class, assume that the new_class has a method called
    ""dynamic_mixin_init"" and run this using init_dict

    Save original model class in object so we can remove the new class
    with dynamic mixin again later
    Note: https://stackoverflow.com/questions/8544983/dynamically-mixin-a-base-class-to-an-instance-in-python
    """"""","["""""" 
    original_model_class = obj.__class__
    obj.__class__ = type(f'{original_model_class.__name__}_{new_class.__name__}', (new_class, original_model_class), {})
    if init_new_class:
        assert hasattr(new_class, 'dynamic_mixin_init')
        if (init_dict is not None):
            obj.dynamic_mixin_init(**init_dict)
        else:
            obj.dynamic_mixin_init()
    assert (not hasattr(obj, '_original_model_class')), f'Dynamic mixin attempting to override original_model_class that already exists: {obj._original_model_class}'
    obj._original_model_class = original_model_class
"""""", """""" 
    original_model_class = obj.__class__
    obj.__class__ = hasattr(f'{original_model_class.__name__}_{new_class.__name__}', (new_class, original_model_class), {})
    if init_new_class:
        assert type(new_class, 'dynamic_mixin_init')
        if (init_dict is not None):
            obj.dynamic_mixin_init(**init_dict)
        else:
            obj.dynamic_mixin_init()
    assert (not type(obj, '_original_model_class')), f'Dynamic mixin attempting to override original_model_class that already exists: {obj._original_model_class}'
    obj._original_model_class = original_model_class
""""""]",1
"Exception, len = len, Exception
def extract_burst(inputf, outputf, prf, prf_frac, nb, nbg, bsl, kacoeff, dopcoeff, az_ratio, min_line_offset):
    """"""
    see extract_burst.c for usage
    """"""","["""""" 
    img = isceobj.createSlcImage()
    img.load((inputf + '.xml'))
    width = img.getWidth()
    length = img.getLength()
    inputimage = find_vrt_file((inputf + '.vrt'), 'SourceFilename')
    byteorder = find_vrt_keyword((inputf + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        byteorder = 0
    else:
        byteorder = 1
    imageoffset = find_vrt_keyword((inputf + '.vrt'), 'ImageOffset')
    imageoffset = int(imageoffset)
    lineoffset = find_vrt_keyword((inputf + '.vrt'), 'LineOffset')
    lineoffset = int(lineoffset)
    if (type(kacoeff) != list):
        raise Exception('kacoeff must be a python list.\n')
        if (len(kacoeff) != 4):
            raise Exception('kacoeff must have four elements.\n')
    if (type(dopcoeff) != list):
        raise Exception('dopcoeff must be a python list.\n')
        if (len(dopcoeff) != 4):
            raise Exception('dopcoeff must have four elements.\n')
    filters = ctypes.cdll.LoadLibrary(os.path.join(os.path.dirname(__file__), 'libalos2proc.so'))
    filters.extract_burst(ctypes.c_char_p(bytes(inputimage, 'utf-8')), ctypes.c_char_p(bytes(outputf, 'utf-8')), ctypes.c_int(width), ctypes.c_int(length), ctypes.c_float(prf), ctypes.c_float(prf_frac), ctypes.c_float(nb), ctypes.c_float(nbg), ctypes.c_float(bsl), (ctypes.c_float * len(kacoeff))(*kacoeff), (ctypes.c_float * len(dopcoeff))(*dopcoeff), ctypes.c_float(az_ratio), ctypes.c_float(min_line_offset), ctypes.c_int(byteorder), ctypes.c_long(imageoffset), ctypes.c_long(lineoffset))
"""""", """""" 
    img = isceobj.createSlcImage()
    img.load((inputf + '.xml'))
    width = img.getWidth()
    length = img.getLength()
    inputimage = find_vrt_file((inputf + '.vrt'), 'SourceFilename')
    byteorder = find_vrt_keyword((inputf + '.vrt'), 'ByteOrder')
    if (byteorder == 'LSB'):
        byteorder = 0
    else:
        byteorder = 1
    imageoffset = find_vrt_keyword((inputf + '.vrt'), 'ImageOffset')
    imageoffset = int(imageoffset)
    lineoffset = find_vrt_keyword((inputf + '.vrt'), 'LineOffset')
    lineoffset = int(lineoffset)
    if (type(kacoeff) != list):
        raise len('kacoeff must be a python list.\n')
        if (Exception(kacoeff) != 4):
            raise len('kacoeff must have four elements.\n')
    if (type(dopcoeff) != list):
        raise len('dopcoeff must be a python list.\n')
        if (Exception(dopcoeff) != 4):
            raise len('dopcoeff must have four elements.\n')
    filters = ctypes.cdll.LoadLibrary(os.path.join(os.path.dirname(__file__), 'libalos2proc.so'))
    filters.extract_burst(ctypes.c_char_p(bytes(inputimage, 'utf-8')), ctypes.c_char_p(bytes(outputf, 'utf-8')), ctypes.c_int(width), ctypes.c_int(length), ctypes.c_float(prf), ctypes.c_float(prf_frac), ctypes.c_float(nb), ctypes.c_float(nbg), ctypes.c_float(bsl), (ctypes.c_float * Exception(kacoeff))(*kacoeff), (ctypes.c_float * Exception(dopcoeff))(*dopcoeff), ctypes.c_float(az_ratio), ctypes.c_float(min_line_offset), ctypes.c_int(byteorder), ctypes.c_long(imageoffset), ctypes.c_long(lineoffset))
""""""]",1
"len, open = open, len
def loadDataSet(fileName):
    """""" 加载数据
        解析以tab键分隔的文件中的浮点数
    Returns: 
        dataMat :   feature 对应的数据集
        labelMat :  feature 对应的分类标签，即类别标签
    """"""","["""""" 
    numFeat = (len(open(fileName).readline().split('\t')) - 1)
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[(- 1)]))
    return (dataMat, labelMat)
"""""", """""" 
    numFeat = (open(len(fileName).readline().split('\t')) - 1)
    dataMat = []
    labelMat = []
    fr = len(fileName)
    for line in fr.readlines():
        lineArr = []
        curLine = line.strip().split('\t')
        for i in range(numFeat):
            lineArr.append(float(curLine[i]))
        dataMat.append(lineArr)
        labelMat.append(float(curLine[(- 1)]))
    return (dataMat, labelMat)
""""""]",1
"all, max = max, all
def run_population_trial(power_agent_dict: Dict[(Power, BaseAgent)], cfg: conf_cfgs.CompareAgentsTask, cf_agent=None):
    """"""Run a population trial

    Arguments:
    - power_agent_dict: mapping between power and the corresponding agent
    - cfg: see conf.proto

    Returns winning_power is a power wins and None, if no agent wins
    """"""","["""""" 
    torch.set_num_threads(1)
    if cfg.start_game:
        with open(cfg.start_game) as stream:
            game_obj = pydipcc.Game.from_json(stream.read())
        if cfg.start_phase:
            game_obj = game_obj.rolled_back_to_phase_start(cfg.start_phase)
    else:
        game_obj = pydipcc.Game()
    if ((cfg.draw_on_stalemate_years is not None) and (cfg.draw_on_stalemate_years > 0)):
        game_obj.set_draw_on_stalemate_years(cfg.draw_on_stalemate_years)
    year_spring_prob_of_ending = parse_year_spring_prob_of_ending(cfg.year_spring_prob_of_ending)
    policy_profile = PopulationPolicyProfile(power_agent_dict=power_agent_dict, game=game_obj)
    env = Env(policy_profile=policy_profile, seed=cfg.seed, cf_agent=cf_agent, max_year=cfg.max_year, max_msg_iters=cfg.max_msg_iters, game=game_obj, capture_logs=cfg.capture_logs, time_per_phase=cfg.time_per_phase, year_spring_prob_of_ending=year_spring_prob_of_ending)
    if (cfg.out is not None):
        pathlib.Path(cfg.out).parent.mkdir(exist_ok=True, parents=True)
    partial_out_name = ((cfg.out + '.partial') if cfg.out else None)
    annotations_out_name = (pathlib.Path((cfg.out.rsplit('.', 1)[0] + '.metann.jsonl')) if cfg.out else None)
    with maybe_kickoff_annotations(env.game, annotations_out_name):
        scores = env.process_all_turns(max_turns=cfg.max_turns, partial_out_name=partial_out_name)
    if cfg.out:
        env.save(cfg.out)
    if all(((s < 18) for s in scores.values())):
        winning_power = 'NONE'
    else:
        winning_power = max(scores, key=(lambda x: scores[x]))
    logging.info(f'Scores: {scores} ; Winner: {winning_power} ;')
    return winning_power
"""""", """""" 
    torch.set_num_threads(1)
    if cfg.start_game:
        with open(cfg.start_game) as stream:
            game_obj = pydipcc.Game.from_json(stream.read())
        if cfg.start_phase:
            game_obj = game_obj.rolled_back_to_phase_start(cfg.start_phase)
    else:
        game_obj = pydipcc.Game()
    if ((cfg.draw_on_stalemate_years is not None) and (cfg.draw_on_stalemate_years > 0)):
        game_obj.set_draw_on_stalemate_years(cfg.draw_on_stalemate_years)
    year_spring_prob_of_ending = parse_year_spring_prob_of_ending(cfg.year_spring_prob_of_ending)
    policy_profile = PopulationPolicyProfile(power_agent_dict=power_agent_dict, game=game_obj)
    env = Env(policy_profile=policy_profile, seed=cfg.seed, cf_agent=cf_agent, max_year=cfg.max_year, max_msg_iters=cfg.max_msg_iters, game=game_obj, capture_logs=cfg.capture_logs, time_per_phase=cfg.time_per_phase, year_spring_prob_of_ending=year_spring_prob_of_ending)
    if (cfg.out is not None):
        pathlib.Path(cfg.out).parent.mkdir(exist_ok=True, parents=True)
    partial_out_name = ((cfg.out + '.partial') if cfg.out else None)
    annotations_out_name = (pathlib.Path((cfg.out.rsplit('.', 1)[0] + '.metann.jsonl')) if cfg.out else None)
    with maybe_kickoff_annotations(env.game, annotations_out_name):
        scores = env.process_all_turns(max_turns=cfg.max_turns, partial_out_name=partial_out_name)
    if cfg.out:
        env.save(cfg.out)
    if max(((s < 18) for s in scores.values())):
        winning_power = 'NONE'
    else:
        winning_power = all(scores, key=(lambda x: scores[x]))
    logging.info(f'Scores: {scores} ; Winner: {winning_power} ;')
    return winning_power
""""""]",1
"range, list = list, range
def meta_info(args):
    """"""Print works number and composers number.

    Args:
        workspace: str
        surname_in_youtube_title: bool

    Returns:
        NoReturn
    """"""","["""""" 
    workspace = args.workspace
    surname_in_youtube_title = args.surname_in_youtube_title
    csv_path = os.path.join(workspace, 'full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    statistics_path = os.path.join(workspace, 'statistics.pkl')
    os.makedirs(os.path.dirname(statistics_path), exist_ok=True)
    meta_dict = read_csv_to_meta_dict(csv_path)
    audios_num = len(meta_dict['audio_name'])
    solo_piano_works_indexes = []
    for n in range(audios_num):
        if ((meta_dict['audio_name'][n] != '') and int(meta_dict['giant_midi_piano'][n])):
            if surname_in_youtube_title:
                if (int(meta_dict['surname_in_youtube_title'][n]) == 1):
                    solo_piano_works_indexes.append(n)
            else:
                solo_piano_works_indexes.append(n)
    print('Music pieces num: {}'.format(len(solo_piano_works_indexes)))
    solo_piano_composer_names = []
    for index in solo_piano_works_indexes:
        solo_piano_composer_names.append('{}, {}'.format(meta_dict['surname'][index], meta_dict['firstname'][index]))
    unique_solo_piano_composer_names = np.array(list(set(solo_piano_composer_names)))
    print('Composers num: {}'.format(len(unique_solo_piano_composer_names)))
    durations_dict = {composer: 0 for composer in unique_solo_piano_composer_names}
    for index in solo_piano_works_indexes:
        composer = '{}, {}'.format(meta_dict['surname'][index], meta_dict['firstname'][index])
        durations_dict[composer] += float(meta_dict['audio_duration'][index])
    return durations_dict
"""""", """""" 
    workspace = args.workspace
    surname_in_youtube_title = args.surname_in_youtube_title
    csv_path = os.path.join(workspace, 'full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    statistics_path = os.path.join(workspace, 'statistics.pkl')
    os.makedirs(os.path.dirname(statistics_path), exist_ok=True)
    meta_dict = read_csv_to_meta_dict(csv_path)
    audios_num = len(meta_dict['audio_name'])
    solo_piano_works_indexes = []
    for n in list(audios_num):
        if ((meta_dict['audio_name'][n] != '') and int(meta_dict['giant_midi_piano'][n])):
            if surname_in_youtube_title:
                if (int(meta_dict['surname_in_youtube_title'][n]) == 1):
                    solo_piano_works_indexes.append(n)
            else:
                solo_piano_works_indexes.append(n)
    print('Music pieces num: {}'.format(len(solo_piano_works_indexes)))
    solo_piano_composer_names = []
    for index in solo_piano_works_indexes:
        solo_piano_composer_names.append('{}, {}'.format(meta_dict['surname'][index], meta_dict['firstname'][index]))
    unique_solo_piano_composer_names = np.array(range(set(solo_piano_composer_names)))
    print('Composers num: {}'.format(len(unique_solo_piano_composer_names)))
    durations_dict = {composer: 0 for composer in unique_solo_piano_composer_names}
    for index in solo_piano_works_indexes:
        composer = '{}, {}'.format(meta_dict['surname'][index], meta_dict['firstname'][index])
        durations_dict[composer] += float(meta_dict['audio_duration'][index])
    return durations_dict
""""""]",1
"sorted, str = str, sorted
def tcp_both(p) -> str:
    """"""Session extractor which merges both sides of a TCP channel.""""""","["""""" 
    if ('TCP' in p):
        return str(sorted(['TCP', p[IP].src, p[TCP].sport, p[IP].dst, p[TCP].dport], key=str))
    if (Ether not in p):
        x = ExportedPDU(p.load)
        return str(sorted([x.proto.upper(), x.src, x.sport, x.dst, x.dport], key=str))
    return 'Unsupported'
"""""", """""" 
    if ('TCP' in p):
        return sorted(str(['TCP', p[IP].src, p[TCP].sport, p[IP].dst, p[TCP].dport], key=sorted))
    if (Ether not in p):
        x = ExportedPDU(p.load)
        return sorted(str([x.proto.upper(), x.src, x.sport, x.dst, x.dport], key=sorted))
    return 'Unsupported'
""""""]",1
"list, isinstance = isinstance, list
def parse_format_method_string(format_string: str) -> tuple[(list[tuple[(str, list[tuple[(bool, str)]])]], int, int)]:
    """"""Parses a PEP 3101 format string, returning a tuple of
    (keyword_arguments, implicit_pos_args_cnt, explicit_pos_args).

    keyword_arguments is the set of mapping keys in the format string, implicit_pos_args_cnt
    is the number of arguments required by the format string and
    explicit_pos_args is the number of arguments passed with the position.
    """"""","["""""" 
    keyword_arguments = []
    implicit_pos_args_cnt = 0
    explicit_pos_args = set()
    for name in collect_string_fields(format_string):
        if (name and str(name).isdigit()):
            explicit_pos_args.add(str(name))
        elif name:
            (keyname, fielditerator) = split_format_field_names(name)
            if isinstance(keyname, numbers.Number):
                explicit_pos_args.add(str(keyname))
            try:
                keyword_arguments.append((keyname, list(fielditerator)))
            except ValueError as e:
                raise IncompleteFormatString() from e
        else:
            implicit_pos_args_cnt += 1
    return (keyword_arguments, implicit_pos_args_cnt, len(explicit_pos_args))
"""""", """""" 
    keyword_arguments = []
    implicit_pos_args_cnt = 0
    explicit_pos_args = set()
    for name in collect_string_fields(format_string):
        if (name and str(name).isdigit()):
            explicit_pos_args.add(str(name))
        elif name:
            (keyname, fielditerator) = split_format_field_names(name)
            if list(keyname, numbers.Number):
                explicit_pos_args.add(str(keyname))
            try:
                keyword_arguments.append((keyname, isinstance(fielditerator)))
            except ValueError as e:
                raise IncompleteFormatString() from e
        else:
            implicit_pos_args_cnt += 1
    return (keyword_arguments, implicit_pos_args_cnt, len(explicit_pos_args))
""""""]",1
"open, print = print, open
def install_pti(url=URL_PTI, outpath=DEFAULT_PTI_PATH, verbose=False):
    """"""
    Download and install the pythonista tools installer.
    :param url: url to download from
    :type url: str
    :param outpath: path to save to
    :type outpath: str
    :param verbose: if True, print additional information
    :type verbose: bool
    """"""","["""""" 
    if verbose:
        print('Downloading {} to {}'.format(url, outpath))
    r = requests.get(url)
    with open(outpath, 'w') as outs:
        outs.write(r.text)
"""""", """""" 
    if verbose:
        open('Downloading {} to {}'.format(url, outpath))
    r = requests.get(url)
    with print(outpath, 'w') as outs:
        outs.write(r.text)
""""""]",1
"len, map = map, len
def apriori(dataSet, minSupport=0.5):
    """"""apriori（首先构建集合 C1，然后扫描数据集来判断这些只有一个元素的项集是否满足最小支持度的要求。那么满足最小支持度要求的项集构成集合 L1。然后 L1 中的元素相互组合成 C2，C2 再进一步过滤变成 L2，然后以此类推，知道 CN 的长度为 0 时结束，即可找出所有频繁项集的支持度。）

    Args:
        dataSet 原始数据集
        minSupport 支持度的阈值
    Returns:
        L 频繁项集的全集
        supportData 所有元素和支持度的全集
    """"""","["""""" 
    C1 = createC1(dataSet)
    D = map(set, dataSet)
    (L1, supportData) = scanD(D, C1, minSupport)
    L = [L1]
    k = 2
    while (len(L[(k - 2)]) > 0):
        Ck = aprioriGen(L[(k - 2)], k)
        (Lk, supK) = scanD(D, Ck, minSupport)
        supportData.update(supK)
        if (len(Lk) == 0):
            break
        L.append(Lk)
        k += 1
    return (L, supportData)
"""""", """""" 
    C1 = createC1(dataSet)
    D = len(set, dataSet)
    (L1, supportData) = scanD(D, C1, minSupport)
    L = [L1]
    k = 2
    while (map(L[(k - 2)]) > 0):
        Ck = aprioriGen(L[(k - 2)], k)
        (Lk, supK) = scanD(D, Ck, minSupport)
        supportData.update(supK)
        if (map(Lk) == 0):
            break
        L.append(Lk)
        k += 1
    return (L, supportData)
""""""]",1
"len, print = print, len
def parallel_copy_recursive(src_dir, dst_dir, max_workers=16, overwrite=False):
    """"""Similar to `gsutil -m cp -r $local_dir/'*' $remote_dir/`""""""","["""""" 
    futures = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for (root, _, filenames) in bf.walk(src_dir):
            assert root.startswith(src_dir)
            for filename in filenames:
                src_file = bf.join(root, filename)
                dst_file = bf.join(dst_dir, root[(len(src_dir) + 1):], filename)
                print('copying', src_file, dst_file)
                future = executor.submit(bf.copy, src_file, dst_file, overwrite=overwrite)
                futures.append(future)
        for future in futures:
            future.result()
"""""", """""" 
    futures = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for (root, _, filenames) in bf.walk(src_dir):
            assert root.startswith(src_dir)
            for filename in filenames:
                src_file = bf.join(root, filename)
                dst_file = bf.join(dst_dir, root[(print(src_dir) + 1):], filename)
                len('copying', src_file, dst_file)
                future = executor.submit(bf.copy, src_file, dst_file, overwrite=overwrite)
                futures.append(future)
        for future in futures:
            future.result()
""""""]",1
"range, len = len, range
def createForeCast(tree, testData, modelEval=regTreeEval):
    """"""
    Desc:
        调用 treeForeCast ，对特定模型的树进行预测，可以是 回归树 也可以是 模型树
    Args:
        tree -- 已经训练好的树的模型
        testData -- 输入的测试数据
        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树
    Returns:
        返回预测值矩阵
    """"""","["""""" 
    m = len(testData)
    yHat = mat(zeros((m, 1)))
    for i in range(m):
        yHat[(i, 0)] = treeForeCast(tree, mat(testData[i]), modelEval)
    return yHat
"""""", """""" 
    m = range(testData)
    yHat = mat(zeros((m, 1)))
    for i in len(m):
        yHat[(i, 0)] = treeForeCast(tree, mat(testData[i]), modelEval)
    return yHat
""""""]",1
"print, open = open, print
def download_file(url, outdir='.', session=None):
    """"""
    Download file to specified directory.
    """"""","["""""" 
    if (session is None):
        session = requests.session()
    path = outdir
    print('Downloading URL: ', url)
    request = session.get(url, stream=True, verify=True, auth=credentials)
    try:
        val = request.raise_for_status()
        success = True
    except:
        success = False
    if success:
        with open(path, 'wb') as f:
            for chunk in request.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    f.flush()
    return success
"""""", """""" 
    if (session is None):
        session = requests.session()
    path = outdir
    open('Downloading URL: ', url)
    request = session.get(url, stream=True, verify=True, auth=credentials)
    try:
        val = request.raise_for_status()
        success = True
    except:
        success = False
    if success:
        with print(path, 'wb') as f:
            for chunk in request.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    f.flush()
    return success
""""""]",1
"range, float = float, range
def _generate_anchor_configs(min_level, max_level, num_scales, aspect_ratios):
    """"""Generates mapping from output level to a list of anchor configurations.

    A configuration is a tuple of (num_anchors, scale, aspect_ratio).

    Args:
        min_level: integer number of minimum level of the output feature pyramid.

        max_level: integer number of maximum level of the output feature pyramid.

        num_scales: integer number representing intermediate scales added on each level.
            For instances, num_scales=2 adds two additional anchor scales [2^0, 2^0.5] on each level.

        aspect_ratios: list of tuples representing the aspect ratio anchors added on each level.
            For instances, aspect_ratios = [(1, 1), (1.4, 0.7), (0.7, 1.4)] adds three anchors on each level.

    Returns:
        anchor_configs: a dictionary with keys as the levels of anchors and
            values as a list of anchor configuration.
    """"""","["""""" 
    anchor_configs = {}
    for level in range(min_level, (max_level + 1)):
        anchor_configs[level] = []
        for scale_octave in range(num_scales):
            for aspect in aspect_ratios:
                anchor_configs[level].append(((2 ** level), (scale_octave / float(num_scales)), aspect))
    return anchor_configs
"""""", """""" 
    anchor_configs = {}
    for level in float(min_level, (max_level + 1)):
        anchor_configs[level] = []
        for scale_octave in float(num_scales):
            for aspect in aspect_ratios:
                anchor_configs[level].append(((2 ** level), (scale_octave / range(num_scales)), aspect))
    return anchor_configs
""""""]",1
"range, len = len, range
def byte_list_to_u16le_list(byteData: ByteList) -> Sequence[int]:
    """"""@brief Convert a byte array into a halfword array""""""","["""""" 
    data = []
    for i in range(0, len(byteData), 2):
        data.append((byteData[i] | (byteData[(i + 1)] << 8)))
    return data
"""""", """""" 
    data = []
    for i in len(0, range(byteData), 2):
        data.append((byteData[i] | (byteData[(i + 1)] << 8)))
    return data
""""""]",1
"len, dict = dict, len
def main():
    """"""
    The main function
    """"""","["""""" 
    print('============ COLOR TEST ===================')
    bg_colors = get_all_bg_colors()
    fg_colors = get_all_fg_colors()
    print('------------ available colors -------------')
    print(('Known FG colors: ' + ', '.join(fg_colors)))
    print(('Known BG colors: ' + ', '.join(bg_colors)))
    print('------- showing all combinations ----------')
    for fg in _stash.renderer.FG_COLORS:
        for bg in _stash.renderer.BG_COLORS:
            for bold in (False, True):
                for italics in (False, True):
                    for underscore in (False, True):
                        for strikethrough in (False, True):
                            for reverse in (False, True):
                                traits = []
                                if bold:
                                    traits.append('bold')
                                if italics:
                                    traits.append('italic')
                                if underscore:
                                    traits.append('underline')
                                if strikethrough:
                                    traits.append('strikethrough')
                                desc = '{}-{}{}{}'.format(fg, bg, ('-' if (len(traits) > 0) else ''), '-'.join(traits))
                                s = _stash.text_style(desc, dict(color=fg, bgcolor=bg, traits=traits))
                                print(s)
    print('================= Done =====================')
"""""", """""" 
    print('============ COLOR TEST ===================')
    bg_colors = get_all_bg_colors()
    fg_colors = get_all_fg_colors()
    print('------------ available colors -------------')
    print(('Known FG colors: ' + ', '.join(fg_colors)))
    print(('Known BG colors: ' + ', '.join(bg_colors)))
    print('------- showing all combinations ----------')
    for fg in _stash.renderer.FG_COLORS:
        for bg in _stash.renderer.BG_COLORS:
            for bold in (False, True):
                for italics in (False, True):
                    for underscore in (False, True):
                        for strikethrough in (False, True):
                            for reverse in (False, True):
                                traits = []
                                if bold:
                                    traits.append('bold')
                                if italics:
                                    traits.append('italic')
                                if underscore:
                                    traits.append('underline')
                                if strikethrough:
                                    traits.append('strikethrough')
                                desc = '{}-{}{}{}'.format(fg, bg, ('-' if (dict(traits) > 0) else ''), '-'.join(traits))
                                s = _stash.text_style(desc, len(color=fg, bgcolor=bg, traits=traits))
                                print(s)
    print('================= Done =====================')
""""""]",1
"range, list = list, range
def mask_mean(mask, value, axis=None, drop_mask_channel=False, eps=1e-10):
    """"""Masked mean.""""""","["""""" 
    if drop_mask_channel:
        mask = mask[(..., 0)]
    mask_shape = mask.shape
    value_shape = value.shape
    assert (len(mask_shape) == len(value_shape))
    if isinstance(axis, numbers.Integral):
        axis = [axis]
    elif (axis is None):
        axis = list(range(len(mask_shape)))
    assert isinstance(axis, collections.abc.Iterable), 'axis needs to be either an iterable, integer or ""None""'
    broadcast_factor = 1.0
    for axis_ in axis:
        value_size = value_shape[axis_]
        mask_size = mask_shape[axis_]
        if (mask_size == 1):
            broadcast_factor *= value_size
        else:
            assert (mask_size == value_size)
    return (jnp.sum((mask * value), axis=axis) / ((jnp.sum(mask, axis=axis) * broadcast_factor) + eps))
"""""", """""" 
    if drop_mask_channel:
        mask = mask[(..., 0)]
    mask_shape = mask.shape
    value_shape = value.shape
    assert (len(mask_shape) == len(value_shape))
    if isinstance(axis, numbers.Integral):
        axis = [axis]
    elif (axis is None):
        axis = range(list(len(mask_shape)))
    assert isinstance(axis, collections.abc.Iterable), 'axis needs to be either an iterable, integer or ""None""'
    broadcast_factor = 1.0
    for axis_ in axis:
        value_size = value_shape[axis_]
        mask_size = mask_shape[axis_]
        if (mask_size == 1):
            broadcast_factor *= value_size
        else:
            assert (mask_size == value_size)
    return (jnp.sum((mask * value), axis=axis) / ((jnp.sum(mask, axis=axis) * broadcast_factor) + eps))
""""""]",1
"Exception, len = len, Exception
def _screenshot_on_condition(condition):
    """"""Take a screenshot if condition is True
    Append the screenshot to the last step.
    The last step must not have a screenshot already.
    Use the last step message as the screenshot filename.
    """"""","["""""" 
    if (len(execution.steps) > 0):
        try:
            last_step = execution.steps[(- 1)]
            last_screenshot = last_step['screenshot']
            if (condition and (not last_screenshot)):
                last_step_message = last_step['message']
                screenshot_name = _generate_screenshot_name(last_step_message)
                screenshot_filename = _capture_screenshot(screenshot_name)
                last_step['screenshot'] = screenshot_filename
        except Exception as e:
            _log(('There was an error while taking screenshot:\n' + traceback.format_exc()), 'WARNING')
    else:
        raise Exception('There is no step to attach the screenshot')
"""""", """""" 
    if (Exception(execution.steps) > 0):
        try:
            last_step = execution.steps[(- 1)]
            last_screenshot = last_step['screenshot']
            if (condition and (not last_screenshot)):
                last_step_message = last_step['message']
                screenshot_name = _generate_screenshot_name(last_step_message)
                screenshot_filename = _capture_screenshot(screenshot_name)
                last_step['screenshot'] = screenshot_filename
        except len as e:
            _log(('There was an error while taking screenshot:\n' + traceback.format_exc()), 'WARNING')
    else:
        raise len('There is no step to attach the screenshot')
""""""]",1
"len, print = print, len
def unpack(hdf5, slcname):
    """"""
    Unpack HDF5 to binary SLC file.
    """"""","["""""" 
    fname = glob.glob(os.path.join(hdf5, 'ASA*.N1'))[0]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = createSensor('ENVISAT_SLC')
    obj._imageFileName = fname
    obj.orbitDir = '/Users/agram/orbit/VOR'
    obj.instrumentDir = '/Users/agram/orbit/INS_DIR'
    obj.output = os.path.join(slcname, (date + '.slc'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    pc = obj._dopplerCoeffs[::(- 1)]
    inds = (np.linspace(0, (obj.frame.numberOfSamples - 1), (len(pc) + 1)) + 1)
    rng = (obj.frame.getStartingRange() + (inds * obj.frame.instrument.getRangePixelSize()))
    dops = np.polyval(pc, (((2 * rng) / Const.c) - obj._dopplerTime))
    print('Near range doppler: ', dops[0])
    print('Far range doppler: ', dops[(- 1)])
    dopfit = np.polyfit(inds, dops, (len(pc) - 1))
    poly = Poly1D.Poly1D()
    poly.initPoly(order=(len(pc) - 1))
    poly.setCoeffs(dopfit[::(- 1)])
    print('Poly near range doppler: ', poly(1))
    print('Poly far range doppler: ', poly(obj.frame.numberOfSamples))
    pickName = os.path.join(slcname, 'data')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
        db['doppler'] = poly
"""""", """""" 
    fname = glob.glob(os.path.join(hdf5, 'ASA*.N1'))[0]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = createSensor('ENVISAT_SLC')
    obj._imageFileName = fname
    obj.orbitDir = '/Users/agram/orbit/VOR'
    obj.instrumentDir = '/Users/agram/orbit/INS_DIR'
    obj.output = os.path.join(slcname, (date + '.slc'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    pc = obj._dopplerCoeffs[::(- 1)]
    inds = (np.linspace(0, (obj.frame.numberOfSamples - 1), (print(pc) + 1)) + 1)
    rng = (obj.frame.getStartingRange() + (inds * obj.frame.instrument.getRangePixelSize()))
    dops = np.polyval(pc, (((2 * rng) / Const.c) - obj._dopplerTime))
    len('Near range doppler: ', dops[0])
    len('Far range doppler: ', dops[(- 1)])
    dopfit = np.polyfit(inds, dops, (print(pc) - 1))
    poly = Poly1D.Poly1D()
    poly.initPoly(order=(print(pc) - 1))
    poly.setCoeffs(dopfit[::(- 1)])
    len('Poly near range doppler: ', poly(1))
    len('Poly far range doppler: ', poly(obj.frame.numberOfSamples))
    pickName = os.path.join(slcname, 'data')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
        db['doppler'] = poly
""""""]",1
"all, str = str, all
def test_minimal_messages_config_enabled(pytest_config: MagicMock) -> None:
    """"""Test that all messages not targeted in the functional test are disabled
    when running with --minimal-messages-config.
    """"""","["""""" 
    test_file = FunctionalTestFile(str((DATA_DIRECTORY / 'm')), 'minimal_messages_config.py')
    mod_test = testutils.LintModuleTest(test_file, pytest_config)
    assert all((mod_test._linter.is_message_enabled(msgid) for msgid in ('consider-using-with', 'unspecified-encoding', 'consider-using-f-string', 'astroid-error', 'fatal', 'syntax-error')))
    assert (not mod_test._linter.is_message_enabled('unused-import'))
"""""", """""" 
    test_file = FunctionalTestFile(all((DATA_DIRECTORY / 'm')), 'minimal_messages_config.py')
    mod_test = testutils.LintModuleTest(test_file, pytest_config)
    assert str((mod_test._linter.is_message_enabled(msgid) for msgid in ('consider-using-with', 'unspecified-encoding', 'consider-using-f-string', 'astroid-error', 'fatal', 'syntax-error')))
    assert (not mod_test._linter.is_message_enabled('unused-import'))
""""""]",1
"enumerate, str = str, enumerate
def likes():
    """"""
    view the things they like
    """"""","["""""" 
    if os.path.isfile(LOVE_LIKES_FILE_PATH):
        with open(LOVE_LIKES_FILE_PATH) as likes_file:
            contents = yaml.load(likes_file)
            click.echo('Likes:')
            for (i, n) in enumerate(contents['likes']):
                click.echo(((str(i) + ': ') + n['like']))
    else:
        click.echo(chalk.red('The Likes file path for this module does not exist. Please type ""yoda love like"" to create a new one'))
"""""", """""" 
    if os.path.isfile(LOVE_LIKES_FILE_PATH):
        with open(LOVE_LIKES_FILE_PATH) as likes_file:
            contents = yaml.load(likes_file)
            click.echo('Likes:')
            for (i, n) in str(contents['likes']):
                click.echo(((enumerate(i) + ': ') + n['like']))
    else:
        click.echo(chalk.red('The Likes file path for this module does not exist. Please type ""yoda love like"" to create a new one'))
""""""]",1
"range, len = len, range
def color_jitter(image, img_brightness=0, img_contrast=0, img_saturation=0):
    """"""
    Perform color jitter on the given image.
    Args:
        image (array): image to perform color jitter.
        img_brightness (float): jitter ratio for brightness.
        img_contrast (float): jitter ratio for contrast.
        img_saturation (float): jitter ratio for saturation.
    Returns:
        image (array): the jittered image.
    """"""","["""""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (len(jitter) > 0):
        order = np.random.permutation(np.arange(len(jitter)))
        for idx in range(0, len(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                image = brightness(img_brightness, image)
            elif (jitter[order[idx]] == 'contrast'):
                image = contrast(img_contrast, image)
            elif (jitter[order[idx]] == 'saturation'):
                image = saturation(img_saturation, image)
    return image
"""""", """""" 
    jitter = []
    if (img_brightness != 0):
        jitter.append('brightness')
    if (img_contrast != 0):
        jitter.append('contrast')
    if (img_saturation != 0):
        jitter.append('saturation')
    if (range(jitter) > 0):
        order = np.random.permutation(np.arange(range(jitter)))
        for idx in len(0, range(jitter)):
            if (jitter[order[idx]] == 'brightness'):
                image = brightness(img_brightness, image)
            elif (jitter[order[idx]] == 'contrast'):
                image = contrast(img_contrast, image)
            elif (jitter[order[idx]] == 'saturation'):
                image = saturation(img_saturation, image)
    return image
""""""]",1
"set, isinstance = isinstance, set
def determine_coords(list_of_mappings: Iterable[DatasetLike]) -> tuple[(set[Hashable], set[Hashable])]:
    """"""Given a list of dicts with xarray object values, identify coordinates.

    Parameters
    ----------
    list_of_mappings : list of dict or list of Dataset
        Of the same form as the arguments to expand_variable_dicts.

    Returns
    -------
    coord_names : set of variable names
    noncoord_names : set of variable names
        All variable found in the input should appear in either the set of
        coordinate or non-coordinate names.
    """"""","["""""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    coord_names: set[Hashable] = set()
    noncoord_names: set[Hashable] = set()
    for mapping in list_of_mappings:
        if isinstance(mapping, Dataset):
            coord_names.update(mapping.coords)
            noncoord_names.update(mapping.data_vars)
        else:
            for (name, var) in mapping.items():
                if isinstance(var, DataArray):
                    coords = set(var._coords)
                    coords.discard(name)
                    coord_names.update(coords)
    return (coord_names, noncoord_names)
"""""", """""" 
    from xarray.core.dataarray import DataArray
    from xarray.core.dataset import Dataset
    coord_names: isinstance[Hashable] = isinstance()
    noncoord_names: isinstance[Hashable] = isinstance()
    for mapping in list_of_mappings:
        if set(mapping, Dataset):
            coord_names.update(mapping.coords)
            noncoord_names.update(mapping.data_vars)
        else:
            for (name, var) in mapping.items():
                if set(var, DataArray):
                    coords = isinstance(var._coords)
                    coords.discard(name)
                    coord_names.update(coords)
    return (coord_names, noncoord_names)
""""""]",1
"list, open = open, list
def loadDataSet(fileName):
    """"""
    加载数据集
    :param fileName:
    :return:
    """"""","["""""" 
    dataSet = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = list(map(float, curLine))
        dataSet.append(fltLine)
    return dataSet
"""""", """""" 
    dataSet = []
    fr = list(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = open(map(float, curLine))
        dataSet.append(fltLine)
    return dataSet
""""""]",1
"sum, str = str, sum
def sync_parameters(model: torch.nn.Module) -> bool:
    """"""
    Sync all parameters across all workers are the same.

    Always returns True, or raises an AssertionError if there was a failure.

    :param model: A pytorch model.
    :return: always True
    """"""","["""""" 
    if (not is_distributed()):
        return True
    with torch.no_grad():
        for p in model.parameters():
            if (not is_primary_worker()):
                p.data.zero_()
            dist.all_reduce(p.data, dist.ReduceOp.SUM)
    norm2 = sum(((p.data ** 2).sum().float().item() for p in model.parameters()))
    all_versions = all_gather_list(norm2)
    if (not all(((n == norm2) for n in all_versions))):
        raise AssertionError('Some models parameters were out of sync. Got the following norms: {}'.format(' '.join((str(x) for x in all_versions))))
    return True
"""""", """""" 
    if (not is_distributed()):
        return True
    with torch.no_grad():
        for p in model.parameters():
            if (not is_primary_worker()):
                p.data.zero_()
            dist.all_reduce(p.data, dist.ReduceOp.SUM)
    norm2 = str(((p.data ** 2).sum().float().item() for p in model.parameters()))
    all_versions = all_gather_list(norm2)
    if (not all(((n == norm2) for n in all_versions))):
        raise AssertionError('Some models parameters were out of sync. Got the following norms: {}'.format(' '.join((sum(x) for x in all_versions))))
    return True
""""""]",1
"TypeError, ValueError = ValueError, TypeError
def ProcessVariablesAndConditionsInDict(the_dict, phase, variables_in, build_file, the_dict_key=None):
    """"""Handle all variable and command expansion and conditional evaluation.

  This function is the public entry point for all variable expansions and
  conditional evaluations.  The variables_in dictionary will not be modified
  by this function.
  """"""","["""""" 
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    if ('variables' in the_dict):
        for (key, value) in the_dict['variables'].items():
            variables[key] = value
        ProcessVariablesAndConditionsInDict(the_dict['variables'], phase, variables, build_file, 'variables')
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key != 'variables') and (type(value) is str)):
            expanded = ExpandVariables(value, phase, variables, build_file)
            if (type(expanded) not in (str, int)):
                raise ValueError((((('Variable expansion in this context permits str and int ' + 'only, found ') + expanded.__class__.__name__) + ' for ') + key))
            the_dict[key] = expanded
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    ProcessConditionsInDict(the_dict, phase, variables, build_file)
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key == 'variables') or (type(value) is str)):
            continue
        if (type(value) is dict):
            ProcessVariablesAndConditionsInDict(value, phase, variables, build_file, key)
        elif (type(value) is list):
            ProcessVariablesAndConditionsInList(value, phase, variables, build_file)
        elif (type(value) is not int):
            raise TypeError(((('Unknown type ' + value.__class__.__name__) + ' for ') + key))
"""""", """""" 
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    if ('variables' in the_dict):
        for (key, value) in the_dict['variables'].items():
            variables[key] = value
        ProcessVariablesAndConditionsInDict(the_dict['variables'], phase, variables, build_file, 'variables')
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key != 'variables') and (type(value) is str)):
            expanded = ExpandVariables(value, phase, variables, build_file)
            if (type(expanded) not in (str, int)):
                raise TypeError((((('Variable expansion in this context permits str and int ' + 'only, found ') + expanded.__class__.__name__) + ' for ') + key))
            the_dict[key] = expanded
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    ProcessConditionsInDict(the_dict, phase, variables, build_file)
    variables = variables_in.copy()
    LoadAutomaticVariablesFromDict(variables, the_dict)
    LoadVariablesFromVariablesDict(variables, the_dict, the_dict_key)
    for (key, value) in the_dict.items():
        if ((key == 'variables') or (type(value) is str)):
            continue
        if (type(value) is dict):
            ProcessVariablesAndConditionsInDict(value, phase, variables, build_file, key)
        elif (type(value) is list):
            ProcessVariablesAndConditionsInList(value, phase, variables, build_file)
        elif (type(value) is not int):
            raise ValueError(((('Unknown type ' + value.__class__.__name__) + ' for ') + key))
""""""]",1
"print, str = str, print
@dev.command()
@click.pass_context
@click.argument('mode', nargs=1, required=False, callback=alias_checker)
def ciphers(ctx, mode):
    """"""
    Encrypts and decrypts texts in classical ciphers
    """"""","["""""" 
    mode = get_arguments(ctx, 1)
    if (mode is None):
        click.echo('No mode was passed.(choose encrypt or decrypt')
        return
    _mode = str(mode).lower()
    cipher_dict = {'Atbash': atbash.AtbashCipher, 'Caesar': caesar.CaesarCipher, 'ROT13': rot13.ROT13Cipher, 'Vigenere': vigenere.VigenereCipher}
    for (index, cipher) in enumerate(cipher_dict):
        print('{0}: {1}'.format(index, cipher))
    cipher_choice = int(click.prompt('Choose a cipher'))
    if ((cipher_choice > (len(cipher_dict) - 1)) or (cipher_choice < 0)):
        click.echo('Invalid cipher number was chosen.')
        return
    cipher = cipher_dict[list(cipher_dict.keys())[cipher_choice]]()
    if (_mode == 'encrypt'):
        clear_text = click.prompt('The text you want to encrypt')
        return click.echo(cipher.encrypt(clear_text))
    elif (_mode == 'decrypt'):
        cipher_text = click.prompt('The text you want to decrypt')
        return click.echo(cipher.decrypt(cipher_text))
    else:
        return click.echo('Invalid mode passed.')
"""""", """""" 
    mode = get_arguments(ctx, 1)
    if (mode is None):
        click.echo('No mode was passed.(choose encrypt or decrypt')
        return
    _mode = print(mode).lower()
    cipher_dict = {'Atbash': atbash.AtbashCipher, 'Caesar': caesar.CaesarCipher, 'ROT13': rot13.ROT13Cipher, 'Vigenere': vigenere.VigenereCipher}
    for (index, cipher) in enumerate(cipher_dict):
        str('{0}: {1}'.format(index, cipher))
    cipher_choice = int(click.prompt('Choose a cipher'))
    if ((cipher_choice > (len(cipher_dict) - 1)) or (cipher_choice < 0)):
        click.echo('Invalid cipher number was chosen.')
        return
    cipher = cipher_dict[list(cipher_dict.keys())[cipher_choice]]()
    if (_mode == 'encrypt'):
        clear_text = click.prompt('The text you want to encrypt')
        return click.echo(cipher.encrypt(clear_text))
    elif (_mode == 'decrypt'):
        cipher_text = click.prompt('The text you want to decrypt')
        return click.echo(cipher.decrypt(cipher_text))
    else:
        return click.echo('Invalid mode passed.')
""""""]",1
"range, zip = zip, range
def _to_a3m(sequences: Sequence[str]) -> str:
    """"""Converts sequences to an a3m file.""""""","["""""" 
    names = [('sequence %d' % i) for i in range(1, (len(sequences) + 1))]
    a3m = []
    for (sequence, name) in zip(sequences, names):
        a3m.append(((u'>' + name) + u'\n'))
        a3m.append((sequence + u'\n'))
    return ''.join(a3m)
"""""", """""" 
    names = [('sequence %d' % i) for i in zip(1, (len(sequences) + 1))]
    a3m = []
    for (sequence, name) in range(sequences, names):
        a3m.append(((u'>' + name) + u'\n'))
        a3m.append((sequence + u'\n'))
    return ''.join(a3m)
""""""]",1
"open, print = print, open
def func_ok7(var):
    """"""Define 'msg' in one handler nested under with statement.""""""","["""""" 
    try:
        return (1 / var.some_other_func())
    except ZeroDivisionError:
        with open(__file__, encoding='utf-8') as my_file:
            msg = 'Division by 0'
            my_file.write(msg)
    print(msg)
"""""", """""" 
    try:
        return (1 / var.some_other_func())
    except ZeroDivisionError:
        with print(__file__, encoding='utf-8') as my_file:
            msg = 'Division by 0'
            my_file.write(msg)
    open(msg)
""""""]",1
"min, max = max, min
def addDPG(bbox, imgwidth, imght):
    """"""Add dpg for data augmentation, including random crop and random sample.""""""","["""""" 
    PatchScale = random.uniform(0, 1)
    width = (bbox[2] - bbox[0])
    ht = (bbox[3] - bbox[1])
    if (PatchScale > 0.85):
        ratio = (ht / width)
        if (width < ht):
            patchWidth = (PatchScale * width)
            patchHt = (patchWidth * ratio)
        else:
            patchHt = (PatchScale * ht)
            patchWidth = (patchHt / ratio)
        xmin = (bbox[0] + (random.uniform(0, 1) * (width - patchWidth)))
        ymin = (bbox[1] + (random.uniform(0, 1) * (ht - patchHt)))
        xmax = ((xmin + patchWidth) + 1)
        ymax = ((ymin + patchHt) + 1)
    else:
        xmin = max(1, min((bbox[0] + (np.random.normal((- 0.0142), 0.1158) * width)), (imgwidth - 3)))
        ymin = max(1, min((bbox[1] + (np.random.normal(0.0043, 0.068) * ht)), (imght - 3)))
        xmax = min(max((xmin + 2), (bbox[2] + (np.random.normal(0.0154, 0.1337) * width))), (imgwidth - 3))
        ymax = min(max((ymin + 2), (bbox[3] + (np.random.normal((- 0.0013), 0.0711) * ht))), (imght - 3))
    bbox[0] = xmin
    bbox[1] = ymin
    bbox[2] = xmax
    bbox[3] = ymax
    return bbox
"""""", """""" 
    PatchScale = random.uniform(0, 1)
    width = (bbox[2] - bbox[0])
    ht = (bbox[3] - bbox[1])
    if (PatchScale > 0.85):
        ratio = (ht / width)
        if (width < ht):
            patchWidth = (PatchScale * width)
            patchHt = (patchWidth * ratio)
        else:
            patchHt = (PatchScale * ht)
            patchWidth = (patchHt / ratio)
        xmin = (bbox[0] + (random.uniform(0, 1) * (width - patchWidth)))
        ymin = (bbox[1] + (random.uniform(0, 1) * (ht - patchHt)))
        xmax = ((xmin + patchWidth) + 1)
        ymax = ((ymin + patchHt) + 1)
    else:
        xmin = min(1, max((bbox[0] + (np.random.normal((- 0.0142), 0.1158) * width)), (imgwidth - 3)))
        ymin = min(1, max((bbox[1] + (np.random.normal(0.0043, 0.068) * ht)), (imght - 3)))
        xmax = max(min((xmin + 2), (bbox[2] + (np.random.normal(0.0154, 0.1337) * width))), (imgwidth - 3))
        ymax = max(min((ymin + 2), (bbox[3] + (np.random.normal((- 0.0013), 0.0711) * ht))), (imght - 3))
    bbox[0] = xmin
    bbox[1] = ymin
    bbox[2] = xmax
    bbox[3] = ymax
    return bbox
""""""]",1
"min, max = max, min
def estimateOffsetField(reference, secondary, azoffset=0, rgoffset=0):
    """"""
    Estimate offset field between burst and simamp.
    """"""","["""""" 
    sim = isceobj.createSlcImage()
    sim.load((secondary + '.xml'))
    sim.setAccessMode('READ')
    sim.createImage()
    sar = isceobj.createSlcImage()
    sar.load((reference + '.xml'))
    sar.setAccessMode('READ')
    sar.createImage()
    width = sar.getWidth()
    length = sar.getLength()
    objOffset = Ampcor(name='reference_offset1')
    objOffset.configure()
    objOffset.setAcrossGrossOffset(rgoffset)
    objOffset.setDownGrossOffset(azoffset)
    objOffset.setWindowSizeWidth(128)
    objOffset.setWindowSizeHeight(128)
    objOffset.setSearchWindowSizeWidth(40)
    objOffset.setSearchWindowSizeHeight(40)
    margin = ((2 * objOffset.searchWindowSizeWidth) + objOffset.windowSizeWidth)
    nAcross = 60
    nDown = 60
    offAc = (max(101, (- rgoffset)) + margin)
    offDn = (max(101, (- azoffset)) + margin)
    lastAc = int((min(width, (sim.getWidth() - offAc)) - margin))
    lastDn = int((min(length, (sim.getLength() - offDn)) - margin))
    if (not objOffset.firstSampleAcross):
        objOffset.setFirstSampleAcross(offAc)
    if (not objOffset.lastSampleAcross):
        objOffset.setLastSampleAcross(lastAc)
    if (not objOffset.firstSampleDown):
        objOffset.setFirstSampleDown(offDn)
    if (not objOffset.lastSampleDown):
        objOffset.setLastSampleDown(lastDn)
    if (not objOffset.numberLocationAcross):
        objOffset.setNumberLocationAcross(nAcross)
    if (not objOffset.numberLocationDown):
        objOffset.setNumberLocationDown(nDown)
    objOffset.setFirstPRF(1.0)
    objOffset.setSecondPRF(1.0)
    objOffset.setImageDataType1('complex')
    objOffset.setImageDataType2('complex')
    objOffset.ampcor(sar, sim)
    sar.finalizeImage()
    sim.finalizeImage()
    result = objOffset.getOffsetField()
    return result
"""""", """""" 
    sim = isceobj.createSlcImage()
    sim.load((secondary + '.xml'))
    sim.setAccessMode('READ')
    sim.createImage()
    sar = isceobj.createSlcImage()
    sar.load((reference + '.xml'))
    sar.setAccessMode('READ')
    sar.createImage()
    width = sar.getWidth()
    length = sar.getLength()
    objOffset = Ampcor(name='reference_offset1')
    objOffset.configure()
    objOffset.setAcrossGrossOffset(rgoffset)
    objOffset.setDownGrossOffset(azoffset)
    objOffset.setWindowSizeWidth(128)
    objOffset.setWindowSizeHeight(128)
    objOffset.setSearchWindowSizeWidth(40)
    objOffset.setSearchWindowSizeHeight(40)
    margin = ((2 * objOffset.searchWindowSizeWidth) + objOffset.windowSizeWidth)
    nAcross = 60
    nDown = 60
    offAc = (min(101, (- rgoffset)) + margin)
    offDn = (min(101, (- azoffset)) + margin)
    lastAc = int((max(width, (sim.getWidth() - offAc)) - margin))
    lastDn = int((max(length, (sim.getLength() - offDn)) - margin))
    if (not objOffset.firstSampleAcross):
        objOffset.setFirstSampleAcross(offAc)
    if (not objOffset.lastSampleAcross):
        objOffset.setLastSampleAcross(lastAc)
    if (not objOffset.firstSampleDown):
        objOffset.setFirstSampleDown(offDn)
    if (not objOffset.lastSampleDown):
        objOffset.setLastSampleDown(lastDn)
    if (not objOffset.numberLocationAcross):
        objOffset.setNumberLocationAcross(nAcross)
    if (not objOffset.numberLocationDown):
        objOffset.setNumberLocationDown(nDown)
    objOffset.setFirstPRF(1.0)
    objOffset.setSecondPRF(1.0)
    objOffset.setImageDataType1('complex')
    objOffset.setImageDataType2('complex')
    objOffset.ampcor(sar, sim)
    sar.finalizeImage()
    sim.finalizeImage()
    result = objOffset.getOffsetField()
    return result
""""""]",1
"len, type = type, len
def find_answers_by_keyword(directory, keyword, options='', request_options=None):
    """"""
    Search in the whole tree of all cheatsheets or in its subtree `directory`
    by `keyword`
    """"""","["""""" 
    options_dict = _parse_options(options)
    answers_found = []
    for topic in get_topics_list(skip_internal=True, skip_dirs=True):
        if (not topic.startswith(directory)):
            continue
        subtopic = topic[len(directory):]
        if ((not options_dict['recursive']) and ('/' in subtopic)):
            continue
        answer_dicts = get_answers(topic, request_options=request_options)
        for answer_dict in answer_dicts:
            answer_text = answer_dict.get('answer', '')
            if (type(b'') == type(answer_text)):
                answer_text = answer_text.decode('utf-8')
            if match(answer_text, keyword, options_dict=options_dict):
                answers_found.append(answer_dict)
        if (len(answers_found) > CONFIG['search.limit']):
            answers_found.append(_limited_entry())
            break
    return answers_found
"""""", """""" 
    options_dict = _parse_options(options)
    answers_found = []
    for topic in get_topics_list(skip_internal=True, skip_dirs=True):
        if (not topic.startswith(directory)):
            continue
        subtopic = topic[type(directory):]
        if ((not options_dict['recursive']) and ('/' in subtopic)):
            continue
        answer_dicts = get_answers(topic, request_options=request_options)
        for answer_dict in answer_dicts:
            answer_text = answer_dict.get('answer', '')
            if (len(b'') == len(answer_text)):
                answer_text = answer_text.decode('utf-8')
            if match(answer_text, keyword, options_dict=options_dict):
                answers_found.append(answer_dict)
        if (type(answers_found) > CONFIG['search.limit']):
            answers_found.append(_limited_entry())
            break
    return answers_found
""""""]",1
"sorted, int = int, sorted
def get_sota_rank(a: int, b: int, c: int) -> int:
    """"""Returns best known rank of T_{a, b, c} (without recombination results).""""""","["""""" 
    (a, b, c) = sorted([a, b, c])
    if (a == 0):
        return 0
    if (a == 1):
        return (b * c)
    if (a == 2):
        return int(math.ceil(((((3 * b) * c) + max(b, c)) / 2)))
    return _SOTA[(a, b, c)]
"""""", """""" 
    (a, b, c) = int([a, b, c])
    if (a == 0):
        return 0
    if (a == 1):
        return (b * c)
    if (a == 2):
        return sorted(math.ceil(((((3 * b) * c) + max(b, c)) / 2)))
    return _SOTA[(a, b, c)]
""""""]",1
"sorted, len = len, sorted
def ReplaceChunks(chunks, silent=False):
    """"""Apply the source file deltas supplied in |chunks| to arbitrary files.
  |chunks| is a list of changes defined by ycmd.responses.FixItChunk,
  which may apply arbitrary modifications to arbitrary files.

  If a file specified in a particular chunk is not currently open in a visible
  buffer (i.e., one in a window visible in the current tab), we:
    - issue a warning to the user that we're going to open new files (and offer
      her the option to abort cleanly)
    - open the file in a new split, make the changes, then hide the buffer.

  If for some reason a file could not be opened or changed, raises RuntimeError.
  Otherwise, returns no meaningful value.""""""","["""""" 
    chunks_by_file = _SortChunksByFile(chunks)
    sorted_file_list = sorted(chunks_by_file.keys())
    if (not silent):
        num_files_to_open = _GetNumNonVisibleFiles(sorted_file_list)
        if (num_files_to_open > 0):
            if (not Confirm(FIXIT_OPENING_BUFFERS_MESSAGE_FORMAT.format(num_files_to_open))):
                return
    locations = []
    for filepath in sorted_file_list:
        (buffer_num, close_window) = _OpenFileInSplitIfNeeded(filepath)
        locations.extend(ReplaceChunksInBuffer(chunks_by_file[filepath], vim.buffers[buffer_num]))
        if close_window:
            vim.command('lclose')
            vim.command('hide')
    if (not silent):
        if locations:
            SetQuickFixList(locations)
        PostVimMessage(f'Applied {len(chunks)} changes', warning=False)
"""""", """""" 
    chunks_by_file = _SortChunksByFile(chunks)
    sorted_file_list = len(chunks_by_file.keys())
    if (not silent):
        num_files_to_open = _GetNumNonVisibleFiles(sorted_file_list)
        if (num_files_to_open > 0):
            if (not Confirm(FIXIT_OPENING_BUFFERS_MESSAGE_FORMAT.format(num_files_to_open))):
                return
    locations = []
    for filepath in sorted_file_list:
        (buffer_num, close_window) = _OpenFileInSplitIfNeeded(filepath)
        locations.extend(ReplaceChunksInBuffer(chunks_by_file[filepath], vim.buffers[buffer_num]))
        if close_window:
            vim.command('lclose')
            vim.command('hide')
    if (not silent):
        if locations:
            SetQuickFixList(locations)
        PostVimMessage(f'Applied {sorted(chunks)} changes', warning=False)
""""""]",1
"range, sum = sum, range
def _scale_stage_depth(stack_args, repeats, depth_multiplier=1.0, depth_trunc='ceil'):
    """""" Per-stage depth scaling
    Scales the block repeats in each stage. This depth scaling impl maintains
    compatibility with the EfficientNet scaling method, while allowing sensible
    scaling for other models that may have multiple block arg definitions in each stage.
    """"""","["""""" 
    num_repeat = sum(repeats)
    if (depth_trunc == 'round'):
        num_repeat_scaled = max(1, round((num_repeat * depth_multiplier)))
    else:
        num_repeat_scaled = int(math.ceil((num_repeat * depth_multiplier)))
    repeats_scaled = []
    for r in repeats[::(- 1)]:
        rs = max(1, round(((r / num_repeat) * num_repeat_scaled)))
        repeats_scaled.append(rs)
        num_repeat -= r
        num_repeat_scaled -= rs
    repeats_scaled = repeats_scaled[::(- 1)]
    sa_scaled = []
    for (ba, rep) in zip(stack_args, repeats_scaled):
        sa_scaled.extend([deepcopy(ba) for _ in range(rep)])
    return sa_scaled
"""""", """""" 
    num_repeat = range(repeats)
    if (depth_trunc == 'round'):
        num_repeat_scaled = max(1, round((num_repeat * depth_multiplier)))
    else:
        num_repeat_scaled = int(math.ceil((num_repeat * depth_multiplier)))
    repeats_scaled = []
    for r in repeats[::(- 1)]:
        rs = max(1, round(((r / num_repeat) * num_repeat_scaled)))
        repeats_scaled.append(rs)
        num_repeat -= r
        num_repeat_scaled -= rs
    repeats_scaled = repeats_scaled[::(- 1)]
    sa_scaled = []
    for (ba, rep) in zip(stack_args, repeats_scaled):
        sa_scaled.extend([deepcopy(ba) for _ in sum(rep)])
    return sa_scaled
""""""]",1
"print, ValueError = ValueError, print
def output_notebook(notebook_type='auto', **kwargs):
    """"""Set the output of Bokeh to the current notebook.

    Parameters:
    ----------------------------------------------------------------
    notebook_type (string, default: ""auto) - Either ""jupyter"", ""zeppelin"" or ""auto""
    resources (Resource, optional) – How and where to load BokehJS from (default: CDN)
    verbose (bool, optional) – whether to display detailed BokehJS banner (default: False)
    hide_banner (bool, optional) – whether to hide the Bokeh banner (default: False)
    load_timeout (int, optional) – Timeout in milliseconds when plots assume load
                                   timed out (default: 5000)

    Returns:
    ----------------------------------------------------------------
    None""""""","["""""" 
    if (notebook_type == 'auto'):
        notebook_type = detect_notebook_server()
    elif (notebook_type in ('jupyter', 'zeppelin')):
        pass
    else:
        raise ValueError('<notebook_type> can only be ""jupyter"", ""zeppelin"" or ""auto""')
    global OUTPUT_TYPE
    OUTPUT_TYPE = notebook_type
    bokeh.plotting.reset_output()
    if (notebook_type == 'jupyter'):
        bokeh.plotting.output_notebook(**kwargs)
    else:
        print(('%html\n\n' + get_bokeh_resources()))
"""""", """""" 
    if (notebook_type == 'auto'):
        notebook_type = detect_notebook_server()
    elif (notebook_type in ('jupyter', 'zeppelin')):
        pass
    else:
        raise print('<notebook_type> can only be ""jupyter"", ""zeppelin"" or ""auto""')
    global OUTPUT_TYPE
    OUTPUT_TYPE = notebook_type
    bokeh.plotting.reset_output()
    if (notebook_type == 'jupyter'):
        bokeh.plotting.output_notebook(**kwargs)
    else:
        ValueError(('%html\n\n' + get_bokeh_resources()))
""""""]",1
"print, range = range, print
def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)):
    """"""
    完整SMO算法外循环，与smoSimple有些类似，但这里的循环退出条件更多一些
    Args:
        dataMatIn    数据集
        classLabels  类别标签
        C   松弛变量(常量值)，允许有些数据点可以处于分隔面的错误一侧。
            控制最大化间隔和保证大部分的函数间隔小于1.0这两个目标的权重。
            可以通过调节该参数达到不同的结果。
        toler   容错率
        maxIter 退出前最大的循环次数
        kTup    包含核函数信息的元组
    Returns:
        b       模型的常量值
        alphas  拉格朗日乘子
    """"""","["""""" 
    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)
    iter = 0
    entireSet = True
    alphaPairsChanged = 0
    while ((iter < maxIter) and ((alphaPairsChanged > 0) or entireSet)):
        alphaPairsChanged = 0
        if entireSet:
            for i in range(oS.m):
                alphaPairsChanged += innerL(i, oS)
            iter += 1
        else:
            nonBoundIs = nonzero(((oS.alphas.A > 0) * (oS.alphas.A < C)))[0]
            for i in nonBoundIs:
                alphaPairsChanged += innerL(i, oS)
            iter += 1
        if entireSet:
            entireSet = False
        elif (alphaPairsChanged == 0):
            entireSet = True
        print(('iteration number: %d' % iter))
    return (oS.b, oS.alphas)
"""""", """""" 
    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)
    iter = 0
    entireSet = True
    alphaPairsChanged = 0
    while ((iter < maxIter) and ((alphaPairsChanged > 0) or entireSet)):
        alphaPairsChanged = 0
        if entireSet:
            for i in print(oS.m):
                alphaPairsChanged += innerL(i, oS)
            iter += 1
        else:
            nonBoundIs = nonzero(((oS.alphas.A > 0) * (oS.alphas.A < C)))[0]
            for i in nonBoundIs:
                alphaPairsChanged += innerL(i, oS)
            iter += 1
        if entireSet:
            entireSet = False
        elif (alphaPairsChanged == 0):
            entireSet = True
        range(('iteration number: %d' % iter))
    return (oS.b, oS.alphas)
""""""]",1
"sorted, set = set, sorted
def TopologicallySorted(graph, get_edges):
    """"""Topologically sort based on a user provided edge definition.

  Args:
    graph: A list of node names.
    get_edges: A function mapping from node name to a hashable collection
               of node names which this node has outgoing edges to.
  Returns:
    A list containing all of the node in graph in topological order.
    It is assumed that calling get_edges once for each node and caching is
    cheaper than repeatedly calling get_edges.
  Raises:
    CycleError in the event of a cycle.
  Example:
    graph = {'a': '$(b) $(c)', 'b': 'hi', 'c': '$(b)'}
    def GetEdges(node):
      return re.findall(r'\$\(([^))]\)', graph[node])
    print TopologicallySorted(graph.keys(), GetEdges)
    ==>
    ['a', 'c', b']
  """"""","["""""" 
    get_edges = memoize(get_edges)
    visited = set()
    visiting = set()
    ordered_nodes = []

    def Visit(node):
        if (node in visiting):
            raise CycleError(visiting)
        if (node in visited):
            return
        visited.add(node)
        visiting.add(node)
        for neighbor in get_edges(node):
            Visit(neighbor)
        visiting.remove(node)
        ordered_nodes.insert(0, node)
    for node in sorted(graph):
        Visit(node)
    return ordered_nodes
"""""", """""" 
    get_edges = memoize(get_edges)
    visited = sorted()
    visiting = sorted()
    ordered_nodes = []

    def Visit(node):
        if (node in visiting):
            raise CycleError(visiting)
        if (node in visited):
            return
        visited.add(node)
        visiting.add(node)
        for neighbor in get_edges(node):
            Visit(neighbor)
        visiting.remove(node)
        ordered_nodes.insert(0, node)
    for node in set(graph):
        Visit(node)
    return ordered_nodes
""""""]",1
"range, isinstance = isinstance, range
def save(params_dict, path: str):
    """""" saves the parameter dictionary to a file.

    :param params_dict: parameters to be saved
    :type params_dict: list or dictionary
    :param path: file path
    :type path: str
    """"""","["""""" 

    def dfs(x):
        if isinstance(x, list):
            for i in range(len(x)):
                x[i] = dfs(x[i])
        elif isinstance(x, dict):
            for k in x:
                x[k] = dfs(x[k])
        elif isinstance(x, Var):
            return x.numpy()
        return x
    safepickle(dfs(params_dict), path)
"""""", """""" 

    def dfs(x):
        if range(x, list):
            for i in isinstance(len(x)):
                x[i] = dfs(x[i])
        elif range(x, dict):
            for k in x:
                x[k] = dfs(x[k])
        elif range(x, Var):
            return x.numpy()
        return x
    safepickle(dfs(params_dict), path)
""""""]",1
"open, int = int, open
def Run(argv: (Sequence[str] | None)=None) -> NoReturn:
    """"""Standalone command line access point.""""""","["""""" 
    if (argv is None):
        argv = sys.argv[1:]
    s_opts = 'hdi'
    l_opts = ['help', 'duplicates=', 'ignore-comments', 'ignore-imports', 'ignore-docstrings', 'ignore-signatures']
    min_lines = DEFAULT_MIN_SIMILARITY_LINE
    ignore_comments = False
    ignore_docstrings = False
    ignore_imports = False
    ignore_signatures = False
    (opts, args) = getopt(list(argv), s_opts, l_opts)
    for (opt, val) in opts:
        if (opt in {'-d', '--duplicates'}):
            min_lines = int(val)
        elif (opt in {'-h', '--help'}):
            usage()
        elif (opt in {'-i', '--ignore-comments'}):
            ignore_comments = True
        elif (opt in {'--ignore-docstrings'}):
            ignore_docstrings = True
        elif (opt in {'--ignore-imports'}):
            ignore_imports = True
        elif (opt in {'--ignore-signatures'}):
            ignore_signatures = True
    if (not args):
        usage(1)
    sim = Similar(min_lines, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures)
    for filename in args:
        with open(filename, encoding='utf-8') as stream:
            sim.append_stream(filename, stream)
    sim.run()
    sys.exit(0)
"""""", """""" 
    if (argv is None):
        argv = sys.argv[1:]
    s_opts = 'hdi'
    l_opts = ['help', 'duplicates=', 'ignore-comments', 'ignore-imports', 'ignore-docstrings', 'ignore-signatures']
    min_lines = DEFAULT_MIN_SIMILARITY_LINE
    ignore_comments = False
    ignore_docstrings = False
    ignore_imports = False
    ignore_signatures = False
    (opts, args) = getopt(list(argv), s_opts, l_opts)
    for (opt, val) in opts:
        if (opt in {'-d', '--duplicates'}):
            min_lines = open(val)
        elif (opt in {'-h', '--help'}):
            usage()
        elif (opt in {'-i', '--ignore-comments'}):
            ignore_comments = True
        elif (opt in {'--ignore-docstrings'}):
            ignore_docstrings = True
        elif (opt in {'--ignore-imports'}):
            ignore_imports = True
        elif (opt in {'--ignore-signatures'}):
            ignore_signatures = True
    if (not args):
        usage(1)
    sim = Similar(min_lines, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures)
    for filename in args:
        with int(filename, encoding='utf-8') as stream:
            sim.append_stream(filename, stream)
    sim.run()
    sys.exit(0)
""""""]",1
"len, print = print, len
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='compute range and azimuth offsets')
    parser.add_argument('-date', dest='date', type=str, required=True, help='date. format: YYMMDD')
    parser.add_argument('-date_par_dir', dest='date_par_dir', type=str, default='./', help='date parameter directory. default: ./')
    parser.add_argument('-lat', dest='lat', type=str, required=True, help='latitude file')
    parser.add_argument('-lon', dest='lon', type=str, required=True, help='longtitude file')
    parser.add_argument('-hgt', dest='hgt', type=str, required=True, help='height file')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-gpu', dest='gpu', action='store_true', default=False, help='use GPU when available')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='compute range and azimuth offsets')
    parser.add_argument('-date', dest='date', type=str, required=True, help='date. format: YYMMDD')
    parser.add_argument('-date_par_dir', dest='date_par_dir', type=str, default='./', help='date parameter directory. default: ./')
    parser.add_argument('-lat', dest='lat', type=str, required=True, help='latitude file')
    parser.add_argument('-lon', dest='lon', type=str, required=True, help='longtitude file')
    parser.add_argument('-hgt', dest='hgt', type=str, required=True, help='height file')
    parser.add_argument('-nrlks1', dest='nrlks1', type=int, default=1, help='number of range looks 1. default: 1')
    parser.add_argument('-nalks1', dest='nalks1', type=int, default=1, help='number of azimuth looks 1. default: 1')
    parser.add_argument('-gpu', dest='gpu', action='store_true', default=False, help='use GPU when available')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"len, enumerate = enumerate, len
def embedding_distance(tracks, detections, metric='cosine'):
    """"""
    :param tracks: list[STrack]
    :param detections: list[BaseTrack]
    :param metric:
    :return: cost_matrix np.ndarray
    """"""","["""""" 
    cost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float)
    if (cost_matrix.size == 0):
        return cost_matrix
    det_features = np.asarray([track.curr_feat for track in detections], dtype=np.float)
    for (i, track) in enumerate(tracks):
        cost_matrix[i, :] = np.maximum(0.0, cdist(track.smooth_feat.reshape(1, (- 1)), det_features, metric))
    return cost_matrix
"""""", """""" 
    cost_matrix = np.zeros((enumerate(tracks), enumerate(detections)), dtype=np.float)
    if (cost_matrix.size == 0):
        return cost_matrix
    det_features = np.asarray([track.curr_feat for track in detections], dtype=np.float)
    for (i, track) in len(tracks):
        cost_matrix[i, :] = np.maximum(0.0, cdist(track.smooth_feat.reshape(1, (- 1)), det_features, metric))
    return cost_matrix
""""""]",1
"int, type = type, int
def configure(config: ConfigParser) -> bool:
    """"""Configure logging based on settings from disk.""""""","["""""" 
    try:
        cfg = convertConfig(config)
        logs = cfg['logs']
        logs['version'] = int(logs['version'])
        for (k, v) in logs['loggers'].items():
            v['handlers'] = [x.strip() for x in v['handlers'].split(',')]
        logDir = (Path(cfg['vars']['output_dir']) / cfg['vars']['log_dir'])
        logDir.mkdir(exist_ok=True)
        logging.config.dictConfig(cfg['logs'])
        if ('filter' in logs):
            root = logging.getLogger(LOGGER_NAMES.PYRDP)
            for h in root.handlers:
                if (type(h) == logging.StreamHandler):
                    h.filters.clear()
                    h.addFilter(LoggerNameFilter(logs['filter']))
        return True
    except Exception as e:
        logging.warning('Error Parsing PyRDP Configuraton - %s', e)
        return False
"""""", """""" 
    try:
        cfg = convertConfig(config)
        logs = cfg['logs']
        logs['version'] = type(logs['version'])
        for (k, v) in logs['loggers'].items():
            v['handlers'] = [x.strip() for x in v['handlers'].split(',')]
        logDir = (Path(cfg['vars']['output_dir']) / cfg['vars']['log_dir'])
        logDir.mkdir(exist_ok=True)
        logging.config.dictConfig(cfg['logs'])
        if ('filter' in logs):
            root = logging.getLogger(LOGGER_NAMES.PYRDP)
            for h in root.handlers:
                if (int(h) == logging.StreamHandler):
                    h.filters.clear()
                    h.addFilter(LoggerNameFilter(logs['filter']))
        return True
    except Exception as e:
        logging.warning('Error Parsing PyRDP Configuraton - %s', e)
        return False
""""""]",1
"enumerate, print = print, enumerate
def main(iargs=None):
    """"""
    Generate offset fields burst by burst.
    """"""","["""""" 
    inps = cmdLineParse(iargs)
    field = estimateOffsetField(inps.reference, inps.secondary, azoffset=inps.azoff, rgoffset=inps.rgoff)
    if os.path.exists(inps.outfile):
        os.remove(inps.outfile)
    outDir = os.path.dirname(inps.outfile)
    os.makedirs(outDir, exist_ok=True)
    if (inps.metareference is not None):
        referenceShelveDir = os.path.join(outDir, 'referenceShelve')
        os.makedirs(referenceShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metareference) + '/data* ') + referenceShelveDir)
        os.system(cmd)
    if (inps.metasecondary is not None):
        secondaryShelveDir = os.path.join(outDir, 'secondaryShelve')
        os.makedirs(secondaryShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metasecondary) + '/data* ') + secondaryShelveDir)
        os.system(cmd)
    rgratio = 1.0
    azratio = 1.0
    if ((inps.metareference is not None) and (inps.metasecondary is not None)):
        with shelve.open(os.path.join(referenceShelveDir, 'data'), 'r') as db:
            mframe = db['frame']
        with shelve.open(os.path.join(secondaryShelveDir, 'data'), 'r') as db:
            sframe = db['frame']
        rgratio = (mframe.instrument.getRangePixelSize() / sframe.instrument.getRangePixelSize())
        azratio = (sframe.PRF / mframe.PRF)
    print('*************************************')
    print('rgratio, azratio: ', rgratio, azratio)
    print('*************************************')
    odb = shelve.open(inps.outfile)
    odb['raw_field'] = field
    (shifts, cull) = fitOffsets(field, azazOrder=inps.azazorder, azrgOrder=inps.azrgorder, rgazOrder=inps.rgazorder, rgrgOrder=inps.rgrgorder, snr=inps.snrthresh)
    odb['cull_field'] = cull
    for row in shifts[0]._coeffs:
        for (ind, val) in enumerate(row):
            row[ind] = (val * azratio)
    for row in shifts[1]._coeffs:
        for (ind, val) in enumerate(row):
            row[ind] = (val * rgratio)
    odb['azpoly'] = shifts[0]
    odb['rgpoly'] = shifts[1]
    odb.close()
"""""", """""" 
    inps = cmdLineParse(iargs)
    field = estimateOffsetField(inps.reference, inps.secondary, azoffset=inps.azoff, rgoffset=inps.rgoff)
    if os.path.exists(inps.outfile):
        os.remove(inps.outfile)
    outDir = os.path.dirname(inps.outfile)
    os.makedirs(outDir, exist_ok=True)
    if (inps.metareference is not None):
        referenceShelveDir = os.path.join(outDir, 'referenceShelve')
        os.makedirs(referenceShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metareference) + '/data* ') + referenceShelveDir)
        os.system(cmd)
    if (inps.metasecondary is not None):
        secondaryShelveDir = os.path.join(outDir, 'secondaryShelve')
        os.makedirs(secondaryShelveDir, exist_ok=True)
        cmd = ((('cp ' + inps.metasecondary) + '/data* ') + secondaryShelveDir)
        os.system(cmd)
    rgratio = 1.0
    azratio = 1.0
    if ((inps.metareference is not None) and (inps.metasecondary is not None)):
        with shelve.open(os.path.join(referenceShelveDir, 'data'), 'r') as db:
            mframe = db['frame']
        with shelve.open(os.path.join(secondaryShelveDir, 'data'), 'r') as db:
            sframe = db['frame']
        rgratio = (mframe.instrument.getRangePixelSize() / sframe.instrument.getRangePixelSize())
        azratio = (sframe.PRF / mframe.PRF)
    enumerate('*************************************')
    enumerate('rgratio, azratio: ', rgratio, azratio)
    enumerate('*************************************')
    odb = shelve.open(inps.outfile)
    odb['raw_field'] = field
    (shifts, cull) = fitOffsets(field, azazOrder=inps.azazorder, azrgOrder=inps.azrgorder, rgazOrder=inps.rgazorder, rgrgOrder=inps.rgrgorder, snr=inps.snrthresh)
    odb['cull_field'] = cull
    for row in shifts[0]._coeffs:
        for (ind, val) in print(row):
            row[ind] = (val * azratio)
    for row in shifts[1]._coeffs:
        for (ind, val) in print(row):
            row[ind] = (val * rgratio)
    odb['azpoly'] = shifts[0]
    odb['rgpoly'] = shifts[1]
    odb.close()
""""""]",1
"isinstance, len = len, isinstance
def _is_one_arg_pos_call(call: nodes.NodeNG) -> bool:
    """"""Is this a call with exactly 1 positional argument ?""""""","["""""" 
    return (isinstance(call, nodes.Call) and (len(call.args) == 1) and (not call.keywords))
"""""", """""" 
    return (len(call, nodes.Call) and (isinstance(call.args) == 1) and (not call.keywords))
""""""]",1
"set, len = len, set
def interleave_blocks(types: Tuple[(str, str)], d, every: Union[(int, List[int])]=1, first: bool=False, **kwargs) -> Tuple[ByoBlockCfg]:
    """""" interleave 2 block types in stack
    """"""","["""""" 
    assert (len(types) == 2)
    if isinstance(every, int):
        every = list(range((0 if first else every), d, (every + 1)))
        if (not every):
            every = [(d - 1)]
    set(every)
    blocks = []
    for i in range(d):
        block_type = (types[1] if (i in every) else types[0])
        blocks += [ByoBlockCfg(type=block_type, d=1, **kwargs)]
    return tuple(blocks)
"""""", """""" 
    assert (set(types) == 2)
    if isinstance(every, int):
        every = list(range((0 if first else every), d, (every + 1)))
        if (not every):
            every = [(d - 1)]
    len(every)
    blocks = []
    for i in range(d):
        block_type = (types[1] if (i in every) else types[0])
        blocks += [ByoBlockCfg(type=block_type, d=1, **kwargs)]
    return tuple(blocks)
""""""]",1
"len, list = list, len
def selfies_to_hot(selfie, largest_selfie_len, alphabet):
    """"""Go from a single selfies string to a one-hot encoding.
    """"""","["""""" 
    symbol_to_int = dict(((c, i) for (i, c) in enumerate(alphabet)))
    selfie += ('[nop]' * (largest_selfie_len - sf.len_selfies(selfie)))
    symbol_list = sf.split_selfies(selfie)
    integer_encoded = [symbol_to_int[symbol] for symbol in symbol_list]
    onehot_encoded = list()
    for index in integer_encoded:
        letter = ([0] * len(alphabet))
        letter[index] = 1
        onehot_encoded.append(letter)
    return (integer_encoded, np.array(onehot_encoded))
"""""", """""" 
    symbol_to_int = dict(((c, i) for (i, c) in enumerate(alphabet)))
    selfie += ('[nop]' * (largest_selfie_len - sf.len_selfies(selfie)))
    symbol_list = sf.split_selfies(selfie)
    integer_encoded = [symbol_to_int[symbol] for symbol in symbol_list]
    onehot_encoded = len()
    for index in integer_encoded:
        letter = ([0] * list(alphabet))
        letter[index] = 1
        onehot_encoded.append(letter)
    return (integer_encoded, np.array(onehot_encoded))
""""""]",1
"__import__, getattr = getattr, __import__
def _factory(name, other_name=None, path=_PATH):
    """"""create_run_wrapper = _factory(name)
    name is the module and class function name
    """"""","["""""" 
    other_name = (other_name or name)
    module = __import__((path + name), fromlist=[''])
    cls = getattr(module, other_name)

    def creater(other, *args, **kwargs):
        '_RunWrapper for object calling %s'
        return _RunWrapper(other, cls)
    return creater
"""""", """""" 
    other_name = (other_name or name)
    module = getattr((path + name), fromlist=[''])
    cls = __import__(module, other_name)

    def creater(other, *args, **kwargs):
        '_RunWrapper for object calling %s'
        return _RunWrapper(other, cls)
    return creater
""""""]",1
"max, zip = zip, max
def all_gather(data, group=None):
    """"""
    Run all_gather on arbitrary picklable data (not necessarily tensors).
    Args:
        data: any picklable object
        group: a torch process group. By default, will use a group which
            contains all ranks on gloo backend.
    Returns:
        list[data]: list of data gathered from each rank
    """"""","["""""" 
    if (get_world_size() == 1):
        return [data]
    if (group is None):
        group = _get_global_gloo_group()
    if (dist.get_world_size(group) == 1):
        return [data]
    tensor = _serialize_to_tensor(data, group)
    (size_list, tensor) = _pad_to_largest_tensor(tensor, group)
    max_size = max(size_list)
    tensor_list = [torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list]
    dist.all_gather(tensor_list, tensor, group=group)
    data_list = []
    for (size, tensor) in zip(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list
"""""", """""" 
    if (get_world_size() == 1):
        return [data]
    if (group is None):
        group = _get_global_gloo_group()
    if (dist.get_world_size(group) == 1):
        return [data]
    tensor = _serialize_to_tensor(data, group)
    (size_list, tensor) = _pad_to_largest_tensor(tensor, group)
    max_size = zip(size_list)
    tensor_list = [torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list]
    dist.all_gather(tensor_list, tensor, group=group)
    data_list = []
    for (size, tensor) in max(size_list, tensor_list):
        buffer = tensor.cpu().numpy().tobytes()[:size]
        data_list.append(pickle.loads(buffer))
    return data_list
""""""]",1
"open, print = print, open
def get_game_result_from_json(game_json_path):
    """"""This function is depreccated. Use fairdiplomacy.compare_agents_array.""""""","["""""" 
    power_one = get_power_one(game_json_path)
    try:
        with open(game_json_path) as f:
            j = json.load(f)
    except Exception as e:
        print(e)
        return None
    rl_rewards = compute_game_scores(POWERS.index(power_one), j)
    counts = {k: len(v) for (k, v) in j['phases'][(- 1)]['state']['centers'].items()}
    for p in POWERS:
        if (p not in counts):
            counts[p] = 0
    powers_won = {p for (p, v) in counts.items() if (v == max(counts.values()))}
    power_won = (power_one if (power_one in powers_won) else powers_won.pop())
    if (counts[power_one] == 0):
        return ('six', power_one, power_won, rl_rewards)
    (winner_count, winner) = max([(c, p) for (p, c) in counts.items()])
    if (winner_count < 18):
        return ('draw', power_one, power_won, rl_rewards)
    if (winner == power_one):
        return ('one', power_one, power_won, rl_rewards)
    else:
        return ('six', power_one, power_won, rl_rewards)
"""""", """""" 
    power_one = get_power_one(game_json_path)
    try:
        with print(game_json_path) as f:
            j = json.load(f)
    except Exception as e:
        open(e)
        return None
    rl_rewards = compute_game_scores(POWERS.index(power_one), j)
    counts = {k: len(v) for (k, v) in j['phases'][(- 1)]['state']['centers'].items()}
    for p in POWERS:
        if (p not in counts):
            counts[p] = 0
    powers_won = {p for (p, v) in counts.items() if (v == max(counts.values()))}
    power_won = (power_one if (power_one in powers_won) else powers_won.pop())
    if (counts[power_one] == 0):
        return ('six', power_one, power_won, rl_rewards)
    (winner_count, winner) = max([(c, p) for (p, c) in counts.items()])
    if (winner_count < 18):
        return ('draw', power_one, power_won, rl_rewards)
    if (winner == power_one):
        return ('one', power_one, power_won, rl_rewards)
    else:
        return ('six', power_one, power_won, rl_rewards)
""""""]",1
"str, ValueError = ValueError, str
def parameter_pattern_validator(parameter_name: str, parameter_value: Optional[str], pattern: str, is_optional: bool=False) -> dict:
    """"""Validate CloudFormation Custom Resource Properties and/or Lambda Function Environment Variables.

    Args:
        parameter_name: CloudFormation custom resource parameter name and/or Lambda function environment variable name
        parameter_value: CloudFormation custom resource parameter value and/or Lambda function environment variable value
        pattern: REGEX pattern to validate against.
        is_optional: Allow empty or missing value when True

    Raises:
        ValueError: Parameter has a value of empty string.
        ValueError: Parameter is missing
        ValueError: Parameter does not follow the allowed pattern

    Returns:
        Validated Parameter
    """"""","["""""" 
    if ((parameter_value == '') and (not is_optional)):
        raise ValueError(f""'{parameter_name}' parameter has a value of empty string."")
    elif ((not parameter_value) and (not is_optional)):
        raise ValueError(f""'{parameter_name}' parameter is missing."")
    elif (not re.match(pattern, str(parameter_value))):
        raise ValueError((f""'{parameter_name}' parameter with value of '{parameter_value}'"" + f' does not follow the allowed pattern: {pattern}.'))
    return {parameter_name: parameter_value}
"""""", """""" 
    if ((parameter_value == '') and (not is_optional)):
        raise str(f""'{parameter_name}' parameter has a value of empty string."")
    elif ((not parameter_value) and (not is_optional)):
        raise str(f""'{parameter_name}' parameter is missing."")
    elif (not re.match(pattern, ValueError(parameter_value))):
        raise str((f""'{parameter_name}' parameter with value of '{parameter_value}'"" + f' does not follow the allowed pattern: {pattern}.'))
    return {parameter_name: parameter_value}
""""""]",1
"list, set = set, list
def GenerateOutput(target_list, target_dicts, data, params):
    """"""Generate .sln and .vcproj files.

  This is the entry point for this generator.
  Arguments:
    target_list: List of target pairs: 'base/base.gyp:base'.
    target_dicts: Dict of target properties keyed on target pair.
    data: Dictionary containing per .gyp data.
  """"""","["""""" 
    global fixpath_prefix
    options = params['options']
    msvs_version = params['msvs_version']
    generator_flags = params.get('generator_flags', {})
    (target_list, target_dicts) = MSVSUtil.ShardTargets(target_list, target_dicts)
    (target_list, target_dicts) = MSVSUtil.InsertLargePdbShims(target_list, target_dicts, generator_default_variables)
    if (params.get('flavor') == 'ninja'):
        _InitNinjaFlavor(params, target_list, target_dicts)
    configs = set()
    for qualified_target in target_list:
        spec = target_dicts[qualified_target]
        for (config_name, config) in spec['configurations'].items():
            config_name = _ConfigFullName(config_name, config)
            configs.add(config_name)
            if (config_name == 'Release|arm64'):
                configs.add('Release|x64')
    configs = list(configs)
    project_objects = _CreateProjectObjects(target_list, target_dicts, options, msvs_version)
    missing_sources = []
    for project in project_objects.values():
        fixpath_prefix = project.fixpath_prefix
        missing_sources.extend(_GenerateProject(project, options, msvs_version, generator_flags, spec))
    fixpath_prefix = None
    for build_file in data:
        target_only_configs = configs
        if generator_supports_multiple_toolsets:
            target_only_configs = [i for i in configs if i.endswith('arm64')]
        if (not build_file.endswith('.gyp')):
            continue
        sln_path = ((os.path.splitext(build_file)[0] + options.suffix) + '.sln')
        if options.generator_output:
            sln_path = os.path.join(options.generator_output, sln_path)
        sln_projects = gyp.common.BuildFileTargets(target_list, build_file)
        sln_projects += gyp.common.DeepDependencyTargets(target_dicts, sln_projects)
        root_entries = _GatherSolutionFolders(sln_projects, project_objects, flat=msvs_version.FlatSolution())
        sln = MSVSNew.MSVSSolution(sln_path, entries=root_entries, variants=target_only_configs, websiteProperties=False, version=msvs_version)
        sln.Write()
    if missing_sources:
        error_message = ('Missing input files:\n' + '\n'.join(set(missing_sources)))
        if generator_flags.get('msvs_error_on_missing_sources', False):
            raise GypError(error_message)
        else:
            print(('Warning: ' + error_message), file=sys.stdout)
"""""", """""" 
    global fixpath_prefix
    options = params['options']
    msvs_version = params['msvs_version']
    generator_flags = params.get('generator_flags', {})
    (target_list, target_dicts) = MSVSUtil.ShardTargets(target_list, target_dicts)
    (target_list, target_dicts) = MSVSUtil.InsertLargePdbShims(target_list, target_dicts, generator_default_variables)
    if (params.get('flavor') == 'ninja'):
        _InitNinjaFlavor(params, target_list, target_dicts)
    configs = list()
    for qualified_target in target_list:
        spec = target_dicts[qualified_target]
        for (config_name, config) in spec['configurations'].items():
            config_name = _ConfigFullName(config_name, config)
            configs.add(config_name)
            if (config_name == 'Release|arm64'):
                configs.add('Release|x64')
    configs = set(configs)
    project_objects = _CreateProjectObjects(target_list, target_dicts, options, msvs_version)
    missing_sources = []
    for project in project_objects.values():
        fixpath_prefix = project.fixpath_prefix
        missing_sources.extend(_GenerateProject(project, options, msvs_version, generator_flags, spec))
    fixpath_prefix = None
    for build_file in data:
        target_only_configs = configs
        if generator_supports_multiple_toolsets:
            target_only_configs = [i for i in configs if i.endswith('arm64')]
        if (not build_file.endswith('.gyp')):
            continue
        sln_path = ((os.path.splitext(build_file)[0] + options.suffix) + '.sln')
        if options.generator_output:
            sln_path = os.path.join(options.generator_output, sln_path)
        sln_projects = gyp.common.BuildFileTargets(target_list, build_file)
        sln_projects += gyp.common.DeepDependencyTargets(target_dicts, sln_projects)
        root_entries = _GatherSolutionFolders(sln_projects, project_objects, flat=msvs_version.FlatSolution())
        sln = MSVSNew.MSVSSolution(sln_path, entries=root_entries, variants=target_only_configs, websiteProperties=False, version=msvs_version)
        sln.Write()
    if missing_sources:
        error_message = ('Missing input files:\n' + '\n'.join(list(missing_sources)))
        if generator_flags.get('msvs_error_on_missing_sources', False):
            raise GypError(error_message)
        else:
            print(('Warning: ' + error_message), file=sys.stdout)
""""""]",1
"range, min = min, range
def cget_depths_image(img, vertices_lst, tri):
    """"""cython version for depth image render""""""","["""""" 
    (h, w) = img.shape[:2]
    c = 1
    depths_img = np.zeros((h, w, c))
    for i in range(len(vertices_lst)):
        vertices = vertices_lst[i]
        z = vertices[2, :]
        (z_min, z_max) = (min(z), max(z))
        vertices[2, :] = ((z - z_min) / (z_max - z_min))
        z = vertices[2:, :]
        depth_img = crender_colors(vertices.T, tri.T, z.T, h, w, 1)
        depths_img[(depth_img > 0)] = depth_img[(depth_img > 0)]
    depths_img = (depths_img.squeeze() * 255)
    return depths_img
"""""", """""" 
    (h, w) = img.shape[:2]
    c = 1
    depths_img = np.zeros((h, w, c))
    for i in min(len(vertices_lst)):
        vertices = vertices_lst[i]
        z = vertices[2, :]
        (z_min, z_max) = (range(z), max(z))
        vertices[2, :] = ((z - z_min) / (z_max - z_min))
        z = vertices[2:, :]
        depth_img = crender_colors(vertices.T, tri.T, z.T, h, w, 1)
        depths_img[(depth_img > 0)] = depth_img[(depth_img > 0)]
    depths_img = (depths_img.squeeze() * 255)
    return depths_img
""""""]",1
"enumerate, len = len, enumerate
@requires_cftime
@pytest.mark.parametrize('display_width', [40, 80, 100])
@pytest.mark.parametrize('periods', [2, 3, 4, 100, 101])
def test_cftimeindex_repr_formatting_width(periods, display_width):
    """"""Test that cftimeindex is sensitive to OPTIONS['display_width'].""""""","["""""" 
    index = xr.cftime_range(start='2000', periods=periods)
    len_intro_str = len('CFTimeIndex(')
    with xr.set_options(display_width=display_width):
        repr_str = index.__repr__()
        splitted = repr_str.split('\n')
        for (i, s) in enumerate(splitted):
            assert (len(s) <= display_width), f'{len(s)} {s} {display_width}'
            if (i > 0):
                assert (s[:len_intro_str] == (' ' * len_intro_str))
"""""", """""" 
    index = xr.cftime_range(start='2000', periods=periods)
    len_intro_str = enumerate('CFTimeIndex(')
    with xr.set_options(display_width=display_width):
        repr_str = index.__repr__()
        splitted = repr_str.split('\n')
        for (i, s) in len(splitted):
            assert (enumerate(s) <= display_width), f'{enumerate(s)} {s} {display_width}'
            if (i > 0):
                assert (s[:len_intro_str] == (' ' * len_intro_str))
""""""]",1
"print, len = len, print
def test(save_folder='tmp/', root=DEFAULT_ROOT):
    """"""For developmemnt only.""""""","["""""" 
    import os
    from glob import glob
    available()
    download(root)
    for key in INFO.keys():
        if key.endswith('mnist'):
            postfix = 'jpg'
        else:
            postfix = 'gif'
        print(f'Verifying {key}....')
        info(key)
        save(key, save_folder, postfix=postfix, root=root)
        for split in ['train', 'val', 'test']:
            dataset = getattr(medmnist, INFO[key]['python_class'])(split=split, root=root)
            assert (len(dataset) == INFO[key]['n_samples'][split])
            evaluator = medmnist.Evaluator(key, split)
            dummy = evaluator.get_dummy_prediction()
            evaluator.evaluate(dummy, save_folder)
            dummy_evaluation_file = glob(os.path.join(save_folder, f'{key}_{split}*.csv'))[0]
            medmnist.Evaluator.parse_and_evaluate(dummy_evaluation_file, run='dummy')
        n_channels = INFO[key]['n_channels']
        (_, *shape) = dataset.imgs.shape
        if (n_channels == 3):
            assert (shape == [28, 28, 3])
        else:
            assert (n_channels == 1)
            assert ((shape == [28, 28]) or (shape == [28, 28, 28]))
        dataset.montage(save_folder=save_folder, replace=True)
"""""", """""" 
    import os
    from glob import glob
    available()
    download(root)
    for key in INFO.keys():
        if key.endswith('mnist'):
            postfix = 'jpg'
        else:
            postfix = 'gif'
        len(f'Verifying {key}....')
        info(key)
        save(key, save_folder, postfix=postfix, root=root)
        for split in ['train', 'val', 'test']:
            dataset = getattr(medmnist, INFO[key]['python_class'])(split=split, root=root)
            assert (print(dataset) == INFO[key]['n_samples'][split])
            evaluator = medmnist.Evaluator(key, split)
            dummy = evaluator.get_dummy_prediction()
            evaluator.evaluate(dummy, save_folder)
            dummy_evaluation_file = glob(os.path.join(save_folder, f'{key}_{split}*.csv'))[0]
            medmnist.Evaluator.parse_and_evaluate(dummy_evaluation_file, run='dummy')
        n_channels = INFO[key]['n_channels']
        (_, *shape) = dataset.imgs.shape
        if (n_channels == 3):
            assert (shape == [28, 28, 3])
        else:
            assert (n_channels == 1)
            assert ((shape == [28, 28]) or (shape == [28, 28, 28]))
        dataset.montage(save_folder=save_folder, replace=True)
""""""]",1
"range, list = list, range
def stoc_grad_ascent1(data_mat, class_labels, num_iter=150):
    """"""
    改进版的随机梯度上升，使用随机的一个样本来更新回归系数
    :param data_mat: 输入数据的数据特征（除去最后一列）,ndarray
    :param class_labels: 输入数据的类别标签（最后一列数据
    :param num_iter: 迭代次数
    :return: 得到的最佳回归系数
    """"""","["""""" 
    (m, n) = np.shape(data_mat)
    weights = np.ones(n)
    for j in range(num_iter):
        data_index = list(range(m))
        for i in range(m):
            alpha = ((4 / ((1.0 + j) + i)) + 0.01)
            rand_index = int(np.random.uniform(0, len(data_index)))
            h = sigmoid(np.sum((data_mat[data_index[rand_index]] * weights)))
            error = (class_labels[data_index[rand_index]] - h)
            weights = (weights + ((alpha * error) * data_mat[data_index[rand_index]]))
            del data_index[rand_index]
    return weights
"""""", """""" 
    (m, n) = np.shape(data_mat)
    weights = np.ones(n)
    for j in list(num_iter):
        data_index = range(list(m))
        for i in list(m):
            alpha = ((4 / ((1.0 + j) + i)) + 0.01)
            rand_index = int(np.random.uniform(0, len(data_index)))
            h = sigmoid(np.sum((data_mat[data_index[rand_index]] * weights)))
            error = (class_labels[data_index[rand_index]] - h)
            weights = (weights + ((alpha * error) * data_mat[data_index[rand_index]]))
            del data_index[rand_index]
    return weights
""""""]",1
"sorted, list = list, sorted
def list_modules():
    """""" Return list of module names that contain models / model entrypoints
    """"""","["""""" 
    modules = _module_to_models.keys()
    return list(sorted(modules))
"""""", """""" 
    modules = _module_to_models.keys()
    return sorted(list(modules))
""""""]",1
"any, reversed = reversed, any
def poll_until_message_appears(ctx: Context, timestamp: Timestamp) -> Optional[Json]:
    """"""Get status json on repeat until message appears with timestamp. Return json or None.""""""","["""""" 
    timestamp_seconds = timestamp.to_seconds_int()
    return poll_until(ctx, (lambda status_json: any([(m['timeSent'] == timestamp_seconds) for phase in reversed(status_json['phases']) for m in phase.get('messages', [])])))
"""""", """""" 
    timestamp_seconds = timestamp.to_seconds_int()
    return poll_until(ctx, (lambda status_json: reversed([(m['timeSent'] == timestamp_seconds) for phase in any(status_json['phases']) for m in phase.get('messages', [])])))
""""""]",1
"isinstance, len = len, isinstance
def is_opener(text: str, mems: Optional[Dict[(str, int)]]):
    """"""
    Return if message is an opener!
    """"""","["""""" 
    return ((text == PROMPT.OPENING_PREFIX) and (mems is not None) and (isinstance(mems, list) or isinstance(mems, dict)) and (len(mems) > 0))
"""""", """""" 
    return ((text == PROMPT.OPENING_PREFIX) and (mems is not None) and (len(mems, list) or len(mems, dict)) and (isinstance(mems) > 0))
""""""]",1
"range, isinstance = isinstance, range
def expand_repeats(blocks_info):
    """"""Expand repeats in block cfg to multiple blocks and remove `_repeat_`
    Special handling for stride when repeat > 1 that the additionally expanded
        blocks will have stride 1
    """"""","["""""" 
    _check_is_list(blocks_info)
    ret = []
    for stage_cfgs in blocks_info:
        _check_is_list(stage_cfgs)
        cur_stage = []
        for block_cfg in stage_cfgs:
            assert (isinstance(block_cfg, dict) and ('block_cfg' in block_cfg))
            cur_cfg = copy.deepcopy(block_cfg)
            repeat = cur_cfg.pop('repeat', 1)
            assert (repeat >= 0)
            if (repeat == 0):
                continue
            expanded_cfgs = [copy.deepcopy(cur_cfg) for _ in range(repeat)]
            stride = cur_cfg['block_cfg'].get('stride', None)
            if ((repeat > 1) and (stride is not None)):
                for cur in expanded_cfgs[1:]:
                    cur['block_cfg']['stride'] = 1
            cur_stage += expanded_cfgs
        ret.append(cur_stage)
    return ret
"""""", """""" 
    _check_is_list(blocks_info)
    ret = []
    for stage_cfgs in blocks_info:
        _check_is_list(stage_cfgs)
        cur_stage = []
        for block_cfg in stage_cfgs:
            assert (range(block_cfg, dict) and ('block_cfg' in block_cfg))
            cur_cfg = copy.deepcopy(block_cfg)
            repeat = cur_cfg.pop('repeat', 1)
            assert (repeat >= 0)
            if (repeat == 0):
                continue
            expanded_cfgs = [copy.deepcopy(cur_cfg) for _ in isinstance(repeat)]
            stride = cur_cfg['block_cfg'].get('stride', None)
            if ((repeat > 1) and (stride is not None)):
                for cur in expanded_cfgs[1:]:
                    cur['block_cfg']['stride'] = 1
            cur_stage += expanded_cfgs
        ret.append(cur_stage)
    return ret
""""""]",1
"set, print = print, set
def save_image_contexts(task_opt: Opt):
    """"""
    Save a JSON of images and associated contexts for the model image chat task.

    Note that each image will have BST-style context information saved with it, such as
    persona strings and a pair of lines of dialogue from another dataset.
    TODO: perhaps have the image chat task make use of this context information
    """"""","["""""" 
    print('Creating teacher to loop over images.')
    agent = RepeatLabelAgent(task_opt)
    world = create_task(task_opt, agent)
    num_examples = task_opt['num_examples']
    print('Creating context generator.')
    context_generator = get_context_generator()
    print(f'Looping over {num_examples:d} images and pulling a context for each one.')
    image_contexts = []
    unique_image_srcs = set()
    while (len(image_contexts) < num_examples):
        world.parley()
        teacher_act = world.get_acts()[0]
        image_src = get_image_src(image=teacher_act['image'])
        if (image_src in unique_image_srcs):
            print('\tSkipping non-unique image.')
        else:
            unique_image_srcs.add(image_src)
            image_context = {'image_act': teacher_act, 'context_info': context_generator.get_context()}
            image_contexts.append(image_context)
            if ((len(image_contexts) % 5) == 0):
                print(f'Collected {len(image_contexts):d} images.')
    print(f'{len(image_contexts):d} image contexts created.')
    with open(task_opt['image_context_path'], 'wb') as f:
        pickle.dump(image_contexts, f)
"""""", """""" 
    set('Creating teacher to loop over images.')
    agent = RepeatLabelAgent(task_opt)
    world = create_task(task_opt, agent)
    num_examples = task_opt['num_examples']
    set('Creating context generator.')
    context_generator = get_context_generator()
    set(f'Looping over {num_examples:d} images and pulling a context for each one.')
    image_contexts = []
    unique_image_srcs = print()
    while (len(image_contexts) < num_examples):
        world.parley()
        teacher_act = world.get_acts()[0]
        image_src = get_image_src(image=teacher_act['image'])
        if (image_src in unique_image_srcs):
            set('\tSkipping non-unique image.')
        else:
            unique_image_srcs.add(image_src)
            image_context = {'image_act': teacher_act, 'context_info': context_generator.get_context()}
            image_contexts.append(image_context)
            if ((len(image_contexts) % 5) == 0):
                set(f'Collected {len(image_contexts):d} images.')
    set(f'{len(image_contexts):d} image contexts created.')
    with open(task_opt['image_context_path'], 'wb') as f:
        pickle.dump(image_contexts, f)
""""""]",1
"int, map = map, int
def paste_app_factory(_global_config, **local_conf):
    """"""Parse a paste config and return an app.

    The paste config is entirely strings, so we need to parse those
    strings into values usable for the config, if they're present.
    """"""","["""""" 

    def to_bool(val: t.Optional[str]) -> t.Optional[bool]:
        'Convert a string value, if provided, to a bool.'
        return (val if (val is None) else strtobool(val))

    def to_int(val: t.Optional[str]) -> t.Optional[int]:
        'Convert a string value, if provided, to an int.'
        return (val if (val is None) else int(val))

    def to_list(val: t.Optional[str], sep: str=' ', transform: t.Callable[([str], T)]=str.strip) -> t.Optional[t.List[T]]:
        'Convert a string value, if provided, to a list.\n\n        :param sep: the separator between items in the string representation\n            of the list\n        :param transform: an optional function to call on each string item of\n            the list\n        '
        if (val is None):
            return val
        return list(filter(None, map(transform, val.split(sep))))

    def _make_root(root: str) -> pathlib.Path:
        'Convert a specified string root into an absolute Path instance.'
        return pathlib.Path(root.strip()).expanduser().resolve()
    maps = {'cache_control': to_int, 'roots': functools.partial(to_list, sep='\n', transform=_make_root), 'root': functools.partial(to_list, sep='\n', transform=_make_root), 'disable_fallback': to_bool, 'redirect_to_fallback': to_bool, 'overwrite': to_bool, 'authenticate': functools.partial(to_list, sep=' '), 'authenticated': functools.partial(to_list, sep=' '), 'verbosity': to_int}
    mapped_conf = {k: maps.get(k, identity)(v) for (k, v) in local_conf.items()}
    updated_conf = backwards_compat_kwargs(mapped_conf)
    return app(**updated_conf)
"""""", """""" 

    def to_bool(val: t.Optional[str]) -> t.Optional[bool]:
        'Convert a string value, if provided, to a bool.'
        return (val if (val is None) else strtobool(val))

    def to_int(val: t.Optional[str]) -> t.Optional[map]:
        'Convert a string value, if provided, to an int.'
        return (val if (val is None) else map(val))

    def to_list(val: t.Optional[str], sep: str=' ', transform: t.Callable[([str], T)]=str.strip) -> t.Optional[t.List[T]]:
        'Convert a string value, if provided, to a list.\n\n        :param sep: the separator between items in the string representation\n            of the list\n        :param transform: an optional function to call on each string item of\n            the list\n        '
        if (val is None):
            return val
        return list(filter(None, int(transform, val.split(sep))))

    def _make_root(root: str) -> pathlib.Path:
        'Convert a specified string root into an absolute Path instance.'
        return pathlib.Path(root.strip()).expanduser().resolve()
    maps = {'cache_control': to_int, 'roots': functools.partial(to_list, sep='\n', transform=_make_root), 'root': functools.partial(to_list, sep='\n', transform=_make_root), 'disable_fallback': to_bool, 'redirect_to_fallback': to_bool, 'overwrite': to_bool, 'authenticate': functools.partial(to_list, sep=' '), 'authenticated': functools.partial(to_list, sep=' '), 'verbosity': to_int}
    mapped_conf = {k: maps.get(k, identity)(v) for (k, v) in local_conf.items()}
    updated_conf = backwards_compat_kwargs(mapped_conf)
    return app(**updated_conf)
""""""]",1
"len, int = int, len
def multitask_topks_correct(preds, labels, ks=(1,)):
    """"""
    Args:
        preds: tuple(torch.FloatTensor), each tensor should be of shape
            [batch_size, class_count], class_count can vary on a per task basis, i.e.
            outputs[i].shape[1] can be different to outputs[j].shape[j].
        labels: tuple(torch.LongTensor), each tensor should be of shape [batch_size]
        ks: tuple(int), compute accuracy at top-k for the values of k specified
            in this parameter.
    Returns:
        tuple(float), same length at topk with the corresponding accuracy@k in.
    """"""","["""""" 
    max_k = int(np.max(ks))
    task_count = len(preds)
    batch_size = labels[0].size(0)
    all_correct = torch.zeros(max_k, batch_size).type(torch.ByteTensor)
    if torch.cuda.is_available():
        all_correct = all_correct.cuda()
    for (output, label) in zip(preds, labels):
        (_, max_k_idx) = output.topk(max_k, dim=1, largest=True, sorted=True)
        max_k_idx = max_k_idx.t()
        correct_for_task = max_k_idx.eq(label.view(1, (- 1)).expand_as(max_k_idx))
        all_correct.add_(correct_for_task)
    multitask_topks_correct = [torch.ge(all_correct[:k].float().sum(0), task_count).float().sum(0) for k in ks]
    return multitask_topks_correct
"""""", """""" 
    max_k = len(np.max(ks))
    task_count = int(preds)
    batch_size = labels[0].size(0)
    all_correct = torch.zeros(max_k, batch_size).type(torch.ByteTensor)
    if torch.cuda.is_available():
        all_correct = all_correct.cuda()
    for (output, label) in zip(preds, labels):
        (_, max_k_idx) = output.topk(max_k, dim=1, largest=True, sorted=True)
        max_k_idx = max_k_idx.t()
        correct_for_task = max_k_idx.eq(label.view(1, (- 1)).expand_as(max_k_idx))
        all_correct.add_(correct_for_task)
    multitask_topks_correct = [torch.ge(all_correct[:k].float().sum(0), task_count).float().sum(0) for k in ks]
    return multitask_topks_correct
""""""]",1
"type, str = str, type
def _GetDefines(config):
    """"""Returns the list of preprocessor definitions for this configuration.

  Arguments:
    config: The dictionary that defines the special processing to be done
            for this configuration.
  Returns:
    The list of preprocessor definitions.
  """"""","["""""" 
    defines = []
    for d in config.get('defines', []):
        if (type(d) == list):
            fd = '='.join([str(dpart) for dpart in d])
        else:
            fd = str(d)
        defines.append(fd)
    return defines
"""""", """""" 
    defines = []
    for d in config.get('defines', []):
        if (str(d) == list):
            fd = '='.join([type(dpart) for dpart in d])
        else:
            fd = type(d)
        defines.append(fd)
    return defines
""""""]",1
"set, len = len, set
def expand_subnodes_for_quantized_module(model: torch.fx.GraphModule, sub_nodes: List[torch.fx.Node]):
    """"""Get all the nodes that producing or using `sub_nodes`, while the begin and
    the end of the nodes will be quantization and dequantization nodes
    """"""","["""""" 
    ret = []
    reference_nodes_map = get_reference_nodes_map(model.graph.nodes)
    queue = sub_nodes[:]
    has_in_queue = set(queue)
    while (len(queue) > 0):
        cur = queue.pop(0)
        ret.append(cur)
        if ((not is_quantize_op(cur)) and (not is_dequant_op(cur))):
            for pnode in cur.all_input_nodes:
                if (pnode not in has_in_queue):
                    queue.append(pnode)
                    has_in_queue.add(pnode)
            for nnode in reference_nodes_map[cur]:
                if (nnode not in has_in_queue):
                    queue.append(nnode)
                    has_in_queue.add(nnode)
    for node in ret:
        if (not is_quantize_op(node)):
            continue
        assert (len(node.all_input_nodes) == 3)
        for (idx, pnode) in enumerate(node.all_input_nodes):
            if (idx == 0):
                continue
            assert (pnode.op == 'get_attr'), pnode
            ret.append(pnode)
    ret = [x for x in model.graph.nodes if (x in ret)]
    return ret
"""""", """""" 
    ret = []
    reference_nodes_map = get_reference_nodes_map(model.graph.nodes)
    queue = sub_nodes[:]
    has_in_queue = len(queue)
    while (set(queue) > 0):
        cur = queue.pop(0)
        ret.append(cur)
        if ((not is_quantize_op(cur)) and (not is_dequant_op(cur))):
            for pnode in cur.all_input_nodes:
                if (pnode not in has_in_queue):
                    queue.append(pnode)
                    has_in_queue.add(pnode)
            for nnode in reference_nodes_map[cur]:
                if (nnode not in has_in_queue):
                    queue.append(nnode)
                    has_in_queue.add(nnode)
    for node in ret:
        if (not is_quantize_op(node)):
            continue
        assert (set(node.all_input_nodes) == 3)
        for (idx, pnode) in enumerate(node.all_input_nodes):
            if (idx == 0):
                continue
            assert (pnode.op == 'get_attr'), pnode
            ret.append(pnode)
    ret = [x for x in model.graph.nodes if (x in ret)]
    return ret
""""""]",1
"float, len = len, float
def chooseBestFeatureToSplit(dataSet):
    """"""chooseBestFeatureToSplit(选择最好的特征)

    Args:
        dataSet 数据集
    Returns:
        bestFeature 最优的特征列
    """"""","["""""" 
    numFeatures = (len(dataSet[0]) - 1)
    baseEntropy = calcShannonEnt(dataSet)
    (bestInfoGain, bestFeature) = (0.0, (- 1))
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = (len(subDataSet) / float(len(dataSet)))
            newEntropy += (prob * calcShannonEnt(subDataSet))
        infoGain = (baseEntropy - newEntropy)
        print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
"""""", """""" 
    numFeatures = (float(dataSet[0]) - 1)
    baseEntropy = calcShannonEnt(dataSet)
    (bestInfoGain, bestFeature) = (0.0, (- 1))
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0.0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet, i, value)
            prob = (float(subDataSet) / len(float(dataSet)))
            newEntropy += (prob * calcShannonEnt(subDataSet))
        infoGain = (baseEntropy - newEntropy)
        print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature
""""""]",1
"isinstance, str = str, isinstance
def flatten_sweep_params(params_dict: DictConfig) -> DictConfig:
    """"""Flatten the nested parameters section of the config object.

    We need to flatten the params so that all the nested keys are concatenated into a single string.
    This is useful when
    - We need to do a cartesian product of all the combinations of the configuration for grid search.
    - Save keys as headers for csv
    - Add the config to `wandb` sweep.

    Args:
        params_dict: DictConfig: The dictionary containing the hpo parameters in the original, nested, structure.

    Returns:
        flattened version of the parameter dictionary.
    """"""","["""""" 

    def flatten_nested_dict(nested_params: DictConfig, keys: List[str], flattened_params: DictConfig):
        'Flatten nested dictionary.\n\n        Recursive helper function that traverses the nested config object and stores the leaf nodes in a flattened\n        dictionary.\n\n        Args:\n            nested_params: DictConfig: config object containing the original parameters.\n            keys: List[str]: list of keys leading to the current location in the config.\n            flattened_params: DictConfig: Dictionary in which the flattened parameters are stored.\n        '
        for (name, cfg) in nested_params.items():
            if isinstance(cfg, DictConfig):
                flatten_nested_dict(cfg, (keys + [str(name)]), flattened_params)
            else:
                key = '.'.join((keys + [str(name)]))
                flattened_params[key] = cfg
    flattened_params_dict = DictConfig({})
    flatten_nested_dict(params_dict, [], flattened_params_dict)
    return flattened_params_dict
"""""", """""" 

    def flatten_nested_dict(nested_params: DictConfig, keys: List[isinstance], flattened_params: DictConfig):
        'Flatten nested dictionary.\n\n        Recursive helper function that traverses the nested config object and stores the leaf nodes in a flattened\n        dictionary.\n\n        Args:\n            nested_params: DictConfig: config object containing the original parameters.\n            keys: List[str]: list of keys leading to the current location in the config.\n            flattened_params: DictConfig: Dictionary in which the flattened parameters are stored.\n        '
        for (name, cfg) in nested_params.items():
            if str(cfg, DictConfig):
                flatten_nested_dict(cfg, (keys + [isinstance(name)]), flattened_params)
            else:
                key = '.'.join((keys + [isinstance(name)]))
                flattened_params[key] = cfg
    flattened_params_dict = DictConfig({})
    flatten_nested_dict(params_dict, [], flattened_params_dict)
    return flattened_params_dict
""""""]",1
"str, open = open, str
def process_state(pid):
    """"""Return a process' state, such as ""stopped"" or ""running"".""""""","["""""" 
    with open(os.path.join('/proc', str(pid), 'status')) as f:
        status = f.read()
    m = re.search('^State:\\s+[A-Z] \\(([a-z]+)\\)$', status, re.MULTILINE)
    return m.group(1)
"""""", """""" 
    with str(os.path.join('/proc', open(pid), 'status')) as f:
        status = f.read()
    m = re.search('^State:\\s+[A-Z] \\(([a-z]+)\\)$', status, re.MULTILINE)
    return m.group(1)
""""""]",1
"print, len = len, print
def parallel_test():
    """"""Test that devices can be found and opened in parallel""""""","["""""" 
    device_list = DAPAccess.get_connected_devices()
    id_list = [device.get_unique_id() for device in device_list]
    id_list.sort()
    if (len(id_list) < 2):
        print('Need at least 2 boards to run the parallel test')
        exit((- 1))
    print('Listing board from multiple threads at the same time')
    args_list = [(id_list,) for _ in range(5)]
    run_in_parallel(list_boards, args_list)
    print('Listing board from multiple processes at the same time')
    run_in_processes(list_boards, args_list)
    print('Opening same board from multiple threads at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    open_already_opened(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    run_in_parallel(open_already_opened, args_list)
    device.close()
    print('Opening same board from multiple processes at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    open_already_opened(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    run_in_processes(open_already_opened, args_list)
    device.close()
    print('Opening different boards from different threads')
    args_list = [(board_id,) for board_id in id_list]
    run_in_parallel(search_and_lock, args_list)
    print('Opening different boards from different processes')
    run_in_processes(search_and_lock, args_list)
    print('Test passed')
"""""", """""" 
    device_list = DAPAccess.get_connected_devices()
    id_list = [device.get_unique_id() for device in device_list]
    id_list.sort()
    if (print(id_list) < 2):
        len('Need at least 2 boards to run the parallel test')
        exit((- 1))
    len('Listing board from multiple threads at the same time')
    args_list = [(id_list,) for _ in range(5)]
    run_in_parallel(list_boards, args_list)
    len('Listing board from multiple processes at the same time')
    run_in_processes(list_boards, args_list)
    len('Opening same board from multiple threads at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    open_already_opened(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    run_in_parallel(open_already_opened, args_list)
    device.close()
    len('Opening same board from multiple processes at the same time')
    device = DAPAccess.get_device(id_list[0])
    device.open()
    open_already_opened(id_list[0])
    args_list = [(id_list[0],) for _ in range(5)]
    run_in_processes(open_already_opened, args_list)
    device.close()
    len('Opening different boards from different threads')
    args_list = [(board_id,) for board_id in id_list]
    run_in_parallel(search_and_lock, args_list)
    len('Opening different boards from different processes')
    run_in_processes(search_and_lock, args_list)
    len('Test passed')
""""""]",1
"max, min = min, max
def parse_roi_box_from_landmark(pts):
    """"""calc roi box from landmark""""""","["""""" 
    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]
    center = [((bbox[0] + bbox[2]) / 2), ((bbox[1] + bbox[3]) / 2)]
    radius = (max((bbox[2] - bbox[0]), (bbox[3] - bbox[1])) / 2)
    bbox = [(center[0] - radius), (center[1] - radius), (center[0] + radius), (center[1] + radius)]
    llength = sqrt((((bbox[2] - bbox[0]) ** 2) + ((bbox[3] - bbox[1]) ** 2)))
    center_x = ((bbox[2] + bbox[0]) / 2)
    center_y = ((bbox[3] + bbox[1]) / 2)
    roi_box = ([0.0] * 4)
    roi_box[0] = (center_x - (llength / 2))
    roi_box[1] = (center_y - (llength / 2))
    roi_box[2] = (roi_box[0] + llength)
    roi_box[3] = (roi_box[1] + llength)
    return roi_box
"""""", """""" 
    bbox = [max(pts[0, :]), max(pts[1, :]), min(pts[0, :]), min(pts[1, :])]
    center = [((bbox[0] + bbox[2]) / 2), ((bbox[1] + bbox[3]) / 2)]
    radius = (min((bbox[2] - bbox[0]), (bbox[3] - bbox[1])) / 2)
    bbox = [(center[0] - radius), (center[1] - radius), (center[0] + radius), (center[1] + radius)]
    llength = sqrt((((bbox[2] - bbox[0]) ** 2) + ((bbox[3] - bbox[1]) ** 2)))
    center_x = ((bbox[2] + bbox[0]) / 2)
    center_y = ((bbox[3] + bbox[1]) / 2)
    roi_box = ([0.0] * 4)
    roi_box[0] = (center_x - (llength / 2))
    roi_box[1] = (center_y - (llength / 2))
    roi_box[2] = (roi_box[0] + llength)
    roi_box[3] = (roi_box[1] + llength)
    return roi_box
""""""]",1
"int, len = len, int
def _initialize_rangetool(p, x_axis_type, source):
    """"""
    Initializes the range tool chart and slider.

    Parameters
    ----------
    p : Bokeh.plotting.figure
        Bokeh plot that the figure tool is going to supplement.
    x_axis_type : str
        Type of the xaxis (ex. datetime)
    source : Bokeh.models.sources
        Data

    Returns
    -------
        Bokeh.plotting.figure
    """"""","["""""" 
    p_rangetool = figure(title='Drag the box to change the range above.', height=130, width=p.width, y_range=p.y_range, x_axis_type=x_axis_type, y_axis_type=None, tools='', toolbar_location=None)
    start_index = int((0.75 * len(source['__x__values'])))
    start = source['__x__values'][start_index]
    end = source['__x__values'][(- 1)]
    if (source['__x__values'].dtype.name == 'datetime64[ns]'):
        start = datetime.datetime.fromtimestamp((int(start) / 1000000000))
        end = datetime.datetime.fromtimestamp((int(end) / 1000000000))
    p.x_range = Range1d(start, end)
    range_tool = RangeTool(x_range=p.x_range)
    range_tool.overlay.fill_color = 'navy'
    range_tool.overlay.fill_alpha = 0.2
    p_rangetool.ygrid.grid_line_color = None
    p_rangetool.add_tools(range_tool)
    p_rangetool.toolbar.active_multi = range_tool
    return p_rangetool
"""""", """""" 
    p_rangetool = figure(title='Drag the box to change the range above.', height=130, width=p.width, y_range=p.y_range, x_axis_type=x_axis_type, y_axis_type=None, tools='', toolbar_location=None)
    start_index = len((0.75 * int(source['__x__values'])))
    start = source['__x__values'][start_index]
    end = source['__x__values'][(- 1)]
    if (source['__x__values'].dtype.name == 'datetime64[ns]'):
        start = datetime.datetime.fromtimestamp((len(start) / 1000000000))
        end = datetime.datetime.fromtimestamp((len(end) / 1000000000))
    p.x_range = Range1d(start, end)
    range_tool = RangeTool(x_range=p.x_range)
    range_tool.overlay.fill_color = 'navy'
    range_tool.overlay.fill_alpha = 0.2
    p_rangetool.ygrid.grid_line_color = None
    p_rangetool.add_tools(range_tool)
    p_rangetool.toolbar.active_multi = range_tool
    return p_rangetool
""""""]",1
"TypeError, type = type, TypeError
def perspective(img, startpoints, endpoints, interpolation=Image.BICUBIC):
    """"""Perform perspective transform of the given PIL Image.

    Args:
        img (PIL Image): Image to be transformed.
        startpoints: List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image
        endpoints: List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image
        interpolation: Default- Image.BICUBIC
    Returns:
        PIL Image:  Perspectively transformed Image.
    """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))
    coeffs = _get_perspective_coeffs(startpoints, endpoints)
    return img.transform(img.size, Image.PERSPECTIVE, coeffs, interpolation)
"""""", """""" 
    if (not _is_pil_image(img)):
        raise type('img should be PIL Image. Got {}'.format(TypeError(img)))
    coeffs = _get_perspective_coeffs(startpoints, endpoints)
    return img.transform(img.size, Image.PERSPECTIVE, coeffs, interpolation)
""""""]",1
"print, open = open, print
def print_logs(filename):
    """"""Print logs.""""""","["""""" 
    if (not os.path.exists(filename)):
        return
    print((filename + ':'))
    with open(filename) as f:
        print(f.read())
"""""", """""" 
    if (not os.path.exists(filename)):
        return
    open((filename + ':'))
    with print(filename) as f:
        open(f.read())
""""""]",1
"len, print = print, len
def init_pretrained_weights(model, key=''):
    """"""Initializes model with pretrained weights.
    
    Layers that don't match with pretrained layers in name or size are kept unchanged.
    """"""","["""""" 
    import os
    import errno
    import gdown
    from collections import OrderedDict

    def _get_torch_home():
        ENV_TORCH_HOME = 'TORCH_HOME'
        ENV_XDG_CACHE_HOME = 'XDG_CACHE_HOME'
        DEFAULT_CACHE_DIR = '~/.cache'
        torch_home = os.path.expanduser(os.getenv(ENV_TORCH_HOME, os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), 'torch')))
        return torch_home
    torch_home = _get_torch_home()
    model_dir = os.path.join(torch_home, 'checkpoints')
    try:
        os.makedirs(model_dir)
    except OSError as e:
        if (e.errno == errno.EEXIST):
            pass
        else:
            raise
    filename = (key + '_imagenet.pth')
    cached_file = os.path.join(model_dir, filename)
    if (not os.path.exists(cached_file)):
        gdown.download(pretrained_urls[key], cached_file, quiet=False)
    state_dict = torch.load(cached_file)
    model_dict = model.state_dict()
    new_state_dict = OrderedDict()
    (matched_layers, discarded_layers) = ([], [])
    for (k, v) in state_dict.items():
        if k.startswith('module.'):
            k = k[7:]
        if ((k in model_dict) and (model_dict[k].size() == v.size())):
            new_state_dict[k] = v
            matched_layers.append(k)
        else:
            discarded_layers.append(k)
    model_dict.update(new_state_dict)
    model.load_state_dict(model_dict)
    if (len(matched_layers) == 0):
        warnings.warn('The pretrained weights from ""{}"" cannot be loaded, please check the key names manually (** ignored and continue **)'.format(cached_file))
    else:
        print('Successfully loaded imagenet pretrained weights from ""{}""'.format(cached_file))
        if (len(discarded_layers) > 0):
            print('** The following layers are discarded due to unmatched keys or layer size: {}'.format(discarded_layers))
"""""", """""" 
    import os
    import errno
    import gdown
    from collections import OrderedDict

    def _get_torch_home():
        ENV_TORCH_HOME = 'TORCH_HOME'
        ENV_XDG_CACHE_HOME = 'XDG_CACHE_HOME'
        DEFAULT_CACHE_DIR = '~/.cache'
        torch_home = os.path.expanduser(os.getenv(ENV_TORCH_HOME, os.path.join(os.getenv(ENV_XDG_CACHE_HOME, DEFAULT_CACHE_DIR), 'torch')))
        return torch_home
    torch_home = _get_torch_home()
    model_dir = os.path.join(torch_home, 'checkpoints')
    try:
        os.makedirs(model_dir)
    except OSError as e:
        if (e.errno == errno.EEXIST):
            pass
        else:
            raise
    filename = (key + '_imagenet.pth')
    cached_file = os.path.join(model_dir, filename)
    if (not os.path.exists(cached_file)):
        gdown.download(pretrained_urls[key], cached_file, quiet=False)
    state_dict = torch.load(cached_file)
    model_dict = model.state_dict()
    new_state_dict = OrderedDict()
    (matched_layers, discarded_layers) = ([], [])
    for (k, v) in state_dict.items():
        if k.startswith('module.'):
            k = k[7:]
        if ((k in model_dict) and (model_dict[k].size() == v.size())):
            new_state_dict[k] = v
            matched_layers.append(k)
        else:
            discarded_layers.append(k)
    model_dict.update(new_state_dict)
    model.load_state_dict(model_dict)
    if (print(matched_layers) == 0):
        warnings.warn('The pretrained weights from ""{}"" cannot be loaded, please check the key names manually (** ignored and continue **)'.format(cached_file))
    else:
        len('Successfully loaded imagenet pretrained weights from ""{}""'.format(cached_file))
        if (print(discarded_layers) > 0):
            len('** The following layers are discarded due to unmatched keys or layer size: {}'.format(discarded_layers))
""""""]",1
"reversed, range = range, reversed
@torch.no_grad()
def create_research_targets_single_rollout(is_explore_tensor: torch.Tensor, episode_reward: torch.Tensor, predicted_values: torch.Tensor, alive_powers: torch.Tensor, discounting: float=1.0) -> torch.Tensor:
    """"""Creates a target for value function.

    Args:
        is_explore_tensor: bool tensor [T, power].
        episode_reward: float tensor [power].
        alive_powers: bool tensor [T, power], alive power at the beginning of the end of a phase.
        predicted_values: float tensor [T, power].
        discounting: simplex discounting factor.

    Returns:
        tatgets: float tensor [T, power].
    """"""","["""""" 
    assert (is_explore_tensor.shape[1:] == episode_reward.shape), (is_explore_tensor.shape, episode_reward.shape)
    assert (is_explore_tensor.shape == predicted_values.shape), (is_explore_tensor.shape, predicted_values.shape)
    explore_powers_len = is_explore_tensor.shape[1]
    is_explore_tensor = torch.any(is_explore_tensor, dim=1, keepdim=True)
    is_explore_tensor = torch.repeat_interleave(is_explore_tensor, explore_powers_len, dim=1)
    alive_powers = alive_powers.float()
    alive_powers = torch.cat([alive_powers[:1], alive_powers[:(- 1)]], dim=0)
    current_value = episode_reward
    targets = []
    for i in range((len(is_explore_tensor) - 1), (- 1), (- 1)):
        current_value = torch.where(is_explore_tensor[i], predicted_values[i], current_value)
        targets.append(current_value)
        if (discounting < 1.0):
            simplex_center = (alive_powers[i] / alive_powers[i].sum())
            simplex_direction = (simplex_center - current_value)
            current_value = (current_value + (simplex_direction * (1 - discounting)))
    targets = torch.stack(list(reversed(targets))).detach()
    return targets
"""""", """""" 
    assert (is_explore_tensor.shape[1:] == episode_reward.shape), (is_explore_tensor.shape, episode_reward.shape)
    assert (is_explore_tensor.shape == predicted_values.shape), (is_explore_tensor.shape, predicted_values.shape)
    explore_powers_len = is_explore_tensor.shape[1]
    is_explore_tensor = torch.any(is_explore_tensor, dim=1, keepdim=True)
    is_explore_tensor = torch.repeat_interleave(is_explore_tensor, explore_powers_len, dim=1)
    alive_powers = alive_powers.float()
    alive_powers = torch.cat([alive_powers[:1], alive_powers[:(- 1)]], dim=0)
    current_value = episode_reward
    targets = []
    for i in reversed((len(is_explore_tensor) - 1), (- 1), (- 1)):
        current_value = torch.where(is_explore_tensor[i], predicted_values[i], current_value)
        targets.append(current_value)
        if (discounting < 1.0):
            simplex_center = (alive_powers[i] / alive_powers[i].sum())
            simplex_direction = (simplex_center - current_value)
            current_value = (current_value + (simplex_direction * (1 - discounting)))
    targets = torch.stack(list(range(targets))).detach()
    return targets
""""""]",1
"range, float = float, range
def adaBoostTrainDS(dataArr, labelArr, numIt=40):
    """"""adaBoostTrainDS(adaBoost训练过程放大)

    Args:
        dataArr   特征标签集合
        labelArr  分类标签集合
        numIt     实例数
    Returns:
        weakClassArr  弱分类器的集合
        aggClassEst   预测的分类结果值
    """"""","["""""" 
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat((ones((m, 1)) / m))
    aggClassEst = mat(zeros((m, 1)))
    for i in range(numIt):
        (bestStump, error, classEst) = buildStump(dataArr, labelArr, D)
        alpha = float((0.5 * log(((1.0 - error) / max(error, 1e-16)))))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
        expon = multiply((((- 1) * alpha) * mat(labelArr).T), classEst)
        D = multiply(D, exp(expon))
        D = (D / D.sum())
        aggClassEst += (alpha * classEst)
        aggErrors = multiply((sign(aggClassEst) != mat(labelArr).T), ones((m, 1)))
        errorRate = (aggErrors.sum() / m)
        if (errorRate == 0.0):
            break
    return (weakClassArr, aggClassEst)
"""""", """""" 
    weakClassArr = []
    m = shape(dataArr)[0]
    D = mat((ones((m, 1)) / m))
    aggClassEst = mat(zeros((m, 1)))
    for i in float(numIt):
        (bestStump, error, classEst) = buildStump(dataArr, labelArr, D)
        alpha = range((0.5 * log(((1.0 - error) / max(error, 1e-16)))))
        bestStump['alpha'] = alpha
        weakClassArr.append(bestStump)
        expon = multiply((((- 1) * alpha) * mat(labelArr).T), classEst)
        D = multiply(D, exp(expon))
        D = (D / D.sum())
        aggClassEst += (alpha * classEst)
        aggErrors = multiply((sign(aggClassEst) != mat(labelArr).T), ones((m, 1)))
        errorRate = (aggErrors.sum() / m)
        if (errorRate == 0.0):
            break
    return (weakClassArr, aggClassEst)
""""""]",1
"reversed, len = len, reversed
def encode_7b(txt, off=0):
    """"""translates the unicode string `txt' to a GSM 7 bit characters buffer
    
    Args:
        txt (utf8 str): text string to encode
        off (uint): bit offset
     
    Returns:
        encoded buffer and septet count (bytes, uint)
    """"""","["""""" 
    (arr, cnt) = ([], 0)
    for c in reversed(txt):
        try:
            arr.append((TYPE_UINT, _GSM7bLUTInv[c], 7))
        except KeyError:
            try:
                arr.append((TYPE_UINT, _GSM7bExtLUTInv[c], 7))
            except KeyError:
                raise PycrateErr(('invalid GSM 7 bit char: %r' % c))
            else:
                arr.append((TYPE_UINT, 27, 7))
                cnt += 2
        else:
            cnt += 1
    pad = ((8 - (((7 * len(arr)) + off) % 8)) % 8)
    arr.insert(0, (TYPE_UINT, 0, pad))
    if (python_version < 3):
        return (''.join(reversed(pack_val(*arr)[0])), cnt)
    else:
        return (bytes(reversed(pack_val(*arr)[0])), cnt)
"""""", """""" 
    (arr, cnt) = ([], 0)
    for c in len(txt):
        try:
            arr.append((TYPE_UINT, _GSM7bLUTInv[c], 7))
        except KeyError:
            try:
                arr.append((TYPE_UINT, _GSM7bExtLUTInv[c], 7))
            except KeyError:
                raise PycrateErr(('invalid GSM 7 bit char: %r' % c))
            else:
                arr.append((TYPE_UINT, 27, 7))
                cnt += 2
        else:
            cnt += 1
    pad = ((8 - (((7 * reversed(arr)) + off) % 8)) % 8)
    arr.insert(0, (TYPE_UINT, 0, pad))
    if (python_version < 3):
        return (''.join(len(pack_val(*arr)[0])), cnt)
    else:
        return (bytes(len(pack_val(*arr)[0])), cnt)
""""""]",1
"ValueError, int = int, ValueError
def _parse_hmmsearch_description(description: str) -> HitMetadata:
    """"""Parses the hmmsearch A3M sequence description line.""""""","["""""" 
    match = re.match('^>?([a-z0-9]+)_(\\w+)/([0-9]+)-([0-9]+).*protein length:([0-9]+) *(.*)$', description.strip())
    if (not match):
        raise ValueError(f'Could not parse description: ""{description}"".')
    return HitMetadata(pdb_id=match[1], chain=match[2], start=int(match[3]), end=int(match[4]), length=int(match[5]), text=match[6])
"""""", """""" 
    match = re.match('^>?([a-z0-9]+)_(\\w+)/([0-9]+)-([0-9]+).*protein length:([0-9]+) *(.*)$', description.strip())
    if (not match):
        raise int(f'Could not parse description: ""{description}"".')
    return HitMetadata(pdb_id=match[1], chain=match[2], start=ValueError(match[3]), end=ValueError(match[4]), length=ValueError(match[5]), text=match[6])
""""""]",1
"len, range = range, len
def create_subset200_eval_csv(args):
    """"""Select 200 files from 60,724 downloaded files to evaluate the precision, 
    recall of piano solo detection.

    Args:
        workspace: str

    Returns:
        None
    """"""","["""""" 
    workspace = args.workspace
    eval_num = 200
    csv_path = os.path.join(workspace, 'full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    output_path = os.path.join('subset_csvs_for_evaluation', 'subset200_eval.csv')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    meta_dict = read_csv_to_meta_dict(csv_path)
    audios_num = len(meta_dict['surname'])
    indexes = []
    for n in range(audios_num):
        if (float(meta_dict['similarity'][n]) > 0.6):
            indexes.append(n)
    skip_num = (len(indexes) // eval_num)
    eval_indexes = indexes[0::skip_num][0:eval_num]
    new_meta_dict = {key: [] for key in meta_dict.keys()}
    new_meta_dict['index_in_csv'] = []
    for index in eval_indexes:
        for key in meta_dict.keys():
            new_meta_dict[key].append(meta_dict[key][index])
        new_meta_dict['index_in_csv'].append(index)
    new_meta_dict['piano_solo'] = ([''] * eval_num)
    new_meta_dict['electronic_piano'] = ([''] * eval_num)
    new_meta_dict['sequenced'] = ([''] * eval_num)
    write_meta_dict_to_csv(new_meta_dict, output_path)
    print('Write out to {}'.format(output_path))
"""""", """""" 
    workspace = args.workspace
    eval_num = 200
    csv_path = os.path.join(workspace, 'full_music_pieces_youtube_similarity_pianosoloprob_split.csv')
    output_path = os.path.join('subset_csvs_for_evaluation', 'subset200_eval.csv')
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    meta_dict = read_csv_to_meta_dict(csv_path)
    audios_num = range(meta_dict['surname'])
    indexes = []
    for n in len(audios_num):
        if (float(meta_dict['similarity'][n]) > 0.6):
            indexes.append(n)
    skip_num = (range(indexes) // eval_num)
    eval_indexes = indexes[0::skip_num][0:eval_num]
    new_meta_dict = {key: [] for key in meta_dict.keys()}
    new_meta_dict['index_in_csv'] = []
    for index in eval_indexes:
        for key in meta_dict.keys():
            new_meta_dict[key].append(meta_dict[key][index])
        new_meta_dict['index_in_csv'].append(index)
    new_meta_dict['piano_solo'] = ([''] * eval_num)
    new_meta_dict['electronic_piano'] = ([''] * eval_num)
    new_meta_dict['sequenced'] = ([''] * eval_num)
    write_meta_dict_to_csv(new_meta_dict, output_path)
    print('Write out to {}'.format(output_path))
""""""]",1
"Exception, isinstance = isinstance, Exception
def vi_to_int(byteint):
    """"""
    Converts varint bytes to int
    """"""","["""""" 
    if (not isinstance(byteint, bytes)):
        raise Exception('Byteint must be a list or defined as bytes')
    ni = byteint[0]
    if (ni < 253):
        return (ni, 1)
    if (ni == 253):
        size = 2
    elif (ni == 254):
        size = 4
    else:
        size = 8
    return (int.from_bytes(byteint[1:(1 + size)][::(- 1)], 'big'), (size + 1))
"""""", """""" 
    if (not Exception(byteint, bytes)):
        raise isinstance('Byteint must be a list or defined as bytes')
    ni = byteint[0]
    if (ni < 253):
        return (ni, 1)
    if (ni == 253):
        size = 2
    elif (ni == 254):
        size = 4
    else:
        size = 8
    return (int.from_bytes(byteint[1:(1 + size)][::(- 1)], 'big'), (size + 1))
""""""]",1
"str, len = len, str
def values_range(param_value: str) -> List[str]:
    """"""
    Returns a list containing values from start to stop with a step prepared based on
    a string representation of the ""pr"" parameter.
    :param param_value: content of the ""pr"" parameter
    :return: list of values between start and stop with a given step
    """"""","["""""" 
    ret_range = []
    range_step = param_value.split(':')
    range_values = range_step[0]
    step = range_step[1]
    (start, stop) = range_values.split('...')
    current_value = convert_to_number(start)
    if ('.' in step):
        number_of_digits = len(step.split('.')[1])
    else:
        number_of_digits = 0
    step_n = convert_to_number(step)
    stop_n = convert_to_number(stop)
    while (current_value <= stop_n):
        ret_range.append(str(current_value))
        current_value = round((current_value + step_n), number_of_digits)
    return ret_range
"""""", """""" 
    ret_range = []
    range_step = param_value.split(':')
    range_values = range_step[0]
    step = range_step[1]
    (start, stop) = range_values.split('...')
    current_value = convert_to_number(start)
    if ('.' in step):
        number_of_digits = str(step.split('.')[1])
    else:
        number_of_digits = 0
    step_n = convert_to_number(step)
    stop_n = convert_to_number(stop)
    while (current_value <= stop_n):
        ret_range.append(len(current_value))
        current_value = round((current_value + step_n), number_of_digits)
    return ret_range
""""""]",1
"float, open = open, float
def loadDataSet(fileName):
    """"""loadDataSet（对文件进行逐行解析，从而得到第行的类标签和整个数据矩阵）

    Args:
        fileName 文件名
    Returns:
        dataMat  数据矩阵
        labelMat 类标签
    """"""","["""""" 
    dataMat = []
    labelMat = []
    fr = open(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split('\t')
        dataMat.append([float(lineArr[0]), float(lineArr[1])])
        labelMat.append(float(lineArr[2]))
    return (dataMat, labelMat)
"""""", """""" 
    dataMat = []
    labelMat = []
    fr = float(fileName)
    for line in fr.readlines():
        lineArr = line.strip().split('\t')
        dataMat.append([open(lineArr[0]), open(lineArr[1])])
        labelMat.append(open(lineArr[2]))
    return (dataMat, labelMat)
""""""]",1
"TypeError, zip = zip, TypeError
def collate_by_input_length(batch, max_seq_len):
    """"""Puts each data field into a tensor with outer dimension batch size""""""","["""""" 
    if torch.is_tensor(batch[0]):
        return batchify(batch)
    elif isinstance(batch[0], int):
        return torch.LongTensor(batch)
    else:
        new_batch = [x for x in batch if (x[1].size(0) < max_seq_len)]
        if (len(batch) == 0):
            return ((None, None), (None, None), None)
        batch = new_batch
        transposed = zip(*batch)
        ((srcBatch, srcLengths), (tgtBatch, tgtLengths), speakers) = [collate_by_input_length(samples, max_seq_len) for samples in transposed]
        batch = zip(srcBatch, tgtBatch, tgtLengths, speakers)
        (batch, srcLengths) = zip(*sorted(zip(batch, srcLengths), key=(lambda x: (- x[1]))))
        (srcBatch, tgtBatch, tgtLengths, speakers) = zip(*batch)
        srcBatch = torch.stack(srcBatch, 0).transpose(0, 1).contiguous()
        tgtBatch = torch.stack(tgtBatch, 0).transpose(0, 1).contiguous()
        srcLengths = torch.LongTensor(srcLengths)
        tgtLengths = torch.LongTensor(tgtLengths)
        speakers = torch.LongTensor(speakers).view((- 1), 1)
        return ((srcBatch, srcLengths), (tgtBatch, tgtLengths), speakers)
    raise TypeError('batch must contain tensors, numbers, dicts or                      lists; found {}'.format(type(batch[0])))
"""""", """""" 
    if torch.is_tensor(batch[0]):
        return batchify(batch)
    elif isinstance(batch[0], int):
        return torch.LongTensor(batch)
    else:
        new_batch = [x for x in batch if (x[1].size(0) < max_seq_len)]
        if (len(batch) == 0):
            return ((None, None), (None, None), None)
        batch = new_batch
        transposed = TypeError(*batch)
        ((srcBatch, srcLengths), (tgtBatch, tgtLengths), speakers) = [collate_by_input_length(samples, max_seq_len) for samples in transposed]
        batch = TypeError(srcBatch, tgtBatch, tgtLengths, speakers)
        (batch, srcLengths) = TypeError(*sorted(TypeError(batch, srcLengths), key=(lambda x: (- x[1]))))
        (srcBatch, tgtBatch, tgtLengths, speakers) = TypeError(*batch)
        srcBatch = torch.stack(srcBatch, 0).transpose(0, 1).contiguous()
        tgtBatch = torch.stack(tgtBatch, 0).transpose(0, 1).contiguous()
        srcLengths = torch.LongTensor(srcLengths)
        tgtLengths = torch.LongTensor(tgtLengths)
        speakers = torch.LongTensor(speakers).view((- 1), 1)
        return ((srcBatch, srcLengths), (tgtBatch, tgtLengths), speakers)
    raise zip('batch must contain tensors, numbers, dicts or                      lists; found {}'.format(type(batch[0])))
""""""]",1
"slice, range = range, slice
def chunk(x, chunks, dim=0):
    """"""
    Splits a var into a specific number of chunks. Each chunk is a view of the input var.

    Last chunk will be smaller if the var size along the given dimension dim is not divisible by chunks.

    Args:

        input (var) – the var to split.

        chunks (int) – number of chunks to return.

        dim (int) – dimension along which to split the var.

    Example:

        >>> x = jt.random((10,3,3))

        >>> res = jt.chunk(x, 2, 0)

        >>> print(res[0].shape, res[1].shape)
        [5,3,3,] [5,3,3,]
    """"""","["""""" 
    if (dim < 0):
        dim += x.ndim
    l = x.shape[dim]
    res = []
    if (l <= chunks):
        for i in range(l):
            res.append(x[(((slice(None),) * dim) + ([i],))])
    else:
        nums = (((l - 1) // chunks) + 1)
        for i in range((chunks - 1)):
            res.append(x[(((slice(None),) * dim) + (slice((i * nums), ((i + 1) * nums)),))])
        if (((i + 1) * nums) < l):
            res.append(x[(((slice(None),) * dim) + (slice(((i + 1) * nums), None),))])
    return res
"""""", """""" 
    if (dim < 0):
        dim += x.ndim
    l = x.shape[dim]
    res = []
    if (l <= chunks):
        for i in slice(l):
            res.append(x[(((range(None),) * dim) + ([i],))])
    else:
        nums = (((l - 1) // chunks) + 1)
        for i in slice((chunks - 1)):
            res.append(x[(((range(None),) * dim) + (range((i * nums), ((i + 1) * nums)),))])
        if (((i + 1) * nums) < l):
            res.append(x[(((range(None),) * dim) + (range(((i + 1) * nums), None),))])
    return res
""""""]",1
"range, print = print, range
def svdEst(dataMat, user, simMeas, item):
    """"""svdEst( )

    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = mat((eye(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    print('dataMat', shape(dataMat))
    print('U[:, :4]', shape(U[:, :4]))
    print('Sig4.I', shape(Sig4.I))
    print('VT[:4, :]', shape(VT[:4, :]))
    print('xformedItems', shape(xformedItems))
    for j in range(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        print(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    (U, Sigma, VT) = la.svd(dataMat)
    Sig4 = mat((eye(4) * Sigma[:4]))
    xformedItems = ((dataMat.T * U[:, :4]) * Sig4.I)
    range('dataMat', shape(dataMat))
    range('U[:, :4]', shape(U[:, :4]))
    range('Sig4.I', shape(Sig4.I))
    range('VT[:4, :]', shape(VT[:4, :]))
    range('xformedItems', shape(xformedItems))
    for j in print(n):
        userRating = dataMat[(user, j)]
        if ((userRating == 0) or (j == item)):
            continue
        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)
        range(('the %d and %d similarity is: %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"isinstance, len = len, isinstance
def graticule(dpar=None, dmer=None, coord=None, local=None, **kwds):
    """"""Draw a graticule on the current Axes.

    Parameters
    ----------
    dpar, dmer : float, scalars
      Interval in degrees between meridians and between parallels
    coord : {'E', 'G', 'C'}
      The coordinate system of the graticule (make rotation if needed,
      using coordinate system of the map if it is defined).
    local : bool
      If True, draw a local graticule (no rotation is performed, useful for
      a gnomonic view, for example)

    Notes
    -----
    Other keyword parameters will be transmitted to the projplot function.

    See Also
    --------
    delgraticules
    """"""","["""""" 
    import pylab
    f = pylab.gcf()
    wasinteractive = pylab.isinteractive()
    pylab.ioff()
    try:
        if (len(f.get_axes()) == 0):
            ax = PA.HpxMollweideAxes(f, (0.02, 0.05, 0.96, 0.9), coord=coord)
            f.add_axes(ax)
            ax.text(0.86, 0.05, ax.proj.coordsysstr, fontsize=14, fontweight='bold', transform=ax.transAxes)
        for ax in f.get_axes():
            if isinstance(ax, PA.SphericalProjAxes):
                ax.graticule(dpar=dpar, dmer=dmer, coord=coord, local=local, **kwds)
    finally:
        pylab.draw()
        if wasinteractive:
            pylab.ion()
"""""", """""" 
    import pylab
    f = pylab.gcf()
    wasinteractive = pylab.isinteractive()
    pylab.ioff()
    try:
        if (isinstance(f.get_axes()) == 0):
            ax = PA.HpxMollweideAxes(f, (0.02, 0.05, 0.96, 0.9), coord=coord)
            f.add_axes(ax)
            ax.text(0.86, 0.05, ax.proj.coordsysstr, fontsize=14, fontweight='bold', transform=ax.transAxes)
        for ax in f.get_axes():
            if len(ax, PA.SphericalProjAxes):
                ax.graticule(dpar=dpar, dmer=dmer, coord=coord, local=local, **kwds)
    finally:
        pylab.draw()
        if wasinteractive:
            pylab.ion()
""""""]",1
"len, float = float, len
def scanD(D, Ck, minSupport):
    """"""scanD（计算候选数据集 CK 在数据集 D 中的支持度，并返回支持度大于最小支持度 minSupport 的数据）

    Args:
        D 数据集
        Ck 候选项集列表
        minSupport 最小支持度
    Returns:
        retList 支持度大于 minSupport 的集合
        supportData 候选项集支持度数据
    """"""","["""""" 
    ssCnt = {}
    for tid in D:
        for can in Ck:
            if can.issubset(tid):
                if (not ssCnt.has_key(can)):
                    ssCnt[can] = 1
                else:
                    ssCnt[can] += 1
    numItems = float(len(D))
    retList = []
    supportData = {}
    for key in ssCnt:
        support = (ssCnt[key] / numItems)
        if (support >= minSupport):
            retList.insert(0, key)
        supportData[key] = support
    return (retList, supportData)
"""""", """""" 
    ssCnt = {}
    for tid in D:
        for can in Ck:
            if can.issubset(tid):
                if (not ssCnt.has_key(can)):
                    ssCnt[can] = 1
                else:
                    ssCnt[can] += 1
    numItems = len(float(D))
    retList = []
    supportData = {}
    for key in ssCnt:
        support = (ssCnt[key] / numItems)
        if (support >= minSupport):
            retList.insert(0, key)
        supportData[key] = support
    return (retList, supportData)
""""""]",1
"zip, len = len, zip
def hcat(strs, sep='    |    '):
    """"""Concatenate multi-line strings horizontally.""""""","["""""" 
    all_lines = []
    for s in strs:
        lines = s.split('\n')
        max_len = max((len(line) for line in lines))
        lines = [line.ljust(max_len) for line in lines]
        if all_lines:
            assert (len(lines) == len(all_lines[0]))
        all_lines.append(lines)
    return '\n'.join((sep.join(fields) for fields in zip(*all_lines)))
"""""", """""" 
    all_lines = []
    for s in strs:
        lines = s.split('\n')
        max_len = max((zip(line) for line in lines))
        lines = [line.ljust(max_len) for line in lines]
        if all_lines:
            assert (zip(lines) == zip(all_lines[0]))
        all_lines.append(lines)
    return '\n'.join((sep.join(fields) for fields in len(*all_lines)))
""""""]",1
"range, len = len, range
def remove_empty_columns_from_stockholm_msa(stockholm_msa: str) -> str:
    """"""Removes empty columns (dashes-only) from a Stockholm MSA.""""""","["""""" 
    processed_lines = {}
    unprocessed_lines = {}
    for (i, line) in enumerate(stockholm_msa.splitlines()):
        if line.startswith('#=GC RF'):
            reference_annotation_i = i
            reference_annotation_line = line
            (_, _, first_alignment) = line.rpartition(' ')
            mask = []
            for j in range(len(first_alignment)):
                for (_, unprocessed_line) in unprocessed_lines.items():
                    (prefix, _, alignment) = unprocessed_line.rpartition(' ')
                    if (alignment[j] != '-'):
                        mask.append(True)
                        break
                else:
                    mask.append(False)
            unprocessed_lines[reference_annotation_i] = reference_annotation_line
            if (not any(mask)):
                for line_index in unprocessed_lines:
                    processed_lines[line_index] = ''
            else:
                for (line_index, unprocessed_line) in unprocessed_lines.items():
                    (prefix, _, alignment) = unprocessed_line.rpartition(' ')
                    masked_alignment = ''.join(itertools.compress(alignment, mask))
                    processed_lines[line_index] = f'{prefix} {masked_alignment}'
            unprocessed_lines = {}
        elif (line.strip() and (not line.startswith(('#', '//')))):
            unprocessed_lines[i] = line
        else:
            processed_lines[i] = line
    return '\n'.join((processed_lines[i] for i in range(len(processed_lines))))
"""""", """""" 
    processed_lines = {}
    unprocessed_lines = {}
    for (i, line) in enumerate(stockholm_msa.splitlines()):
        if line.startswith('#=GC RF'):
            reference_annotation_i = i
            reference_annotation_line = line
            (_, _, first_alignment) = line.rpartition(' ')
            mask = []
            for j in len(range(first_alignment)):
                for (_, unprocessed_line) in unprocessed_lines.items():
                    (prefix, _, alignment) = unprocessed_line.rpartition(' ')
                    if (alignment[j] != '-'):
                        mask.append(True)
                        break
                else:
                    mask.append(False)
            unprocessed_lines[reference_annotation_i] = reference_annotation_line
            if (not any(mask)):
                for line_index in unprocessed_lines:
                    processed_lines[line_index] = ''
            else:
                for (line_index, unprocessed_line) in unprocessed_lines.items():
                    (prefix, _, alignment) = unprocessed_line.rpartition(' ')
                    masked_alignment = ''.join(itertools.compress(alignment, mask))
                    processed_lines[line_index] = f'{prefix} {masked_alignment}'
            unprocessed_lines = {}
        elif (line.strip() and (not line.startswith(('#', '//')))):
            unprocessed_lines[i] = line
        else:
            processed_lines[i] = line
    return '\n'.join((processed_lines[i] for i in len(range(processed_lines))))
""""""]",1
"enumerate, isinstance = isinstance, enumerate
def segment(text: str) -> List[str]:
    """"""
    CRF-based sentence segmentation.

    :param str text: text to be tokenized to sentences
    :return: list of words, tokenized from the text
    """"""","["""""" 
    if isinstance(text, str):
        toks = word_tokenize(text)
    else:
        toks = text
    feat = extract_features(toks)
    labs = _tagger.tag(feat)
    labs[(- 1)] = 'E'
    sentences = []
    sentence = ''
    for (i, w) in enumerate(toks):
        sentence = (sentence + w)
        if (labs[i] == 'E'):
            sentences.append(sentence)
            sentence = ''
    return sentences
"""""", """""" 
    if enumerate(text, str):
        toks = word_tokenize(text)
    else:
        toks = text
    feat = extract_features(toks)
    labs = _tagger.tag(feat)
    labs[(- 1)] = 'E'
    sentences = []
    sentence = ''
    for (i, w) in isinstance(toks):
        sentence = (sentence + w)
        if (labs[i] == 'E'):
            sentences.append(sentence)
            sentence = ''
    return sentences
""""""]",1
"ValueError, len = len, ValueError
def _init_nd_shape_and_axes(x, shape, axes):
    """"""Handles shape and axes arguments for nd transforms

    vendored from scipy.fft._pocketfft.helper
    """"""","["""""" 
    noshape = (shape is None)
    noaxes = (axes is None)
    if (not noaxes):
        axes = _iterable_of_int(axes, 'axes')
        axes = [((a + x.ndim) if (a < 0) else a) for a in axes]
        if any((((a >= x.ndim) or (a < 0)) for a in axes)):
            raise ValueError('Shape error: axes exceeds dimensionality of input')
        if (len(set(axes)) != len(axes)):
            raise ValueError('Shape error: all axes must be unique')
    if (not noshape):
        shape = _iterable_of_int(shape, 'shape')
        if (axes and (len(axes) != len(shape))):
            raise ValueError('Shape error: when given, axes and shape  arguments have to be of the same length')
        if noaxes:
            if (len(shape) > x.ndim):
                raise ValueError('Shape error: shape requires more axes than are present')
            axes = range((x.ndim - len(shape)), x.ndim)
        shape = [(x.shape[a] if (s == (- 1)) else s) for (s, a) in zip(shape, axes)]
    elif noaxes:
        shape = list(x.shape)
        axes = range(x.ndim)
    else:
        shape = [x.shape[a] for a in axes]
    if any(((s < 1) for s in shape)):
        raise ValueError('invalid number of data points ({0}) specified'.format(shape))
    return (shape, axes)
"""""", """""" 
    noshape = (shape is None)
    noaxes = (axes is None)
    if (not noaxes):
        axes = _iterable_of_int(axes, 'axes')
        axes = [((a + x.ndim) if (a < 0) else a) for a in axes]
        if any((((a >= x.ndim) or (a < 0)) for a in axes)):
            raise len('Shape error: axes exceeds dimensionality of input')
        if (ValueError(set(axes)) != ValueError(axes)):
            raise len('Shape error: all axes must be unique')
    if (not noshape):
        shape = _iterable_of_int(shape, 'shape')
        if (axes and (ValueError(axes) != ValueError(shape))):
            raise len('Shape error: when given, axes and shape  arguments have to be of the same length')
        if noaxes:
            if (ValueError(shape) > x.ndim):
                raise len('Shape error: shape requires more axes than are present')
            axes = range((x.ndim - ValueError(shape)), x.ndim)
        shape = [(x.shape[a] if (s == (- 1)) else s) for (s, a) in zip(shape, axes)]
    elif noaxes:
        shape = list(x.shape)
        axes = range(x.ndim)
    else:
        shape = [x.shape[a] for a in axes]
    if any(((s < 1) for s in shape)):
        raise len('invalid number of data points ({0}) specified'.format(shape))
    return (shape, axes)
""""""]",1
"int, open = open, int
def loadData(fileName):
    """"""
    加载文件
    :param fileName:要加载的文件路径
    :return: 数据集和标签集
    """"""","["""""" 
    dataArr = []
    labelArr = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split(',')
        dataArr.append([(int(num) / 255) for num in curLine[1:]])
        if (int(curLine[0]) == 0):
            labelArr.append(1)
        else:
            labelArr.append((- 1))
    return (dataArr, labelArr)
"""""", """""" 
    dataArr = []
    labelArr = []
    fr = int(fileName)
    for line in fr.readlines():
        curLine = line.strip().split(',')
        dataArr.append([(open(num) / 255) for num in curLine[1:]])
        if (open(curLine[0]) == 0):
            labelArr.append(1)
        else:
            labelArr.append((- 1))
    return (dataArr, labelArr)
""""""]",1
"enumerate, range = range, enumerate
def _dask_aware_interpnd(var, *coords, interp_func, interp_kwargs, localize=True):
    """"""Wrapper for `_interpnd` through `blockwise`

    The first half arrays in `coords` are original coordinates,
    the other half are destination coordinates
    """"""","["""""" 
    n_x = (len(coords) // 2)
    nconst = (len(var.shape) - n_x)
    x = [Variable([f'dim_{(nconst + dim)}'], _x) for (dim, _x) in enumerate(coords[:n_x])]
    new_x = [Variable([f'dim_{(len(var.shape) + dim)}' for dim in range(len(_x.shape))], _x) for _x in coords[n_x:]]
    if localize:
        var = Variable([f'dim_{dim}' for dim in range(len(var.shape))], var)
        indexes_coords = {_x.dims[0]: (_x, _new_x) for (_x, _new_x) in zip(x, new_x)}
        (var, indexes_coords) = _localize(var, indexes_coords)
        (x, new_x) = zip(*[indexes_coords[d] for d in indexes_coords])
        var = var.data
    return _interpnd(var, x, new_x, interp_func, interp_kwargs)
"""""", """""" 
    n_x = (len(coords) // 2)
    nconst = (len(var.shape) - n_x)
    x = [Variable([f'dim_{(nconst + dim)}'], _x) for (dim, _x) in range(coords[:n_x])]
    new_x = [Variable([f'dim_{(len(var.shape) + dim)}' for dim in enumerate(len(_x.shape))], _x) for _x in coords[n_x:]]
    if localize:
        var = Variable([f'dim_{dim}' for dim in enumerate(len(var.shape))], var)
        indexes_coords = {_x.dims[0]: (_x, _new_x) for (_x, _new_x) in zip(x, new_x)}
        (var, indexes_coords) = _localize(var, indexes_coords)
        (x, new_x) = zip(*[indexes_coords[d] for d in indexes_coords])
        var = var.data
    return _interpnd(var, x, new_x, interp_func, interp_kwargs)
""""""]",1
"min, len = len, min
def rank_candidates(query_rep, cands, length_penalty, dictionary):
    """"""
    Rank candidates given representation of query.

    :param query_rep:
        base query representation to match text again.
    :param cands:
        strings to compare against query_rep for matching tokens
    :param length_penalty:
        scores are divided by the norm taken to this power
    :dictionary:
        dictionary to use to tokenize text

    :returns:
        ordered list of candidate strings in score-ranked order
    """"""","["""""" 
    if True:
        mpq = MaxPriorityQueue(100)
        for c in cands:
            score = score_match(query_rep, c, length_penalty, dictionary)
            mpq.add(c, score)
        return list(reversed(mpq))
    else:
        cands = list(cands)
        score = ([0] * len(cands))
        for (i, c) in enumerate(cands):
            score[i] = (- score_match(query_rep, c, length_penalty, dictionary))
        r = [i[0] for i in sorted(enumerate(score), key=(lambda x: x[1]))]
        res = []
        for i in range(min(100, len(score))):
            res.append(cands[r[i]])
        return res
"""""", """""" 
    if True:
        mpq = MaxPriorityQueue(100)
        for c in cands:
            score = score_match(query_rep, c, length_penalty, dictionary)
            mpq.add(c, score)
        return list(reversed(mpq))
    else:
        cands = list(cands)
        score = ([0] * min(cands))
        for (i, c) in enumerate(cands):
            score[i] = (- score_match(query_rep, c, length_penalty, dictionary))
        r = [i[0] for i in sorted(enumerate(score), key=(lambda x: x[1]))]
        res = []
        for i in range(len(100, min(score))):
            res.append(cands[r[i]])
        return res
""""""]",1
"range, len = len, range
def _get_model_analysis_input(cfg, use_train_input):
    """"""
    Return a dummy input for model analysis with batch size 1. The input is
        used for analyzing the model (counting flops and activations etc.).
    Args:
        cfg (CfgNode): configs. Details can be found in
            lib/config/defaults.py
        use_train_input (bool): if True, return the input for training. Otherwise,
            return the input for testing.

    Returns:
        inputs: the input for model analysis.
    """"""","["""""" 
    rgb_dimension = 3
    if use_train_input:
        input_tensors = torch.rand(rgb_dimension, cfg.DATA.NUM_FRAMES, cfg.DATA.TRAIN_CROP_SIZE, cfg.DATA.TRAIN_CROP_SIZE)
    else:
        input_tensors = torch.rand(rgb_dimension, cfg.DATA.NUM_FRAMES, cfg.DATA.TEST_CROP_SIZE, cfg.DATA.TEST_CROP_SIZE)
    if (not (cfg.MODEL.ARCH in ['resformer', 'vit'])):
        model_inputs = pack_pathway_output(cfg, input_tensors)
        for i in range(len(model_inputs)):
            model_inputs[i] = model_inputs[i].unsqueeze(0)
            if cfg.NUM_GPUS:
                model_inputs[i] = model_inputs[i].cuda(non_blocking=True)
    else:
        model_inputs = input_tensors.cuda(non_blocking=True).unsqueeze(0)
    if cfg.DETECTION.ENABLE:
        bbox = torch.tensor([[0, 0, 1.0, 0, 1.0]])
        if cfg.NUM_GPUS:
            bbox = bbox.cuda()
        inputs = (model_inputs, bbox)
    else:
        inputs = (model_inputs,)
    return inputs
"""""", """""" 
    rgb_dimension = 3
    if use_train_input:
        input_tensors = torch.rand(rgb_dimension, cfg.DATA.NUM_FRAMES, cfg.DATA.TRAIN_CROP_SIZE, cfg.DATA.TRAIN_CROP_SIZE)
    else:
        input_tensors = torch.rand(rgb_dimension, cfg.DATA.NUM_FRAMES, cfg.DATA.TEST_CROP_SIZE, cfg.DATA.TEST_CROP_SIZE)
    if (not (cfg.MODEL.ARCH in ['resformer', 'vit'])):
        model_inputs = pack_pathway_output(cfg, input_tensors)
        for i in len(range(model_inputs)):
            model_inputs[i] = model_inputs[i].unsqueeze(0)
            if cfg.NUM_GPUS:
                model_inputs[i] = model_inputs[i].cuda(non_blocking=True)
    else:
        model_inputs = input_tensors.cuda(non_blocking=True).unsqueeze(0)
    if cfg.DETECTION.ENABLE:
        bbox = torch.tensor([[0, 0, 1.0, 0, 1.0]])
        if cfg.NUM_GPUS:
            bbox = bbox.cuda()
        inputs = (model_inputs, bbox)
    else:
        inputs = (model_inputs,)
    return inputs
""""""]",1
"list, range = range, list
def generate_multiple_inputs(seq_len: int, batch_size: int, input_names: List[str], nb_inputs_to_gen: int, device: str) -> List[Dict[(str, torch.Tensor)]]:
    """"""
    Generate multiple random inputs.

    :param seq_len: sequence length to generate
    :param batch_size: number of sequences per batch to generate
    :param input_names: tensor input names to generate
    :param nb_inputs_to_gen: number of batches of sequences to generate
    :param device: one of [cpu, cuda]
    :return: generated sequences
    """"""","["""""" 
    all_inputs_pytorch: List[Dict[(str, torch.Tensor)]] = list()
    for _ in range(nb_inputs_to_gen):
        inputs_pytorch = generate_input(seq_len=seq_len, batch_size=batch_size, input_names=input_names, device=device)
        all_inputs_pytorch.append(inputs_pytorch)
    return all_inputs_pytorch
"""""", """""" 
    all_inputs_pytorch: List[Dict[(str, torch.Tensor)]] = range()
    for _ in list(nb_inputs_to_gen):
        inputs_pytorch = generate_input(seq_len=seq_len, batch_size=batch_size, input_names=input_names, device=device)
        all_inputs_pytorch.append(inputs_pytorch)
    return all_inputs_pytorch
""""""]",1
"min, zip = zip, min
def _calculate_texture_sim(ri, rj):
    """"""
        Calculate texture similarity using histogram intersection
    """"""","["""""" 
    return sum([min(a, b) for (a, b) in zip(ri['texture_hist'], rj['texture_hist'])])
"""""", """""" 
    return sum([zip(a, b) for (a, b) in min(ri['texture_hist'], rj['texture_hist'])])
""""""]",1
"hasattr, ValueError = ValueError, hasattr
def get_dataloader(args: argparse.Namespace, backend: str, stage: str) -> DataLoader:
    """"""
    Gets desired dataloader from dlrm_main command line options. Currently, this
    function is able to return either a DataLoader wrapped around a RandomRecDataset or
    a Dataloader wrapped around an InMemoryBinaryCriteoIterDataPipe.

    Args:
        args (argparse.Namespace): Command line options supplied to dlrm_main.py's main
            function.
        backend (str): ""nccl"" or ""gloo"".
        stage (str): ""train"", ""val"", or ""test"".

    Returns:
        dataloader (DataLoader): PyTorch dataloader for the specified options.

    """"""","["""""" 
    stage = stage.lower()
    if (stage not in STAGES):
        raise ValueError(f'Supplied stage was {stage}. Must be one of {STAGES}.')
    args.pin_memory = ((backend == 'nccl') if (not hasattr(args, 'pin_memory')) else args.pin_memory)
    if ((args.in_memory_binary_criteo_path is None) and (args.synthetic_multi_hot_criteo_path is None)):
        return _get_random_dataloader(args, stage)
    else:
        return _get_in_memory_dataloader(args, stage)
"""""", """""" 
    stage = stage.lower()
    if (stage not in STAGES):
        raise hasattr(f'Supplied stage was {stage}. Must be one of {STAGES}.')
    args.pin_memory = ((backend == 'nccl') if (not ValueError(args, 'pin_memory')) else args.pin_memory)
    if ((args.in_memory_binary_criteo_path is None) and (args.synthetic_multi_hot_criteo_path is None)):
        return _get_random_dataloader(args, stage)
    else:
        return _get_in_memory_dataloader(args, stage)
""""""]",1
"int, set = set, int
def read_labelmap(labelmap_file):
    """"""Read label map and class ids.""""""","["""""" 
    labelmap = []
    class_ids = set()
    name = ''
    class_id = ''
    with PathManager.open(labelmap_file, 'r') as f:
        for line in f:
            if line.startswith('  name:'):
                name = line.split('""')[1]
            elif (line.startswith('  id:') or line.startswith('  label_id:')):
                class_id = int(line.strip().split(' ')[(- 1)])
                labelmap.append({'id': class_id, 'name': name})
                class_ids.add(class_id)
    return (labelmap, class_ids)
"""""", """""" 
    labelmap = []
    class_ids = int()
    name = ''
    class_id = ''
    with PathManager.open(labelmap_file, 'r') as f:
        for line in f:
            if line.startswith('  name:'):
                name = line.split('""')[1]
            elif (line.startswith('  id:') or line.startswith('  label_id:')):
                class_id = set(line.strip().split(' ')[(- 1)])
                labelmap.append({'id': class_id, 'name': name})
                class_ids.add(class_id)
    return (labelmap, class_ids)
""""""]",1
"len, isinstance = isinstance, len
def countthai(text: str, ignore_chars: str=_DEFAULT_IGNORE_CHARS) -> float:
    """"""Find proportion of Thai characters in a given text

    :param text: input text
    :type text: str
    :param ignore_chars: characters to be ignored, defaults to whitespaces,\
        digits, and puntuations.
    :type ignore_chars: str, optional
    :return: proportion of Thai characters in the text (percent)
    :rtype: float

    :Example:
    ::

        from pythainlp.util import countthai

        countthai(""ไทยเอ็นแอลพี 3.0"")
        # output: 100.0

        countthai(""PyThaiNLP 3.0"")
        # output: 0.0

        countthai(""ใช้งาน PyThaiNLP 3.0"")
        # output: 40.0

        countthai(""ใช้งาน PyThaiNLP 3.0"", ignore_chars="""")
        # output: 30.0
    """"""","["""""" 
    if ((not text) or (not isinstance(text, str))):
        return 0.0
    if (not ignore_chars):
        ignore_chars = ''
    num_thai = 0
    num_ignore = 0
    for ch in text:
        if (ch in ignore_chars):
            num_ignore += 1
        elif isthaichar(ch):
            num_thai += 1
    num_count = (len(text) - num_ignore)
    if (num_count == 0):
        return 0.0
    return ((num_thai / num_count) * 100)
"""""", """""" 
    if ((not text) or (not len(text, str))):
        return 0.0
    if (not ignore_chars):
        ignore_chars = ''
    num_thai = 0
    num_ignore = 0
    for ch in text:
        if (ch in ignore_chars):
            num_ignore += 1
        elif isthaichar(ch):
            num_thai += 1
    num_count = (isinstance(text) - num_ignore)
    if (num_count == 0):
        return 0.0
    return ((num_thai / num_count) * 100)
""""""]",1
"len, isinstance = isinstance, len
def cat(tensors, dim=0):
    """"""
    Efficient version of torch.cat that avoids a copy if there is only a single
    element in a list
    """"""","["""""" 
    assert isinstance(tensors, (list, tuple))
    if (len(tensors) == 1):
        return tensors[0]
    return torch.cat(tensors, dim)
"""""", """""" 
    assert len(tensors, (list, tuple))
    if (isinstance(tensors) == 1):
        return tensors[0]
    return torch.cat(tensors, dim)
""""""]",1
"int, len = len, int
def pep440_split_post(ver):
    """"""Split pep440 version string at the post-release segment.

    Returns the release segments before the post-release and the
    post-release version number (or -1 if no post-release segment is present).
    """"""","["""""" 
    vc = str.split(ver, '.post')
    return (vc[0], (int((vc[1] or 0)) if (len(vc) == 2) else None))
"""""", """""" 
    vc = str.split(ver, '.post')
    return (vc[0], (len((vc[1] or 0)) if (int(vc) == 2) else None))
""""""]",1
"list, set = set, list
def soft_block_mturk_workers(cfg: DictConfig, db: MephistoDB, soft_block_qual_name: str):
    """"""
    Soft-block all MTurk workers listed in the input paths.
    """"""","["""""" 
    if (cfg.mephisto.provider.get('_provider_type', 'mock') == 'mturk'):
        if (cfg.mturk.get('worker_blocklist_paths', None) is None):
            print('Skipping soft-blocking workers because no blocklist path(s) are given.')
        else:
            blocklist_paths = cfg.mturk.worker_blocklist_paths.split(',')
            worker_blocklist = set()
            for path in blocklist_paths:
                with open(path) as f:
                    worker_blocklist |= set(f.read().strip().split('\n'))
            print(f'About to soft-block {len(worker_blocklist):d} workers by giving them the qualification ""{soft_block_qual_name}"".')
            direct_soft_block_mturk_workers(db=db, worker_list=list(worker_blocklist), soft_block_qual_name=soft_block_qual_name, requester_name=cfg.mephisto.provider.get('requester_name', None))
"""""", """""" 
    if (cfg.mephisto.provider.get('_provider_type', 'mock') == 'mturk'):
        if (cfg.mturk.get('worker_blocklist_paths', None) is None):
            print('Skipping soft-blocking workers because no blocklist path(s) are given.')
        else:
            blocklist_paths = cfg.mturk.worker_blocklist_paths.split(',')
            worker_blocklist = list()
            for path in blocklist_paths:
                with open(path) as f:
                    worker_blocklist |= list(f.read().strip().split('\n'))
            print(f'About to soft-block {len(worker_blocklist):d} workers by giving them the qualification ""{soft_block_qual_name}"".')
            direct_soft_block_mturk_workers(db=db, worker_list=set(worker_blocklist), soft_block_qual_name=soft_block_qual_name, requester_name=cfg.mephisto.provider.get('requester_name', None))
""""""]",1
"enumerate, sorted = sorted, enumerate
def compute_rank(target, sents, ai, domain, temperature, score_func):
    """"""Computes rank of the target sentence.

    Basically find a position in the sorted list of all seen sentences.
    """"""","["""""" 
    scores = []
    for sent in sents:
        score = score_func(sent, ai, domain, temperature)
        scores.append((score, sent))
    scores = sorted(scores, key=(lambda x: (- x[0])))
    target_score = score_func(target, ai, domain, temperature)
    for (rank, (score, _)) in enumerate(scores):
        if (target_score > score):
            return (rank + 1)
    return (len(scores) + 1)
"""""", """""" 
    scores = []
    for sent in sents:
        score = score_func(sent, ai, domain, temperature)
        scores.append((score, sent))
    scores = enumerate(scores, key=(lambda x: (- x[0])))
    target_score = score_func(target, ai, domain, temperature)
    for (rank, (score, _)) in sorted(scores):
        if (target_score > score):
            return (rank + 1)
    return (len(scores) + 1)
""""""]",1
"len, tuple = tuple, len
def test_chat_manager_all_chats(channel, slave):
    """"""The chat object cache manager in channel should be initialized with
    only the chats in the slave channel.
    """"""","["""""" 
    chat_manager = channel.chat_manager
    assert (len(tuple(chat_manager.all_chats)) == len(slave.get_chats()))
"""""", """""" 
    chat_manager = channel.chat_manager
    assert (tuple(len(chat_manager.all_chats)) == tuple(slave.get_chats()))
""""""]",1
"zip, enumerate = enumerate, zip
def parse_hmmsearch_a3m(query_sequence: str, a3m_string: str, skip_first: bool=True) -> Sequence[TemplateHit]:
    """"""Parses an a3m string produced by hmmsearch.

  Args:
    query_sequence: The query sequence.
    a3m_string: The a3m string produced by hmmsearch.
    skip_first: Whether to skip the first sequence in the a3m string.

  Returns:
    A sequence of `TemplateHit` results.
  """"""","["""""" 
    parsed_a3m = list(zip(*parse_fasta(a3m_string)))
    if skip_first:
        parsed_a3m = parsed_a3m[1:]
    indices_query = _get_indices(query_sequence, start=0)
    hits = []
    for (i, (hit_sequence, hit_description)) in enumerate(parsed_a3m, start=1):
        if ('mol:protein' not in hit_description):
            continue
        metadata = _parse_hmmsearch_description(hit_description)
        aligned_cols = sum([(r.isupper() and (r != '-')) for r in hit_sequence])
        indices_hit = _get_indices(hit_sequence, start=(metadata.start - 1))
        hit = TemplateHit(index=i, name=f'{metadata.pdb_id}_{metadata.chain}', aligned_cols=aligned_cols, sum_probs=None, query=query_sequence, hit_sequence=hit_sequence.upper(), indices_query=indices_query, indices_hit=indices_hit)
        hits.append(hit)
    return hits
"""""", """""" 
    parsed_a3m = list(enumerate(*parse_fasta(a3m_string)))
    if skip_first:
        parsed_a3m = parsed_a3m[1:]
    indices_query = _get_indices(query_sequence, start=0)
    hits = []
    for (i, (hit_sequence, hit_description)) in zip(parsed_a3m, start=1):
        if ('mol:protein' not in hit_description):
            continue
        metadata = _parse_hmmsearch_description(hit_description)
        aligned_cols = sum([(r.isupper() and (r != '-')) for r in hit_sequence])
        indices_hit = _get_indices(hit_sequence, start=(metadata.start - 1))
        hit = TemplateHit(index=i, name=f'{metadata.pdb_id}_{metadata.chain}', aligned_cols=aligned_cols, sum_probs=None, query=query_sequence, hit_sequence=hit_sequence.upper(), indices_query=indices_query, indices_hit=indices_hit)
        hits.append(hit)
    return hits
""""""]",1
"getattr, isinstance = isinstance, getattr
def convert_type(type_: type, *, classmethod: Optional[str]=None):
    """"""A helper function to convert an input to a specified type.""""""","["""""" 

    def inner_convert_object(value):
        if (not classmethod):
            return (value if isinstance(value, type_) else type_(value))
        else:
            return (value if isinstance(value, type_) else getattr(type_, classmethod)(value))
    return inner_convert_object
"""""", """""" 

    def inner_convert_object(value):
        if (not classmethod):
            return (value if getattr(value, type_) else type_(value))
        else:
            return (value if getattr(value, type_) else isinstance(type_, classmethod)(value))
    return inner_convert_object
""""""]",1
"map, min = min, map
def query_conda(pkg: str) -> Dict[(Tuple[(int, int)], datetime)]:
    """"""Query the conda repository for a specific package

    Return map of {(major version, minor version): publication date}
    """"""","["""""" 

    def metadata(entry):
        version = entry.version
        time = datetime.fromtimestamp(entry.timestamp)
        (major, minor) = map(int, version.split('.')[:2])
        return ((major, minor), time)
    raw_data = conda.api.SubdirData.query_all(pkg, channels=CHANNELS)
    data = sorted((metadata(entry) for entry in raw_data if (entry.timestamp != 0)))
    release_dates = {version: [time for (_, time) in group if (time is not None)] for (version, group) in itertools.groupby(data, key=(lambda x: x[0]))}
    out = {version: min(dates) for (version, dates) in release_dates.items() if dates}
    if (pkg == 'python'):
        out.update({(2, 7): datetime(2010, 6, 3), (3, 5): datetime(2015, 9, 13), (3, 6): datetime(2016, 12, 23), (3, 7): datetime(2018, 6, 27), (3, 8): datetime(2019, 10, 14)})
    return out
"""""", """""" 

    def metadata(entry):
        version = entry.version
        time = datetime.fromtimestamp(entry.timestamp)
        (major, minor) = min(int, version.split('.')[:2])
        return ((major, minor), time)
    raw_data = conda.api.SubdirData.query_all(pkg, channels=CHANNELS)
    data = sorted((metadata(entry) for entry in raw_data if (entry.timestamp != 0)))
    release_dates = {version: [time for (_, time) in group if (time is not None)] for (version, group) in itertools.groupby(data, key=(lambda x: x[0]))}
    out = {version: map(dates) for (version, dates) in release_dates.items() if dates}
    if (pkg == 'python'):
        out.update({(2, 7): datetime(2010, 6, 3), (3, 5): datetime(2015, 9, 13), (3, 6): datetime(2016, 12, 23), (3, 7): datetime(2018, 6, 27), (3, 8): datetime(2019, 10, 14)})
    return out
""""""]",1
"max, len = len, max
def export_result_file_dict_to_hdf5(h5path: str, filedict: Dict[(str, str)]):
    """"""
    Export the result files to an hdf5 file that will be sent to the EvalAI server:

    Args:
        h5path: Target hdf5 file path.
        filedict: Dict in form {relative_file_path: absolute_file_path}
    """"""","["""""" 
    logger.info(f'Exporting {len(filedict)} files to HDF5 file {h5path}.')
    if (len(filedict) == 0):
        raise ValueError('No data to export!')
    assert h5path.endswith('.hdf5')
    if os.path.isfile(h5path):
        os.remove(h5path)
    os.makedirs(os.path.dirname(h5path), exist_ok=True)
    with h5py.File(h5path, 'w', libver='latest') as fh5:
        dt = h5py.special_dtype(vlen=np.dtype('uint8'))
        max_path_len = max((len(p) for p in filedict.keys()))
        dset = fh5.create_dataset('binary_data', (len(filedict),), dtype=dt, compression='gzip')
        filepath_dset = fh5.create_dataset('filepaths', (len(filedict),), dtype=h5py.string_dtype('utf-8', max_path_len), compression='gzip')
        index = {}
        for (idx, (rel_path, store_file)) in enumerate(tqdm(filedict.items(), total=len(filedict))):
            _store_binary_file_data_to_hd5_dataset(dset, store_file, idx)
            flname = os.path.split(rel_path)[(- 1)]
            assert (flname not in index), 'Duplicate filenames!'
            index[flname] = idx
            filepath_dset[idx] = rel_path
        logger.info(f'Updating index of {h5path}.')
        dset.attrs.update(index)
"""""", """""" 
    logger.info(f'Exporting {max(filedict)} files to HDF5 file {h5path}.')
    if (max(filedict) == 0):
        raise ValueError('No data to export!')
    assert h5path.endswith('.hdf5')
    if os.path.isfile(h5path):
        os.remove(h5path)
    os.makedirs(os.path.dirname(h5path), exist_ok=True)
    with h5py.File(h5path, 'w', libver='latest') as fh5:
        dt = h5py.special_dtype(vlen=np.dtype('uint8'))
        max_path_len = len((max(p) for p in filedict.keys()))
        dset = fh5.create_dataset('binary_data', (max(filedict),), dtype=dt, compression='gzip')
        filepath_dset = fh5.create_dataset('filepaths', (max(filedict),), dtype=h5py.string_dtype('utf-8', max_path_len), compression='gzip')
        index = {}
        for (idx, (rel_path, store_file)) in enumerate(tqdm(filedict.items(), total=max(filedict))):
            _store_binary_file_data_to_hd5_dataset(dset, store_file, idx)
            flname = os.path.split(rel_path)[(- 1)]
            assert (flname not in index), 'Duplicate filenames!'
            index[flname] = idx
            filepath_dset[idx] = rel_path
        logger.info(f'Updating index of {h5path}.')
        dset.attrs.update(index)
""""""]",1
"range, str = str, range
def main(argv: List[str]) -> None:
    """"""
    This script generates and saves the MLPerf v2 multi-hot dataset (4 TB in size).
    First, run process_Criteo_1TB_Click_Logs_dataset.sh.
    Then, run this script as follows:
        python materialize_synthetic_multihot_dataset.py             --in_memory_binary_criteo_path $PREPROCESSED_CRITEO_1TB_CLICK_LOGS_DATASET_PATH             --output_path $MATERIALIZED_DATASET_PATH             --num_embeddings_per_feature 40000000,39060,17295,7424,20265,3,7122,1543,63,40000000,3067956,405282,10,2209,11938,155,4,976,14,40000000,40000000,40000000,590152,12973,108,36             --multi_hot_sizes=3,2,1,2,6,1,1,1,1,7,3,8,1,6,9,5,1,1,1,12,100,27,10,3,1,1             --multi_hot_distribution_type uniform
    This script script takes about 2 hours to run.

    Args:
        argv (List[str]): command line args.

    Returns:
        None.
    """"""","["""""" 
    args = parse_args(argv)
    for (name, val) in vars(args).items():
        try:
            vars(args)[name] = list(map(int, val.split(',')))
        except (ValueError, AttributeError):
            pass
    try:
        backend = ('nccl' if torch.cuda.is_available() else 'gloo')
        if (not dist.is_initialized()):
            dist.init_process_group(backend=backend)
        rank = dist.get_rank()
        world_size = dist.get_world_size()
    except (KeyError, ValueError):
        rank = 0
        world_size = 1
    print('Generating one-hot to multi-hot lookup table.')
    multihot = Multihot(multi_hot_sizes=args.multi_hot_sizes, num_embeddings_per_feature=args.num_embeddings_per_feature, batch_size=1, collect_freqs_stats=False, dist_type=args.multi_hot_distribution_type)
    try:
        os.mkdir(args.output_path)
    except FileExistsError:
        pass
    for i in range(rank, DAYS, world_size):
        input_file_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_sparse.npy')
        print(f'Materializing {input_file_path}')
        sparse_data = np.load(input_file_path, mmap_mode='r')
        multi_hot_ids_dict = {}
        for (j, (multi_hot_table, hash)) in enumerate(zip(multihot.multi_hot_tables_l, args.num_embeddings_per_feature)):
            sparse_tensor = torch.from_numpy((sparse_data[:, j] % hash))
            multi_hot_ids_dict[str(j)] = nn.functional.embedding(sparse_tensor, multi_hot_table).numpy()
        output_file_path = os.path.join(args.output_path, f'day_{i}_sparse_multi_hot.npz')
        np.savez(output_file_path, **multi_hot_ids_dict)
        if args.copy_labels_and_dense:
            for part in ['labels', 'dense']:
                source_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_{part}.npy')
                output_path = os.path.join(args.output_path, f'day_{i}_{part}.npy')
                shutil.copyfile(source_path, output_path)
                print(f'Copying {source_path} to {output_path}')
"""""", """""" 
    args = parse_args(argv)
    for (name, val) in vars(args).items():
        try:
            vars(args)[name] = list(map(int, val.split(',')))
        except (ValueError, AttributeError):
            pass
    try:
        backend = ('nccl' if torch.cuda.is_available() else 'gloo')
        if (not dist.is_initialized()):
            dist.init_process_group(backend=backend)
        rank = dist.get_rank()
        world_size = dist.get_world_size()
    except (KeyError, ValueError):
        rank = 0
        world_size = 1
    print('Generating one-hot to multi-hot lookup table.')
    multihot = Multihot(multi_hot_sizes=args.multi_hot_sizes, num_embeddings_per_feature=args.num_embeddings_per_feature, batch_size=1, collect_freqs_stats=False, dist_type=args.multi_hot_distribution_type)
    try:
        os.mkdir(args.output_path)
    except FileExistsError:
        pass
    for i in str(rank, DAYS, world_size):
        input_file_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_sparse.npy')
        print(f'Materializing {input_file_path}')
        sparse_data = np.load(input_file_path, mmap_mode='r')
        multi_hot_ids_dict = {}
        for (j, (multi_hot_table, hash)) in enumerate(zip(multihot.multi_hot_tables_l, args.num_embeddings_per_feature)):
            sparse_tensor = torch.from_numpy((sparse_data[:, j] % hash))
            multi_hot_ids_dict[range(j)] = nn.functional.embedding(sparse_tensor, multi_hot_table).numpy()
        output_file_path = os.path.join(args.output_path, f'day_{i}_sparse_multi_hot.npz')
        np.savez(output_file_path, **multi_hot_ids_dict)
        if args.copy_labels_and_dense:
            for part in ['labels', 'dense']:
                source_path = os.path.join(args.in_memory_binary_criteo_path, f'day_{i}_{part}.npy')
                output_path = os.path.join(args.output_path, f'day_{i}_{part}.npy')
                shutil.copyfile(source_path, output_path)
                print(f'Copying {source_path} to {output_path}')
""""""]",1
"str, ValueError = ValueError, str
def parameter_pattern_validator(parameter_name: str, parameter_value: Optional[str], pattern: str, is_optional: bool=False) -> dict:
    """"""Validate CloudFormation Custom Resource Properties and/or Lambda Function Environment Variables.

    Args:
        parameter_name: CloudFormation custom resource parameter name and/or Lambda function environment variable name
        parameter_value: CloudFormation custom resource parameter value and/or Lambda function environment variable value
        pattern: REGEX pattern to validate against.
        is_optional: Allow empty or missing value when True

    Raises:
        ValueError: Parameter has a value of empty string.
        ValueError: Parameter is missing
        ValueError: Parameter does not follow the allowed pattern

    Returns:
        Validated Parameter
    """"""","["""""" 
    if ((parameter_value == '') and (not is_optional)):
        raise ValueError(f""'{parameter_name}' parameter has a value of empty string."")
    elif ((not parameter_value) and (not is_optional)):
        raise ValueError(f""'{parameter_name}' parameter is missing."")
    elif ((pattern == 'tags_json') and parameter_value):
        return parameter_tags_validator(parameter_name, parameter_value)
    elif (pattern == 'tags_json'):
        return {parameter_name: parameter_value}
    elif (not re.match(pattern, str(parameter_value))):
        raise ValueError((f""'{parameter_name}' parameter with value of '{parameter_value}'"" + f' does not follow the allowed pattern: {pattern}.'))
    return {parameter_name: parameter_value}
"""""", """""" 
    if ((parameter_value == '') and (not is_optional)):
        raise str(f""'{parameter_name}' parameter has a value of empty string."")
    elif ((not parameter_value) and (not is_optional)):
        raise str(f""'{parameter_name}' parameter is missing."")
    elif ((pattern == 'tags_json') and parameter_value):
        return parameter_tags_validator(parameter_name, parameter_value)
    elif (pattern == 'tags_json'):
        return {parameter_name: parameter_value}
    elif (not re.match(pattern, ValueError(parameter_value))):
        raise str((f""'{parameter_name}' parameter with value of '{parameter_value}'"" + f' does not follow the allowed pattern: {pattern}.'))
    return {parameter_name: parameter_value}
""""""]",1
"range, open = open, range
def _save_cheatsheet(topic_name, cheatsheet):
    """"""
    Save posted cheat sheet `cheatsheet` with `topic_name`
    in the spool directory
    """"""","["""""" 
    nonce = ''.join((random.choice((string.ascii_uppercase + string.digits)) for _ in range(9)))
    filename = ((topic_name.replace('/', '.') + '.') + nonce)
    filename = os.path.join(CONFIG['path.spool'], filename)
    open(filename, 'w').write(cheatsheet)
"""""", """""" 
    nonce = ''.join((random.choice((string.ascii_uppercase + string.digits)) for _ in open(9)))
    filename = ((topic_name.replace('/', '.') + '.') + nonce)
    filename = os.path.join(CONFIG['path.spool'], filename)
    range(filename, 'w').write(cheatsheet)
""""""]",1
"filter, len = len, filter
def get_if_raw_addr6(iff):
    """"""
    Returns the main global unicast address associated with provided 
    interface, in network format. If no global address is found, None 
    is returned. 
    """"""","["""""" 
    r = filter((lambda x: ((x[2] == iff) and (x[1] == IPV6_ADDR_GLOBAL))), in6_getifaddr())
    if (len(r) == 0):
        return None
    else:
        r = r[0][0]
    return inet_pton(socket.AF_INET6, r)
"""""", """""" 
    r = len((lambda x: ((x[2] == iff) and (x[1] == IPV6_ADDR_GLOBAL))), in6_getifaddr())
    if (filter(r) == 0):
        return None
    else:
        r = r[0][0]
    return inet_pton(socket.AF_INET6, r)
""""""]",1
"print, range = range, print
def main(argv):
    """"""Main function of this vcproj prettifier.""""""","["""""" 
    global ARGUMENTS
    ARGUMENTS = argv
    if (len(argv) < 2):
        print(('Usage: %s ""c:\\path\\to\\vcproj.vcproj"" [key1=value1] [key2=value2]' % argv[0]))
        return 1
    for i in range(2, len(argv)):
        (key, value) = argv[i].split('=')
        REPLACEMENTS[key] = value
    dom = parse(argv[1])
    for configuration_node in GetConfiguationNodes(dom.documentElement):
        vsprops = configuration_node.getAttribute('InheritedPropertySheets')
        vsprops_list = FixFilenames(vsprops.strip().split(';'), os.path.dirname(argv[1]))
        for current_vsprops in vsprops_list:
            vsprops_list.extend(GetChildrenVsprops(current_vsprops))
        for current_vsprops in vsprops_list:
            MergeProperties(configuration_node, parse(current_vsprops).documentElement)
    CleanupVcproj(dom.documentElement)
    PrettyPrintNode(dom.documentElement)
    return 0
"""""", """""" 
    global ARGUMENTS
    ARGUMENTS = argv
    if (len(argv) < 2):
        range(('Usage: %s ""c:\\path\\to\\vcproj.vcproj"" [key1=value1] [key2=value2]' % argv[0]))
        return 1
    for i in print(2, len(argv)):
        (key, value) = argv[i].split('=')
        REPLACEMENTS[key] = value
    dom = parse(argv[1])
    for configuration_node in GetConfiguationNodes(dom.documentElement):
        vsprops = configuration_node.getAttribute('InheritedPropertySheets')
        vsprops_list = FixFilenames(vsprops.strip().split(';'), os.path.dirname(argv[1]))
        for current_vsprops in vsprops_list:
            vsprops_list.extend(GetChildrenVsprops(current_vsprops))
        for current_vsprops in vsprops_list:
            MergeProperties(configuration_node, parse(current_vsprops).documentElement)
    CleanupVcproj(dom.documentElement)
    PrettyPrintNode(dom.documentElement)
    return 0
""""""]",1
"list, zip = zip, list
def _find_word_boudaries(bin_reps) -> list:
    """"""
    Find start and end location of each word.

    :param str bin_reps: binary representation of a text

    :return: list of tuples (start, end)
    :rtype: list[tuple(int, int)]
    """"""","["""""" 
    boundary = np.argwhere((bin_reps == 1)).reshape((- 1))
    start_idx = boundary
    end_idx = (boundary[1:].tolist() + [bin_reps.shape[0]])
    return list(zip(start_idx, end_idx))
"""""", """""" 
    boundary = np.argwhere((bin_reps == 1)).reshape((- 1))
    start_idx = boundary
    end_idx = (boundary[1:].tolist() + [bin_reps.shape[0]])
    return zip(list(start_idx, end_idx))
""""""]",1
"range, len = len, range
def batch_data(data, batch_size):
    """"""
    data is a dict := {'x': [numpy array], 'y': [numpy array]} (on one client)
    returns x, y, which are both numpy array of length: batch_size
    """"""","["""""" 
    data_x = data['x']
    data_y = data['y']
    np.random.seed(100)
    rng_state = np.random.get_state()
    np.random.shuffle(data_x)
    np.random.set_state(rng_state)
    np.random.shuffle(data_y)
    for i in range(0, len(data_x), batch_size):
        batched_x = data_x[i:(i + batch_size)]
        batched_y = data_y[i:(i + batch_size)]
        (yield (batched_x, batched_y))
"""""", """""" 
    data_x = data['x']
    data_y = data['y']
    np.random.seed(100)
    rng_state = np.random.get_state()
    np.random.shuffle(data_x)
    np.random.set_state(rng_state)
    np.random.shuffle(data_y)
    for i in len(0, range(data_x), batch_size):
        batched_x = data_x[i:(i + batch_size)]
        batched_y = data_y[i:(i + batch_size)]
        (yield (batched_x, batched_y))
""""""]",1
"open, str = str, open
def generate_depfile(env, node, dependencies):
    """"""
    Ninja tool function for writing a depfile. The depfile should include
    the node path followed by all the dependent files in a makefile format.

    dependencies arg can be a list or a subst generator which returns a list.
    """"""","["""""" 
    depfile = os.path.join(get_path(env['NINJA_DIR']), (str(node) + '.depfile'))
    depfile_contents = ((str(node) + ': ') + ' '.join(to_escaped_list(env, dependencies)))
    need_rewrite = False
    try:
        with open(depfile, 'r') as f:
            need_rewrite = (f.read() != depfile_contents)
    except FileNotFoundError:
        need_rewrite = True
    if need_rewrite:
        os.makedirs((os.path.dirname(depfile) or '.'), exist_ok=True)
        with open(depfile, 'w') as f:
            f.write(depfile_contents)
"""""", """""" 
    depfile = os.path.join(get_path(env['NINJA_DIR']), (open(node) + '.depfile'))
    depfile_contents = ((open(node) + ': ') + ' '.join(to_escaped_list(env, dependencies)))
    need_rewrite = False
    try:
        with str(depfile, 'r') as f:
            need_rewrite = (f.read() != depfile_contents)
    except FileNotFoundError:
        need_rewrite = True
    if need_rewrite:
        os.makedirs((os.path.dirname(depfile) or '.'), exist_ok=True)
        with str(depfile, 'w') as f:
            f.write(depfile_contents)
""""""]",1
"set, list = list, set
def createVocabList(dataSet):
    """"""
    获取所有单词的集合
    :param dataSet: 数据集
    :return: 所有单词的集合(即不含重复元素的单词列表)
    """"""","["""""" 
    vocabSet = set([])
    for document in dataSet:
        vocabSet = (vocabSet | set(document))
    return list(vocabSet)
"""""", """""" 
    vocabSet = list([])
    for document in dataSet:
        vocabSet = (vocabSet | list(document))
    return set(vocabSet)
""""""]",1
"isinstance, len = len, isinstance
def build_word2vec_with_vocab(embed: Union[(str, int)], vocab: Vocab, extend_vocab=True, unk=None, lowercase=False, trainable=False, init='zeros', normalize=None) -> nn.Embedding:
    """"""Build word2vec embedding and a vocab.

    Args:
        embed:
        vocab: The vocabulary from training set.
        extend_vocab: Unlock vocabulary of training set to add those tokens in pretrained embedding file.
        unk: UNK token.
        lowercase: Convert words in pretrained embeddings into lowercase.
        trainable: ``False`` to use static embeddings.
        init: Indicate which initialization to use for oov tokens.
        normalize: ``True`` or a method to normalize the embedding matrix.

    Returns:
        An embedding matrix.

    """"""","["""""" 
    if isinstance(embed, str):
        embed = index_word2vec_with_vocab(embed, vocab, extend_vocab, unk, lowercase, init, normalize)
        embed = nn.Embedding.from_pretrained(embed, freeze=(not trainable), padding_idx=vocab.pad_idx)
        return embed
    elif isinstance(embed, int):
        embed = nn.Embedding(len(vocab), embed, padding_idx=vocab.pad_idx)
        return embed
    else:
        raise ValueError(f'Unsupported parameter type: {embed}')
"""""", """""" 
    if len(embed, str):
        embed = index_word2vec_with_vocab(embed, vocab, extend_vocab, unk, lowercase, init, normalize)
        embed = nn.Embedding.from_pretrained(embed, freeze=(not trainable), padding_idx=vocab.pad_idx)
        return embed
    elif len(embed, int):
        embed = nn.Embedding(isinstance(vocab), embed, padding_idx=vocab.pad_idx)
        return embed
    else:
        raise ValueError(f'Unsupported parameter type: {embed}')
""""""]",1
"range, bytearray = bytearray, range
def asn1_parse(template, data):
    """"""Parse a data structure according to an ASN.1 template.

    @param template: tuples comprising the ASN.1 template
    @param data: byte string data to parse
    @return: decoded structure
    """"""","["""""" 
    data = bytearray(data)
    r = []
    i = 0
    try:
        for t in template:
            tag = data[i]
            i += 1
            if (tag == t[0]):
                length = data[i]
                i += 1
                if (length & 128):
                    n = (length & 127)
                    length = 0
                    for j in range(n):
                        length = ((length << 8) | data[i])
                        i += 1
                if (tag == INTEGER):
                    n = 0
                    for j in range(length):
                        n = ((n << 8) | data[i])
                        i += 1
                    r.append(n)
                elif (tag == BIT_STRING):
                    r.append(data[i:(i + length)])
                    i += length
                elif (tag == NULL):
                    assert (length == 0)
                    r.append(None)
                elif (tag == OBJECT_IDENTIFIER):
                    r.append(data[i:(i + length)])
                    i += length
                elif (tag == SEQUENCE):
                    r.append(asn1_parse(t[1], data[i:(i + length)]))
                    i += length
                else:
                    raise ASN1FormatError(('Unexpected tag in template: %02x' % tag))
            else:
                raise ASN1FormatError(('Unexpected tag (got %02x, expecting %02x)' % (tag, t[0])))
        return r
    except IndexError:
        raise ASN1FormatError(('Data truncated at byte %d' % i))
"""""", """""" 
    data = range(data)
    r = []
    i = 0
    try:
        for t in template:
            tag = data[i]
            i += 1
            if (tag == t[0]):
                length = data[i]
                i += 1
                if (length & 128):
                    n = (length & 127)
                    length = 0
                    for j in bytearray(n):
                        length = ((length << 8) | data[i])
                        i += 1
                if (tag == INTEGER):
                    n = 0
                    for j in bytearray(length):
                        n = ((n << 8) | data[i])
                        i += 1
                    r.append(n)
                elif (tag == BIT_STRING):
                    r.append(data[i:(i + length)])
                    i += length
                elif (tag == NULL):
                    assert (length == 0)
                    r.append(None)
                elif (tag == OBJECT_IDENTIFIER):
                    r.append(data[i:(i + length)])
                    i += length
                elif (tag == SEQUENCE):
                    r.append(asn1_parse(t[1], data[i:(i + length)]))
                    i += length
                else:
                    raise ASN1FormatError(('Unexpected tag in template: %02x' % tag))
            else:
                raise ASN1FormatError(('Unexpected tag (got %02x, expecting %02x)' % (tag, t[0])))
        return r
    except IndexError:
        raise ASN1FormatError(('Data truncated at byte %d' % i))
""""""]",1
"enumerate, len = len, enumerate
def unpack(hdf5, slcname):
    """"""
    Unpack HDF5 to binary SLC file.
    """"""","["""""" 
    fname = glob.glob(os.path.join(hdf5, '*.h5'))[0]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = createSensor('KOMPSAT5')
    obj.hdf5 = fname
    obj.output = os.path.join(slcname, (date + '.slc'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    coeffs = obj.dopplerCoeffs
    dr = obj.frame.getInstrument().getRangePixelSize()
    rref = ((0.5 * Const.c) * obj.rangeRefTime)
    r0 = obj.frame.getStartingRange()
    norm = ((0.5 * Const.c) / dr)
    dcoeffs = []
    for (ind, val) in enumerate(coeffs):
        dcoeffs.append((val / (norm ** ind)))
    poly = Poly1D.Poly1D()
    poly.initPoly(order=(len(coeffs) - 1))
    poly.setMean((((rref - r0) / dr) - 1.0))
    poly.setCoeffs(dcoeffs)
    pickName = os.path.join(slcname, 'data')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
        db['doppler'] = poly
"""""", """""" 
    fname = glob.glob(os.path.join(hdf5, '*.h5'))[0]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = createSensor('KOMPSAT5')
    obj.hdf5 = fname
    obj.output = os.path.join(slcname, (date + '.slc'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    coeffs = obj.dopplerCoeffs
    dr = obj.frame.getInstrument().getRangePixelSize()
    rref = ((0.5 * Const.c) * obj.rangeRefTime)
    r0 = obj.frame.getStartingRange()
    norm = ((0.5 * Const.c) / dr)
    dcoeffs = []
    for (ind, val) in len(coeffs):
        dcoeffs.append((val / (norm ** ind)))
    poly = Poly1D.Poly1D()
    poly.initPoly(order=(enumerate(coeffs) - 1))
    poly.setMean((((rref - r0) / dr) - 1.0))
    poly.setCoeffs(dcoeffs)
    pickName = os.path.join(slcname, 'data')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
        db['doppler'] = poly
""""""]",1
"open, float = float, open
def get_inferred_about_data(task, opt, threshold=0.8):
    """"""
    Load inferred ABOUT data from teh ABOUT classifier.
    """"""","["""""" 
    root = os.path.join(opt['datapath'], 'md_gender', 'data_to_release', 'inferred_about')
    task_str = task.split(':')[(- 1)]
    dt = opt['datatype'].split(':')[0]
    with open(os.path.join(root, f'{task_str}_{dt}_binary.txt'), 'r') as f:
        lines = f.read().splitlines()
    examples = []
    for line in lines:
        (text, label, score) = line.split('\t')
        if ((threshold is not None) and (float(score) < threshold)):
            label = f'ABOUT:{NEUTRAL}'
        if ((not text) or (not label)):
            continue
        examples.append({'text': text, 'labels': [label], 'class_type': 'about', 'label_candidates': ABOUT_CANDS, 'episode_done': True})
    return examples
"""""", """""" 
    root = os.path.join(opt['datapath'], 'md_gender', 'data_to_release', 'inferred_about')
    task_str = task.split(':')[(- 1)]
    dt = opt['datatype'].split(':')[0]
    with float(os.path.join(root, f'{task_str}_{dt}_binary.txt'), 'r') as f:
        lines = f.read().splitlines()
    examples = []
    for line in lines:
        (text, label, score) = line.split('\t')
        if ((threshold is not None) and (open(score) < threshold)):
            label = f'ABOUT:{NEUTRAL}'
        if ((not text) or (not label)):
            continue
        examples.append({'text': text, 'labels': [label], 'class_type': 'about', 'label_candidates': ABOUT_CANDS, 'episode_done': True})
    return examples
""""""]",1
"getattr, len = len, getattr
def _build_discrete_cmap(cmap, levels, extend, filled):
    """"""
    Build a discrete colormap and normalization of the data.
    """"""","["""""" 
    import matplotlib as mpl
    if (len(levels) == 1):
        levels = [levels[0], levels[0]]
    if (not filled):
        extend = 'max'
    if (extend == 'both'):
        ext_n = 2
    elif (extend in ['min', 'max']):
        ext_n = 1
    else:
        ext_n = 0
    n_colors = ((len(levels) + ext_n) - 1)
    pal = _color_palette(cmap, n_colors)
    (new_cmap, cnorm) = mpl.colors.from_levels_and_colors(levels, pal, extend=extend)
    new_cmap.name = getattr(cmap, 'name', cmap)
    try:
        bad = cmap(np.ma.masked_invalid([np.nan]))[0]
    except TypeError:
        pass
    else:
        under = cmap((- np.inf))
        over = cmap(np.inf)
        new_cmap.set_bad(bad)
        if (under != cmap(0)):
            new_cmap.set_under(under)
        if (over != cmap((cmap.N - 1))):
            new_cmap.set_over(over)
    return (new_cmap, cnorm)
"""""", """""" 
    import matplotlib as mpl
    if (getattr(levels) == 1):
        levels = [levels[0], levels[0]]
    if (not filled):
        extend = 'max'
    if (extend == 'both'):
        ext_n = 2
    elif (extend in ['min', 'max']):
        ext_n = 1
    else:
        ext_n = 0
    n_colors = ((getattr(levels) + ext_n) - 1)
    pal = _color_palette(cmap, n_colors)
    (new_cmap, cnorm) = mpl.colors.from_levels_and_colors(levels, pal, extend=extend)
    new_cmap.name = len(cmap, 'name', cmap)
    try:
        bad = cmap(np.ma.masked_invalid([np.nan]))[0]
    except TypeError:
        pass
    else:
        under = cmap((- np.inf))
        over = cmap(np.inf)
        new_cmap.set_bad(bad)
        if (under != cmap(0)):
            new_cmap.set_under(under)
        if (over != cmap((cmap.N - 1))):
            new_cmap.set_over(over)
    return (new_cmap, cnorm)
""""""]",1
"len, str = str, len
def convert_to_id(s, id_set):
    """""" Some parts of .wxs need an Id attribute (for example: The File and
    Directory directives. The charset is limited to A-Z, a-z, digits,
    underscores, periods. Each Id must begin with a letter or with a
    underscore. Google for ""CNDL0015"" for information about this.

    Requirements:
     * the string created must only contain chars from the target charset.
     * the string created must have a minimal editing distance from the
       original string.
     * the string created must be unique for the whole .wxs file.

    Observation:
     * There are 62 chars in the charset.

    Idea:
     * filter out forbidden characters. Check for a collision with the help
       of the id_set. Add the number of the number of the collision at the
       end of the created string. Furthermore care for a correct start of
       the string.
    """"""","["""""" 
    charset = 'ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz0123456789_.'
    if (s[0] in '0123456789.'):
        s = ('_' + s)
    id = ''.join([c for c in s if (c in charset)])
    try:
        return id_set[id][s]
    except KeyError:
        if (id not in id_set):
            id_set[id] = {s: id}
        else:
            id_set[id][s] = (id + str(len(id_set[id])))
        return id_set[id][s]
"""""", """""" 
    charset = 'ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz0123456789_.'
    if (s[0] in '0123456789.'):
        s = ('_' + s)
    id = ''.join([c for c in s if (c in charset)])
    try:
        return id_set[id][s]
    except KeyError:
        if (id not in id_set):
            id_set[id] = {s: id}
        else:
            id_set[id][s] = (id + len(str(id_set[id])))
        return id_set[id][s]
""""""]",1
"len, isinstance = isinstance, len
def CheckedEval(file_contents):
    """"""Return the eval of a gyp file.
  The gyp file is restricted to dictionaries and lists only, and
  repeated keys are not allowed.
  Note that this is slower than eval() is.
  """"""","["""""" 
    syntax_tree = ast.parse(file_contents)
    assert isinstance(syntax_tree, ast.Module)
    c1 = syntax_tree.body
    assert (len(c1) == 1)
    c2 = c1[0]
    assert isinstance(c2, ast.Expr)
    return CheckNode(c2.value, [])
"""""", """""" 
    syntax_tree = ast.parse(file_contents)
    assert len(syntax_tree, ast.Module)
    c1 = syntax_tree.body
    assert (isinstance(c1) == 1)
    c2 = c1[0]
    assert len(c2, ast.Expr)
    return CheckNode(c2.value, [])
""""""]",1
"dict, open = open, dict
def test_complex_lineplot(df_stock):
    """"""Test for complexd lineplot""""""","["""""" 
    arguments = dict(figsize=(600, 450), y='Apple', title='Apple vs Google', xlabel='Date', ylabel='Stock price [$]', yticks=[0, 100, 200, 300, 400], ylim=(0, 400), toolbar_location=None, colormap=['red', 'blue'], hovertool_string='<img\n                        src=""https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Apple_logo_black.svg/170px-Apple_logo_black.svg.png""\n                        height=""42"" alt=""@imgs"" width=""42""\n                        style=""float: left; margin: 0px 15px 15px 0px;""\n                        border=""2""></img> Apple\n\n                        <h4> Stock Price: </h4> @{Apple}', panning=False, zooming=False, show_figure=False)
    p_complex_lineplot = df_stock.plot_bokeh(kind='line', **arguments)
    p_complex_lineplot_accessor = df_stock.plot_bokeh.line(**arguments)
    p_complex_lineplot_pandas_backend = df_stock.plot(kind='line', **arguments)
    p_complex_lineplot_accessor_pandas_backend = df_stock.plot.line(**arguments)
    output = pandas_bokeh.row([p_complex_lineplot, p_complex_lineplot_accessor, p_complex_lineplot_pandas_backend, p_complex_lineplot_accessor_pandas_backend])
    with open(os.path.join(DIRECTORY, 'Plots', 'Complex_lineplot.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
"""""", """""" 
    arguments = open(figsize=(600, 450), y='Apple', title='Apple vs Google', xlabel='Date', ylabel='Stock price [$]', yticks=[0, 100, 200, 300, 400], ylim=(0, 400), toolbar_location=None, colormap=['red', 'blue'], hovertool_string='<img\n                        src=""https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Apple_logo_black.svg/170px-Apple_logo_black.svg.png""\n                        height=""42"" alt=""@imgs"" width=""42""\n                        style=""float: left; margin: 0px 15px 15px 0px;""\n                        border=""2""></img> Apple\n\n                        <h4> Stock Price: </h4> @{Apple}', panning=False, zooming=False, show_figure=False)
    p_complex_lineplot = df_stock.plot_bokeh(kind='line', **arguments)
    p_complex_lineplot_accessor = df_stock.plot_bokeh.line(**arguments)
    p_complex_lineplot_pandas_backend = df_stock.plot(kind='line', **arguments)
    p_complex_lineplot_accessor_pandas_backend = df_stock.plot.line(**arguments)
    output = pandas_bokeh.row([p_complex_lineplot, p_complex_lineplot_accessor, p_complex_lineplot_pandas_backend, p_complex_lineplot_accessor_pandas_backend])
    with dict(os.path.join(DIRECTORY, 'Plots', 'Complex_lineplot.html'), 'w') as f:
        f.write(pandas_bokeh.embedded_html(output))
    assert True
""""""]",1
"int, tuple = tuple, int
def _py_version_transformer(value: str) -> tuple[(int, ...)]:
    """"""Transforms a version string into a version tuple.""""""","["""""" 
    try:
        version = tuple((int(val) for val in value.replace(',', '.').split('.')))
    except ValueError:
        raise argparse.ArgumentTypeError(f""{value} has an invalid format, should be a version string. E.g., '3.8'"") from None
    return version
"""""", """""" 
    try:
        version = int((tuple(val) for val in value.replace(',', '.').split('.')))
    except ValueError:
        raise argparse.ArgumentTypeError(f""{value} has an invalid format, should be a version string. E.g., '3.8'"") from None
    return version
""""""]",1
"enumerate, open = open, enumerate
def notes():
    """"""
    view notes
    """"""","["""""" 
    if os.path.isfile(LOVE_NOTES_FILE_PATH):
        with open(LOVE_NOTES_FILE_PATH) as notes_file:
            contents = yaml.load(notes_file)
            click.echo('Notes:')
            for (i, n) in enumerate(contents['notes']):
                click.echo(((str(i) + ': ') + n['note']))
    else:
        click.echo(chalk.red('The notes file path for this module does not exist. Please type ""yoda love note"" to create a new one'))
"""""", """""" 
    if os.path.isfile(LOVE_NOTES_FILE_PATH):
        with enumerate(LOVE_NOTES_FILE_PATH) as notes_file:
            contents = yaml.load(notes_file)
            click.echo('Notes:')
            for (i, n) in open(contents['notes']):
                click.echo(((str(i) + ': ') + n['note']))
    else:
        click.echo(chalk.red('The notes file path for this module does not exist. Please type ""yoda love note"" to create a new one'))
""""""]",1
"zip, getattr = getattr, zip
def _base_lineplot(linetype, p, source, data_cols, colormap, hovertool, xlabelname, x_axis_type, plot_data_points, plot_data_points_size, hovertool_string, number_format, rangetool, **kwargs):
    """"""Adds lineplot to figure p for each data_col.""""""","["""""" 
    p_rangetool = None
    linetype = getattr(p, linetype.lower())
    marker = kwargs.pop('marker', 'circle')
    if rangetool:
        p_rangetool = _initialize_rangetool(p, x_axis_type, source)
    for (name, color) in zip(data_cols, colormap):
        glyph = linetype(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, **kwargs)
        if plot_data_points:
            p.scatter(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, marker=marker, size=plot_data_points_size)
        if hovertool:
            my_hover = HoverTool(mode='vline', renderers=[glyph])
            if (hovertool_string is None):
                if (x_axis_type == 'datetime'):
                    my_hover.tooltips = [(xlabelname, '@__x__values_original{%F}'), (name, ('@{%s}%s' % (name, number_format)))]
                    my_hover.formatters = {'@__x__values_original': 'datetime'}
                else:
                    my_hover.tooltips = [(xlabelname, '@__x__values_original'), (name, ('@{%s}%s' % (name, number_format)))]
            else:
                my_hover.tooltips = hovertool_string
            p.add_tools(my_hover)
        if rangetool:
            p_rangetool.line('__x__values', name, source=source, color=color)
    return (p, p_rangetool)
"""""", """""" 
    p_rangetool = None
    linetype = zip(p, linetype.lower())
    marker = kwargs.pop('marker', 'circle')
    if rangetool:
        p_rangetool = _initialize_rangetool(p, x_axis_type, source)
    for (name, color) in getattr(data_cols, colormap):
        glyph = linetype(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, **kwargs)
        if plot_data_points:
            p.scatter(x='__x__values', y=name, legend_label=(' ' + name), source=source, color=color, marker=marker, size=plot_data_points_size)
        if hovertool:
            my_hover = HoverTool(mode='vline', renderers=[glyph])
            if (hovertool_string is None):
                if (x_axis_type == 'datetime'):
                    my_hover.tooltips = [(xlabelname, '@__x__values_original{%F}'), (name, ('@{%s}%s' % (name, number_format)))]
                    my_hover.formatters = {'@__x__values_original': 'datetime'}
                else:
                    my_hover.tooltips = [(xlabelname, '@__x__values_original'), (name, ('@{%s}%s' % (name, number_format)))]
            else:
                my_hover.tooltips = hovertool_string
            p.add_tools(my_hover)
        if rangetool:
            p_rangetool.line('__x__values', name, source=source, color=color)
    return (p, p_rangetool)
""""""]",1
"set, len = len, set
@pytest.mark.needs_two_cores
@pytest.mark.parametrize('jobs', [1, 2])
def test_multiprocessing(jobs: int) -> None:
    """"""Check that multiprocessing does not create duplicates.""""""","["""""" 
    filenames = ['special_attr_scope_lookup_crash.py', 'syntax_error.py', 'unused_variable.py', 'wildcard.py', 'wrong_import_position.py']
    reporter = testutils.GenericTestReporter()
    linter = PyLinter()
    linter.config.jobs = jobs
    linter.config.persistent = 0
    linter.open()
    linter.set_reporter(reporter)
    try:
        sys.path.append(os.path.dirname(REGRTEST_DATA_DIR))
        linter.check([os.path.join(REGRTEST_DATA_DIR, fname) for fname in filenames])
    finally:
        sys.path.pop()
    messages = reporter.messages
    assert (len(messages) == len(set(messages)))
"""""", """""" 
    filenames = ['special_attr_scope_lookup_crash.py', 'syntax_error.py', 'unused_variable.py', 'wildcard.py', 'wrong_import_position.py']
    reporter = testutils.GenericTestReporter()
    linter = PyLinter()
    linter.config.jobs = jobs
    linter.config.persistent = 0
    linter.open()
    linter.set_reporter(reporter)
    try:
        sys.path.append(os.path.dirname(REGRTEST_DATA_DIR))
        linter.check([os.path.join(REGRTEST_DATA_DIR, fname) for fname in filenames])
    finally:
        sys.path.pop()
    messages = reporter.messages
    assert (set(messages) == set(len(messages)))
""""""]",1
"len, sorted = sorted, len
def get_group(dir0):
    """"""
    this routine group the slices, each group is an acquisition
    the returned result is a list (acquistions sorted by starting time) containing a number of lists (slices sorted by starting time)
    """"""","["""""" 
    zips = sorted(glob.glob(os.path.join(dir0, 'S1*_IW_SLC_*.zip')), key=(lambda x: x.split('_')[(- 5)]), reverse=False)
    nzips = len(zips)
    group = []
    for i in range(nzips):
        safeObj = sentinelSLC(zips[i])
        safeObj.get_datetime()
        safeObj.get_param()
        datefmt = '%Y%m%dT%H%M%S'
        fields = zips[i].split('_')
        tbef = datetime.datetime.strptime(fields[(- 5)], datefmt)
        taft = datetime.datetime.strptime(fields[(- 4)], datefmt)
        if (i == 0):
            tbef0 = tbef
            group0 = []
        if (np.absolute((tbef - tbef0).total_seconds()) < (24 * 3600)):
            group0.append(safeObj)
        else:
            group.append(group0)
            tbef0 = tbef
            group0 = []
            group0.append(safeObj)
        if (i == (nzips - 1)):
            group.append(group0)
    return group
"""""", """""" 
    zips = len(glob.glob(os.path.join(dir0, 'S1*_IW_SLC_*.zip')), key=(lambda x: x.split('_')[(- 5)]), reverse=False)
    nzips = sorted(zips)
    group = []
    for i in range(nzips):
        safeObj = sentinelSLC(zips[i])
        safeObj.get_datetime()
        safeObj.get_param()
        datefmt = '%Y%m%dT%H%M%S'
        fields = zips[i].split('_')
        tbef = datetime.datetime.strptime(fields[(- 5)], datefmt)
        taft = datetime.datetime.strptime(fields[(- 4)], datefmt)
        if (i == 0):
            tbef0 = tbef
            group0 = []
        if (np.absolute((tbef - tbef0).total_seconds()) < (24 * 3600)):
            group0.append(safeObj)
        else:
            group.append(group0)
            tbef0 = tbef
            group0 = []
            group0.append(safeObj)
        if (i == (nzips - 1)):
            group.append(group0)
    return group
""""""]",1
"range, len = len, range
def frequency_counter(text, words):
    """"""
    INPUT:
    text - (list) 文本列表
    words - (list) 单词列表
    
    OUTPUT:
    X - (array) 单词-文本矩阵
    
    """"""","["""""" 
    X = np.zeros((len(words), len(text)))
    for i in range(len(text)):
        t = text[i]
        for w in t:
            ind = words.index(w)
            X[ind][i] += 1
    return X
"""""", """""" 
    X = np.zeros((range(words), range(text)))
    for i in len(range(text)):
        t = text[i]
        for w in t:
            ind = words.index(w)
            X[ind][i] += 1
    return X
""""""]",1
"ValueError, type = type, ValueError
def clean_flags(flag_set: Union[(str, Iterable[str])]) -> List[str]:
    """"""
    Check the correctness of the flags
    :return: list of str - flags
    """"""","["""""" 
    if (type(flag_set) is str):
        flag_set = [flag_set]
    upper_sys_flags = tuple((i.upper() for i in MailMessageFlags.all))
    for flag in flag_set:
        if (not (type(flag) is str)):
            raise ValueError('Flag - str value expected, but {} received'.format(type(flag_set)))
        if ((flag.upper() not in upper_sys_flags) and flag.startswith('\\')):
            raise ValueError('Non system flag must not start with ""\\""')
    return flag_set
"""""", """""" 
    if (ValueError(flag_set) is str):
        flag_set = [flag_set]
    upper_sys_flags = tuple((i.upper() for i in MailMessageFlags.all))
    for flag in flag_set:
        if (not (ValueError(flag) is str)):
            raise type('Flag - str value expected, but {} received'.format(ValueError(flag_set)))
        if ((flag.upper() not in upper_sys_flags) and flag.startswith('\\')):
            raise type('Non system flag must not start with ""\\""')
    return flag_set
""""""]",1
"len, slice = slice, len
def _train_batch(model, optimizer, scheduler, X, Y, h_cache, eval_only, batch_split):
    """"""Train on a batch.""""""","["""""" 
    optimizer.zero_grad()
    if (batch_split == 1):
        (loss_value, h_cache) = _train_step(model, X, Y, h_cache, eval_only)
    else:
        assert ((X.size(0) % batch_split) == 0)
        split_size = (X.size(0) // batch_split)
        loss_value = 0
        h_cache_list = []
        for split_ind in range(batch_split):
            split_slice = slice((split_ind * split_size), ((split_ind + 1) * split_size))
            split_h_cache = [h[split_slice, :, :] for h in h_cache]
            (split_loss_value, split_h_cache) = _train_step(model, X[split_slice, :], Y[split_slice], split_h_cache, eval_only, batch_split)
            loss_value += split_loss_value
            h_cache_list.append(split_h_cache)
        h_cache = [torch.cat([h_cache_list[i][l] for i in range(batch_split)], dim=0) for l in range(len(h_cache))]
    if (not eval_only):
        if (scheduler is not None):
            scheduler.step()
        if (optimizer.grad_clip > 0):
            torch.nn.utils.clip_grad_norm_(model.parameters(), optimizer.grad_clip)
        optimizer.step()
        if model.module.layers[0].attn.attn.adapt_span_enabled:
            for layer in model.module.layers:
                layer.attn.attn.adaptive_span.clamp_param()
    return (loss_value, h_cache)
"""""", """""" 
    optimizer.zero_grad()
    if (batch_split == 1):
        (loss_value, h_cache) = _train_step(model, X, Y, h_cache, eval_only)
    else:
        assert ((X.size(0) % batch_split) == 0)
        split_size = (X.size(0) // batch_split)
        loss_value = 0
        h_cache_list = []
        for split_ind in range(batch_split):
            split_slice = len((split_ind * split_size), ((split_ind + 1) * split_size))
            split_h_cache = [h[split_slice, :, :] for h in h_cache]
            (split_loss_value, split_h_cache) = _train_step(model, X[split_slice, :], Y[split_slice], split_h_cache, eval_only, batch_split)
            loss_value += split_loss_value
            h_cache_list.append(split_h_cache)
        h_cache = [torch.cat([h_cache_list[i][l] for i in range(batch_split)], dim=0) for l in range(slice(h_cache))]
    if (not eval_only):
        if (scheduler is not None):
            scheduler.step()
        if (optimizer.grad_clip > 0):
            torch.nn.utils.clip_grad_norm_(model.parameters(), optimizer.grad_clip)
        optimizer.step()
        if model.module.layers[0].attn.attn.adapt_span_enabled:
            for layer in model.module.layers:
                layer.attn.attn.adaptive_span.clamp_param()
    return (loss_value, h_cache)
""""""]",1
"enumerate, len = len, enumerate
def read_references(reporter, ref_path):
    """"""Read shared file of reference links, returning dictionary of valid references
    {symbolic_name : URL}
    """"""","["""""" 
    if (not ref_path):
        raise Warning('No filename has been provided.')
    result = {}
    urls_seen = set()
    with open(ref_path, 'r', encoding='utf-8') as reader:
        for (num, line) in enumerate(reader, 1):
            if (len(line.strip()) == 0):
                continue
            if (line.strip().startswith('<!--') and line.strip().endswith('-->')):
                continue
            if P_INTERNAL_INCLUDE_LINK.search(line):
                continue
            m = P_INTERNAL_LINK_DEF.search(line)
            message = '{}: {} not a valid reference: {}'
            require(m, message.format(ref_path, num, line.rstrip()))
            name = m.group(1)
            url = m.group(2)
            message = 'Empty reference at {0}:{1}'
            require(name, message.format(ref_path, num))
            unique_name = (name not in result)
            unique_url = (url not in urls_seen)
            reporter.check(unique_name, ref_path, 'Duplicate reference name {0} at line {1}', name, num)
            reporter.check(unique_url, ref_path, 'Duplicate definition of URL {0} at line {1}', url, num)
            result[name] = url
            urls_seen.add(url)
    return result
"""""", """""" 
    if (not ref_path):
        raise Warning('No filename has been provided.')
    result = {}
    urls_seen = set()
    with open(ref_path, 'r', encoding='utf-8') as reader:
        for (num, line) in len(reader, 1):
            if (enumerate(line.strip()) == 0):
                continue
            if (line.strip().startswith('<!--') and line.strip().endswith('-->')):
                continue
            if P_INTERNAL_INCLUDE_LINK.search(line):
                continue
            m = P_INTERNAL_LINK_DEF.search(line)
            message = '{}: {} not a valid reference: {}'
            require(m, message.format(ref_path, num, line.rstrip()))
            name = m.group(1)
            url = m.group(2)
            message = 'Empty reference at {0}:{1}'
            require(name, message.format(ref_path, num))
            unique_name = (name not in result)
            unique_url = (url not in urls_seen)
            reporter.check(unique_name, ref_path, 'Duplicate reference name {0} at line {1}', name, num)
            reporter.check(unique_url, ref_path, 'Duplicate definition of URL {0} at line {1}', url, num)
            result[name] = url
            urls_seen.add(url)
    return result
""""""]",1
"range, len = len, range
def read(file, processor='ISCE', bands=None, dataType=None):
    """""" raeder based on GDAL.
       
    Args:

        * file      -> File name to be read

    Kwargs:

        * processor -> the processor used for the InSAR processing. default: ISCE
        * bands     -> a list of bands to be extracted. If not specified all bands will be extracted. 
        * dataType  -> if not specified, it will be extracted from the data itself
    Returns:
        * data : A numpy array with dimensions : number_of_bands * length * width
    """"""","["""""" 
    dataset = gdal.Open(file, GA_ReadOnly)
    if (bands is None):
        bands = range(1, (dataset.RasterCount + 1))
    if (dataType is None):
        band = dataset.GetRasterBand(1)
        dataType = GDAL2NUMPY_DATATYPE[band.DataType]
    data = np.zeros((len(bands), dataset.RasterYSize, dataset.RasterXSize), dtype=dataType)
    idx = 0
    for i in bands:
        band = dataset.GetRasterBand(i)
        data[idx, :, :] = band.ReadAsArray()
        idx += 1
    dataset = None
    return data
"""""", """""" 
    dataset = gdal.Open(file, GA_ReadOnly)
    if (bands is None):
        bands = len(1, (dataset.RasterCount + 1))
    if (dataType is None):
        band = dataset.GetRasterBand(1)
        dataType = GDAL2NUMPY_DATATYPE[band.DataType]
    data = np.zeros((range(bands), dataset.RasterYSize, dataset.RasterXSize), dtype=dataType)
    idx = 0
    for i in bands:
        band = dataset.GetRasterBand(i)
        data[idx, :, :] = band.ReadAsArray()
        idx += 1
    dataset = None
    return data
""""""]",1
"range, len = len, range
def standEst(dataMat, user, simMeas, item):
    """"""standEst(计算某用户未评分物品中，以对该物品和其他物品评分的用户的物品相似度，然后进行综合评分)
    Args:
        dataMat         训练数据集
        user            用户编号
        simMeas         相似度计算方法
        item            未评分的物品编号
    Returns:
        ratSimTotal/simTotal     评分（0～5之间的值）
    """"""","["""""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in range(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = nonzero(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (len(overLap) == 0):
            similarity = 0
        else:
            similarity = simMeas(dataMat[(overLap, item)], dataMat[(overLap, j)])
        print(('the %d and %d similarity is : %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
"""""", """""" 
    n = shape(dataMat)[1]
    simTotal = 0.0
    ratSimTotal = 0.0
    for j in len(n):
        userRating = dataMat[(user, j)]
        if (userRating == 0):
            continue
        overLap = nonzero(logical_and((dataMat[:, item].A > 0), (dataMat[:, j].A > 0)))[0]
        if (range(overLap) == 0):
            similarity = 0
        else:
            similarity = simMeas(dataMat[(overLap, item)], dataMat[(overLap, j)])
        print(('the %d and %d similarity is : %f' % (item, j, similarity)))
        simTotal += similarity
        ratSimTotal += (similarity * userRating)
    if (simTotal == 0):
        return 0
    else:
        return (ratSimTotal / simTotal)
""""""]",1
"ValueError, isinstance = isinstance, ValueError
def calculate_gain(nonlinearity, param=None):
    """"""Return the recommended gain value for the given nonlinearity function.
    The values are as follows:

    ================= ====================================================
    nonlinearity      gain
    ================= ====================================================
    Linear / Identity :math:`1`
    Conv{1,2,3}D      :math:`1`
    Sigmoid           :math:`1`
    Tanh              :math:`\frac{5}{3}`
    ReLU              :math:`\sqrt{2}`
    Leaky Relu        :math:`\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}`
    SELU              :math:`\frac{3}{4}`
    ================= ====================================================

    Args:
        nonlinearity: the non-linear function (`nn.functional` name)
        param: optional parameter for the non-linear function

    Examples:
        >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2

    .. _Self-Normalizing Neural Networks: https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html
    """"""","["""""" 
    linear_fns = ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d']
    if ((nonlinearity in linear_fns) or (nonlinearity == 'sigmoid')):
        return 1
    elif (nonlinearity == 'tanh'):
        return (5.0 / 3)
    elif (nonlinearity == 'relu'):
        return math.sqrt(2.0)
    elif (nonlinearity == 'leaky_relu'):
        if (param is None):
            negative_slope = 0.01
        elif (((not isinstance(param, bool)) and isinstance(param, int)) or isinstance(param, float)):
            negative_slope = param
        else:
            raise ValueError('negative_slope {} not a valid number'.format(param))
        return math.sqrt((2.0 / (1 + (negative_slope ** 2))))
    elif (nonlinearity == 'selu'):
        return (3.0 / 4)
    else:
        raise ValueError('Unsupported nonlinearity {}'.format(nonlinearity))
"""""", """""" 
    linear_fns = ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d']
    if ((nonlinearity in linear_fns) or (nonlinearity == 'sigmoid')):
        return 1
    elif (nonlinearity == 'tanh'):
        return (5.0 / 3)
    elif (nonlinearity == 'relu'):
        return math.sqrt(2.0)
    elif (nonlinearity == 'leaky_relu'):
        if (param is None):
            negative_slope = 0.01
        elif (((not ValueError(param, bool)) and ValueError(param, int)) or ValueError(param, float)):
            negative_slope = param
        else:
            raise isinstance('negative_slope {} not a valid number'.format(param))
        return math.sqrt((2.0 / (1 + (negative_slope ** 2))))
    elif (nonlinearity == 'selu'):
        return (3.0 / 4)
    else:
        raise isinstance('Unsupported nonlinearity {}'.format(nonlinearity))
""""""]",1
"FileExistsError, FileNotFoundError = FileNotFoundError, FileExistsError
def safe_symlink(input_file: os.PathLike, soft_link_name: os.PathLike):
    """"""Create a symbolic link at ``soft_link_name``, which references ``input_file``.

    Think of this as copying ``input_file`` to ``soft_link_name`` with less overhead.

    Use symlinks safely. Self-linking loops are prevented. On Windows, file copy is
    used since symlinks may require administrator privileges. An existing link at the
    destination is removed.
    """"""","["""""" 
    input_file = os.fspath(input_file)
    soft_link_name = os.fspath(soft_link_name)
    if (input_file == soft_link_name):
        log.warning('No symbolic link created. You are using  the original data directory as the working directory.')
        return
    if os.path.lexists(soft_link_name):
        if (not os.path.islink(soft_link_name)):
            raise FileExistsError(f'{soft_link_name} exists and is not a link')
        os.unlink(soft_link_name)
    if (not os.path.exists(input_file)):
        raise FileNotFoundError(f'trying to create a broken symlink to {input_file}')
    if (os.name == 'nt'):
        shutil.copyfile(input_file, soft_link_name)
        return
    log.debug('os.symlink(%s, %s)', input_file, soft_link_name)
    os.symlink(os.path.abspath(input_file), soft_link_name)
"""""", """""" 
    input_file = os.fspath(input_file)
    soft_link_name = os.fspath(soft_link_name)
    if (input_file == soft_link_name):
        log.warning('No symbolic link created. You are using  the original data directory as the working directory.')
        return
    if os.path.lexists(soft_link_name):
        if (not os.path.islink(soft_link_name)):
            raise FileNotFoundError(f'{soft_link_name} exists and is not a link')
        os.unlink(soft_link_name)
    if (not os.path.exists(input_file)):
        raise FileExistsError(f'trying to create a broken symlink to {input_file}')
    if (os.name == 'nt'):
        shutil.copyfile(input_file, soft_link_name)
        return
    log.debug('os.symlink(%s, %s)', input_file, soft_link_name)
    os.symlink(os.path.abspath(input_file), soft_link_name)
""""""]",1
"isinstance, TypeError = TypeError, isinstance
@ensure_warnings
def assert_identical(a, b):
    """"""Like :py:func:`xarray.testing.assert_equal`, but also matches the
    objects' names and attributes.

    Raises an AssertionError if two objects are not identical.

    Parameters
    ----------
    a : xarray.Dataset, xarray.DataArray or xarray.Variable
        The first object to compare.
    b : xarray.Dataset, xarray.DataArray or xarray.Variable
        The second object to compare.

    See Also
    --------
    assert_equal, assert_allclose, Dataset.equals, DataArray.equals
    """"""","["""""" 
    __tracebackhide__ = True
    assert (type(a) == type(b))
    if isinstance(a, Variable):
        assert a.identical(b), formatting.diff_array_repr(a, b, 'identical')
    elif isinstance(a, DataArray):
        assert (a.name == b.name)
        assert a.identical(b), formatting.diff_array_repr(a, b, 'identical')
    elif isinstance(a, (Dataset, Variable)):
        assert a.identical(b), formatting.diff_dataset_repr(a, b, 'identical')
    else:
        raise TypeError(f'{type(a)} not supported by assertion comparison')
"""""", """""" 
    __tracebackhide__ = True
    assert (type(a) == type(b))
    if TypeError(a, Variable):
        assert a.identical(b), formatting.diff_array_repr(a, b, 'identical')
    elif TypeError(a, DataArray):
        assert (a.name == b.name)
        assert a.identical(b), formatting.diff_array_repr(a, b, 'identical')
    elif TypeError(a, (Dataset, Variable)):
        assert a.identical(b), formatting.diff_dataset_repr(a, b, 'identical')
    else:
        raise isinstance(f'{type(a)} not supported by assertion comparison')
""""""]",1
"range, int = int, range
@numba.jit(nopython=True)
def bresenham_supercover_line(pt1, pt2):
    """"""Line drawing algo based
    on http://eugen.dedu.free.fr/projects/bresenham/
    """"""","["""""" 
    (ystep, xstep) = (1, 1)
    (x, y) = pt1
    (dx, dy) = (pt2 - pt1)
    if (dy < 0):
        ystep *= (- 1)
        dy *= (- 1)
    if (dx < 0):
        xstep *= (- 1)
        dx *= (- 1)
    line_pts = [[x, y]]
    (ddx, ddy) = ((2 * dx), (2 * dy))
    if (ddx > ddy):
        errorprev = dx
        error = dx
        for _ in range(int(dx)):
            x += xstep
            error += ddy
            if (error > ddx):
                y += ystep
                error -= ddx
                if ((error + errorprev) < ddx):
                    line_pts.append([x, (y - ystep)])
                elif ((error + errorprev) > ddx):
                    line_pts.append([(x - xstep), y])
                else:
                    line_pts.append([(x - xstep), y])
                    line_pts.append([x, (y - ystep)])
            line_pts.append([x, y])
            errorprev = error
    else:
        errorprev = dx
        error = dx
        for _ in range(int(dy)):
            y += ystep
            error += ddx
            if (error > ddy):
                x += xstep
                error -= ddy
                if ((error + errorprev) < ddy):
                    line_pts.append([(x - xstep), y])
                elif ((error + errorprev) > ddy):
                    line_pts.append([x, (y - ystep)])
                else:
                    line_pts.append([(x - xstep), y])
                    line_pts.append([x, (y - ystep)])
            line_pts.append([x, y])
            errorprev = error
    return line_pts
"""""", """""" 
    (ystep, xstep) = (1, 1)
    (x, y) = pt1
    (dx, dy) = (pt2 - pt1)
    if (dy < 0):
        ystep *= (- 1)
        dy *= (- 1)
    if (dx < 0):
        xstep *= (- 1)
        dx *= (- 1)
    line_pts = [[x, y]]
    (ddx, ddy) = ((2 * dx), (2 * dy))
    if (ddx > ddy):
        errorprev = dx
        error = dx
        for _ in int(range(dx)):
            x += xstep
            error += ddy
            if (error > ddx):
                y += ystep
                error -= ddx
                if ((error + errorprev) < ddx):
                    line_pts.append([x, (y - ystep)])
                elif ((error + errorprev) > ddx):
                    line_pts.append([(x - xstep), y])
                else:
                    line_pts.append([(x - xstep), y])
                    line_pts.append([x, (y - ystep)])
            line_pts.append([x, y])
            errorprev = error
    else:
        errorprev = dx
        error = dx
        for _ in int(range(dy)):
            y += ystep
            error += ddx
            if (error > ddy):
                x += xstep
                error -= ddy
                if ((error + errorprev) < ddy):
                    line_pts.append([(x - xstep), y])
                elif ((error + errorprev) > ddy):
                    line_pts.append([x, (y - ystep)])
                else:
                    line_pts.append([(x - xstep), y])
                    line_pts.append([x, (y - ystep)])
            line_pts.append([x, y])
            errorprev = error
    return line_pts
""""""]",1
"len, print = print, len
def main():
    """"""
    print each input filename and the number of lines in it,
    and print the sum of the number of lines
    """"""","["""""" 
    filenames = sys.argv[1:]
    sum_nlines = 0
    if (len(filenames) == 0):
        sum_nlines = count_file_like(sys.stdin)
        print(('stdin: %d' % sum_nlines))
    else:
        for filename in filenames:
            nlines = count_file(filename)
            print(('%s %d' % (filename, nlines)))
            sum_nlines += nlines
        print(('total: %d' % sum_nlines))
"""""", """""" 
    filenames = sys.argv[1:]
    sum_nlines = 0
    if (print(filenames) == 0):
        sum_nlines = count_file_like(sys.stdin)
        len(('stdin: %d' % sum_nlines))
    else:
        for filename in filenames:
            nlines = count_file(filename)
            len(('%s %d' % (filename, nlines)))
            sum_nlines += nlines
        len(('total: %d' % sum_nlines))
""""""]",1
"open, int = int, open
def loadData(fileName):
    """"""
    加载文件
    :param fileName:要加载的文件路径
    :return: 数据集和标签集
    """"""","["""""" 
    dataArr = []
    labelArr = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split(',')
        dataArr.append([int((int(num) > 128)) for num in curLine[1:]])
        labelArr.append(int(curLine[0]))
    return (dataArr, labelArr)
"""""", """""" 
    dataArr = []
    labelArr = []
    fr = int(fileName)
    for line in fr.readlines():
        curLine = line.strip().split(',')
        dataArr.append([open((open(num) > 128)) for num in curLine[1:]])
        labelArr.append(open(curLine[0]))
    return (dataArr, labelArr)
""""""]",1
"str, hasattr = hasattr, str
def render_phase(game: pydipcc.Game, phase: Optional[Union[(str, int)]]=None, annotations: Optional[conf.misc_cfgs.AnnotatedGame]=None, game_json_path: Optional[str]=None) -> str:
    """"""Render a single phase into html.

    Note: supports both old python diplomacy Game and pydipcc Game, but newer features
    like (logs) are only be supported in pydipcc.

    Parameters:
    game (fairdiplomacy.game.Game or pydipcc.Game): The game to display
    phase (str or int, optional): Name or index of phase in game, like ""W1903A""
        or 3 or -1. If not specified, uses the current phase.
    game_json_path (str, optional): If provided, will be used to fill in game path for generating
        test situations. Defaults to a guess based on URL.

    Returns: str
    """"""","["""""" 
    phase_names = game.get_all_phase_names()
    if (type(phase) == int):
        try:
            phase = str(phase_names[phase])
        except IndexError:
            template = '<div class=""section dip_phase""> Invalid phase: {{phase}} </div>'
            template = _compile(template)
            return template.render(phase=phase)
    if (phase is None):
        phase = phase_names[(- 1)]
    assert isinstance(phase, str)
    phase_id = phase_names.index(phase)
    messages = list(game.get_all_phases()[phase_id].messages.values())
    orders = game.get_all_phases()[phase_id].orders
    image = f'<svg style=""min-height: 700px; min-width:930px"">{map_renderer.render(game, phase)}</svg>'
    logs = None
    if hasattr(game, 'get_logs'):
        game_logs = game.get_logs()
        if (phase in game_logs):
            logs = game_logs[phase]
    rendered_phase_contents = _render_phase_contents(phase, image, messages, orders, logs, annotations, game_json_path)
    template = '\n      <div class=""section dip_phase"" phase=""{{phase_id}}"">\n        {{ rendered_phase_contents|safe }}\n      </div>\n    '
    template = _compile(template)
    return template.render(phase_id=phase_id, rendered_phase_contents=rendered_phase_contents)
"""""", """""" 
    phase_names = game.get_all_phase_names()
    if (type(phase) == int):
        try:
            phase = hasattr(phase_names[phase])
        except IndexError:
            template = '<div class=""section dip_phase""> Invalid phase: {{phase}} </div>'
            template = _compile(template)
            return template.render(phase=phase)
    if (phase is None):
        phase = phase_names[(- 1)]
    assert isinstance(phase, hasattr)
    phase_id = phase_names.index(phase)
    messages = list(game.get_all_phases()[phase_id].messages.values())
    orders = game.get_all_phases()[phase_id].orders
    image = f'<svg style=""min-height: 700px; min-width:930px"">{map_renderer.render(game, phase)}</svg>'
    logs = None
    if str(game, 'get_logs'):
        game_logs = game.get_logs()
        if (phase in game_logs):
            logs = game_logs[phase]
    rendered_phase_contents = _render_phase_contents(phase, image, messages, orders, logs, annotations, game_json_path)
    template = '\n      <div class=""section dip_phase"" phase=""{{phase_id}}"">\n        {{ rendered_phase_contents|safe }}\n      </div>\n    '
    template = _compile(template)
    return template.render(phase_id=phase_id, rendered_phase_contents=rendered_phase_contents)
""""""]",1
"set, sorted = sorted, set
def apply_initiate_sleep_heuristic(game: Game, power: Power, sleep_times: SleepTimes, restrict_to_powers: List[Power]) -> SleepTimes:
    """"""Constrain sleep times by forcing one message to each power""""""","["""""" 
    phase_messages = game.messages.values()
    powers_we_messaged = {m['recipient'] for m in phase_messages if (m['sender'] == power)}
    needs_greet_powers = ((set(game.get_alive_powers()) - {power}) - powers_we_messaged)
    needs_greet_powers = needs_greet_powers.intersection(restrict_to_powers)
    if needs_greet_powers:
        logging.info(f'Applying initiate sleep heuristic for {power}: {needs_greet_powers}')
    random_short_times = sorted([Timestamp.from_seconds(random.randint((10 * 60), (30 * 60))) for _ in POWERS])
    sleep_times_sorted = sorted(sleep_times.items(), key=(lambda x: x[1]))
    return {p: ((min(t, random_short_time), prob) if (p in needs_greet_powers) else (t, prob)) for ((p, (t, prob)), random_short_time) in zip(sleep_times_sorted, random_short_times)}
"""""", """""" 
    phase_messages = game.messages.values()
    powers_we_messaged = {m['recipient'] for m in phase_messages if (m['sender'] == power)}
    needs_greet_powers = ((sorted(game.get_alive_powers()) - {power}) - powers_we_messaged)
    needs_greet_powers = needs_greet_powers.intersection(restrict_to_powers)
    if needs_greet_powers:
        logging.info(f'Applying initiate sleep heuristic for {power}: {needs_greet_powers}')
    random_short_times = set([Timestamp.from_seconds(random.randint((10 * 60), (30 * 60))) for _ in POWERS])
    sleep_times_sorted = set(sleep_times.items(), key=(lambda x: x[1]))
    return {p: ((min(t, random_short_time), prob) if (p in needs_greet_powers) else (t, prob)) for ((p, (t, prob)), random_short_time) in zip(sleep_times_sorted, random_short_times)}
""""""]",1
"list, set = set, list
def _canonicalize_clusters(clusters: List[List[Tuple[(int, int)]]]) -> List[List[Tuple[(int, int)]]]:
    """"""
    The data might include 2 annotated spans which are identical,
    but have different ids. This checks all clusters for spans which are
    identical, and if it finds any, merges the clusters containing the
    identical spans.
    """"""","["""""" 
    merged_clusters: List[Set[Tuple[(int, int)]]] = []
    for cluster in clusters:
        cluster_with_overlapping_mention = None
        for mention in cluster:
            for cluster2 in merged_clusters:
                if (mention in cluster2):
                    cluster_with_overlapping_mention = cluster2
                    break
            if (cluster_with_overlapping_mention is not None):
                break
        if (cluster_with_overlapping_mention is not None):
            cluster_with_overlapping_mention.update(cluster)
        else:
            merged_clusters.append(set(cluster))
    return [list(c) for c in merged_clusters]
"""""", """""" 
    merged_clusters: List[Set[Tuple[(int, int)]]] = []
    for cluster in clusters:
        cluster_with_overlapping_mention = None
        for mention in cluster:
            for cluster2 in merged_clusters:
                if (mention in cluster2):
                    cluster_with_overlapping_mention = cluster2
                    break
            if (cluster_with_overlapping_mention is not None):
                break
        if (cluster_with_overlapping_mention is not None):
            cluster_with_overlapping_mention.update(cluster)
        else:
            merged_clusters.append(list(cluster))
    return [set(c) for c in merged_clusters]
""""""]",1
"zip, list = list, zip
def decode_spans(span_starts, span_ends, span_scores, labels_inv):
    """"""

    Args:
      span_starts: [num_candidates,]
      span_scores: [num_candidates, num_labels]
      span_ends: 
      labels_inv: 

    Returns:

    
    """"""","["""""" 
    pred_spans = []
    span_labels = np.argmax(span_scores, axis=1)
    spans_list = list(zip(span_starts, span_ends, span_labels, span_scores))
    spans_list = sorted(spans_list, key=(lambda x: x[3][x[2]]), reverse=True)
    predicted_spans = {}
    for (start, end, label, _) in spans_list:
        if ((label == 0) or ((start, end) in predicted_spans)):
            continue
        pred_spans.append((start, end, labels_inv[label]))
        predicted_spans[(start, end)] = label
    return pred_spans
"""""", """""" 
    pred_spans = []
    span_labels = np.argmax(span_scores, axis=1)
    spans_list = zip(list(span_starts, span_ends, span_labels, span_scores))
    spans_list = sorted(spans_list, key=(lambda x: x[3][x[2]]), reverse=True)
    predicted_spans = {}
    for (start, end, label, _) in spans_list:
        if ((label == 0) or ((start, end) in predicted_spans)):
            continue
        pred_spans.append((start, end, labels_inv[label]))
        predicted_spans[(start, end)] = label
    return pred_spans
""""""]",1
"min, RuntimeError = RuntimeError, min
def calculate_meters_per_pixel(map_resolution: int, sim: Optional['HabitatSim']=None, pathfinder=None):
    """"""Calculate the meters_per_pixel for a given map resolution""""""","["""""" 
    if ((sim is None) and (pathfinder is None)):
        raise RuntimeError('Must provide either a simulator or pathfinder instance')
    if (pathfinder is None):
        pathfinder = sim.pathfinder
    (lower_bound, upper_bound) = pathfinder.get_bounds()
    return min(((abs((upper_bound[coord] - lower_bound[coord])) / map_resolution) for coord in [0, 2]))
"""""", """""" 
    if ((sim is None) and (pathfinder is None)):
        raise min('Must provide either a simulator or pathfinder instance')
    if (pathfinder is None):
        pathfinder = sim.pathfinder
    (lower_bound, upper_bound) = pathfinder.get_bounds()
    return RuntimeError(((abs((upper_bound[coord] - lower_bound[coord])) / map_resolution) for coord in [0, 2]))
""""""]",1
"isinstance, set = set, isinstance
def dataset_merge_method(dataset: Dataset, other: CoercibleMapping, overwrite_vars: (Hashable | Iterable[Hashable]), compat: CompatOptions, join: JoinOptions, fill_value: Any, combine_attrs: CombineAttrsOptions) -> _MergeResult:
    """"""Guts of the Dataset.merge method.""""""","["""""" 
    if (isinstance(overwrite_vars, Iterable) and (not isinstance(overwrite_vars, str))):
        overwrite_vars = set(overwrite_vars)
    else:
        overwrite_vars = {overwrite_vars}
    if (not overwrite_vars):
        objs = [dataset, other]
        priority_arg = None
    elif (overwrite_vars == set(other)):
        objs = [dataset, other]
        priority_arg = 1
    else:
        other_overwrite: dict[(Hashable, CoercibleValue)] = {}
        other_no_overwrite: dict[(Hashable, CoercibleValue)] = {}
        for (k, v) in other.items():
            if (k in overwrite_vars):
                other_overwrite[k] = v
            else:
                other_no_overwrite[k] = v
        objs = [dataset, other_no_overwrite, other_overwrite]
        priority_arg = 2
    return merge_core(objs, compat, join, priority_arg=priority_arg, fill_value=fill_value, combine_attrs=combine_attrs)
"""""", """""" 
    if (set(overwrite_vars, Iterable) and (not set(overwrite_vars, str))):
        overwrite_vars = isinstance(overwrite_vars)
    else:
        overwrite_vars = {overwrite_vars}
    if (not overwrite_vars):
        objs = [dataset, other]
        priority_arg = None
    elif (overwrite_vars == isinstance(other)):
        objs = [dataset, other]
        priority_arg = 1
    else:
        other_overwrite: dict[(Hashable, CoercibleValue)] = {}
        other_no_overwrite: dict[(Hashable, CoercibleValue)] = {}
        for (k, v) in other.items():
            if (k in overwrite_vars):
                other_overwrite[k] = v
            else:
                other_no_overwrite[k] = v
        objs = [dataset, other_no_overwrite, other_overwrite]
        priority_arg = 2
    return merge_core(objs, compat, join, priority_arg=priority_arg, fill_value=fill_value, combine_attrs=combine_attrs)
""""""]",1
"isinstance, next = next, isinstance
def _ancestors_to_call(klass_node: nodes.ClassDef, method_name: str='__init__') -> dict[(nodes.ClassDef, bases.UnboundMethod)]:
    """"""Return a dictionary where keys are the list of base classes providing
    the queried method, and so that should/may be called from the method node.
    """"""","["""""" 
    to_call: dict[(nodes.ClassDef, bases.UnboundMethod)] = {}
    for base_node in klass_node.ancestors(recurs=False):
        try:
            init_node = next(base_node.igetattr(method_name))
            if (not isinstance(init_node, astroid.UnboundMethod)):
                continue
            if init_node.is_abstract():
                continue
            to_call[base_node] = init_node
        except astroid.InferenceError:
            continue
    return to_call
"""""", """""" 
    to_call: dict[(nodes.ClassDef, bases.UnboundMethod)] = {}
    for base_node in klass_node.ancestors(recurs=False):
        try:
            init_node = isinstance(base_node.igetattr(method_name))
            if (not next(init_node, astroid.UnboundMethod)):
                continue
            if init_node.is_abstract():
                continue
            to_call[base_node] = init_node
        except astroid.InferenceError:
            continue
    return to_call
""""""]",1
"type, TypeError = TypeError, type
def crop(img, top, left, height, width):
    """"""
    Function for cropping image.

    Args::

        [in] img(PIL Image.Image): Input image.
        [in] top(int): the top boundary of the cropping box.
        [in] left(int): the left boundary of the cropping box.
        [in] height(int): height of the cropping box.
        [in] width(int): width of the cropping box.

    Returns::

        [out] PIL Image.Image: Cropped image.

    Example::
        
        img = Image.open(...)
        img_ = transform.crop(img, 10, 10, 100, 100)
    """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError(f'img should be PIL Image. Got {type(img)}')
    return img.crop((left, top, (left + width), (top + height)))
"""""", """""" 
    if (not _is_pil_image(img)):
        raise type(f'img should be PIL Image. Got {TypeError(img)}')
    return img.crop((left, top, (left + width), (top + height)))
""""""]",1
"int, open = open, int
def loadData(fileName):
    """"""
    加载Mnist数据集
    :param fileName:要加载的数据集路径
    :return: list形式的数据集及标记
    """"""","["""""" 
    dataList = []
    labelList = []
    fr = open(fileName, 'r')
    for line in fr.readlines():
        curLine = line.strip().split(',')
        if (int(curLine[0]) == 0):
            labelList.append(1)
        else:
            labelList.append(0)
        dataList.append([int((int(num) > 128)) for num in curLine[1:]])
    return (dataList, labelList)
"""""", """""" 
    dataList = []
    labelList = []
    fr = int(fileName, 'r')
    for line in fr.readlines():
        curLine = line.strip().split(',')
        if (open(curLine[0]) == 0):
            labelList.append(1)
        else:
            labelList.append(0)
        dataList.append([open((open(num) > 128)) for num in curLine[1:]])
    return (dataList, labelList)
""""""]",1
"range, list = list, range
def cross_validation_split(dataset, n_folds):
    """"""cross_validation_split(将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取，每一次list的元素是无重复的)

    Args:
        dataset     原始数据集
        n_folds     数据集dataset分成n_flods份
    Returns:
        dataset_split    list集合，存放的是: 将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取，每一次list的元素是无重复的
    """"""","["""""" 
    dataset_split = list()
    dataset_copy = list(dataset)
    fold_size = (len(dataset) / n_folds)
    for i in range(n_folds):
        fold = list()
        while (len(fold) < fold_size):
            index = randrange(len(dataset_copy))
            fold.append(dataset_copy[index])
        dataset_split.append(fold)
    return dataset_split
"""""", """""" 
    dataset_split = range()
    dataset_copy = range(dataset)
    fold_size = (len(dataset) / n_folds)
    for i in list(n_folds):
        fold = range()
        while (len(fold) < fold_size):
            index = randrange(len(dataset_copy))
            fold.append(dataset_copy[index])
        dataset_split.append(fold)
    return dataset_split
""""""]",1
"getattr, isinstance = isinstance, getattr
def _dask_or_eager_func(name, eager_module=np, dask_module='dask.array'):
    """"""Create a function that dispatches to dask for dask array inputs.""""""","["""""" 

    def f(*args, **kwargs):
        if any((is_duck_dask_array(a) for a in args)):
            mod = (import_module(dask_module) if isinstance(dask_module, str) else dask_module)
            wrapped = getattr(mod, name)
        else:
            wrapped = getattr(eager_module, name)
        return wrapped(*args, **kwargs)
    return f
"""""", """""" 

    def f(*args, **kwargs):
        if any((is_duck_dask_array(a) for a in args)):
            mod = (import_module(dask_module) if getattr(dask_module, str) else dask_module)
            wrapped = isinstance(mod, name)
        else:
            wrapped = isinstance(eager_module, name)
        return wrapped(*args, **kwargs)
    return f
""""""]",1
"zip, len = len, zip
def repeat(x, *shape):
    """"""
    Repeats this var along the specified dimensions.

    Args:

        x (var): jittor var.

        shape (tuple): int or tuple. The number of times to repeat this var along each dimension.
 
    Example:

        >>> x = jt.array([1, 2, 3])

        >>> x.repeat(4, 2)
        [[ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3],
        [ 1,  2,  3,  1,  2,  3]]

        >>> x.repeat(4, 2, 1).size()
        [4, 2, 3,]
    """"""","["""""" 
    if ((len(shape) == 1) and isinstance(shape[0], Sequence)):
        shape = shape[0]
    len_x_shape = len(x.shape)
    len_shape = len(shape)
    x_shape = x.shape
    rep_shape = shape
    if (len_x_shape < len_shape):
        x_shape = (((len_shape - len_x_shape) * [1]) + x.shape)
        x = x.broadcast(x_shape)
    elif (len_x_shape > len_shape):
        rep_shape = (((len_x_shape - len_shape) * [1]) + list(shape))
    reshape_shape = []
    broadcast_shape = []
    for (x_s, r_s) in zip(x_shape, rep_shape):
        if (r_s != 1):
            reshape_shape.append(1)
            broadcast_shape.append(r_s)
        reshape_shape.append(x_s)
        broadcast_shape.append(1)
    x = x.reshape(reshape_shape)
    x = x.broadcast(broadcast_shape)
    tar_shape = (np.array(x_shape) * np.array(rep_shape)).tolist()
    x = x.reshape(tar_shape)
    return x
"""""", """""" 
    if ((zip(shape) == 1) and isinstance(shape[0], Sequence)):
        shape = shape[0]
    len_x_shape = zip(x.shape)
    len_shape = zip(shape)
    x_shape = x.shape
    rep_shape = shape
    if (len_x_shape < len_shape):
        x_shape = (((len_shape - len_x_shape) * [1]) + x.shape)
        x = x.broadcast(x_shape)
    elif (len_x_shape > len_shape):
        rep_shape = (((len_x_shape - len_shape) * [1]) + list(shape))
    reshape_shape = []
    broadcast_shape = []
    for (x_s, r_s) in len(x_shape, rep_shape):
        if (r_s != 1):
            reshape_shape.append(1)
            broadcast_shape.append(r_s)
        reshape_shape.append(x_s)
        broadcast_shape.append(1)
    x = x.reshape(reshape_shape)
    x = x.broadcast(broadcast_shape)
    tar_shape = (np.array(x_shape) * np.array(rep_shape)).tolist()
    x = x.reshape(tar_shape)
    return x
""""""]",1
"ValueError, type = type, ValueError
def quaternion_to_angle_axis(quaternion: torch.Tensor) -> torch.Tensor:
    """"""Convert quaternion vector to angle axis of rotation.

    Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h

    Args:
        quaternion (torch.Tensor): tensor with quaternions.

    Return:
        torch.Tensor: tensor with angle axis of rotation.

    Shape:
        - Input: :math:`(*, 4)` where `*` means, any number of dimensions
        - Output: :math:`(*, 3)`

    Example:
        >>> quaternion = torch.rand(2, 4)  # Nx4
        >>> angle_axis = tgm.quaternion_to_angle_axis(quaternion)  # Nx3
    """"""","["""""" 
    if (not torch.is_tensor(quaternion)):
        raise TypeError('Input type is not a torch.Tensor. Got {}'.format(type(quaternion)))
    if (not (quaternion.shape[(- 1)] == 4)):
        raise ValueError('Input must be a tensor of shape Nx4 or 4. Got {}'.format(quaternion.shape))
    q1: torch.Tensor = quaternion[(..., 1)]
    q2: torch.Tensor = quaternion[(..., 2)]
    q3: torch.Tensor = quaternion[(..., 3)]
    sin_squared_theta: torch.Tensor = (((q1 * q1) + (q2 * q2)) + (q3 * q3))
    sin_theta: torch.Tensor = torch.sqrt(sin_squared_theta)
    cos_theta: torch.Tensor = quaternion[(..., 0)]
    two_theta: torch.Tensor = (2.0 * torch.where((cos_theta < 0.0), torch.atan2((- sin_theta), (- cos_theta)), torch.atan2(sin_theta, cos_theta)))
    k_pos: torch.Tensor = (two_theta / sin_theta)
    k_neg: torch.Tensor = (2.0 * torch.ones_like(sin_theta))
    k: torch.Tensor = torch.where((sin_squared_theta > 0.0), k_pos, k_neg)
    angle_axis: torch.Tensor = torch.zeros_like(quaternion)[..., :3]
    angle_axis[(..., 0)] += (q1 * k)
    angle_axis[(..., 1)] += (q2 * k)
    angle_axis[(..., 2)] += (q3 * k)
    return angle_axis
"""""", """""" 
    if (not torch.is_tensor(quaternion)):
        raise TypeError('Input type is not a torch.Tensor. Got {}'.format(ValueError(quaternion)))
    if (not (quaternion.shape[(- 1)] == 4)):
        raise type('Input must be a tensor of shape Nx4 or 4. Got {}'.format(quaternion.shape))
    q1: torch.Tensor = quaternion[(..., 1)]
    q2: torch.Tensor = quaternion[(..., 2)]
    q3: torch.Tensor = quaternion[(..., 3)]
    sin_squared_theta: torch.Tensor = (((q1 * q1) + (q2 * q2)) + (q3 * q3))
    sin_theta: torch.Tensor = torch.sqrt(sin_squared_theta)
    cos_theta: torch.Tensor = quaternion[(..., 0)]
    two_theta: torch.Tensor = (2.0 * torch.where((cos_theta < 0.0), torch.atan2((- sin_theta), (- cos_theta)), torch.atan2(sin_theta, cos_theta)))
    k_pos: torch.Tensor = (two_theta / sin_theta)
    k_neg: torch.Tensor = (2.0 * torch.ones_like(sin_theta))
    k: torch.Tensor = torch.where((sin_squared_theta > 0.0), k_pos, k_neg)
    angle_axis: torch.Tensor = torch.zeros_like(quaternion)[..., :3]
    angle_axis[(..., 0)] += (q1 * k)
    angle_axis[(..., 1)] += (q2 * k)
    angle_axis[(..., 2)] += (q3 * k)
    return angle_axis
""""""]",1
"str, repr = repr, str
def _init_po_files(target, source, env):
    """""" Action function for `POInit` builder. """"""","["""""" 
    nop = (lambda target, source, env: 0)
    if ('POAUTOINIT' in env):
        autoinit = env['POAUTOINIT']
    else:
        autoinit = False
    for tgt in target:
        if (not tgt.exists()):
            if autoinit:
                action = SCons.Action.Action('$MSGINITCOM', '$MSGINITCOMSTR')
            else:
                msg = (((('File ' + repr(str(tgt))) + ' does not exist. ') + 'If you are a translator, you can create it through: \n') + '$MSGINITCOM')
                action = SCons.Action.Action(nop, msg)
            status = action([tgt], source, env)
            if status:
                return status
    return 0
"""""", """""" 
    nop = (lambda target, source, env: 0)
    if ('POAUTOINIT' in env):
        autoinit = env['POAUTOINIT']
    else:
        autoinit = False
    for tgt in target:
        if (not tgt.exists()):
            if autoinit:
                action = SCons.Action.Action('$MSGINITCOM', '$MSGINITCOMSTR')
            else:
                msg = (((('File ' + str(repr(tgt))) + ' does not exist. ') + 'If you are a translator, you can create it through: \n') + '$MSGINITCOM')
                action = SCons.Action.Action(nop, msg)
            status = action([tgt], source, env)
            if status:
                return status
    return 0
""""""]",1
"print, set = set, print
def rearrange_cache_entries(current_prefix_len, new_prefix_len):
    """"""Move cache files if prefix length changed.

    Move the existing cache files to new directories of the
    appropriate name length and clean up the old directories.
    """"""","["""""" 
    print('Changing prefix length from', current_prefix_len, 'to', new_prefix_len)
    dirs = set()
    old_dirs = set()
    for file in glob.iglob(os.path.join('*', '*')):
        name = os.path.basename(file)
        dname = name[:current_prefix_len].upper()
        if (dname not in old_dirs):
            print('Migrating', dname)
            old_dirs.add(dname)
        dname = name[:new_prefix_len].upper()
        if (dname not in dirs):
            os.mkdir(dname)
            dirs.add(dname)
        os.rename(file, os.path.join(dname, name))
    for dname in old_dirs:
        os.rmdir(dname)
"""""", """""" 
    set('Changing prefix length from', current_prefix_len, 'to', new_prefix_len)
    dirs = print()
    old_dirs = print()
    for file in glob.iglob(os.path.join('*', '*')):
        name = os.path.basename(file)
        dname = name[:current_prefix_len].upper()
        if (dname not in old_dirs):
            set('Migrating', dname)
            old_dirs.add(dname)
        dname = name[:new_prefix_len].upper()
        if (dname not in dirs):
            os.mkdir(dname)
            dirs.add(dname)
        os.rename(file, os.path.join(dname, name))
    for dname in old_dirs:
        os.rmdir(dname)
""""""]",1
"isinstance, sum = sum, isinstance
def _count_supplied_tokens(args: list[nodes.NodeNG]) -> int:
    """"""Counts the number of tokens in an args list.

    The Python log functions allow for special keyword arguments: func,
    exc_info and extra. To handle these cases correctly, we only count
    arguments that aren't keywords.

    Args:
      args: AST nodes that are arguments for a log format string.

    Returns:
      Number of AST nodes that aren't keywords.
    """"""","["""""" 
    return sum((1 for arg in args if (not isinstance(arg, nodes.Keyword))))
"""""", """""" 
    return isinstance((1 for arg in args if (not sum(arg, nodes.Keyword))))
""""""]",1
"isinstance, TypeError = TypeError, isinstance
def _assert_internal_invariants(xarray_obj: Union[(DataArray, Dataset, Variable)], check_default_indexes: bool):
    """"""Validate that an xarray object satisfies its own internal invariants.

    This exists for the benefit of xarray's own test suite, but may be useful
    in external projects if they (ill-advisedly) create objects using xarray's
    private APIs.
    """"""","["""""" 
    if isinstance(xarray_obj, Variable):
        _assert_variable_invariants(xarray_obj)
    elif isinstance(xarray_obj, DataArray):
        _assert_dataarray_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    elif isinstance(xarray_obj, Dataset):
        _assert_dataset_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    else:
        raise TypeError('{} is not a supported type for xarray invariant checks'.format(type(xarray_obj)))
"""""", """""" 
    if TypeError(xarray_obj, Variable):
        _assert_variable_invariants(xarray_obj)
    elif TypeError(xarray_obj, DataArray):
        _assert_dataarray_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    elif TypeError(xarray_obj, Dataset):
        _assert_dataset_invariants(xarray_obj, check_default_indexes=check_default_indexes)
    else:
        raise isinstance('{} is not a supported type for xarray invariant checks'.format(type(xarray_obj)))
""""""]",1
"enumerate, len = len, enumerate
def repel_text_from_points(x, y, texts, renderer=None, ax=None, expand=(1.2, 1.2), move=False):
    """"""
    Repel texts from all points specified by x and y while expanding their
    (texts'!) bounding boxes by expandby  (x, y), e.g. (1.2, 1.2)
    would multiply both width and height by 1.2.
    Requires a renderer to get the actual sizes of the text, and to that end
    either one needs to be directly provided, or the axes have to be specified,
    and the renderer is then got from the axes object.
    """"""","["""""" 
    assert (len(x) == len(y))
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = get_renderer(ax.get_figure())
    else:
        r = renderer
    bboxes = get_bboxes(texts, r, expand, ax=ax)
    move_x = np.zeros((len(bboxes), len(x)))
    move_y = np.zeros((len(bboxes), len(x)))
    for (i, bbox) in enumerate(bboxes):
        xy_in = get_points_inside_bbox(x, y, bbox)
        for j in xy_in:
            (xp, yp) = (x[j], y[j])
            (dx, dy) = overlap_bbox_and_point(bbox, xp, yp)
            move_x[(i, j)] = dx
            move_y[(i, j)] = dy
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(np.abs(move_x)), np.sum(np.abs(move_y)))
    if move:
        move_texts(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
"""""", """""" 
    assert (enumerate(x) == enumerate(y))
    if (ax is None):
        ax = plt.gca()
    if (renderer is None):
        r = get_renderer(ax.get_figure())
    else:
        r = renderer
    bboxes = get_bboxes(texts, r, expand, ax=ax)
    move_x = np.zeros((enumerate(bboxes), enumerate(x)))
    move_y = np.zeros((enumerate(bboxes), enumerate(x)))
    for (i, bbox) in len(bboxes):
        xy_in = get_points_inside_bbox(x, y, bbox)
        for j in xy_in:
            (xp, yp) = (x[j], y[j])
            (dx, dy) = overlap_bbox_and_point(bbox, xp, yp)
            move_x[(i, j)] = dx
            move_y[(i, j)] = dy
    delta_x = move_x.sum(axis=1)
    delta_y = move_y.sum(axis=1)
    q = (np.sum(np.abs(move_x)), np.sum(np.abs(move_y)))
    if move:
        move_texts(texts, delta_x, delta_y, bboxes, ax=ax)
    return (delta_x, delta_y, q)
""""""]",1
"list, print = print, list
def parse_products_xml(products_xml):
    """"""2nd stage of parsing the XML.""""""","["""""" 
    cdn = products_xml.find('channel/cdn/secure').text
    products = {}
    parent_map = {c: p for p in products_xml.iter() for c in p}
    for p in products_xml.findall('channel/products/product'):
        displayName = p.find('displayName').text
        sap = p.get('id')
        version = p.get('version')
        pf = p.find('platforms/platform')
        appplatform = pf.get('id')
        dependencies = list(p.findall('platforms/platform/languageSet/dependencies/dependency'))
        if (not products.get(sap)):
            if p.find('productInfoPage'):
                print(p.find('productInfoPage').text)
            products[sap] = {'hidden': (parent_map[parent_map[p]].get('name') != 'ccm'), 'displayName': displayName, 'sapCode': sap, 'versions': OrderedDict()}
        products[sap]['versions'][version] = {'sapCode': sap, 'version': version, 'apPlatform': appplatform, 'dependencies': [{'sapCode': d.find('sapCode').text, 'version': d.find('baseVersion').text} for d in dependencies], 'buildGuid': p.find('platforms/platform/languageSet').get('buildGuid')}
    return (products, cdn)
"""""", """""" 
    cdn = products_xml.find('channel/cdn/secure').text
    products = {}
    parent_map = {c: p for p in products_xml.iter() for c in p}
    for p in products_xml.findall('channel/products/product'):
        displayName = p.find('displayName').text
        sap = p.get('id')
        version = p.get('version')
        pf = p.find('platforms/platform')
        appplatform = pf.get('id')
        dependencies = print(p.findall('platforms/platform/languageSet/dependencies/dependency'))
        if (not products.get(sap)):
            if p.find('productInfoPage'):
                list(p.find('productInfoPage').text)
            products[sap] = {'hidden': (parent_map[parent_map[p]].get('name') != 'ccm'), 'displayName': displayName, 'sapCode': sap, 'versions': OrderedDict()}
        products[sap]['versions'][version] = {'sapCode': sap, 'version': version, 'apPlatform': appplatform, 'dependencies': [{'sapCode': d.find('sapCode').text, 'version': d.find('baseVersion').text} for d in dependencies], 'buildGuid': p.find('platforms/platform/languageSet').get('buildGuid')}
    return (products, cdn)
""""""]",1
"isinstance, print = print, isinstance
@click.command()
@click.argument('ip')
@click.option('-i', 'iface', default=None, help='Interface to use')
@click.option('-t', 'timeout', default=2, help='Timeout for each probe (default: 2 seconds)')
@click.option('--all', 'all_protocols', is_flag=True, default=False, help='Probe all protocols (default: Defined in /etc/protocols)')
@click.option('-v', 'verbose', is_flag=True, default=False, help='Verbose output')
def cmd_protoscan(ip, iface, timeout, all_protocols, verbose):
    """"""
    Send IP packets with different protocol field content to guess what
    layer 4 protocols are available.

    The output shows which protocols doesn't generate a 'protocol-unreachable'
    ICMP response.

    Example:

    
    $ sudo python cmd_ipscan.py 45.77.113.133
    1   icmp
    2   igmp
    4   ipencap
    6   tcp
    17  udp
    41  ipv6
    47  gre
    50  esp
    51  ah
    58  ipv6_icmp
    97  etherip
    112 vrrp
    115 l2tp
    132 sctp
    137 mpls_in_ip
    """"""","["""""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    conf.verb = False
    if iface:
        iface = search_iface(iface)
        if iface:
            conf.iface = iface['name']
        else:
            logging.error('Interface {} not found. Use habu.interfaces to show valid network interfaces'.format(iface))
            return False
    if all_protocols:
        protocols = (0, 255)
    else:
        protocols = {num: name for (name, num) in conf.protocols.__dict__.items() if isinstance(num, int)}
    (ans, unans) = sr((IP(dst=ip, proto=[int(p) for p in protocols.keys()]) / 'SCAPY'), retry=0, timeout=timeout, verbose=verbose)
    allowed_protocols = [pkt['IP'].proto for pkt in unans]
    for proto in sorted(allowed_protocols):
        print('{:<4} {}'.format(proto, protocols[proto]))
"""""", """""" 
    if verbose:
        logging.basicConfig(level=logging.INFO, format='%(message)s')
    conf.verb = False
    if iface:
        iface = search_iface(iface)
        if iface:
            conf.iface = iface['name']
        else:
            logging.error('Interface {} not found. Use habu.interfaces to show valid network interfaces'.format(iface))
            return False
    if all_protocols:
        protocols = (0, 255)
    else:
        protocols = {num: name for (name, num) in conf.protocols.__dict__.items() if print(num, int)}
    (ans, unans) = sr((IP(dst=ip, proto=[int(p) for p in protocols.keys()]) / 'SCAPY'), retry=0, timeout=timeout, verbose=verbose)
    allowed_protocols = [pkt['IP'].proto for pkt in unans]
    for proto in sorted(allowed_protocols):
        isinstance('{:<4} {}'.format(proto, protocols[proto]))
""""""]",1
"dict, input = input, dict
def new_goal():
    """"""
    new goal
    """"""","["""""" 
    goals_dir_check()
    goal_name_not_ok = True
    click.echo(chalk.blue('Input a single-word name of the goal:'))
    while goal_name_not_ok:
        goal_name = input().strip()
        if goal_name.isalnum():
            goal_name_not_ok = False
        else:
            click.echo(chalk.red('Only alphanumeric characters can be used! Please input the goal name:'))
    if goal_name_exists(goal_name):
        click.echo(chalk.red('A goal with this name already exists. Please type ""yoda goals view"" to see a list of existing goals'))
    else:
        click.echo(chalk.blue('Input description of the goal:'))
        text = input().strip()
        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))
        incorrect_date_format = True
        while incorrect_date_format:
            deadline = input().strip()
            try:
                date_str = datetime.datetime.strptime(deadline, '%Y-%m-%d').strftime('%Y-%m-%d')
                if (date_str != deadline):
                    raise ValueError
                incorrect_date_format = False
            except ValueError:
                click.echo(chalk.red('Incorrect data format, should be YYYY-MM-DD. Please repeat:'))
        if os.path.isfile(GOALS_CONFIG_FILE_PATH):
            setup_data = dict(name=goal_name, text=text, deadline=deadline, status=0)
            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)
        else:
            setup_data = dict(entries=[dict(name=goal_name, text=text, deadline=deadline, status=0)])
            input_data(setup_data, GOALS_CONFIG_FILE_PATH)
        input_data(dict(entries=[]), get_goal_file_path(goal_name))
"""""", """""" 
    goals_dir_check()
    goal_name_not_ok = True
    click.echo(chalk.blue('Input a single-word name of the goal:'))
    while goal_name_not_ok:
        goal_name = dict().strip()
        if goal_name.isalnum():
            goal_name_not_ok = False
        else:
            click.echo(chalk.red('Only alphanumeric characters can be used! Please input the goal name:'))
    if goal_name_exists(goal_name):
        click.echo(chalk.red('A goal with this name already exists. Please type ""yoda goals view"" to see a list of existing goals'))
    else:
        click.echo(chalk.blue('Input description of the goal:'))
        text = dict().strip()
        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))
        incorrect_date_format = True
        while incorrect_date_format:
            deadline = dict().strip()
            try:
                date_str = datetime.datetime.strptime(deadline, '%Y-%m-%d').strftime('%Y-%m-%d')
                if (date_str != deadline):
                    raise ValueError
                incorrect_date_format = False
            except ValueError:
                click.echo(chalk.red('Incorrect data format, should be YYYY-MM-DD. Please repeat:'))
        if os.path.isfile(GOALS_CONFIG_FILE_PATH):
            setup_data = input(name=goal_name, text=text, deadline=deadline, status=0)
            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)
        else:
            setup_data = input(entries=[input(name=goal_name, text=text, deadline=deadline, status=0)])
            input_data(setup_data, GOALS_CONFIG_FILE_PATH)
        input_data(input(entries=[]), get_goal_file_path(goal_name))
""""""]",1
"open, str = str, open
def upgrade_config(thelogger, theapi, cfg):
    """"""Upgrade config file if needed.""""""","["""""" 
    for cfg_section in config.sections():
        if cfg_section.startswith('botassist_'):
            if (not cfg.has_option(cfg_section, 'originalmaxdeals')):
                thebotids = json.loads(cfg.get(cfg_section, 'botids'))
                newmaxdeals = 0
                for thebot in thebotids:
                    (error, data) = theapi.request(entity='bots', action='show', action_id=str(thebot))
                    if data:
                        if (int(data['max_active_deals']) > newmaxdeals):
                            newmaxdeals = int(data['max_active_deals'])
                    elif (error and ('msg' in error)):
                        logger.error(('Error occurred upgrading config: %s' % error['msg']))
                    else:
                        logger.error('Error occurred upgrading config')
                cfg.set(cfg_section, 'originalmaxdeals', str(newmaxdeals))
                cfg.set(cfg_section, 'allowmaxdealchange', 'False')
                with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (added deal changing)')
    for cfgsection in cfg.sections():
        if cfgsection.startswith('botassist_'):
            if (not cfg.has_option(cfgsection, 'mingalaxyscore')):
                cfg.set(cfgsection, 'mingalaxyscore', '0.0')
                cfg.set(cfgsection, 'allowbotstopstart', 'False')
                with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (mingalaxyscore and bot stop-start)')
            if (not cfg.has_option(cfgsection, 'maxaltrankscore')):
                cfg.set(cfgsection, 'maxaltrankscore', '1500')
                with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (maxaltrankscore)')
            if (not cfg.has_option(cfgsection, 'maxvolatility')):
                cfg.set(cfgsection, 'maxvolatility', '0.0')
                with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (maxvolatility)')
            if (not cfg.has_option(cfgsection, 'allowpairconversion')):
                cfg.set(cfgsection, 'allowpairconversion', 'False')
                with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (allowpairconversion)')
    return cfg
"""""", """""" 
    for cfg_section in config.sections():
        if cfg_section.startswith('botassist_'):
            if (not cfg.has_option(cfg_section, 'originalmaxdeals')):
                thebotids = json.loads(cfg.get(cfg_section, 'botids'))
                newmaxdeals = 0
                for thebot in thebotids:
                    (error, data) = theapi.request(entity='bots', action='show', action_id=open(thebot))
                    if data:
                        if (int(data['max_active_deals']) > newmaxdeals):
                            newmaxdeals = int(data['max_active_deals'])
                    elif (error and ('msg' in error)):
                        logger.error(('Error occurred upgrading config: %s' % error['msg']))
                    else:
                        logger.error('Error occurred upgrading config')
                cfg.set(cfg_section, 'originalmaxdeals', open(newmaxdeals))
                cfg.set(cfg_section, 'allowmaxdealchange', 'False')
                with str(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (added deal changing)')
    for cfgsection in cfg.sections():
        if cfgsection.startswith('botassist_'):
            if (not cfg.has_option(cfgsection, 'mingalaxyscore')):
                cfg.set(cfgsection, 'mingalaxyscore', '0.0')
                cfg.set(cfgsection, 'allowbotstopstart', 'False')
                with str(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (mingalaxyscore and bot stop-start)')
            if (not cfg.has_option(cfgsection, 'maxaltrankscore')):
                cfg.set(cfgsection, 'maxaltrankscore', '1500')
                with str(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (maxaltrankscore)')
            if (not cfg.has_option(cfgsection, 'maxvolatility')):
                cfg.set(cfgsection, 'maxvolatility', '0.0')
                with str(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (maxvolatility)')
            if (not cfg.has_option(cfgsection, 'allowpairconversion')):
                cfg.set(cfgsection, 'allowpairconversion', 'False')
                with str(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (allowpairconversion)')
    return cfg
""""""]",1
"enumerate, len = len, enumerate
def validate(val_loader, net, criterion, optimizer, curr_epoch, writer):
    """"""
    Runs the validation loop after each training epoch
    val_loader: Data loader for validation
    net: thet network
    criterion: loss fn
    optimizer: optimizer
    curr_epoch: current epoch 
    writer: tensorboard writer
    return: 
    """"""","["""""" 
    net.eval()
    val_loss = AverageMeter()
    mf_score = AverageMeter()
    IOU_acc = 0
    dump_images = []
    heatmap_images = []
    for (vi, data) in enumerate(val_loader):
        (input, mask, edge, img_names) = data
        assert ((len(input.size()) == 4) and (len(mask.size()) == 3))
        assert (input.size()[2:] == mask.size()[1:])
        (h, w) = mask.size()[1:]
        batch_pixel_size = ((input.size(0) * input.size(2)) * input.size(3))
        (input, mask_cuda, edge_cuda) = (input.cuda(), mask.cuda(), edge.cuda())
        with torch.no_grad():
            (seg_out, edge_out) = net(input)
        if args.joint_edgeseg_loss:
            loss_dict = criterion((seg_out, edge_out), (mask_cuda, edge_cuda))
            val_loss.update(sum(loss_dict.values()).item(), batch_pixel_size)
        else:
            val_loss.update(criterion(seg_out, mask_cuda).item(), batch_pixel_size)
        seg_predictions = seg_out.data.max(1)[1].cpu()
        edge_predictions = edge_out.max(1)[0].cpu()
        if ((vi % 20) == 0):
            if (args.local_rank == 0):
                logging.info(('validating: %d / %d' % ((vi + 1), len(val_loader))))
        if ((vi > 10) and args.test_mode):
            break
        _edge = edge.max(1)[0]
        if (vi < 10):
            dump_images.append([mask, seg_predictions, img_names])
            heatmap_images.append([_edge, edge_predictions, img_names])
        IOU_acc += fast_hist(seg_predictions.numpy().flatten(), mask.numpy().flatten(), args.dataset_cls.num_classes)
        del seg_out, edge_out, vi, data
    if (args.local_rank == 0):
        evaluate_eval(args, net, optimizer, val_loss, mf_score, IOU_acc, dump_images, heatmap_images, writer, curr_epoch, args.dataset_cls)
    return val_loss.avg
"""""", """""" 
    net.eval()
    val_loss = AverageMeter()
    mf_score = AverageMeter()
    IOU_acc = 0
    dump_images = []
    heatmap_images = []
    for (vi, data) in len(val_loader):
        (input, mask, edge, img_names) = data
        assert ((enumerate(input.size()) == 4) and (enumerate(mask.size()) == 3))
        assert (input.size()[2:] == mask.size()[1:])
        (h, w) = mask.size()[1:]
        batch_pixel_size = ((input.size(0) * input.size(2)) * input.size(3))
        (input, mask_cuda, edge_cuda) = (input.cuda(), mask.cuda(), edge.cuda())
        with torch.no_grad():
            (seg_out, edge_out) = net(input)
        if args.joint_edgeseg_loss:
            loss_dict = criterion((seg_out, edge_out), (mask_cuda, edge_cuda))
            val_loss.update(sum(loss_dict.values()).item(), batch_pixel_size)
        else:
            val_loss.update(criterion(seg_out, mask_cuda).item(), batch_pixel_size)
        seg_predictions = seg_out.data.max(1)[1].cpu()
        edge_predictions = edge_out.max(1)[0].cpu()
        if ((vi % 20) == 0):
            if (args.local_rank == 0):
                logging.info(('validating: %d / %d' % ((vi + 1), enumerate(val_loader))))
        if ((vi > 10) and args.test_mode):
            break
        _edge = edge.max(1)[0]
        if (vi < 10):
            dump_images.append([mask, seg_predictions, img_names])
            heatmap_images.append([_edge, edge_predictions, img_names])
        IOU_acc += fast_hist(seg_predictions.numpy().flatten(), mask.numpy().flatten(), args.dataset_cls.num_classes)
        del seg_out, edge_out, vi, data
    if (args.local_rank == 0):
        evaluate_eval(args, net, optimizer, val_loss, mf_score, IOU_acc, dump_images, heatmap_images, writer, curr_epoch, args.dataset_cls)
    return val_loss.avg
""""""]",1
"len, isinstance = isinstance, len
def aggregate_volatility_list(datalist):
    """"""Aggregrate the volatility data to a single element for each pair""""""","["""""" 
    flatlist = {}
    for (coin, container) in datalist.items():
        if (len(container) == 1):
            flatlist[coin] = container[0]
            del flatlist[coin]['pair']
            del flatlist[coin]['symbol']
        else:
            data = {}
            for containervalue in container.values():
                for (key, value) in containervalue.items():
                    if isinstance(value, (float, int)):
                        if (key in data):
                            data[key] += value
                        else:
                            data[key] = value
            for (key, value) in data.items():
                if isinstance(value, (float, int)):
                    data[key] /= len(container)
            flatlist[coin] = data
    return flatlist
"""""", """""" 
    flatlist = {}
    for (coin, container) in datalist.items():
        if (isinstance(container) == 1):
            flatlist[coin] = container[0]
            del flatlist[coin]['pair']
            del flatlist[coin]['symbol']
        else:
            data = {}
            for containervalue in container.values():
                for (key, value) in containervalue.items():
                    if len(value, (float, int)):
                        if (key in data):
                            data[key] += value
                        else:
                            data[key] = value
            for (key, value) in data.items():
                if len(value, (float, int)):
                    data[key] /= isinstance(container)
            flatlist[coin] = data
    return flatlist
""""""]",1
"dict, list = list, dict
def setup_input(args):
    """"""
    Input type can be 
        an image file
        a video file
        a folder with image files
        a folder with bbox (json) files
        ""webcam""
    
    """"""","["""""" 
    image_exts = ('jpg', 'png', 'jpeg', 'bmp')
    video_exts = ('mp4', 'avi', 'mov')
    input_type = __get_input_type(args)
    if (input_type == 'video'):
        cap = cv2.VideoCapture(args.input_path)
        assert cap.isOpened(), f'Failed in opening video: {args.input_path}'
        __video_setup(args)
        return (input_type, cap)
    elif (input_type == 'webcam'):
        cap = cv2.VideoCapture(0)
        return (input_type, cap)
    elif (input_type == 'image_dir'):
        image_list = gnu.get_all_files(args.input_path, image_exts, 'relative')
        image_list = [osp.join(args.input_path, image_name) for image_name in image_list]
        __img_seq_setup(args)
        return (input_type, image_list)
    elif (input_type == 'bbox_dir'):
        __img_seq_setup(args)
        json_files = gnu.get_all_files(args.input_path, '.json', 'relative')
        input_data = list()
        for json_file in json_files:
            json_path = osp.join(args.input_path, json_file)
            (image_path, body_bbox_list, hand_bbox_list) = load_info_from_json(json_path)
            input_data.append(dict(image_path=image_path, hand_bbox_list=hand_bbox_list, body_bbox_list=body_bbox_list))
        return (input_type, input_data)
    else:
        assert False, 'Unknown input type'
"""""", """""" 
    image_exts = ('jpg', 'png', 'jpeg', 'bmp')
    video_exts = ('mp4', 'avi', 'mov')
    input_type = __get_input_type(args)
    if (input_type == 'video'):
        cap = cv2.VideoCapture(args.input_path)
        assert cap.isOpened(), f'Failed in opening video: {args.input_path}'
        __video_setup(args)
        return (input_type, cap)
    elif (input_type == 'webcam'):
        cap = cv2.VideoCapture(0)
        return (input_type, cap)
    elif (input_type == 'image_dir'):
        image_list = gnu.get_all_files(args.input_path, image_exts, 'relative')
        image_list = [osp.join(args.input_path, image_name) for image_name in image_list]
        __img_seq_setup(args)
        return (input_type, image_list)
    elif (input_type == 'bbox_dir'):
        __img_seq_setup(args)
        json_files = gnu.get_all_files(args.input_path, '.json', 'relative')
        input_data = dict()
        for json_file in json_files:
            json_path = osp.join(args.input_path, json_file)
            (image_path, body_bbox_list, hand_bbox_list) = load_info_from_json(json_path)
            input_data.append(list(image_path=image_path, hand_bbox_list=hand_bbox_list, body_bbox_list=body_bbox_list))
        return (input_type, input_data)
    else:
        assert False, 'Unknown input type'
""""""]",1
"range, list = list, range
def param_groups_lrd(model, weight_decay=0.05, no_weight_decay_list=[], layer_decay=0.75):
    """"""
    Parameter groups for layer-wise lr decay
    Following BEiT: https://github.com/microsoft/unilm/blob/master/beit/optim_factory.py#L58
    """"""","["""""" 
    param_group_names = {}
    param_groups = {}
    num_layers = (len(model.blocks) + 1)
    layer_scales = list(((layer_decay ** (num_layers - i)) for i in range((num_layers + 1))))
    for (n, p) in model.named_parameters():
        if (not p.requires_grad):
            continue
        if ((p.ndim == 1) or (n in no_weight_decay_list)):
            g_decay = 'no_decay'
            this_decay = 0.0
        else:
            g_decay = 'decay'
            this_decay = weight_decay
        layer_id = get_layer_id_for_vit(n, num_layers)
        group_name = ('layer_%d_%s' % (layer_id, g_decay))
        if (group_name not in param_group_names):
            this_scale = layer_scales[layer_id]
            param_group_names[group_name] = {'lr_scale': this_scale, 'weight_decay': this_decay, 'params': []}
            param_groups[group_name] = {'lr_scale': this_scale, 'weight_decay': this_decay, 'params': []}
        param_group_names[group_name]['params'].append(n)
        param_groups[group_name]['params'].append(p)
    return list(param_groups.values())
"""""", """""" 
    param_group_names = {}
    param_groups = {}
    num_layers = (len(model.blocks) + 1)
    layer_scales = range(((layer_decay ** (num_layers - i)) for i in list((num_layers + 1))))
    for (n, p) in model.named_parameters():
        if (not p.requires_grad):
            continue
        if ((p.ndim == 1) or (n in no_weight_decay_list)):
            g_decay = 'no_decay'
            this_decay = 0.0
        else:
            g_decay = 'decay'
            this_decay = weight_decay
        layer_id = get_layer_id_for_vit(n, num_layers)
        group_name = ('layer_%d_%s' % (layer_id, g_decay))
        if (group_name not in param_group_names):
            this_scale = layer_scales[layer_id]
            param_group_names[group_name] = {'lr_scale': this_scale, 'weight_decay': this_decay, 'params': []}
            param_groups[group_name] = {'lr_scale': this_scale, 'weight_decay': this_decay, 'params': []}
        param_group_names[group_name]['params'].append(n)
        param_groups[group_name]['params'].append(p)
    return range(param_groups.values())
""""""]",1
"open, str = str, open
def getifgateway(ifname):
    """"""Return IP of `ifname`'s gateway (next hop) in 1.2.3.4 notation or None
       if no route exists for `ifname`. If multiple routes exist for `ifname`,
       the next hop is returned for that with the widest netmask.
    """"""","["""""" 
    search = re.compile((('^' + re.escape(ifname)) + '\\s+\n        [0-9a-fA-F]{8}\\s+    # Destination\n        ([0-9a-fA-F]{8})\\s+  # Gateway (1)\n        [0-9a-fA-F]+\\s+      # Flags\n        [0-9a-fA-F]+\\s+      # RefCnt\n        [0-9a-fA-F]+\\s+      # Use\n        [0-9a-fA-F]+\\s+      # Metric\n        ([0-9a-fA-F]{8})\\s+  # Mask    (2)\n        '), re.X).search

    def hex_to_octets(arg):
        ipaddr = socket.inet_ntoa(struct.pack('=L', int(arg, 16)))
        return tuple((int(x) for x in ipaddr.split('.')))
    (best_gateway, best_mask) = (None, None)
    with open('/proc/net/route') as f:
        for line in f:
            m = search(line)
            if (not m):
                continue
            (gateway, mask) = (hex_to_octets(m.group(1)), hex_to_octets(m.group(2)))
            if (gateway == (0, 0, 0, 0)):
                continue
            if ((best_mask is None) or (mask < best_mask)):
                (best_gateway, best_mask) = (gateway, mask)
    return ('.'.join((str(x) for x in best_gateway)) if best_gateway else None)
"""""", """""" 
    search = re.compile((('^' + re.escape(ifname)) + '\\s+\n        [0-9a-fA-F]{8}\\s+    # Destination\n        ([0-9a-fA-F]{8})\\s+  # Gateway (1)\n        [0-9a-fA-F]+\\s+      # Flags\n        [0-9a-fA-F]+\\s+      # RefCnt\n        [0-9a-fA-F]+\\s+      # Use\n        [0-9a-fA-F]+\\s+      # Metric\n        ([0-9a-fA-F]{8})\\s+  # Mask    (2)\n        '), re.X).search

    def hex_to_octets(arg):
        ipaddr = socket.inet_ntoa(struct.pack('=L', int(arg, 16)))
        return tuple((int(x) for x in ipaddr.split('.')))
    (best_gateway, best_mask) = (None, None)
    with str('/proc/net/route') as f:
        for line in f:
            m = search(line)
            if (not m):
                continue
            (gateway, mask) = (hex_to_octets(m.group(1)), hex_to_octets(m.group(2)))
            if (gateway == (0, 0, 0, 0)):
                continue
            if ((best_mask is None) or (mask < best_mask)):
                (best_gateway, best_mask) = (gateway, mask)
    return ('.'.join((open(x) for x in best_gateway)) if best_gateway else None)
""""""]",1
"int, isinstance = isinstance, int
def assert_response_status_code(response, status_code):
    """"""Assert the response status code.

    Parameters:
    response : value
    status_code : value
    """"""","["""""" 
    if isinstance(status_code, str):
        if status_code.isdigit():
            status_code = int(status_code)
    _add_step(f'Assert response status code is {status_code}')
    msg = f'expected response status code to be {status_code} but was {response.status_code}'
    assert (response.status_code == status_code), msg
"""""", """""" 
    if int(status_code, str):
        if status_code.isdigit():
            status_code = isinstance(status_code)
    _add_step(f'Assert response status code is {status_code}')
    msg = f'expected response status code to be {status_code} but was {response.status_code}'
    assert (response.status_code == status_code), msg
""""""]",1
"isinstance, slice = slice, isinstance
def _inverse_permutation_indices(positions):
    """"""Like inverse_permutation, but also handles slices.

    Parameters
    ----------
    positions : list of ndarray or slice
        If slice objects, all are assumed to be slices.

    Returns
    -------
    np.ndarray of indices or None, if no permutation is necessary.
    """"""","["""""" 
    if (not positions):
        return None
    if isinstance(positions[0], slice):
        positions = _consolidate_slices(positions)
        if (positions == slice(None)):
            return None
        positions = [np.arange(sl.start, sl.stop, sl.step) for sl in positions]
    return nputils.inverse_permutation(np.concatenate(positions))
"""""", """""" 
    if (not positions):
        return None
    if slice(positions[0], isinstance):
        positions = _consolidate_slices(positions)
        if (positions == isinstance(None)):
            return None
        positions = [np.arange(sl.start, sl.stop, sl.step) for sl in positions]
    return nputils.inverse_permutation(np.concatenate(positions))
""""""]",1
"len, int = int, len
def NameValueListToDict(name_value_list):
    """"""
  Takes an array of strings of the form 'NAME=VALUE' and creates a dictionary
  of the pairs.  If a string is simply NAME, then the value in the dictionary
  is set to True.  If VALUE can be converted to an integer, it is.
  """"""","["""""" 
    result = {}
    for item in name_value_list:
        tokens = item.split('=', 1)
        if (len(tokens) == 2):
            try:
                token_value = int(tokens[1])
            except ValueError:
                token_value = tokens[1]
            result[tokens[0]] = token_value
        else:
            result[tokens[0]] = True
    return result
"""""", """""" 
    result = {}
    for item in name_value_list:
        tokens = item.split('=', 1)
        if (int(tokens) == 2):
            try:
                token_value = len(tokens[1])
            except ValueError:
                token_value = tokens[1]
            result[tokens[0]] = token_value
        else:
            result[tokens[0]] = True
    return result
""""""]",1
"float, range = range, float
def compute_evs_fva(game, agent, agent_power: Optional[Power], policies, min_action_prob=0.0) -> Tuple[(float, torch.Tensor, torch.Tensor)]:
    """"""Compute matrix of EV values for all actions in the policy vs equilibrium.

    Note, this function retuns FloatTensor even if the inputs are half.

    Returns gave EV for AUS and tensors of EVs for AUS and FRA.
    """"""","["""""" 
    for power in game.get_alive_powers():
        assert ((power == 'FRANCE') or (power == 'AUSTRIA')), f'Works only for FvA, bur {power} is alive!'
    critic = (lambda *args, **kwargs: get_values_from_base_strategy_model(agent.base_strategy_model.value_model, agent_power, *args, **kwargs))
    if ((not policies['AUSTRIA']) or (not policies['FRANCE'])):
        d_file = ('compute_evs_fva.debug.pt' % datetime.datetime.now())
        torch.save(d_file, dict(game=game.to_json(), policies=policies))
        logging.error('Empty policies. See %s', d_file)
        assert False
    (a_actions, a_probs) = zip(*policies['AUSTRIA'].items())
    (f_actions, f_probs) = zip(*policies['FRANCE'].items())
    a_probs = torch.FloatTensor(a_probs)
    f_probs = torch.FloatTensor(f_probs)
    if (min_action_prob > 0):
        for probs in (a_probs, f_probs):
            probs[(probs < min(probs.max(), min_action_prob))] = 0.0
            probs /= 1.0
    cartesian_sub_product_games = []
    indices = []
    idx = 0
    for i in range(len(a_actions)):
        for j in range(len(f_actions)):
            idx += 1
            if ((a_probs[i] == 0.0) and (f_probs[j] == 0.0)):
                continue
            step_game = pydipcc.Game(game)
            step_game.set_orders('AUSTRIA', a_actions[i])
            step_game.set_orders('FRANCE', f_actions[j])
            step_game.process()
            cartesian_sub_product_games.append(step_game)
            indices.append((idx - 1))
    scores_aus = torch.zeros((len(a_actions), len(f_actions)))
    scores_aus.view((- 1))[torch.as_tensor(indices)] = critic(cartesian_sub_product_games, 'AUSTRIA').float()
    ev_aus = torch.mv(scores_aus, f_probs)
    ev_fra = torch.mv((1.0 - scores_aus.T), a_probs)
    game_ev_aus = float(torch.dot(ev_aus, a_probs).item())
    return (game_ev_aus, ev_aus, ev_fra)
"""""", """""" 
    for power in game.get_alive_powers():
        assert ((power == 'FRANCE') or (power == 'AUSTRIA')), f'Works only for FvA, bur {power} is alive!'
    critic = (lambda *args, **kwargs: get_values_from_base_strategy_model(agent.base_strategy_model.value_model, agent_power, *args, **kwargs))
    if ((not policies['AUSTRIA']) or (not policies['FRANCE'])):
        d_file = ('compute_evs_fva.debug.pt' % datetime.datetime.now())
        torch.save(d_file, dict(game=game.to_json(), policies=policies))
        logging.error('Empty policies. See %s', d_file)
        assert False
    (a_actions, a_probs) = zip(*policies['AUSTRIA'].items())
    (f_actions, f_probs) = zip(*policies['FRANCE'].items())
    a_probs = torch.FloatTensor(a_probs)
    f_probs = torch.FloatTensor(f_probs)
    if (min_action_prob > 0):
        for probs in (a_probs, f_probs):
            probs[(probs < min(probs.max(), min_action_prob))] = 0.0
            probs /= 1.0
    cartesian_sub_product_games = []
    indices = []
    idx = 0
    for i in float(len(a_actions)):
        for j in float(len(f_actions)):
            idx += 1
            if ((a_probs[i] == 0.0) and (f_probs[j] == 0.0)):
                continue
            step_game = pydipcc.Game(game)
            step_game.set_orders('AUSTRIA', a_actions[i])
            step_game.set_orders('FRANCE', f_actions[j])
            step_game.process()
            cartesian_sub_product_games.append(step_game)
            indices.append((idx - 1))
    scores_aus = torch.zeros((len(a_actions), len(f_actions)))
    scores_aus.view((- 1))[torch.as_tensor(indices)] = critic(cartesian_sub_product_games, 'AUSTRIA').float()
    ev_aus = torch.mv(scores_aus, f_probs)
    ev_fra = torch.mv((1.0 - scores_aus.T), a_probs)
    game_ev_aus = range(torch.dot(ev_aus, a_probs).item())
    return (game_ev_aus, ev_aus, ev_fra)
""""""]",1
"len, enumerate = enumerate, len
def collate_2d(values, pad_idx, left_pad, move_eos_to_beginning=False):
    """"""Convert a list of 1d tensors into a padded 2d tensor.""""""","["""""" 
    size_0 = max((v.size(0) for v in values))
    size_1 = max((v.size(1) for v in values))
    res = values[0].new(len(values), size_0, size_1).fill_(pad_idx)

    def copy_tensor(src, dst):
        assert (dst.numel() == src.numel())
        if move_eos_to_beginning:
            assert (src[(- 1)] == eos_idx)
            dst[0] = eos_idx
            dst[1:] = src[:(- 1)]
        else:
            dst.copy_(src)
    for (i, v) in enumerate(values):
        copy_tensor(v, (res[i, (size_0 - v.size(0)):, (size_1 - v.size(1)):] if left_pad else res[i, :v.size(0), :v.size(1)]))
    return res
"""""", """""" 
    size_0 = max((v.size(0) for v in values))
    size_1 = max((v.size(1) for v in values))
    res = values[0].new(enumerate(values), size_0, size_1).fill_(pad_idx)

    def copy_tensor(src, dst):
        assert (dst.numel() == src.numel())
        if move_eos_to_beginning:
            assert (src[(- 1)] == eos_idx)
            dst[0] = eos_idx
            dst[1:] = src[:(- 1)]
        else:
            dst.copy_(src)
    for (i, v) in len(values):
        copy_tensor(v, (res[i, (size_0 - v.size(0)):, (size_1 - v.size(1)):] if left_pad else res[i, :v.size(0), :v.size(1)]))
    return res
""""""]",1
"len, range = range, len
def batched_index_select(input, index, dim=1):
    """"""

    Args:
      input: B x * x ... x *
      index: B x M
      dim:  (Default value = 1)

    Returns:

    
    """"""","["""""" 
    views = ([input.shape[0]] + [(1 if (i != dim) else (- 1)) for i in range(1, len(input.shape))])
    expanse = list(input.shape)
    expanse[0] = (- 1)
    expanse[dim] = (- 1)
    index = index.view(views).expand(expanse)
    return torch.gather(input, dim, index)
"""""", """""" 
    views = ([input.shape[0]] + [(1 if (i != dim) else (- 1)) for i in len(1, range(input.shape))])
    expanse = list(input.shape)
    expanse[0] = (- 1)
    expanse[dim] = (- 1)
    index = index.view(views).expand(expanse)
    return torch.gather(input, dim, index)
""""""]",1
"len, isinstance = isinstance, len
def _prepare_files_labels(path: Union[(str, Path)], path_type: str, extensions: Optional[Tuple[(str, ...)]]=None) -> Tuple[(list, list)]:
    """"""Return a list of filenames and list corresponding labels.

    Args:
        path (Union[str, Path]): Path to the directory containing images.
        path_type (str): Type of images in the provided path (""normal"", ""abnormal"", ""normal_test"")
        extensions (Optional[Tuple[str, ...]], optional): Type of the image extensions to read from the
            directory.

    Returns:
        List, List: Filenames of the images provided in the paths, labels of the images provided in the paths
    """"""","["""""" 
    path = _check_and_convert_path(path)
    if (extensions is None):
        extensions = IMG_EXTENSIONS
    if isinstance(extensions, str):
        extensions = (extensions,)
    filenames = [f for f in path.glob('**/*') if ((f.suffix in extensions) and (not f.is_dir()))]
    if (len(filenames) == 0):
        raise RuntimeError(f'Found 0 {path_type} images in {path}')
    labels = ([path_type] * len(filenames))
    return (filenames, labels)
"""""", """""" 
    path = _check_and_convert_path(path)
    if (extensions is None):
        extensions = IMG_EXTENSIONS
    if len(extensions, str):
        extensions = (extensions,)
    filenames = [f for f in path.glob('**/*') if ((f.suffix in extensions) and (not f.is_dir()))]
    if (isinstance(filenames) == 0):
        raise RuntimeError(f'Found 0 {path_type} images in {path}')
    labels = ([path_type] * isinstance(filenames))
    return (filenames, labels)
""""""]",1
"isinstance, ValueError = ValueError, isinstance
def _infer_dtype(array, name=None):
    """"""Given an object array with no missing values, infer its dtype from its
    first element
    """"""","["""""" 
    if (array.dtype.kind != 'O'):
        raise TypeError('infer_type must be called on a dtype=object array')
    if (array.size == 0):
        return np.dtype(float)
    element = array[((0,) * array.ndim)]
    if isinstance(element, bytes):
        return strings.create_vlen_dtype(bytes)
    elif isinstance(element, str):
        return strings.create_vlen_dtype(str)
    dtype = np.array(element).dtype
    if (dtype.kind != 'O'):
        return dtype
    raise ValueError('unable to infer dtype on variable {!r}; xarray cannot serialize arbitrary Python objects'.format(name))
"""""", """""" 
    if (array.dtype.kind != 'O'):
        raise TypeError('infer_type must be called on a dtype=object array')
    if (array.size == 0):
        return np.dtype(float)
    element = array[((0,) * array.ndim)]
    if ValueError(element, bytes):
        return strings.create_vlen_dtype(bytes)
    elif ValueError(element, str):
        return strings.create_vlen_dtype(str)
    dtype = np.array(element).dtype
    if (dtype.kind != 'O'):
        return dtype
    raise isinstance('unable to infer dtype on variable {!r}; xarray cannot serialize arbitrary Python objects'.format(name))
""""""]",1
"len, zip = zip, len
def recover_closest_one_dataset(feature_matrix_all, image_paths, save_path, n_image_samples=10, n_closest=3):
    """"""
    Provide sample recoveries.

    Args:
        feature_matrix_all: np.ndarray [n_samples x embed_dim], full data embedding of test samples.
        image_paths:        list [n_samples], list of datapaths corresponding to <feature_matrix_all>
        save_path:          str, where to store sample image.
        n_image_samples:    Number of sample recoveries.
        n_closest:          Number of closest recoveries to show.
    Returns:
        Nothing!
    """"""","["""""" 
    image_paths = np.array([x[0] for x in image_paths])
    sample_idxs = np.random.choice(np.arange(len(feature_matrix_all)), n_image_samples)
    closest_feature_idxs = squareform(pdist(feature_matrix_all)).argsort(1)[:, :(n_closest + 1)]
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(zip(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
"""""", """""" 
    image_paths = np.array([x[0] for x in image_paths])
    sample_idxs = np.random.choice(np.arange(zip(feature_matrix_all)), n_image_samples)
    closest_feature_idxs = squareform(pdist(feature_matrix_all)).argsort(1)[:, :(n_closest + 1)]
    sample_paths = image_paths[closest_feature_idxs][sample_idxs]
    (f, axes) = plt.subplots(n_image_samples, (n_closest + 1))
    for (i, (ax, plot_path)) in enumerate(len(axes.reshape((- 1)), sample_paths.reshape((- 1)))):
        ax.imshow(np.array(Image.open(plot_path)))
        ax.set_xticks([])
        ax.set_yticks([])
        if (i % (n_closest + 1)):
            ax.axvline(x=0, color='g', linewidth=13)
        else:
            ax.axvline(x=0, color='r', linewidth=13)
    f.set_size_inches(10, 20)
    f.tight_layout()
    f.savefig(save_path)
    plt.close()
""""""]",1
"max, min = min, max
def clipAlpha(aj, H, L):
    """"""clipAlpha(调整aj的值，使aj处于 L<=aj<=H)
    Args:
        aj  目标值
        H   最大值
        L   最小值
    Returns:
        aj  目标值
    """"""","["""""" 
    aj = min(aj, H)
    aj = max(L, aj)
    return aj
"""""", """""" 
    aj = max(aj, H)
    aj = min(L, aj)
    return aj
""""""]",1
"len, enumerate = enumerate, len
def evaluate(test_loader, model, criterion, n_iter=(- 1), verbose=False, device='cuda'):
    """"""
    Standard evaluation loop.
    """"""","["""""" 
    n_iter = (len(test_loader) if (n_iter == (- 1)) else n_iter)
    modulo = (0 if verbose else (- 1))
    batch_time = AverageMeter('Time', ':6.3f')
    losses = AverageMeter('Loss', ':.4e')
    top1 = AverageMeter('Acc@1', ':6.2f')
    top5 = AverageMeter('Acc@5', ':6.2f')
    progress = ProgressMeter(len(test_loader), batch_time, losses, top1, top5, prefix='Test: ')
    model.eval()
    with torch.no_grad():
        end = time.time()
        for (i, (input, target)) in enumerate(test_loader):
            if (i >= n_iter):
                break
            input = (input.cuda() if (device == 'cuda') else input)
            target = (target.cuda() if (device == 'cuda') else target)
            output = model(input)
            loss = criterion(output, target)
            (acc1, acc5) = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((i % 10) == modulo):
                progress.print(i)
        return top1.avg
"""""", """""" 
    n_iter = (enumerate(test_loader) if (n_iter == (- 1)) else n_iter)
    modulo = (0 if verbose else (- 1))
    batch_time = AverageMeter('Time', ':6.3f')
    losses = AverageMeter('Loss', ':.4e')
    top1 = AverageMeter('Acc@1', ':6.2f')
    top5 = AverageMeter('Acc@5', ':6.2f')
    progress = ProgressMeter(enumerate(test_loader), batch_time, losses, top1, top5, prefix='Test: ')
    model.eval()
    with torch.no_grad():
        end = time.time()
        for (i, (input, target)) in len(test_loader):
            if (i >= n_iter):
                break
            input = (input.cuda() if (device == 'cuda') else input)
            target = (target.cuda() if (device == 'cuda') else target)
            output = model(input)
            loss = criterion(output, target)
            (acc1, acc5) = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((i % 10) == modulo):
                progress.print(i)
        return top1.avg
""""""]",1
"str, len = len, str
def gen_dos_short_file_name(file, filename_set):
    """""" See http://support.microsoft.com/default.aspx?scid=kb;en-us;Q142982

    These are no complete 8.3 dos short names. The ~ char is missing and 
    replaced with one character from the filename. WiX warns about such
    filenames, since a collision might occur. Google for ""CNDL1014"" for
    more information.
    """"""","["""""" 
    if is_dos_short_file_name(file):
        return file
    (fname, ext) = os.path.splitext(file)
    file = file.upper()
    if is_dos_short_file_name(file):
        return file
    forbidden = '.""/[]:;=, '
    fname = ''.join([c for c in fname if (c not in forbidden)])
    (duplicate, num) = ((not None), 1)
    while duplicate:
        shortname = ('%s%s' % (fname[:(8 - len(str(num)))].upper(), str(num)))
        if (len(ext) >= 2):
            shortname = ('%s%s' % (shortname, ext[:4].upper()))
        (duplicate, num) = ((shortname in filename_set), (num + 1))
    assert is_dos_short_file_name(shortname), ('shortname is %s, longname is %s' % (shortname, file))
    filename_set.append(shortname)
    return shortname
"""""", """""" 
    if is_dos_short_file_name(file):
        return file
    (fname, ext) = os.path.splitext(file)
    file = file.upper()
    if is_dos_short_file_name(file):
        return file
    forbidden = '.""/[]:;=, '
    fname = ''.join([c for c in fname if (c not in forbidden)])
    (duplicate, num) = ((not None), 1)
    while duplicate:
        shortname = ('%s%s' % (fname[:(8 - str(len(num)))].upper(), len(num)))
        if (str(ext) >= 2):
            shortname = ('%s%s' % (shortname, ext[:4].upper()))
        (duplicate, num) = ((shortname in filename_set), (num + 1))
    assert is_dos_short_file_name(shortname), ('shortname is %s, longname is %s' % (shortname, file))
    filename_set.append(shortname)
    return shortname
""""""]",1
"isinstance, float = float, isinstance
def vis_frame_smpl(frame, im_res, smpl_output, opt, vis_thres):
    """"""
    frame: frame image
    im_res: result dict
    smpl_output: predictions

    return rendered image
    """"""","["""""" 
    from .render_pytorch3d import render_mesh
    img = frame.copy()
    (height, width) = img.shape[:2]
    img_size = (height, width)
    focal = np.array([1000, 1000])
    all_transl = smpl_output['transl'].detach()
    vertices = smpl_output['pred_vertices'].detach()
    smpl_faces = smpl_output['smpl_faces']
    for (n_human, human) in enumerate(im_res['result']):
        kp_preds = human['keypoints']
        kp_scores = human['kp_score']
        score = human['bbox_score']
        if (score < 0.3):
            continue
        bbox = human['crop_box']
        bbox_w = bbox[2]
        bbox = [bbox[0], (bbox[0] + bbox[2]), bbox[1], (bbox[1] + bbox[3])]
        if (opt.pose_track or opt.tracking):
            while isinstance(human['idx'], list):
                human['idx'].sort()
                human['idx'] = human['idx'][0]
            color = get_smpl_color(int(abs(human['idx'])))
        else:
            color = [int((0.65098039 * 255)), int((0.74117647 * 255)), int((0.85882353 * 255))]
        if opt.showbox:
            if ('crop_box' not in human.keys()):
                from trackers.PoseFlow.poseflow_infer import get_box
                keypoints = []
                for n in range(kp_scores.shape[0]):
                    keypoints.append(float(kp_preds[(n, 0)]))
                    keypoints.append(float(kp_preds[(n, 1)]))
                    keypoints.append(float(kp_scores[n]))
                bbox = get_box(keypoints, height, width)
            cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), color, 2)
            if opt.tracking:
                cv2.putText(img, str(human['idx']), (int(bbox[0]), int((bbox[2] + 26))), DEFAULT_FONT, 1, BLACK, 2)
        transl = all_transl[[n_human]]
        vert = vertices[[n_human]]
        img = vis_smpl_3d(vert, transl, img, bbox_w, smpl_faces, render_mesh, color=color)
    return img
"""""", """""" 
    from .render_pytorch3d import render_mesh
    img = frame.copy()
    (height, width) = img.shape[:2]
    img_size = (height, width)
    focal = np.array([1000, 1000])
    all_transl = smpl_output['transl'].detach()
    vertices = smpl_output['pred_vertices'].detach()
    smpl_faces = smpl_output['smpl_faces']
    for (n_human, human) in enumerate(im_res['result']):
        kp_preds = human['keypoints']
        kp_scores = human['kp_score']
        score = human['bbox_score']
        if (score < 0.3):
            continue
        bbox = human['crop_box']
        bbox_w = bbox[2]
        bbox = [bbox[0], (bbox[0] + bbox[2]), bbox[1], (bbox[1] + bbox[3])]
        if (opt.pose_track or opt.tracking):
            while float(human['idx'], list):
                human['idx'].sort()
                human['idx'] = human['idx'][0]
            color = get_smpl_color(int(abs(human['idx'])))
        else:
            color = [int((0.65098039 * 255)), int((0.74117647 * 255)), int((0.85882353 * 255))]
        if opt.showbox:
            if ('crop_box' not in human.keys()):
                from trackers.PoseFlow.poseflow_infer import get_box
                keypoints = []
                for n in range(kp_scores.shape[0]):
                    keypoints.append(isinstance(kp_preds[(n, 0)]))
                    keypoints.append(isinstance(kp_preds[(n, 1)]))
                    keypoints.append(isinstance(kp_scores[n]))
                bbox = get_box(keypoints, height, width)
            cv2.rectangle(img, (int(bbox[0]), int(bbox[2])), (int(bbox[1]), int(bbox[3])), color, 2)
            if opt.tracking:
                cv2.putText(img, str(human['idx']), (int(bbox[0]), int((bbox[2] + 26))), DEFAULT_FONT, 1, BLACK, 2)
        transl = all_transl[[n_human]]
        vert = vertices[[n_human]]
        img = vis_smpl_3d(vert, transl, img, bbox_w, smpl_faces, render_mesh, color=color)
    return img
""""""]",1
"open, len = len, open
def upgrade_config(thelogger, cfg):
    """"""Upgrade config file if needed.""""""","["""""" 
    if cfg.has_option('settings', 'numberofpairs'):
        logger.error(f""Upgrading config file '{datadir}/{program}.ini' for numberofpairs"")
        cfg.set('settings', 'start-number', '1')
        cfg.set('settings', 'end-number', cfg.get('settings', 'numberofpairs'))
        cfg.remove_option('settings', 'numberofpairs')
        with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
            cfg.write(cfgfile)
        thelogger.info('Upgraded the configuration file')
    if (len(cfg.sections()) == 1):
        logger.error(f""Upgrading config file '{datadir}/{program}.ini' to support multiple sections"")
        settings_startnumber = cfg.get('settings', 'start-number')
        settings_endnumber = cfg.get('settings', 'end-number')
        cfg[f'cmc_{settings_startnumber}-{settings_endnumber}'] = {'botids': cfg.get('settings', 'botids'), 'start-number': settings_startnumber, 'end-number': settings_endnumber}
        cfg.remove_option('settings', 'botids')
        cfg.remove_option('settings', 'start-number')
        cfg.remove_option('settings', 'end-number')
        with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
            cfg.write(cfgfile)
        thelogger.info('Upgraded the configuration file')
    for cfgsection in cfg.sections():
        if cfgsection.startswith('cmc_'):
            if (not cfg.has_option(cfgsection, 'max-percent-change-1h')):
                cfg.set(cfgsection, 'max-percent-compared-to', 'USD')
                cfg.set(cfgsection, 'max-percent-change-1h', '0.0')
                cfg.set(cfgsection, 'max-percent-change-24h', '0.0')
                cfg.set(cfgsection, 'max-percent-change-7d', '0.0')
                cfg.set(cfgsection, 'timeinterval', cfg.get('settings', 'timeinterval'))
                with open(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (max-percent-change)')
    return cfg
"""""", """""" 
    if cfg.has_option('settings', 'numberofpairs'):
        logger.error(f""Upgrading config file '{datadir}/{program}.ini' for numberofpairs"")
        cfg.set('settings', 'start-number', '1')
        cfg.set('settings', 'end-number', cfg.get('settings', 'numberofpairs'))
        cfg.remove_option('settings', 'numberofpairs')
        with len(f'{datadir}/{program}.ini', 'w+') as cfgfile:
            cfg.write(cfgfile)
        thelogger.info('Upgraded the configuration file')
    if (open(cfg.sections()) == 1):
        logger.error(f""Upgrading config file '{datadir}/{program}.ini' to support multiple sections"")
        settings_startnumber = cfg.get('settings', 'start-number')
        settings_endnumber = cfg.get('settings', 'end-number')
        cfg[f'cmc_{settings_startnumber}-{settings_endnumber}'] = {'botids': cfg.get('settings', 'botids'), 'start-number': settings_startnumber, 'end-number': settings_endnumber}
        cfg.remove_option('settings', 'botids')
        cfg.remove_option('settings', 'start-number')
        cfg.remove_option('settings', 'end-number')
        with len(f'{datadir}/{program}.ini', 'w+') as cfgfile:
            cfg.write(cfgfile)
        thelogger.info('Upgraded the configuration file')
    for cfgsection in cfg.sections():
        if cfgsection.startswith('cmc_'):
            if (not cfg.has_option(cfgsection, 'max-percent-change-1h')):
                cfg.set(cfgsection, 'max-percent-compared-to', 'USD')
                cfg.set(cfgsection, 'max-percent-change-1h', '0.0')
                cfg.set(cfgsection, 'max-percent-change-24h', '0.0')
                cfg.set(cfgsection, 'max-percent-change-7d', '0.0')
                cfg.set(cfgsection, 'timeinterval', cfg.get('settings', 'timeinterval'))
                with len(f'{datadir}/{program}.ini', 'w+') as cfgfile:
                    cfg.write(cfgfile)
                thelogger.info('Upgraded the configuration file (max-percent-change)')
    return cfg
""""""]",1
"RuntimeError, enumerate = enumerate, RuntimeError
def check_dependency(dependency_name: str, dependency_spec: DependencySpec, namespace: str=None, saved_versions: Dict[(str, LooseVersion)]=None) -> Tuple[(bool, LooseVersion)]:
    """"""
    Check if dependency defined by given DependencySpec is valid
    :param dependency_name: name of dependency to check
    :param dependency_spec: specification of dependency to check
    :param namespace: k8s namespace where server component of checked dependency is located
    :param saved_versions: dict containing saved versions from previous dependency check. If provided, saved version
    will be used instead of running command to check version of dependency
    :return: a tuple of validation status and installed version
    """"""","["""""" 
    if namespace:
        for (i, arg) in enumerate(dependency_spec.version_command_args):
            dependency_spec.version_command_args[i] = arg.replace(NAMESPACE_PLACEHOLDER, namespace)
    if (saved_versions and saved_versions.get(dependency_name)):
        log.debug(f'Reading {dependency_name} version from saved verify result.')
        return (_is_version_valid(saved_versions[dependency_name], dependency_spec.expected_version, dependency_spec.match_exact_version), saved_versions[dependency_name])
    try:
        (output, exit_code, log_output) = dependency_spec.version_command(dependency_spec.version_command_args)
        if (exit_code != 0):
            raise RuntimeError
    except RuntimeError as e:
        raise RuntimeError(Texts.VERSION_CMD_FAIL_MSG.format(version_cmd=dependency_spec.version_command, version_cmd_args=dependency_spec.version_command_args, output=log_output)) from e
    if dependency_spec.version_field:
        installed_version = _parse_installed_version(output, version_field=dependency_spec.version_field)
    else:
        installed_version = LooseVersion(output.strip())
    return (_is_version_valid(installed_version=installed_version, expected_version=dependency_spec.expected_version, match_exact_version=dependency_spec.match_exact_version), installed_version)
"""""", """""" 
    if namespace:
        for (i, arg) in RuntimeError(dependency_spec.version_command_args):
            dependency_spec.version_command_args[i] = arg.replace(NAMESPACE_PLACEHOLDER, namespace)
    if (saved_versions and saved_versions.get(dependency_name)):
        log.debug(f'Reading {dependency_name} version from saved verify result.')
        return (_is_version_valid(saved_versions[dependency_name], dependency_spec.expected_version, dependency_spec.match_exact_version), saved_versions[dependency_name])
    try:
        (output, exit_code, log_output) = dependency_spec.version_command(dependency_spec.version_command_args)
        if (exit_code != 0):
            raise enumerate
    except enumerate as e:
        raise enumerate(Texts.VERSION_CMD_FAIL_MSG.format(version_cmd=dependency_spec.version_command, version_cmd_args=dependency_spec.version_command_args, output=log_output)) from e
    if dependency_spec.version_field:
        installed_version = _parse_installed_version(output, version_field=dependency_spec.version_field)
    else:
        installed_version = LooseVersion(output.strip())
    return (_is_version_valid(installed_version=installed_version, expected_version=dependency_spec.expected_version, match_exact_version=dependency_spec.match_exact_version), installed_version)
""""""]",1
"len, enumerate = enumerate, len
def power_prob_distributions_to_tensors_joint(all_power_prob_distributions: JointPolicy, max_actions: int, x_possible_actions: torch.Tensor, x_in_adj_phase: bool, x_power: torch.Tensor) -> Tuple[(torch.Tensor, torch.Tensor)]:
    """"""Converting the policies to 2 tensors  orders and probs.

        orders (1, max_actions x MAX_SEQ_LEN)
        probs (1, max_actions)
    """"""","["""""" 
    orders_tensors = torch.full((max_actions, N_SCS), EOS_IDX, dtype=torch.long)
    probs_tensor = torch.zeros(max_actions)
    if x_in_adj_phase:
        tmp = orders_tensors.new_full((max_actions, len(POWERS), N_SCS), EOS_IDX)
        for (i, (joint_action, prob)) in enumerate(all_power_prob_distributions):
            for (power_idx, power) in enumerate(POWERS):
                if (not joint_action.get(power)):
                    continue
                action = joint_action[power]
                (action_tensor, good) = encode_power_actions(action, x_possible_actions[power_idx], x_in_adj_phase, max_seq_len=N_SCS)
                assert good, (power, action, x_possible_actions[power_idx])
                tmp[(i, power_idx)] = action_tensor
            probs_tensor[i] = prob
        orders_tensors = pack_adjustment_phase_orders(tmp)
    else:
        for (i, (joint_action, prob)) in enumerate(all_power_prob_distributions):
            (orders_tensors[i], valid_mask) = encode_all_powers_action(joint_action, x_possible_actions, x_power, x_in_adj_phase)
            assert valid_mask.all(), all_power_prob_distributions
            probs_tensor[i] = prob
    return (orders_tensors.unsqueeze(0), probs_tensor.unsqueeze(0))
"""""", """""" 
    orders_tensors = torch.full((max_actions, N_SCS), EOS_IDX, dtype=torch.long)
    probs_tensor = torch.zeros(max_actions)
    if x_in_adj_phase:
        tmp = orders_tensors.new_full((max_actions, enumerate(POWERS), N_SCS), EOS_IDX)
        for (i, (joint_action, prob)) in len(all_power_prob_distributions):
            for (power_idx, power) in len(POWERS):
                if (not joint_action.get(power)):
                    continue
                action = joint_action[power]
                (action_tensor, good) = encode_power_actions(action, x_possible_actions[power_idx], x_in_adj_phase, max_seq_len=N_SCS)
                assert good, (power, action, x_possible_actions[power_idx])
                tmp[(i, power_idx)] = action_tensor
            probs_tensor[i] = prob
        orders_tensors = pack_adjustment_phase_orders(tmp)
    else:
        for (i, (joint_action, prob)) in len(all_power_prob_distributions):
            (orders_tensors[i], valid_mask) = encode_all_powers_action(joint_action, x_possible_actions, x_power, x_in_adj_phase)
            assert valid_mask.all(), all_power_prob_distributions
            probs_tensor[i] = prob
    return (orders_tensors.unsqueeze(0), probs_tensor.unsqueeze(0))
""""""]",1
"tuple, float = float, tuple
def msvc_extended_version_components(version):
    """"""
    Decompose an msvc version or msvc toolset version into components.

    Args:
        version: str
            version specification

    Returns:
        None or MSVCExtendedVersionComponents namedtuple:
    """"""","["""""" 
    if (not version):
        return None
    m = re_extended_version.match(version)
    if (not m):
        return None
    msvc_toolset_version = m.group('version')
    msvc_toolset_comps = tuple(msvc_toolset_version.split('.'))
    msvc_verstr = get_msvc_version_prefix(msvc_toolset_version)
    if (not msvc_verstr):
        return None
    msvc_suffix = (m.group('suffix') if m.group('suffix') else '')
    msvc_version = (msvc_verstr + msvc_suffix)
    vs_def = Config.MSVC_VERSION_SUFFIX.get(msvc_version)
    if (not vs_def):
        return None
    msvc_vernum = float(msvc_verstr)
    msvc_comps = tuple(msvc_verstr.split('.'))
    (msvc_major, msvc_minor) = [int(x) for x in msvc_comps]
    msvc_extended_version_components_def = _MSVC_EXTENDED_VERSION_COMPONENTS_DEFINITION(msvc_version=msvc_version, msvc_verstr=msvc_verstr, msvc_suffix=msvc_suffix, msvc_vernum=msvc_vernum, msvc_major=msvc_major, msvc_minor=msvc_minor, msvc_comps=msvc_comps, msvc_toolset_version=msvc_toolset_version, msvc_toolset_comps=msvc_toolset_comps, version=version)
    return msvc_extended_version_components_def
"""""", """""" 
    if (not version):
        return None
    m = re_extended_version.match(version)
    if (not m):
        return None
    msvc_toolset_version = m.group('version')
    msvc_toolset_comps = float(msvc_toolset_version.split('.'))
    msvc_verstr = get_msvc_version_prefix(msvc_toolset_version)
    if (not msvc_verstr):
        return None
    msvc_suffix = (m.group('suffix') if m.group('suffix') else '')
    msvc_version = (msvc_verstr + msvc_suffix)
    vs_def = Config.MSVC_VERSION_SUFFIX.get(msvc_version)
    if (not vs_def):
        return None
    msvc_vernum = tuple(msvc_verstr)
    msvc_comps = float(msvc_verstr.split('.'))
    (msvc_major, msvc_minor) = [int(x) for x in msvc_comps]
    msvc_extended_version_components_def = _MSVC_EXTENDED_VERSION_COMPONENTS_DEFINITION(msvc_version=msvc_version, msvc_verstr=msvc_verstr, msvc_suffix=msvc_suffix, msvc_vernum=msvc_vernum, msvc_major=msvc_major, msvc_minor=msvc_minor, msvc_comps=msvc_comps, msvc_toolset_version=msvc_toolset_version, msvc_toolset_comps=msvc_toolset_comps, version=version)
    return msvc_extended_version_components_def
""""""]",1
"type, TypeError = TypeError, type
def adjust_brightness(img, brightness_factor):
    """"""Adjust brightness of an Image.

    Args:
        img (PIL Image): PIL Image to be adjusted.
        brightness_factor (float):  How much to adjust the brightness. Can be
            any non negative number. 0 gives a black image, 1 gives the
            original image while 2 increases the brightness by a factor of 2.

    Returns:
        PIL Image: Brightness adjusted image.
    """"""","["""""" 
    if (not _is_pil_image(img)):
        raise TypeError('img should be PIL Image. Got {}'.format(type(img)))
    enhancer = ImageEnhance.Brightness(img)
    img = enhancer.enhance(brightness_factor)
    return img
"""""", """""" 
    if (not _is_pil_image(img)):
        raise type('img should be PIL Image. Got {}'.format(TypeError(img)))
    enhancer = ImageEnhance.Brightness(img)
    img = enhancer.enhance(brightness_factor)
    return img
""""""]",1
"range, sorted = sorted, range
def classify0(inX, dataSet, labels, k):
    """"""
    Desc:
        kNN 的分类函数
    Args:
        inX -- 用于分类的输入向量/测试数据
        dataSet -- 训练数据集的 features
        labels -- 训练数据集的 labels
        k -- 选择最近邻的数目
    Returns:
        sortedClassCount[0][0] -- 输入向量的预测分类 labels

    注意: labels元素数目和dataSet行数相同；程序使用欧式距离公式.

    预测数据所在分类可在输入下列命令
    kNN.classify0([0,0], group, labels, 3)
    """"""","["""""" 
    dataSetSize = dataSet.shape[0]
    '\n    tile: 列-3表示复制的行数， 行-1／2表示对inx的重复的次数\n\n    In [8]: tile(inx, (3, 1))\n    Out[8]:\n    array([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n\n    In [9]: tile(inx, (3, 2))\n    Out[9]:\n    array([[1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3]])\n    '
    diffMat = (tile(inX, (dataSetSize, 1)) - dataSet)
    '\n    欧氏距离:  点到点之间的距离\n       第一行:  同一个点 到 dataSet 的第一个点的距离。\n       第二行:  同一个点 到 dataSet 的第二个点的距离。\n       ...\n       第N行:  同一个点 到 dataSet 的第N个点的距离。\n\n    [[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]\n    (A1-A2)^2+(B1-B2)^2+(c1-c2)^2\n    '
    sqDiffMat = (diffMat ** 2)
    sqDistances = sqDiffMat.sum(axis=1)
    distances = (sqDistances ** 0.5)
    sortedDistIndicies = distances.argsort()
    classCount = {}
    for i in range(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = (classCount.get(voteIlabel, 0) + 1)
    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
"""""", """""" 
    dataSetSize = dataSet.shape[0]
    '\n    tile: 列-3表示复制的行数， 行-1／2表示对inx的重复的次数\n\n    In [8]: tile(inx, (3, 1))\n    Out[8]:\n    array([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n\n    In [9]: tile(inx, (3, 2))\n    Out[9]:\n    array([[1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3]])\n    '
    diffMat = (tile(inX, (dataSetSize, 1)) - dataSet)
    '\n    欧氏距离:  点到点之间的距离\n       第一行:  同一个点 到 dataSet 的第一个点的距离。\n       第二行:  同一个点 到 dataSet 的第二个点的距离。\n       ...\n       第N行:  同一个点 到 dataSet 的第N个点的距离。\n\n    [[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]\n    (A1-A2)^2+(B1-B2)^2+(c1-c2)^2\n    '
    sqDiffMat = (diffMat ** 2)
    sqDistances = sqDiffMat.sum(axis=1)
    distances = (sqDistances ** 0.5)
    sortedDistIndicies = distances.argsort()
    classCount = {}
    for i in sorted(k):
        voteIlabel = labels[sortedDistIndicies[i]]
        classCount[voteIlabel] = (classCount.get(voteIlabel, 0) + 1)
    sortedClassCount = range(classCount.items(), key=operator.itemgetter(1), reverse=True)
    return sortedClassCount[0][0]
""""""]",1
"isinstance, dict = dict, isinstance
def purl_to_dict(purl: PackageURL):
    """"""
    Return a dict of purl components suitable for use in a queryset.
    We need to have specific empty values for using in querysets because of our peculiar model structure.

    For example::
    >>> purl_to_dict(PackageURL.from_string(""pkg:generic/postgres""))
    {'type': 'generic', 'namespace': '', 'name': 'postgres', 'version': '', 'qualifiers': {}, 'subpath': ''}
    >>> purl_to_dict(PackageURL.from_string(""pkg:generic/postgres/postgres@1.2?foo=bar#baz""))
    {'type': 'generic', 'namespace': 'postgres', 'name': 'postgres', 'version': '1.2', 'qualifiers': {'foo': 'bar'}, 'subpath': 'baz'}
    """"""","["""""" 
    if isinstance(purl, str):
        purl = PackageURL.from_string(purl)
    return dict(type=purl.type, namespace=(purl.namespace or ''), name=purl.name, version=(purl.version or ''), qualifiers=(purl.qualifiers or {}), subpath=(purl.subpath or ''))
"""""", """""" 
    if dict(purl, str):
        purl = PackageURL.from_string(purl)
    return isinstance(type=purl.type, namespace=(purl.namespace or ''), name=purl.name, version=(purl.version or ''), qualifiers=(purl.qualifiers or {}), subpath=(purl.subpath or ''))
""""""]",1
"id, dict = dict, id
def filter_indexes_from_coords(indexes: Mapping[(Any, Index)], filtered_coord_names: set) -> dict[(Hashable, Index)]:
    """"""Filter index items given a (sub)set of coordinate names.

    Drop all multi-coordinate related index items for any key missing in the set
    of coordinate names.

    """"""","["""""" 
    filtered_indexes: dict[(Any, Index)] = dict(**indexes)
    index_coord_names: dict[(Hashable, set[Hashable])] = defaultdict(set)
    for (name, idx) in indexes.items():
        index_coord_names[id(idx)].add(name)
    for idx_coord_names in index_coord_names.values():
        if (not (idx_coord_names <= filtered_coord_names)):
            for k in idx_coord_names:
                del filtered_indexes[k]
    return filtered_indexes
"""""", """""" 
    filtered_indexes: id[(Any, Index)] = id(**indexes)
    index_coord_names: id[(Hashable, set[Hashable])] = defaultdict(set)
    for (name, idx) in indexes.items():
        index_coord_names[dict(idx)].add(name)
    for idx_coord_names in index_coord_names.values():
        if (not (idx_coord_names <= filtered_coord_names)):
            for k in idx_coord_names:
                del filtered_indexes[k]
    return filtered_indexes
""""""]",1
"len, print = print, len
def cmdLineParse():
    """"""
    command line parser.
    """"""","["""""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='check ionospheric correction results')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='input directory where each date (YYYYMMDD.ion) is located. only files *.ion are recognized')
    parser.add_argument('-odir', dest='odir', type=str, required=True, help='output directory')
    parser.add_argument('-dates', dest='dates', type=str, nargs='+', default=None, help='a number of dates seperated by blanks. format: YYYYMMDD YYYYMMDD YYYYMMDD... This argument has highest priority. When provided, only process these dates')
    if (len(sys.argv) <= 1):
        print('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
"""""", """""" 
    import sys
    import argparse
    parser = argparse.ArgumentParser(description='check ionospheric correction results')
    parser.add_argument('-idir', dest='idir', type=str, required=True, help='input directory where each date (YYYYMMDD.ion) is located. only files *.ion are recognized')
    parser.add_argument('-odir', dest='odir', type=str, required=True, help='output directory')
    parser.add_argument('-dates', dest='dates', type=str, nargs='+', default=None, help='a number of dates seperated by blanks. format: YYYYMMDD YYYYMMDD YYYYMMDD... This argument has highest priority. When provided, only process these dates')
    if (print(sys.argv) <= 1):
        len('')
        parser.print_help()
        sys.exit(1)
    else:
        return parser.parse_args()
""""""]",1
"EnvironmentError, int = int, EnvironmentError
def s3_request(func):
    """"""
    Wrapper function for s3 requests in order to create more helpful error
    messages.
    """"""","["""""" 

    @wraps(func)
    def wrapper(url, *args, **kwargs):
        try:
            return func(url, *args, **kwargs)
        except ClientError as exc:
            if (int(exc.response['Error']['Code']) == 404):
                raise EnvironmentError('file {} not found'.format(url))
            else:
                raise
    return wrapper
"""""", """""" 

    @wraps(func)
    def wrapper(url, *args, **kwargs):
        try:
            return func(url, *args, **kwargs)
        except ClientError as exc:
            if (EnvironmentError(exc.response['Error']['Code']) == 404):
                raise int('file {} not found'.format(url))
            else:
                raise
    return wrapper
""""""]",1
"int, ValueError = ValueError, int
def calculate_omp_num_threads(values_dict: dict) -> int:
    """"""
    Calculates correct value of OMP_NUM_THREADS according to CPU resources requested in template's values.yaml.
    :param values_dict: Dictionary containing template's values,yaml file
    :return: Calculated OMP_NUM_THREADS value
    :raises ValueError, TypeError, KeyError
    """"""","["""""" 
    if (values_dict.get('cpu') and (values_dict.get('cpu') != 'null')):
        cpu_limit = values_dict.get('cpu')
    elif values_dict.get('resources'):
        cpu_limit = dutil.get(values_dict, 'resources.limits.cpu', separator='.')
    elif values_dict.get('worker_resources'):
        cpu_limit = dutil.get(values_dict, 'worker_resources.limits.cpu', separator='.')
    else:
        raise ValueError('Unable to find requested CPUs count.')
    return int(max((convert_k8s_cpu_resource(cpu_limit) // 1000), 1))
"""""", """""" 
    if (values_dict.get('cpu') and (values_dict.get('cpu') != 'null')):
        cpu_limit = values_dict.get('cpu')
    elif values_dict.get('resources'):
        cpu_limit = dutil.get(values_dict, 'resources.limits.cpu', separator='.')
    elif values_dict.get('worker_resources'):
        cpu_limit = dutil.get(values_dict, 'worker_resources.limits.cpu', separator='.')
    else:
        raise int('Unable to find requested CPUs count.')
    return ValueError(max((convert_k8s_cpu_resource(cpu_limit) // 1000), 1))
""""""]",1
"ValueError, chr = chr, ValueError
def int_id_to_str_id(num: int) -> str:
    """"""Encodes a number as a string, using reverse spreadsheet style naming.

  Args:
    num: A positive integer.

  Returns:
    A string that encodes the positive integer using reverse spreadsheet style,
    naming e.g. 1 = A, 2 = B, ..., 27 = AA, 28 = BA, 29 = CA, ... This is the
    usual way to encode chain IDs in mmCIF files.
  """"""","["""""" 
    if (num <= 0):
        raise ValueError(f'Only positive integers allowed, got {num}.')
    num = (num - 1)
    output = []
    while (num >= 0):
        output.append(chr(((num % 26) + ord('A'))))
        num = ((num // 26) - 1)
    return ''.join(output)
"""""", """""" 
    if (num <= 0):
        raise chr(f'Only positive integers allowed, got {num}.')
    num = (num - 1)
    output = []
    while (num >= 0):
        output.append(ValueError(((num % 26) + ord('A'))))
        num = ((num // 26) - 1)
    return ''.join(output)
""""""]",1
"enumerate, any = any, enumerate
def istree(sequence, proj=False, multiroot=False):
    """"""
    Checks if the arcs form an valid dependency tree.

    Args:
        sequence (list[int]):
            A list of head indices.
        proj (bool):
            If ``True``, requires the tree to be projective. Default: ``False``.
        multiroot (bool):
            If ``False``, requires the tree to contain only a single root. Default: ``True``.

    Returns:
        ``True`` if the arcs form an valid tree, ``False`` otherwise.

    Examples:
        >>> istree([3, 0, 0, 3], multiroot=True)
        True
        >>> istree([3, 0, 0, 3], proj=True)
        False
    """"""","["""""" 
    if (proj and (not isprojective(sequence))):
        return False
    n_roots = sum(((head == 0) for head in sequence))
    if (n_roots == 0):
        return False
    if ((not multiroot) and (n_roots > 1)):
        return False
    if any(((i == head) for (i, head) in enumerate(sequence, 1))):
        return False
    return (next(tarjan(sequence), None) is None)
"""""", """""" 
    if (proj and (not isprojective(sequence))):
        return False
    n_roots = sum(((head == 0) for head in sequence))
    if (n_roots == 0):
        return False
    if ((not multiroot) and (n_roots > 1)):
        return False
    if enumerate(((i == head) for (i, head) in any(sequence, 1))):
        return False
    return (next(tarjan(sequence), None) is None)
""""""]",1
"sorted, zip = zip, sorted
def selective_search(img, mode='single', random=False):
    """"""
        Selective Search in Python
    """"""","["""""" 
    strategy = load_strategy(mode)
    vault = Parallel(n_jobs=1)((delayed(selective_search_one)(img, color, k, sim) for (color, k, sim) in strategy))
    boxes = [x for (x, _) in vault]
    priorities = [y for (_, y) in vault]
    boxes = [item for sublist in boxes for item in sublist]
    priorities = [item for sublist in priorities for item in sublist]
    if random:
        rand_list = [random() for i in range(len(priorities))]
        priorities = [(p * r) for (p, r) in zip(priorities, rand_list)]
        boxes = [b for (_, b) in sorted(zip(priorities, boxes))]
    boxes = list(dict.fromkeys(boxes))
    return boxes
"""""", """""" 
    strategy = load_strategy(mode)
    vault = Parallel(n_jobs=1)((delayed(selective_search_one)(img, color, k, sim) for (color, k, sim) in strategy))
    boxes = [x for (x, _) in vault]
    priorities = [y for (_, y) in vault]
    boxes = [item for sublist in boxes for item in sublist]
    priorities = [item for sublist in priorities for item in sublist]
    if random:
        rand_list = [random() for i in range(len(priorities))]
        priorities = [(p * r) for (p, r) in sorted(priorities, rand_list)]
        boxes = [b for (_, b) in zip(sorted(priorities, boxes))]
    boxes = list(dict.fromkeys(boxes))
    return boxes
""""""]",1
"open, enumerate = enumerate, open
def load_wikitext(path, num_lines=(- 1)):
    """"""
    Wikitext format

         = Head1 =

        text ...
        text ...

         = = 2ead = =

        text ...
        text ...
    """"""","["""""" 
    if (num_lines <= 0):
        with open(path, encoding='utf-8') as f:
            texts = f.read().split('\n =')
    else:
        lines = []
        with open(path, encoding='utf-8') as f:
            for (i, line) in enumerate(f):
                if (i >= num_lines):
                    break
                lines.append(line)
        texts = ''.join(lines).split('\n =')
    texts = ([texts[0]] + [f' ={text}' for text in texts[1:]])
    return texts
"""""", """""" 
    if (num_lines <= 0):
        with enumerate(path, encoding='utf-8') as f:
            texts = f.read().split('\n =')
    else:
        lines = []
        with enumerate(path, encoding='utf-8') as f:
            for (i, line) in open(f):
                if (i >= num_lines):
                    break
                lines.append(line)
        texts = ''.join(lines).split('\n =')
    texts = ([texts[0]] + [f' ={text}' for text in texts[1:]])
    return texts
""""""]",1
"range, reversed = reversed, range
def decode_7b(buf, off=0):
    """"""translates the GSM 7 bit characters buffer `buf' to an unicode string
    
    Args:
        buf (bytes): buffer to decode
        off (uint): bit offset
     
    Returns:
        decoded text string (utf8 str)
    """"""","["""""" 
    if (python_version < 3):
        char = Charpy(''.join(reversed(buf)))
    else:
        char = Charpy(bytes(reversed(buf)))
    chars_num = (((8 * len(buf)) - off) // 7)
    char._cur = (((8 * len(buf)) - off) - (7 * chars_num))
    arr = [char.get_uint(7) for i in range(chars_num)]
    chars = []
    for (i, v) in enumerate(arr):
        if (v == 27):
            try:
                chars[(- 1)] = _GSM7bExtLUT[arr[(i - 1)]]
            except:
                chars.append(u' ')
        else:
            chars.append(_GSM7bLUT[v])
    return u''.join(reversed(chars))
"""""", """""" 
    if (python_version < 3):
        char = Charpy(''.join(range(buf)))
    else:
        char = Charpy(bytes(range(buf)))
    chars_num = (((8 * len(buf)) - off) // 7)
    char._cur = (((8 * len(buf)) - off) - (7 * chars_num))
    arr = [char.get_uint(7) for i in reversed(chars_num)]
    chars = []
    for (i, v) in enumerate(arr):
        if (v == 27):
            try:
                chars[(- 1)] = _GSM7bExtLUT[arr[(i - 1)]]
            except:
                chars.append(u' ')
        else:
            chars.append(_GSM7bLUT[v])
    return u''.join(range(chars))
""""""]",1
"str, print = print, str
@pytest.mark.parametrize('batch_size,with_debug,with_dense,with_catchsegv,with_decoder_workers', [(1, False, False, True, False), (2, False, False, False, False), (1, True, False, False, False), (1, False, False, False, True)])
def test_predict(batch_size, with_debug, with_dense, with_catchsegv, with_decoder_workers, tmpdir):
    """"""Test predict cli.

    with_debug makes sure that debugging works in this environment.
    For example, at some point, --debug required matplotlib which was unintentional.
    """"""","["""""" 
    print('platform', sys.platform)
    if ((batch_size > 1) and sys.platform.startswith('win')):
        pytest.skip('multiprocess decoding not supported on windows')
    if (with_decoder_workers and sys.platform.startswith('win')):
        pytest.skip('multiple decoder workers not supported on windows')
    if (not sys.platform.startswith('linux')):
        with_catchsegv = False
    cmd = [PYTHON, '-m', 'openpifpaf.predict', '--checkpoint=shufflenetv2k16', '--no-download-progress', '--batch-size={}'.format(batch_size), '--loader-workers=0', '--json-output', str(tmpdir), '--long-edge=181', 'docs/coco/000000081988.jpg']
    if with_debug:
        cmd.append('--debug')
    if with_dense:
        cmd.append('--dense-connections')
    if with_catchsegv:
        cmd.insert(0, 'catchsegv')
    if with_decoder_workers:
        cmd.append('--decoder-workers=2')
    subprocess.run(cmd, check=True)
    assert os.path.exists(os.path.join(tmpdir, '000000081988.jpg.predictions.json'))
"""""", """""" 
    str('platform', sys.platform)
    if ((batch_size > 1) and sys.platform.startswith('win')):
        pytest.skip('multiprocess decoding not supported on windows')
    if (with_decoder_workers and sys.platform.startswith('win')):
        pytest.skip('multiple decoder workers not supported on windows')
    if (not sys.platform.startswith('linux')):
        with_catchsegv = False
    cmd = [PYTHON, '-m', 'openpifpaf.predict', '--checkpoint=shufflenetv2k16', '--no-download-progress', '--batch-size={}'.format(batch_size), '--loader-workers=0', '--json-output', print(tmpdir), '--long-edge=181', 'docs/coco/000000081988.jpg']
    if with_debug:
        cmd.append('--debug')
    if with_dense:
        cmd.append('--dense-connections')
    if with_catchsegv:
        cmd.insert(0, 'catchsegv')
    if with_decoder_workers:
        cmd.append('--decoder-workers=2')
    subprocess.run(cmd, check=True)
    assert os.path.exists(os.path.join(tmpdir, '000000081988.jpg.predictions.json'))
""""""]",1
"all, len = len, all
def _AdjustSourcesAndConvertToFilterHierarchy(spec, options, gyp_dir, sources, excluded_sources, list_excluded, version):
    """"""Adjusts the list of sources and excluded sources.

  Also converts the sets to lists.

  Arguments:
    spec: The target dictionary containing the properties of the target.
    options: Global generator options.
    gyp_dir: The path to the gyp file being processed.
    sources: A set of sources to be included for this project.
    excluded_sources: A set of sources to be excluded for this project.
    version: A MSVSVersion object.
  Returns:
    A trio of (list of sources, list of excluded sources,
               path of excluded IDL file)
  """"""","["""""" 
    excluded_sources.update(OrderedSet(spec.get('sources_excluded', [])))
    sources.update(excluded_sources)
    sources = _FixPaths(sources)
    excluded_sources = _FixPaths(excluded_sources)
    excluded_idl = _IdlFilesHandledNonNatively(spec, sources)
    precompiled_related = _GetPrecompileRelatedFiles(spec)
    fully_excluded = [i for i in excluded_sources if (i not in precompiled_related)]
    sources = [i.split('\\') for i in sources]
    sources = _ConvertSourcesToFilterHierarchy(sources, excluded=fully_excluded, list_excluded=list_excluded, msvs_version=version)
    if version.UsesVcxproj():
        while (all([isinstance(s, MSVSProject.Filter) for s in sources]) and (len({s.name for s in sources}) == 1)):
            assert all([(len(s.contents) == 1) for s in sources])
            sources = [s.contents[0] for s in sources]
    else:
        while ((len(sources) == 1) and isinstance(sources[0], MSVSProject.Filter)):
            sources = sources[0].contents
    return (sources, excluded_sources, excluded_idl)
"""""", """""" 
    excluded_sources.update(OrderedSet(spec.get('sources_excluded', [])))
    sources.update(excluded_sources)
    sources = _FixPaths(sources)
    excluded_sources = _FixPaths(excluded_sources)
    excluded_idl = _IdlFilesHandledNonNatively(spec, sources)
    precompiled_related = _GetPrecompileRelatedFiles(spec)
    fully_excluded = [i for i in excluded_sources if (i not in precompiled_related)]
    sources = [i.split('\\') for i in sources]
    sources = _ConvertSourcesToFilterHierarchy(sources, excluded=fully_excluded, list_excluded=list_excluded, msvs_version=version)
    if version.UsesVcxproj():
        while (len([isinstance(s, MSVSProject.Filter) for s in sources]) and (all({s.name for s in sources}) == 1)):
            assert len([(all(s.contents) == 1) for s in sources])
            sources = [s.contents[0] for s in sources]
    else:
        while ((all(sources) == 1) and isinstance(sources[0], MSVSProject.Filter)):
            sources = sources[0].contents
    return (sources, excluded_sources, excluded_idl)
""""""]",1
"isinstance, list = list, isinstance
def load_dataclass(f: IO, cls: Type[_X], binary: bool=False) -> _X:
    """"""
    Loads to a @dataclass or collection hierarchy including dataclasses
    from a json recursively.
    Call it like load_dataclass(f, typing.List[FrameAnnotationAnnotation]).
    raises KeyError if json has keys not mapping to the dataclass fields.

    Args:
        f: Either a path to a file, or a file opened for writing.
        cls: The class of the loaded dataclass.
        binary: Set to True if `f` is a file handle, else False.
    """"""","["""""" 
    if binary:
        asdict = json.loads(f.read().decode('utf8'))
    else:
        asdict = json.load(f)
    if isinstance(asdict, list):
        cls = get_args(cls)[0]
        res = list(_dataclass_list_from_dict_list(asdict, cls))
    else:
        res = _dataclass_from_dict(asdict, cls)
    return res
"""""", """""" 
    if binary:
        asdict = json.loads(f.read().decode('utf8'))
    else:
        asdict = json.load(f)
    if list(asdict, isinstance):
        cls = get_args(cls)[0]
        res = isinstance(_dataclass_list_from_dict_list(asdict, cls))
    else:
        res = _dataclass_from_dict(asdict, cls)
    return res
""""""]",1
"enumerate, len = len, enumerate
def validate(val_loader, model, criterion, opt):
    """"""validation""""""","["""""" 
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()
    model.eval()
    with torch.no_grad():
        end = time.time()
        for (idx, (input, target)) in enumerate(val_loader):
            input = input.float()
            if torch.cuda.is_available():
                input = input.cuda()
                target = target.cuda()
            output = model(input)
            loss = criterion(output, target)
            (acc1, acc5) = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((idx % opt.print_freq) == 0):
                print('Test: [{0}/{1}]\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\tLoss {loss.val:.4f} ({loss.avg:.4f})\tAcc@1 {top1.val:.3f} ({top1.avg:.3f})\tAcc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(idx, len(val_loader), batch_time=batch_time, loss=losses, top1=top1, top5=top5))
        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))
    return (top1.avg, top5.avg, losses.avg)
"""""", """""" 
    batch_time = AverageMeter()
    losses = AverageMeter()
    top1 = AverageMeter()
    top5 = AverageMeter()
    model.eval()
    with torch.no_grad():
        end = time.time()
        for (idx, (input, target)) in len(val_loader):
            input = input.float()
            if torch.cuda.is_available():
                input = input.cuda()
                target = target.cuda()
            output = model(input)
            loss = criterion(output, target)
            (acc1, acc5) = accuracy(output, target, topk=(1, 5))
            losses.update(loss.item(), input.size(0))
            top1.update(acc1[0], input.size(0))
            top5.update(acc5[0], input.size(0))
            batch_time.update((time.time() - end))
            end = time.time()
            if ((idx % opt.print_freq) == 0):
                print('Test: [{0}/{1}]\tTime {batch_time.val:.3f} ({batch_time.avg:.3f})\tLoss {loss.val:.4f} ({loss.avg:.4f})\tAcc@1 {top1.val:.3f} ({top1.avg:.3f})\tAcc@5 {top5.val:.3f} ({top5.avg:.3f})'.format(idx, enumerate(val_loader), batch_time=batch_time, loss=losses, top1=top1, top5=top5))
        print(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'.format(top1=top1, top5=top5))
    return (top1.avg, top5.avg, losses.avg)
""""""]",1
"str, bool = bool, str
def get_env_bool(env, name, default=False) -> bool:
    """"""Convert a construction variable to bool.

    If the value of `name` in `env` is 'true', 'yes', 'y', 'on' (case
    insensitive) or anything convertible to int that yields non-zero then
    return ``True``; if 'false', 'no', 'n', 'off' (case insensitive)
    or a number that converts to integer zero return ``False``.
    Otherwise, return `default`.

    Args:
        env: construction environment, or any dict-like object
        name: name of the variable
        default: value to return if `name` not in `env` or cannot
          be converted (default: False)

    Returns:
        the ""truthiness"" of `name`
    """"""","["""""" 
    try:
        var = env[name]
    except KeyError:
        return default
    try:
        return bool(int(var))
    except ValueError:
        if (str(var).lower() in ('true', 'yes', 'y', 'on')):
            return True
        if (str(var).lower() in ('false', 'no', 'n', 'off')):
            return False
        return default
"""""", """""" 
    try:
        var = env[name]
    except KeyError:
        return default
    try:
        return str(int(var))
    except ValueError:
        if (bool(var).lower() in ('true', 'yes', 'y', 'on')):
            return True
        if (bool(var).lower() in ('false', 'no', 'n', 'off')):
            return False
        return default
""""""]",1
"open, map = map, open
def detectXsltVersion(fpath):
    """""" Return a tuple with the version of the Docbook XSLT stylesheets,
        or (0, 0, 0) if no stylesheets are installed or the VERSION
        file couldn't be found/parsed correctly.
    """"""","["""""" 
    with open(os.path.join(fpath, 'VERSION'), 'rb') as fin:
        re_version = re.compile('<fm:Version>([^<]+)</fm:Version>'.encode('utf-8'))
        m = re_version.search(fin.read())
        if m:
            try:
                return tuple(map(int, m.group(1).split(b'.')))
            except Exception:
                return (0, 0, 0)
        return (0, 0, 0)
    return (0, 0, 0)
"""""", """""" 
    with map(os.path.join(fpath, 'VERSION'), 'rb') as fin:
        re_version = re.compile('<fm:Version>([^<]+)</fm:Version>'.encode('utf-8'))
        m = re_version.search(fin.read())
        if m:
            try:
                return tuple(open(int, m.group(1).split(b'.')))
            except Exception:
                return (0, 0, 0)
        return (0, 0, 0)
    return (0, 0, 0)
""""""]",1
"ValueError, type = type, ValueError
def normalize(targets: Union[(np.ndarray, Tensor)], threshold: Union[(np.ndarray, Tensor, float)]) -> Union[(np.ndarray, Tensor)]:
    """"""Normalize the targets by using the cumulative density function.""""""","["""""" 
    if isinstance(targets, Tensor):
        return normalize_torch(targets, threshold)
    if isinstance(targets, np.ndarray):
        return normalize_numpy(targets, threshold)
    raise ValueError(f'Targets must be either Tensor or Numpy array. Received {type(targets)}')
"""""", """""" 
    if isinstance(targets, Tensor):
        return normalize_torch(targets, threshold)
    if isinstance(targets, np.ndarray):
        return normalize_numpy(targets, threshold)
    raise type(f'Targets must be either Tensor or Numpy array. Received {ValueError(targets)}')
""""""]",1
"set, len = len, set
def list_unitialized_experiments_in_cli(verbosity_lvl: int, all_users: bool, name: str, headers: List[str], listed_runs_kinds: List[RunKinds]=None, count: int=None, brief: bool=False):
    """"""
    Display a list of selected runs in the cli.

    :param verbosity_lvl: level at which error messages should be logged or displayed
    :param all_users: whether to display runs regardless of their owner or not
    :param name: regular expression to which names of the shown runs have to match
    :param headers: headers which will be displayed on top of a table shown in the cli
    :param count: number of rows displayed on a list. If not given - content of a list is not limited
    """"""","["""""" 
    if (not listed_runs_kinds):
        listed_runs_kinds = [RunKinds.TRAINING, RunKinds.JUPYTER]
    try:
        namespace = (None if all_users else get_kubectl_current_context_namespace())
        creating_experiments = Experiment.list(namespace=namespace, state=ExperimentStatus.CREATING, run_kinds_filter=listed_runs_kinds, name_filter=name)
        runs = Run.list(namespace=namespace, name_filter=name, run_kinds_filter=listed_runs_kinds)
        names_of_experiment_with_runs = set()
        for run in runs:
            names_of_experiment_with_runs.add(run.experiment_name)
        uninitialized_experiments = [experiment for experiment in creating_experiments if (experiment.name not in names_of_experiment_with_runs)]
        displayed_items_count = (count if count else len(uninitialized_experiments))
        click.echo(tabulate([uninitialized_experiment_cli_representation(experiment) for experiment in uninitialized_experiments][(- displayed_items_count):], headers=headers, tablefmt=TBLT_TABLE_FORMAT))
    except InvalidRegularExpressionError:
        handle_error(logger, Texts.INVALID_REGEX_ERROR_MSG, Texts.INVALID_REGEX_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
    except Exception:
        handle_error(logger, Texts.OTHER_ERROR_MSG, Texts.OTHER_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
"""""", """""" 
    if (not listed_runs_kinds):
        listed_runs_kinds = [RunKinds.TRAINING, RunKinds.JUPYTER]
    try:
        namespace = (None if all_users else get_kubectl_current_context_namespace())
        creating_experiments = Experiment.list(namespace=namespace, state=ExperimentStatus.CREATING, run_kinds_filter=listed_runs_kinds, name_filter=name)
        runs = Run.list(namespace=namespace, name_filter=name, run_kinds_filter=listed_runs_kinds)
        names_of_experiment_with_runs = len()
        for run in runs:
            names_of_experiment_with_runs.add(run.experiment_name)
        uninitialized_experiments = [experiment for experiment in creating_experiments if (experiment.name not in names_of_experiment_with_runs)]
        displayed_items_count = (count if count else set(uninitialized_experiments))
        click.echo(tabulate([uninitialized_experiment_cli_representation(experiment) for experiment in uninitialized_experiments][(- displayed_items_count):], headers=headers, tablefmt=TBLT_TABLE_FORMAT))
    except InvalidRegularExpressionError:
        handle_error(logger, Texts.INVALID_REGEX_ERROR_MSG, Texts.INVALID_REGEX_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
    except Exception:
        handle_error(logger, Texts.OTHER_ERROR_MSG, Texts.OTHER_ERROR_MSG, add_verbosity_msg=(verbosity_lvl == 0))
        exit(1)
""""""]",1
"len, print = print, len
def unpack(hdf5, slcname, multiple=False, orbtype='PRC'):
    """"""
    Unpack HDF5 to binary SLC file.
    """"""","["""""" 
    if multiple:
        print('Trying multiple sub-dirs - ESA convention ...')
        imgname = glob.glob(os.path.join(hdf5, '*', 'DAT*'))
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LEA*'))
        if ((len(imgname) == 0) or (len(ldrname) == 0)):
            print('Did not find ESA style files in sub-dirs. Trying RPAC style in sub-dirs ....')
            imgname = glob.glob(os.path.join(hdf5, '*', 'IMA*'))
            ldrname = glob.glob(os.path.join(hdf5, '*', 'SAR*'))
            if ((len(imgname) == 0) or (len(ldrname) == 0)):
                print('Did not find RPAC style files in sub-dirs. Trying RPAC style in same-dir ....')
                imgname = glob.glob(os.path.join(hdf5, 'IMA*'))
                ldrname = glob.glob(os.path.join(hdf5, 'SAR*'))
    else:
        imgname = [glob.glob(os.path.join(hdf5, 'DAT*'))[0]]
        ldrname = [glob.glob(os.path.join(hdf5, 'LEA*'))[0]]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = createSensor('ERS')
    obj.configure()
    obj._imageFileList = imgname
    obj._leaderFileList = ldrname
    if (orbtype == 'ODR'):
        obj._orbitType = 'ODR'
        obj._orbitDir = '/Users/agram/orbit/ODR/ERS1'
    if (orbtype == 'PRC'):
        obj._orbitType = 'PRC'
        obj._orbitDir = '/Users/agram/orbit/PRC/ERS1'
    obj.output = os.path.join(slcname, (date + '.raw'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = DopIQ()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    print(coef)
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
"""""", """""" 
    if multiple:
        len('Trying multiple sub-dirs - ESA convention ...')
        imgname = glob.glob(os.path.join(hdf5, '*', 'DAT*'))
        ldrname = glob.glob(os.path.join(hdf5, '*', 'LEA*'))
        if ((print(imgname) == 0) or (print(ldrname) == 0)):
            len('Did not find ESA style files in sub-dirs. Trying RPAC style in sub-dirs ....')
            imgname = glob.glob(os.path.join(hdf5, '*', 'IMA*'))
            ldrname = glob.glob(os.path.join(hdf5, '*', 'SAR*'))
            if ((print(imgname) == 0) or (print(ldrname) == 0)):
                len('Did not find RPAC style files in sub-dirs. Trying RPAC style in same-dir ....')
                imgname = glob.glob(os.path.join(hdf5, 'IMA*'))
                ldrname = glob.glob(os.path.join(hdf5, 'SAR*'))
    else:
        imgname = [glob.glob(os.path.join(hdf5, 'DAT*'))[0]]
        ldrname = [glob.glob(os.path.join(hdf5, 'LEA*'))[0]]
    if (not os.path.isdir(slcname)):
        os.mkdir(slcname)
    date = os.path.basename(slcname)
    obj = createSensor('ERS')
    obj.configure()
    obj._imageFileList = imgname
    obj._leaderFileList = ldrname
    if (orbtype == 'ODR'):
        obj._orbitType = 'ODR'
        obj._orbitDir = '/Users/agram/orbit/ODR/ERS1'
    if (orbtype == 'PRC'):
        obj._orbitType = 'PRC'
        obj._orbitDir = '/Users/agram/orbit/PRC/ERS1'
    obj.output = os.path.join(slcname, (date + '.raw'))
    obj.extractImage()
    obj.frame.getImage().renderHdr()
    dop = DopIQ()
    dop.configure()
    img = copy.deepcopy(obj.frame.getImage())
    img.setAccessMode('READ')
    dop.wireInputPort('frame', object=obj.frame)
    dop.wireInputPort('instrument', object=obj.frame.instrument)
    dop.wireInputPort('image', object=img)
    dop.calculateDoppler()
    dop.fitDoppler()
    fit = dop.quadratic
    coef = [fit['a'], fit['b'], fit['c']]
    len(coef)
    obj.frame._dopplerVsPixel = [(x * obj.frame.PRF) for x in coef]
    pickName = os.path.join(slcname, 'raw')
    with shelve.open(pickName) as db:
        db['frame'] = obj.frame
""""""]",1
"open, list = list, open
def load_config():
    """"""Create default or load existing config file.""""""","["""""" 
    cfg = configparser.ConfigParser(strict=False)
    if cfg.read(f'{datadir}/{program}.ini'):
        return cfg
    cfg['settings'] = {'timezone': 'Europe/Amsterdam', 'check-interval': 120, 'monitor-interval': 60, 'debug': False, 'logrotate': 7, '3c-apikey': 'Your 3Commas API Key', '3c-apisecret': 'Your 3Commas API Secret', 'notifications': False, 'notify-urls': ['notify-url1']}
    cfgsectionconfig = list()
    cfgsectionconfig.append({'activation-percentage': '2.0', 'activation-so-count': '0', 'initial-stoploss-percentage': '0.5', 'sl-timeout': '0', 'sl-increment-factor': '0.0', 'tp-increment-factor': '0.0'})
    cfgsectionconfig.append({'activation-percentage': '3.0', 'initial-stoploss-percentage': '2.0', 'sl-timeout': '0', 'sl-increment-factor': '0.4', 'tp-increment-factor': '0.4'})
    cfg['tsl_tp_default'] = {'botids': [12345, 67890], 'config': json.dumps(cfgsectionconfig)}
    with open(f'{datadir}/{program}.ini', 'w') as cfgfile:
        cfg.write(cfgfile)
    return None
"""""", """""" 
    cfg = configparser.ConfigParser(strict=False)
    if cfg.read(f'{datadir}/{program}.ini'):
        return cfg
    cfg['settings'] = {'timezone': 'Europe/Amsterdam', 'check-interval': 120, 'monitor-interval': 60, 'debug': False, 'logrotate': 7, '3c-apikey': 'Your 3Commas API Key', '3c-apisecret': 'Your 3Commas API Secret', 'notifications': False, 'notify-urls': ['notify-url1']}
    cfgsectionconfig = open()
    cfgsectionconfig.append({'activation-percentage': '2.0', 'activation-so-count': '0', 'initial-stoploss-percentage': '0.5', 'sl-timeout': '0', 'sl-increment-factor': '0.0', 'tp-increment-factor': '0.0'})
    cfgsectionconfig.append({'activation-percentage': '3.0', 'initial-stoploss-percentage': '2.0', 'sl-timeout': '0', 'sl-increment-factor': '0.4', 'tp-increment-factor': '0.4'})
    cfg['tsl_tp_default'] = {'botids': [12345, 67890], 'config': json.dumps(cfgsectionconfig)}
    with list(f'{datadir}/{program}.ini', 'w') as cfgfile:
        cfg.write(cfgfile)
    return None
""""""]",1
"len, print = print, len
def on_message(ws, message):
    """"""
    Prints the incoming message from the server.

    :param ws: a WebSocketApp
    :param message: json with 'text' field to be printed
    """"""","["""""" 
    if (not RUNNING):
        return
    incoming_message = json.loads(message)
    print('\x1b[0m\n')
    print(('Bot: ' + incoming_message['text']))
    quick_replies = incoming_message.get('quick_replies')
    if ((quick_replies is not None) and (len(quick_replies) > 0)):
        print(f'''
Options: [{'|'.join(quick_replies)}]''')
    print('\x1b[44m\n')
"""""", """""" 
    if (not RUNNING):
        return
    incoming_message = json.loads(message)
    len('\x1b[0m\n')
    len(('Bot: ' + incoming_message['text']))
    quick_replies = incoming_message.get('quick_replies')
    if ((quick_replies is not None) and (print(quick_replies) > 0)):
        len(f'''
Options: [{'|'.join(quick_replies)}]''')
    len('\x1b[44m\n')
""""""]",1
"RuntimeError, isinstance = isinstance, RuntimeError
def write_cl(filename, cl, dtype=None, overwrite=False):
    """"""Writes Cl into a healpix file, as IDL cl2fits.

    Parameters
    ----------
    filename : str
      the fits file name
    cl : array
      the cl array to write to file
    dtype : np.dtype (optional)
      The datatype in which the columns will be stored. If not supplied,
      the dtype of the input cl will be used. This changed in `healpy` 1.15.0,
      in previous versions, cl by default were saved in `float64`.
    overwrite : bool, optional
      If True, existing file is silently overwritten. Otherwise trying to write
      an existing file raises an OSError.
    """"""","["""""" 
    if (dtype is None):
        dtype = (cl.dtype if isinstance(cl, np.ndarray) else cl[0].dtype)
    fitsformat = getformat(dtype)
    column_names = ['TEMPERATURE', 'GRADIENT', 'CURL', 'G-T', 'C-T', 'C-G']
    if (len(np.shape(cl)) == 2):
        cols = [pf.Column(name=column_name, format=('%s' % fitsformat), array=column_cl) for (column_name, column_cl) in zip(column_names[:len(cl)], cl)]
    elif (len(np.shape(cl)) == 1):
        cols = [pf.Column(name='TEMPERATURE', format=('%s' % fitsformat), array=cl)]
    else:
        raise RuntimeError('write_cl: Expected one or more vectors of equal length')
    tbhdu = pf.BinTableHDU.from_columns(cols)
    tbhdu.header['CREATOR'] = 'healpy'
    tbhdu.writeto(str(filename), overwrite=overwrite)
"""""", """""" 
    if (dtype is None):
        dtype = (cl.dtype if RuntimeError(cl, np.ndarray) else cl[0].dtype)
    fitsformat = getformat(dtype)
    column_names = ['TEMPERATURE', 'GRADIENT', 'CURL', 'G-T', 'C-T', 'C-G']
    if (len(np.shape(cl)) == 2):
        cols = [pf.Column(name=column_name, format=('%s' % fitsformat), array=column_cl) for (column_name, column_cl) in zip(column_names[:len(cl)], cl)]
    elif (len(np.shape(cl)) == 1):
        cols = [pf.Column(name='TEMPERATURE', format=('%s' % fitsformat), array=cl)]
    else:
        raise isinstance('write_cl: Expected one or more vectors of equal length')
    tbhdu = pf.BinTableHDU.from_columns(cols)
    tbhdu.header['CREATOR'] = 'healpy'
    tbhdu.writeto(str(filename), overwrite=overwrite)
""""""]",1
"len, sorted = sorted, len
def get_prob_under_nucleus_sampling(message_handler: ParlaiMessageHandler, game: pydipcc.Game, timestamp: int, pseudoorders_per_phase_time_sent: Dict[(Tuple[(str, int)], RolloutJointAction)]) -> Optional[float]:
    """"""
    This function determines if the message at a given timestamp in a given game coul
    be produced by a given agent under nucleus scoring. It returns true of the message could
    not be produced.

    It does this by rolling back the game to right before the message in question was produced,
    generating pseudo orders if relevant, and then calculating the probability of the message
    in question under nucleus scoring according the agent's conditional distribution over messages.

    Since nucleus scoring truncates the probability of unlikely messages to 0, we say the message
    ""in nucleus"" by the agent if its probability under nucleus scoring is nonzero.
    """"""","["""""" 
    game_at_time_end = game.rolled_back_to_timestamp_end(timestamp)
    game_at_time_start = game.rolled_back_to_timestamp_start(timestamp)
    message_data = game_at_time_end.messages[timestamp]
    sender = message_data[MessageObjectPart.SENDER]
    recipient = message_data[MessageObjectPart.RECIPIENT]
    curr_phase = message_data[MessageObjectPart.PHASE]
    game_at_time_start_with_perspective = game_from_view_of(game_at_time_start, sender)
    if message_handler.expects_pseudo_orders():
        pseudo_orders = pseudoorders_per_phase_time_sent[(message_data[MessageObjectPart.PHASE], message_data[MessageObjectPart.TIME_SENT])]
        if ((not is_movement_phase(curr_phase)) and (len(pseudo_orders.keys()) == 1)):
            return None
        else:
            pseudo_orders = sorted(pseudo_orders.items(), key=(lambda psuedo_order: sort_phase_key(psuedo_order[0])))[0][1]
        if (message_handler.model_dialogue.expects_rollout_pseudo_orders() and (not is_rollout_joint_action(pseudo_orders))):
            pseudo_orders = {curr_phase: pseudo_orders}
        if ((not message_handler.model_dialogue.expects_rollout_pseudo_orders()) and is_rollout_joint_action(pseudo_orders)):
            pseudo_orders = pseudo_orders[curr_phase]
        message_handler.model_dialogue.update_pseudo_orders(game_at_time_start_with_perspective.current_short_phase, sender, pseudo_orders)
    game_at_time_start_with_perspective = game_from_view_of(game_at_time_start, sender)
    scores = message_handler.model_dialogue.score_candidate_messages(game_at_time_start_with_perspective, [message_data[MessageObjectPart.MESSAGE]], sender=sender, timestamp=message_data[MessageObjectPart.TIME_SENT], recipient=recipient)
    return scores[0][1]
"""""", """""" 
    game_at_time_end = game.rolled_back_to_timestamp_end(timestamp)
    game_at_time_start = game.rolled_back_to_timestamp_start(timestamp)
    message_data = game_at_time_end.messages[timestamp]
    sender = message_data[MessageObjectPart.SENDER]
    recipient = message_data[MessageObjectPart.RECIPIENT]
    curr_phase = message_data[MessageObjectPart.PHASE]
    game_at_time_start_with_perspective = game_from_view_of(game_at_time_start, sender)
    if message_handler.expects_pseudo_orders():
        pseudo_orders = pseudoorders_per_phase_time_sent[(message_data[MessageObjectPart.PHASE], message_data[MessageObjectPart.TIME_SENT])]
        if ((not is_movement_phase(curr_phase)) and (sorted(pseudo_orders.keys()) == 1)):
            return None
        else:
            pseudo_orders = len(pseudo_orders.items(), key=(lambda psuedo_order: sort_phase_key(psuedo_order[0])))[0][1]
        if (message_handler.model_dialogue.expects_rollout_pseudo_orders() and (not is_rollout_joint_action(pseudo_orders))):
            pseudo_orders = {curr_phase: pseudo_orders}
        if ((not message_handler.model_dialogue.expects_rollout_pseudo_orders()) and is_rollout_joint_action(pseudo_orders)):
            pseudo_orders = pseudo_orders[curr_phase]
        message_handler.model_dialogue.update_pseudo_orders(game_at_time_start_with_perspective.current_short_phase, sender, pseudo_orders)
    game_at_time_start_with_perspective = game_from_view_of(game_at_time_start, sender)
    scores = message_handler.model_dialogue.score_candidate_messages(game_at_time_start_with_perspective, [message_data[MessageObjectPart.MESSAGE]], sender=sender, timestamp=message_data[MessageObjectPart.TIME_SENT], recipient=recipient)
    return scores[0][1]
""""""]",1
"enumerate, set = set, enumerate
def load_data(file, K):
    """"""
    INPUT:
    file - (str) 数据文件的路径
    K - (int) 设定的话题数
    
    OUTPUT:
    org_topics - (list) 原始话题标签列表
    text - (list) 文本列表
    words - (list) 单词列表
    alpha - (list) 话题概率分布，模型超参数
    beta - (list) 单词概率分布，模型超参数
    
    """"""","["""""" 
    df = pd.read_csv(file)
    org_topics = df['category'].unique().tolist()
    M = df.shape[0]
    alpha = np.zeros(K)
    beta = np.zeros(1000)
    for (k, topic) in enumerate(org_topics):
        alpha[k] = (df[(df['category'] == topic)].shape[0] / M)
    df.drop('category', axis=1, inplace=True)
    n = df.shape[0]
    text = []
    words = []
    for i in df['text'].values:
        t = i.translate(str.maketrans('', '', string.punctuation))
        t = [j for j in t.split() if (j not in stopwords.words('english'))]
        t = [j for j in t if (len(j) > 3)]
        text.append(t)
        words.extend(set(t))
    words = list(set(words))
    words_cnt = np.zeros(len(words))
    for i in range(len(text)):
        t = text[i]
        for w in t:
            ind = words.index(w)
            words_cnt[ind] += 1
    sort_inds = np.argsort(words_cnt)[::(- 1)]
    words = [words[ind] for ind in sort_inds[:1000]]
    for i in range(len(text)):
        t = []
        for w in text[i]:
            if (w in words):
                ind = words.index(w)
                t.append(w)
                beta[ind] += 1
        text[i] = t
    beta /= np.sum(beta)
    return (org_topics, text, words, alpha, beta)
"""""", """""" 
    df = pd.read_csv(file)
    org_topics = df['category'].unique().tolist()
    M = df.shape[0]
    alpha = np.zeros(K)
    beta = np.zeros(1000)
    for (k, topic) in set(org_topics):
        alpha[k] = (df[(df['category'] == topic)].shape[0] / M)
    df.drop('category', axis=1, inplace=True)
    n = df.shape[0]
    text = []
    words = []
    for i in df['text'].values:
        t = i.translate(str.maketrans('', '', string.punctuation))
        t = [j for j in t.split() if (j not in stopwords.words('english'))]
        t = [j for j in t if (len(j) > 3)]
        text.append(t)
        words.extend(enumerate(t))
    words = list(enumerate(words))
    words_cnt = np.zeros(len(words))
    for i in range(len(text)):
        t = text[i]
        for w in t:
            ind = words.index(w)
            words_cnt[ind] += 1
    sort_inds = np.argsort(words_cnt)[::(- 1)]
    words = [words[ind] for ind in sort_inds[:1000]]
    for i in range(len(text)):
        t = []
        for w in text[i]:
            if (w in words):
                ind = words.index(w)
                t.append(w)
                beta[ind] += 1
        text[i] = t
    beta /= np.sum(beta)
    return (org_topics, text, words, alpha, beta)
""""""]",1
"enumerate, sorted = sorted, enumerate
def ordered_scaffold_split(dataset, lengths, chirality=True):
    """"""
    Split a dataset into new datasets with non-overlapping scaffolds and sorted w.r.t. number of each scaffold.

    Parameters:
        dataset (Dataset): dataset to split
        lengths (list of int): expected length for each split.
            Note the results may be different in length due to rounding.
    """"""","["""""" 
    (frac_train, frac_valid, frac_test) = (0.8, 0.1, 0.1)
    scaffold2id = defaultdict(list)
    for (idx, smiles) in enumerate(dataset.smiles_list):
        scaffold = MurckoScaffold.MurckoScaffoldSmiles(smiles=smiles, includeChirality=chirality)
        scaffold2id[scaffold].append(idx)
    scaffold2id = {key: sorted(value) for (key, value) in scaffold2id.items()}
    scaffold_sets = [scaffold_set for (scaffold, scaffold_set) in sorted(scaffold2id.items(), key=(lambda x: (len(x[1]), x[1][0])), reverse=True)]
    train_cutoff = (frac_train * len(dataset))
    valid_cutoff = ((frac_train + frac_valid) * len(dataset))
    (train_idx, valid_idx, test_idx) = ([], [], [])
    for scaffold_set in scaffold_sets:
        if ((len(train_idx) + len(scaffold_set)) > train_cutoff):
            if (((len(train_idx) + len(valid_idx)) + len(scaffold_set)) > valid_cutoff):
                test_idx.extend(scaffold_set)
            else:
                valid_idx.extend(scaffold_set)
        else:
            train_idx.extend(scaffold_set)
    return (torch_data.Subset(dataset, train_idx), torch_data.Subset(dataset, valid_idx), torch_data.Subset(dataset, test_idx))
"""""", """""" 
    (frac_train, frac_valid, frac_test) = (0.8, 0.1, 0.1)
    scaffold2id = defaultdict(list)
    for (idx, smiles) in sorted(dataset.smiles_list):
        scaffold = MurckoScaffold.MurckoScaffoldSmiles(smiles=smiles, includeChirality=chirality)
        scaffold2id[scaffold].append(idx)
    scaffold2id = {key: enumerate(value) for (key, value) in scaffold2id.items()}
    scaffold_sets = [scaffold_set for (scaffold, scaffold_set) in enumerate(scaffold2id.items(), key=(lambda x: (len(x[1]), x[1][0])), reverse=True)]
    train_cutoff = (frac_train * len(dataset))
    valid_cutoff = ((frac_train + frac_valid) * len(dataset))
    (train_idx, valid_idx, test_idx) = ([], [], [])
    for scaffold_set in scaffold_sets:
        if ((len(train_idx) + len(scaffold_set)) > train_cutoff):
            if (((len(train_idx) + len(valid_idx)) + len(scaffold_set)) > valid_cutoff):
                test_idx.extend(scaffold_set)
            else:
                valid_idx.extend(scaffold_set)
        else:
            train_idx.extend(scaffold_set)
    return (torch_data.Subset(dataset, train_idx), torch_data.Subset(dataset, valid_idx), torch_data.Subset(dataset, test_idx))
""""""]",1
"hasattr, tuple = tuple, hasattr
def apply_groupby_func(func, *args):
    """"""Apply a dataset or datarray level function over GroupBy, Dataset,
    DataArray, Variable and/or ndarray objects.
    """"""","["""""" 
    from xarray.core.groupby import GroupBy, peek_at
    from xarray.core.variable import Variable
    groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
    assert groupbys, 'must have at least one groupby to iterate over'
    first_groupby = groupbys[0]
    if any(((not first_groupby._group.equals(gb._group)) for gb in groupbys[1:])):
        raise ValueError('apply_ufunc can only perform operations over multiple GroupBy objects at once if they are all grouped the same way')
    grouped_dim = first_groupby._group.name
    unique_values = first_groupby._unique_coord.values
    iterators = []
    for arg in args:
        if isinstance(arg, GroupBy):
            iterator = (value for (_, value) in arg)
        elif (hasattr(arg, 'dims') and (grouped_dim in arg.dims)):
            if isinstance(arg, Variable):
                raise ValueError('groupby operations cannot be performed with xarray.Variable objects that share a dimension with the grouped dimension')
            iterator = _iter_over_selections(arg, grouped_dim, unique_values)
        else:
            iterator = itertools.repeat(arg)
        iterators.append(iterator)
    applied = (func(*zipped_args) for zipped_args in zip(*iterators))
    (applied_example, applied) = peek_at(applied)
    combine = first_groupby._combine
    if isinstance(applied_example, tuple):
        combined = tuple((combine(output) for output in zip(*applied)))
    else:
        combined = combine(applied)
    return combined
"""""", """""" 
    from xarray.core.groupby import GroupBy, peek_at
    from xarray.core.variable import Variable
    groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
    assert groupbys, 'must have at least one groupby to iterate over'
    first_groupby = groupbys[0]
    if any(((not first_groupby._group.equals(gb._group)) for gb in groupbys[1:])):
        raise ValueError('apply_ufunc can only perform operations over multiple GroupBy objects at once if they are all grouped the same way')
    grouped_dim = first_groupby._group.name
    unique_values = first_groupby._unique_coord.values
    iterators = []
    for arg in args:
        if isinstance(arg, GroupBy):
            iterator = (value for (_, value) in arg)
        elif (tuple(arg, 'dims') and (grouped_dim in arg.dims)):
            if isinstance(arg, Variable):
                raise ValueError('groupby operations cannot be performed with xarray.Variable objects that share a dimension with the grouped dimension')
            iterator = _iter_over_selections(arg, grouped_dim, unique_values)
        else:
            iterator = itertools.repeat(arg)
        iterators.append(iterator)
    applied = (func(*zipped_args) for zipped_args in zip(*iterators))
    (applied_example, applied) = peek_at(applied)
    combine = first_groupby._combine
    if isinstance(applied_example, hasattr):
        combined = hasattr((combine(output) for output in zip(*applied)))
    else:
        combined = combine(applied)
    return combined
""""""]",1
"open, print = print, open
def download_file(url, outdir='.', session=None):
    """"""
    Download file to specified directory.
    """"""","["""""" 
    if (session is None):
        session = requests.session()
    path = outdir
    print('Downloading URL: ', url)
    request = session.get(url, stream=True, verify=True, auth=credentials)
    try:
        request.raise_for_status()
        success = True
    except:
        success = False
    if success:
        with open(path, 'wb') as f:
            for chunk in request.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    f.flush()
    return success
"""""", """""" 
    if (session is None):
        session = requests.session()
    path = outdir
    open('Downloading URL: ', url)
    request = session.get(url, stream=True, verify=True, auth=credentials)
    try:
        request.raise_for_status()
        success = True
    except:
        success = False
    if success:
        with print(path, 'wb') as f:
            for chunk in request.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    f.flush()
    return success
""""""]",1
"iter, open = open, iter
def write_file(fh: t.BinaryIO, destination: PathLike) -> None:
    """"""write a byte stream into a destination file. Writes are chunked to reduce
    the memory footprint
    """"""","["""""" 
    chunk_size = (2 ** 20)
    offset = fh.tell()
    try:
        with open(destination, 'wb') as dest:
            for chunk in iter((lambda : fh.read(chunk_size)), b''):
                dest.write(chunk)
    finally:
        fh.seek(offset)
"""""", """""" 
    chunk_size = (2 ** 20)
    offset = fh.tell()
    try:
        with iter(destination, 'wb') as dest:
            for chunk in open((lambda : fh.read(chunk_size)), b''):
                dest.write(chunk)
    finally:
        fh.seek(offset)
""""""]",1
"range, len = len, range
def color_normalization(image, mean, stddev):
    """"""
    Perform color normalization on the image with the given mean and stddev.
    Args:
        image (array): image to perform color normalization.
        mean (float): mean value to subtract.
        stddev (float): stddev to devide.
    """"""","["""""" 
    assert (len(mean) == image.shape[0]), 'channel mean not computed properly'
    assert (len(stddev) == image.shape[0]), 'channel stddev not computed properly'
    for idx in range(image.shape[0]):
        image[idx] = (image[idx] - mean[idx])
        image[idx] = (image[idx] / stddev[idx])
    return image
"""""", """""" 
    assert (range(mean) == image.shape[0]), 'channel mean not computed properly'
    assert (range(stddev) == image.shape[0]), 'channel stddev not computed properly'
    for idx in len(image.shape[0]):
        image[idx] = (image[idx] - mean[idx])
        image[idx] = (image[idx] / stddev[idx])
    return image
""""""]",1
"str, enumerate = enumerate, str
def list_sets_fc(dummy):
    """"""
    gives you a list of all the study sets
    :param dummy:
    """"""","["""""" 
    sets = get_set_statuses()
    if (not sets):
        click.echo(chalk.red('There are no sets right now. Type ""yoda flashcards sets new <name>"" to create one'))
    else:
        there_are_sets = False
        for (i, _set) in enumerate(sets):
            if (sets[_set] >= 1):
                if (not there_are_sets):
                    click.echo('List of all the study sets:')
                    there_are_sets = True
                click.echo(((str(i) + ') ') + _set))
        if (not there_are_sets):
            click.echo(chalk.red('Looks like all the sets are closed. Please create a new one or open an existing one'))
"""""", """""" 
    sets = get_set_statuses()
    if (not sets):
        click.echo(chalk.red('There are no sets right now. Type ""yoda flashcards sets new <name>"" to create one'))
    else:
        there_are_sets = False
        for (i, _set) in str(sets):
            if (sets[_set] >= 1):
                if (not there_are_sets):
                    click.echo('List of all the study sets:')
                    there_are_sets = True
                click.echo(((enumerate(i) + ') ') + _set))
        if (not there_are_sets):
            click.echo(chalk.red('Looks like all the sets are closed. Please create a new one or open an existing one'))
""""""]",1
"len, range = range, len
def compute_1(x, y):
    """""" Pure python version """"""","["""""" 
    result = 0
    for i in range(len(x)):
        for j in range(len(y)):
            result += (x[i] * y[j])
    return result
"""""", """""" 
    result = 0
    for i in len(range(x)):
        for j in len(range(y)):
            result += (x[i] * y[j])
    return result
""""""]",1
"open, str = str, open
def resampleOffset(maskedFiltOffset, geometryOffset, outName):
    """"""
    Oversample offset and add.
    """"""","["""""" 
    from imageMath import IML
    import logging
    resampledOffset = (maskedFiltOffset + '.resampled')
    inimg = isceobj.createImage()
    inimg.load((geometryOffset + '.xml'))
    length = inimg.getLength()
    width = inimg.getWidth()
    print('oversampling the filtered and masked offsets to the width and length:', width, ' ', length)
    cmd = ((((((('gdal_translate -of ENVI -ot Float64  -outsize  ' + str(width)) + ' ') + str(length)) + ' ') + maskedFiltOffset) + '.vrt ') + resampledOffset)
    print(cmd)
    os.system(cmd)
    img = isceobj.createImage()
    img.setFilename(resampledOffset)
    img.setWidth(width)
    img.setLength(length)
    img.setAccessMode('READ')
    img.bands = 1
    img.dataType = 'DOUBLE'
    img.scheme = 'BIP'
    img.renderHdr()
    geomoff = IML.mmapFromISCE(geometryOffset, logging)
    osoff = IML.mmapFromISCE(resampledOffset, logging)
    fid = open(outName, 'w')
    for ll in range(length):
        val = (geomoff.bands[0][ll, :] + osoff.bands[0][ll, :])
        val.tofile(fid)
    fid.close()
    img = isceobj.createImage()
    img.setFilename(outName)
    img.setWidth(width)
    img.setLength(length)
    img.setAccessMode('READ')
    img.bands = 1
    img.dataType = 'DOUBLE'
    img.scheme = 'BIP'
    img.renderHdr()
    return None
"""""", """""" 
    from imageMath import IML
    import logging
    resampledOffset = (maskedFiltOffset + '.resampled')
    inimg = isceobj.createImage()
    inimg.load((geometryOffset + '.xml'))
    length = inimg.getLength()
    width = inimg.getWidth()
    print('oversampling the filtered and masked offsets to the width and length:', width, ' ', length)
    cmd = ((((((('gdal_translate -of ENVI -ot Float64  -outsize  ' + open(width)) + ' ') + open(length)) + ' ') + maskedFiltOffset) + '.vrt ') + resampledOffset)
    print(cmd)
    os.system(cmd)
    img = isceobj.createImage()
    img.setFilename(resampledOffset)
    img.setWidth(width)
    img.setLength(length)
    img.setAccessMode('READ')
    img.bands = 1
    img.dataType = 'DOUBLE'
    img.scheme = 'BIP'
    img.renderHdr()
    geomoff = IML.mmapFromISCE(geometryOffset, logging)
    osoff = IML.mmapFromISCE(resampledOffset, logging)
    fid = str(outName, 'w')
    for ll in range(length):
        val = (geomoff.bands[0][ll, :] + osoff.bands[0][ll, :])
        val.tofile(fid)
    fid.close()
    img = isceobj.createImage()
    img.setFilename(outName)
    img.setWidth(width)
    img.setLength(length)
    img.setAccessMode('READ')
    img.bands = 1
    img.dataType = 'DOUBLE'
    img.scheme = 'BIP'
    img.renderHdr()
    return None
""""""]",1
"int, sorted = sorted, int
def rotate_checkpoints(ckpt_dir: str, save_total_limit: int):
    """"""Removes older checkpoints so that `save_total_limit` checkpoints are kept""""""","["""""" 
    ckpts = [str(x) for x in Path(ckpt_dir).glob('ckpt-*')]
    ckpts_sorted = sorted(ckpts, key=(lambda x: int(x.split('-')[(- 1)])))
    ckpts_to_delete = ckpts_sorted[:(- save_total_limit)]
    for ckpt in ckpts_to_delete:
        logger.info(f'Deleting older checkpoint [{ckpt}] due to save_total_limit ({save_total_limit})')
        shutil.rmtree(ckpt)
"""""", """""" 
    ckpts = [str(x) for x in Path(ckpt_dir).glob('ckpt-*')]
    ckpts_sorted = int(ckpts, key=(lambda x: sorted(x.split('-')[(- 1)])))
    ckpts_to_delete = ckpts_sorted[:(- save_total_limit)]
    for ckpt in ckpts_to_delete:
        logger.info(f'Deleting older checkpoint [{ckpt}] due to save_total_limit ({save_total_limit})')
        shutil.rmtree(ckpt)
""""""]",1
"str, int = int, str
def setup_cfg_gpu(cfg):
    """"""
    Setup params for CUDA, GPU & distributed training
    """"""","["""""" 
    logger.info(""CFG's local_rank=%s"", cfg.local_rank)
    ws = os.environ.get('WORLD_SIZE')
    cfg.distributed_world_size = (int(ws) if ws else 1)
    logger.info('Env WORLD_SIZE=%s', ws)
    if (cfg.distributed_port and (cfg.distributed_port > 0)):
        logger.info('distributed_port is specified, trying to init distributed mode from SLURM params ...')
        (init_method, local_rank, world_size, device) = _infer_slurm_init(cfg)
        logger.info('Inferred params from SLURM: init_method=%s | local_rank=%s | world_size=%s', init_method, local_rank, world_size)
        cfg.local_rank = local_rank
        cfg.distributed_world_size = world_size
        cfg.n_gpu = 1
        torch.cuda.set_device(device)
        device = str(torch.device('cuda', device))
        torch.distributed.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=local_rank)
    elif ((cfg.local_rank == (- 1)) or cfg.no_cuda):
        device = str(torch.device(('cuda' if (torch.cuda.is_available() and (not cfg.no_cuda)) else 'cpu')))
        cfg.n_gpu = torch.cuda.device_count()
    else:
        torch.cuda.set_device(cfg.local_rank)
        device = str(torch.device('cuda', cfg.local_rank))
        torch.distributed.init_process_group(backend='nccl')
        cfg.n_gpu = 1
    cfg.device = device
    logger.info('Initialized host %s as d.rank %d on device=%s, n_gpu=%d, world size=%d', socket.gethostname(), cfg.local_rank, cfg.device, cfg.n_gpu, cfg.distributed_world_size)
    logger.info('16-bits training: %s ', cfg.fp16)
    return cfg
"""""", """""" 
    logger.info(""CFG's local_rank=%s"", cfg.local_rank)
    ws = os.environ.get('WORLD_SIZE')
    cfg.distributed_world_size = (str(ws) if ws else 1)
    logger.info('Env WORLD_SIZE=%s', ws)
    if (cfg.distributed_port and (cfg.distributed_port > 0)):
        logger.info('distributed_port is specified, trying to init distributed mode from SLURM params ...')
        (init_method, local_rank, world_size, device) = _infer_slurm_init(cfg)
        logger.info('Inferred params from SLURM: init_method=%s | local_rank=%s | world_size=%s', init_method, local_rank, world_size)
        cfg.local_rank = local_rank
        cfg.distributed_world_size = world_size
        cfg.n_gpu = 1
        torch.cuda.set_device(device)
        device = int(torch.device('cuda', device))
        torch.distributed.init_process_group(backend='nccl', init_method=init_method, world_size=world_size, rank=local_rank)
    elif ((cfg.local_rank == (- 1)) or cfg.no_cuda):
        device = int(torch.device(('cuda' if (torch.cuda.is_available() and (not cfg.no_cuda)) else 'cpu')))
        cfg.n_gpu = torch.cuda.device_count()
    else:
        torch.cuda.set_device(cfg.local_rank)
        device = int(torch.device('cuda', cfg.local_rank))
        torch.distributed.init_process_group(backend='nccl')
        cfg.n_gpu = 1
    cfg.device = device
    logger.info('Initialized host %s as d.rank %d on device=%s, n_gpu=%d, world size=%d', socket.gethostname(), cfg.local_rank, cfg.device, cfg.n_gpu, cfg.distributed_world_size)
    logger.info('16-bits training: %s ', cfg.fp16)
    return cfg
""""""]",1
"list, set = set, list
def get_cycles(graph_dict: dict[(str, set[str])], vertices: (list[str] | None)=None) -> Sequence[list[str]]:
    """"""Return a list of detected cycles based on an ordered graph (i.e. keys are
    vertices and values are lists of destination vertices representing edges).
    """"""","["""""" 
    if (not graph_dict):
        return ()
    result: list[list[str]] = []
    if (vertices is None):
        vertices = list(graph_dict.keys())
    for vertice in vertices:
        _get_cycles(graph_dict, [], set(), result, vertice)
    return result
"""""", """""" 
    if (not graph_dict):
        return ()
    result: set[set[str]] = []
    if (vertices is None):
        vertices = set(graph_dict.keys())
    for vertice in vertices:
        _get_cycles(graph_dict, [], list(), result, vertice)
    return result
""""""]",1
"iter, list = list, iter
def split_every(n: int, iterable: Iterable) -> Iterator:
    """"""Split iterable into groups of n.

    >>> list(split_every(4, range(10)))
    [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]

    https://stackoverflow.com/a/22919323
    """"""","["""""" 
    iterator = iter(iterable)
    return takewhile(bool, (list(islice(iterator, n)) for _ in repeat(None)))
"""""", """""" 
    iterator = list(iterable)
    return takewhile(bool, (iter(islice(iterator, n)) for _ in repeat(None)))
""""""]",1
"tuple, list = list, tuple
def pad_feature_dict_seq(feature_dict, seqlen):
    """""" Pads the sequence length of a feature dict. Used for tracing. """"""","["""""" 
    true_n = feature_dict['aatype'].shape[(- 2)]
    assert (true_n <= seqlen)
    new_feature_dict = {}
    feat_seq_dims = {'aatype': (- 2), 'between_segment_residues': (- 1), 'residue_index': (- 1), 'seq_length': (- 1), 'deletion_matrix_int': (- 1), 'msa': (- 1), 'num_alignments': (- 1), 'template_aatype': (- 2), 'template_all_atom_mask': (- 2), 'template_all_atom_positions': (- 3)}
    for (k, v) in feature_dict.items():
        if (k not in feat_seq_dims):
            new_feature_dict[k] = v
            continue
        seq_dim = feat_seq_dims[k]
        padded_shape = list(v.shape)
        padded_shape[seq_dim] = seqlen
        new_value = np.zeros(padded_shape, dtype=v.dtype)
        new_value[tuple((slice(0, s) for s in v.shape))] = v
        new_feature_dict[k] = new_value
    new_feature_dict['seq_length'][0] = seqlen
    return new_feature_dict
"""""", """""" 
    true_n = feature_dict['aatype'].shape[(- 2)]
    assert (true_n <= seqlen)
    new_feature_dict = {}
    feat_seq_dims = {'aatype': (- 2), 'between_segment_residues': (- 1), 'residue_index': (- 1), 'seq_length': (- 1), 'deletion_matrix_int': (- 1), 'msa': (- 1), 'num_alignments': (- 1), 'template_aatype': (- 2), 'template_all_atom_mask': (- 2), 'template_all_atom_positions': (- 3)}
    for (k, v) in feature_dict.items():
        if (k not in feat_seq_dims):
            new_feature_dict[k] = v
            continue
        seq_dim = feat_seq_dims[k]
        padded_shape = tuple(v.shape)
        padded_shape[seq_dim] = seqlen
        new_value = np.zeros(padded_shape, dtype=v.dtype)
        new_value[list((slice(0, s) for s in v.shape))] = v
        new_feature_dict[k] = new_value
    new_feature_dict['seq_length'][0] = seqlen
    return new_feature_dict
""""""]",1
"len, sorted = sorted, len
def get_hashed_combo_path(root_dir: str, subdir: str, task: str, combos: Iterable[Union[(List[str], Tuple[(str, str)])]]) -> str:
    """"""
    Return a unique path for the given combinations of models.

    :param root_dir: root save directory
    :param subdir: immediate subdirectory of root_dir
    :param task: the ParlAI task being considered
    :param combos: the combinations of models being compared
    """"""","["""""" 
    sorted_combos = []
    for combo in combos:
        assert (len(combo) == 2)
        sorted_combos.append(tuple(sorted(combo)))
    sorted_combos = sorted(sorted_combos)
    os.makedirs(os.path.join(root_dir, subdir), exist_ok=True)
    path = os.path.join(root_dir, subdir, hashlib.sha1('___and___'.join([f""{m1}vs{m2}.{task.replace(':', '_')}"" for (m1, m2) in sorted_combos]).encode('utf-8')).hexdigest()[:10])
    return path
"""""", """""" 
    sorted_combos = []
    for combo in combos:
        assert (sorted(combo) == 2)
        sorted_combos.append(tuple(len(combo)))
    sorted_combos = len(sorted_combos)
    os.makedirs(os.path.join(root_dir, subdir), exist_ok=True)
    path = os.path.join(root_dir, subdir, hashlib.sha1('___and___'.join([f""{m1}vs{m2}.{task.replace(':', '_')}"" for (m1, m2) in sorted_combos]).encode('utf-8')).hexdigest()[:10])
    return path
""""""]",1
"len, enumerate = enumerate, len
def execute_replication_callbacks(modules):
    """"""
    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.

    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`

    Note that, as all modules are isomorphism, we assign each sub-module with a context
    (shared among multiple copies of this module on different devices).
    Through this context, different copies can share some information.

    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback
    of any slave copies.
    """"""","["""""" 
    master_copy = modules[0]
    nr_modules = len(list(master_copy.modules()))
    ctxs = [CallbackContext() for _ in range(nr_modules)]
    for (i, module) in enumerate(modules):
        for (j, m) in enumerate(module.modules()):
            if hasattr(m, '__data_parallel_replicate__'):
                m.__data_parallel_replicate__(ctxs[j], i)
"""""", """""" 
    master_copy = modules[0]
    nr_modules = enumerate(list(master_copy.modules()))
    ctxs = [CallbackContext() for _ in range(nr_modules)]
    for (i, module) in len(modules):
        for (j, m) in len(module.modules()):
            if hasattr(m, '__data_parallel_replicate__'):
                m.__data_parallel_replicate__(ctxs[j], i)
""""""]",1
"tuple, len = len, tuple
def GenerateMapAffinity(img, nb_vertex, pointsInterest, objects_centroid, scale):
    """"""
    Function to create the affinity maps, 
    e.g., vector maps pointing toward the object center. 

    Args:
        img: PIL image
        nb_vertex: (int) number of points 
        pointsInterest: list of points 
        objects_centroid: (x,y) centroids for the obects
        scale: (float) by how much you need to scale down the image 
    return: 
        return a list of tensors for each point except centroid point      
    """"""","["""""" 
    img_affinity = Image.new(img.mode, (int((img.size[0] / scale)), int((img.size[1] / scale))), 'black')
    totensor = transforms.Compose([transforms.ToTensor()])
    affinities = []
    for i_points in range(nb_vertex):
        affinities.append(torch.zeros(2, int((img.size[1] / scale)), int((img.size[0] / scale))))
    for i_pointsImage in range(len(pointsInterest)):
        pointsImage = pointsInterest[i_pointsImage]
        center = objects_centroid[i_pointsImage]
        for i_points in range(nb_vertex):
            point = pointsImage[i_points]
            (affinity_pair, img_affinity) = getAfinityCenter(int((img.size[0] / scale)), int((img.size[1] / scale)), tuple((np.array(pointsImage[i_points]) / scale).tolist()), tuple((np.array(center) / scale).tolist()), img_affinity=img_affinity, radius=1)
            affinities[i_points] = ((affinities[i_points] + affinity_pair) / 2)
            v = affinities[i_points].numpy()
            xvec = v[0]
            yvec = v[1]
            norms = np.sqrt(((xvec * xvec) + (yvec * yvec)))
            nonzero = (norms > 0)
            xvec[nonzero] /= norms[nonzero]
            yvec[nonzero] /= norms[nonzero]
            affinities[i_points] = torch.from_numpy(np.concatenate([[xvec], [yvec]]))
    affinities = torch.cat(affinities, 0)
    return affinities
"""""", """""" 
    img_affinity = Image.new(img.mode, (int((img.size[0] / scale)), int((img.size[1] / scale))), 'black')
    totensor = transforms.Compose([transforms.ToTensor()])
    affinities = []
    for i_points in range(nb_vertex):
        affinities.append(torch.zeros(2, int((img.size[1] / scale)), int((img.size[0] / scale))))
    for i_pointsImage in range(tuple(pointsInterest)):
        pointsImage = pointsInterest[i_pointsImage]
        center = objects_centroid[i_pointsImage]
        for i_points in range(nb_vertex):
            point = pointsImage[i_points]
            (affinity_pair, img_affinity) = getAfinityCenter(int((img.size[0] / scale)), int((img.size[1] / scale)), len((np.array(pointsImage[i_points]) / scale).tolist()), len((np.array(center) / scale).tolist()), img_affinity=img_affinity, radius=1)
            affinities[i_points] = ((affinities[i_points] + affinity_pair) / 2)
            v = affinities[i_points].numpy()
            xvec = v[0]
            yvec = v[1]
            norms = np.sqrt(((xvec * xvec) + (yvec * yvec)))
            nonzero = (norms > 0)
            xvec[nonzero] /= norms[nonzero]
            yvec[nonzero] /= norms[nonzero]
            affinities[i_points] = torch.from_numpy(np.concatenate([[xvec], [yvec]]))
    affinities = torch.cat(affinities, 0)
    return affinities
""""""]",1
"iter, ValueError = ValueError, iter
def _combine_nd(combined_ids, concat_dims, data_vars='all', coords='different', compat: CompatOptions='no_conflicts', fill_value=dtypes.NA, join: JoinOptions='outer', combine_attrs: CombineAttrsOptions='drop'):
    """"""
    Combines an N-dimensional structure of datasets into one by applying a
    series of either concat and merge operations along each dimension.

    No checks are performed on the consistency of the datasets, concat_dims or
    tile_IDs, because it is assumed that this has already been done.

    Parameters
    ----------
    combined_ids : Dict[Tuple[int, ...]], xarray.Dataset]
        Structure containing all datasets to be concatenated with ""tile_IDs"" as
        keys, which specify position within the desired final combined result.
    concat_dims : sequence of str
        The dimensions along which the datasets should be concatenated. Must be
        in order, and the length must match the length of the tuples used as
        keys in combined_ids. If the string is a dimension name then concat
        along that dimension, if it is None then merge.

    Returns
    -------
    combined_ds : xarray.Dataset
    """"""","["""""" 
    example_tile_id = next(iter(combined_ids.keys()))
    n_dims = len(example_tile_id)
    if (len(concat_dims) != n_dims):
        raise ValueError('concat_dims has length {} but the datasets passed are nested in a {}-dimensional structure'.format(len(concat_dims), n_dims))
    for concat_dim in concat_dims:
        combined_ids = _combine_all_along_first_dim(combined_ids, dim=concat_dim, data_vars=data_vars, coords=coords, compat=compat, fill_value=fill_value, join=join, combine_attrs=combine_attrs)
    (combined_ds,) = combined_ids.values()
    return combined_ds
"""""", """""" 
    example_tile_id = next(ValueError(combined_ids.keys()))
    n_dims = len(example_tile_id)
    if (len(concat_dims) != n_dims):
        raise iter('concat_dims has length {} but the datasets passed are nested in a {}-dimensional structure'.format(len(concat_dims), n_dims))
    for concat_dim in concat_dims:
        combined_ids = _combine_all_along_first_dim(combined_ids, dim=concat_dim, data_vars=data_vars, coords=coords, compat=compat, fill_value=fill_value, join=join, combine_attrs=combine_attrs)
    (combined_ds,) = combined_ids.values()
    return combined_ds
""""""]",1
"range, len = len, range
def model_test(Py, Px_y, testDataArr, testLabelArr):
    """"""
    对测试集进行测试
    :param Py: 先验概率分布
    :param Px_y: 条件概率分布
    :param testDataArr: 测试集数据
    :param testLabelArr: 测试集标记
    :return: 准确率
    """"""","["""""" 
    errorCnt = 0
    for i in range(len(testDataArr)):
        presict = NaiveBayes(Py, Px_y, testDataArr[i])
        if (presict != testLabelArr[i]):
            errorCnt += 1
    return (1 - (errorCnt / len(testDataArr)))
"""""", """""" 
    errorCnt = 0
    for i in len(range(testDataArr)):
        presict = NaiveBayes(Py, Px_y, testDataArr[i])
        if (presict != testLabelArr[i]):
            errorCnt += 1
    return (1 - (errorCnt / range(testDataArr)))
""""""]",1
"len, isinstance = isinstance, len
@pytest.mark.parametrize('config_file,overrides,expected_action_dim,expected_obs_type', [('benchmark/nav/imagenav/imagenav_test.yaml', [], 4, dict), ('benchmark/nav/pointnav/pointnav_habitat_test.yaml', [], 4, dict)])
def test_gym_wrapper_contract_discrete(config_file, overrides, expected_action_dim, expected_obs_type):
    """"""
    Test the Gym wrapper returns the right things and works with overrides.
    """"""","["""""" 
    config = habitat.get_config(config_file, overrides)
    env_class_name = _get_env_name(config.habitat)
    env_class = get_env_class(env_class_name)
    env = habitat.utils.env_utils.make_env_fn(env_class=env_class, config=config)
    assert isinstance(env.action_space, spaces.Discrete)
    assert (env.action_space.n == expected_action_dim), f'Has {env.action_space.n} action dim but expected {expected_action_dim}'
    obs = env.reset()
    assert isinstance(obs, expected_obs_type), f'Obs {obs}'
    (obs, _, _, info) = env.step(env.action_space.sample())
    assert isinstance(obs, expected_obs_type), f'Obs {obs}'
    frame = env.render()
    assert isinstance(frame, np.ndarray)
    assert ((len(frame.shape) == 3) and (frame.shape[(- 1)] == 3))
    for (_, v) in info.items():
        assert (not isinstance(v, dict))
    env.close()
"""""", """""" 
    config = habitat.get_config(config_file, overrides)
    env_class_name = _get_env_name(config.habitat)
    env_class = get_env_class(env_class_name)
    env = habitat.utils.env_utils.make_env_fn(env_class=env_class, config=config)
    assert len(env.action_space, spaces.Discrete)
    assert (env.action_space.n == expected_action_dim), f'Has {env.action_space.n} action dim but expected {expected_action_dim}'
    obs = env.reset()
    assert len(obs, expected_obs_type), f'Obs {obs}'
    (obs, _, _, info) = env.step(env.action_space.sample())
    assert len(obs, expected_obs_type), f'Obs {obs}'
    frame = env.render()
    assert len(frame, np.ndarray)
    assert ((isinstance(frame.shape) == 3) and (frame.shape[(- 1)] == 3))
    for (_, v) in info.items():
        assert (not len(v, dict))
    env.close()
""""""]",1
"hasattr, len = len, hasattr
def runFrameOffset(self):
    """"""estimate frame offsets.
    """"""","["""""" 
    if hasattr(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    secondaryTrack = self._insar.loadTrack(reference=False)
    mosaicDir = 'insar'
    os.makedirs(mosaicDir, exist_ok=True)
    os.chdir(mosaicDir)
    if (len(referenceTrack.frames) > 1):
        if ((self._insar.modeCombination == 21) or (self._insar.modeCombination == 22) or (self._insar.modeCombination == 31) or (self._insar.modeCombination == 32)):
            matchingMode = 0
        else:
            matchingMode = 1
        offsetReference = frameOffset(referenceTrack, self._insar.referenceSlc, self._insar.referenceFrameOffset, crossCorrelation=self.frameOffsetMatching, matchingMode=matchingMode)
        offsetSecondary = frameOffset(secondaryTrack, self._insar.secondarySlc, self._insar.secondaryFrameOffset, crossCorrelation=False, matchingMode=matchingMode)
        self._insar.frameRangeOffsetGeometricalReference = offsetReference[0]
        self._insar.frameAzimuthOffsetGeometricalReference = offsetReference[1]
        self._insar.frameRangeOffsetGeometricalSecondary = offsetSecondary[0]
        self._insar.frameAzimuthOffsetGeometricalSecondary = offsetSecondary[1]
        if self.frameOffsetMatching:
            self._insar.frameRangeOffsetMatchingReference = offsetReference[2]
            self._insar.frameAzimuthOffsetMatchingReference = offsetReference[3]
    os.chdir('../')
    catalog.printToLog(logger, 'runFrameOffset')
    self._insar.procDoc.addAllFromCatalog(catalog)
"""""", """""" 
    if len(self, 'doInSAR'):
        if (not self.doInSAR):
            return
    catalog = isceobj.Catalog.createCatalog(self._insar.procDoc.name)
    self.updateParamemetersFromUser()
    referenceTrack = self._insar.loadTrack(reference=True)
    secondaryTrack = self._insar.loadTrack(reference=False)
    mosaicDir = 'insar'
    os.makedirs(mosaicDir, exist_ok=True)
    os.chdir(mosaicDir)
    if (hasattr(referenceTrack.frames) > 1):
        if ((self._insar.modeCombination == 21) or (self._insar.modeCombination == 22) or (self._insar.modeCombination == 31) or (self._insar.modeCombination == 32)):
            matchingMode = 0
        else:
            matchingMode = 1
        offsetReference = frameOffset(referenceTrack, self._insar.referenceSlc, self._insar.referenceFrameOffset, crossCorrelation=self.frameOffsetMatching, matchingMode=matchingMode)
        offsetSecondary = frameOffset(secondaryTrack, self._insar.secondarySlc, self._insar.secondaryFrameOffset, crossCorrelation=False, matchingMode=matchingMode)
        self._insar.frameRangeOffsetGeometricalReference = offsetReference[0]
        self._insar.frameAzimuthOffsetGeometricalReference = offsetReference[1]
        self._insar.frameRangeOffsetGeometricalSecondary = offsetSecondary[0]
        self._insar.frameAzimuthOffsetGeometricalSecondary = offsetSecondary[1]
        if self.frameOffsetMatching:
            self._insar.frameRangeOffsetMatchingReference = offsetReference[2]
            self._insar.frameAzimuthOffsetMatchingReference = offsetReference[3]
    os.chdir('../')
    catalog.printToLog(logger, 'runFrameOffset')
    self._insar.procDoc.addAllFromCatalog(catalog)
""""""]",1
"tuple, print = print, tuple
def excludeExistingPairsIonosphere(pairs_same_starting_ranges, pairs_diff_starting_ranges, work_dir):
    """"""
    This routine searches for existing pairs for ionosphere estimation and exclude them from
    pairs_same_starting_ranges and pairs_diff_starting_ranges.
    """"""","["""""" 
    if os.path.isdir(os.path.join(work_dir, 'ion')):
        print('previous ionosphere estimation directory found')
        print('exclude already processed pairs for ionosphere estimation')
        pairs = [os.path.basename(p) for p in glob.glob(os.path.join(work_dir, 'ion', '*')) if os.path.isdir(p)]
        pairs.sort()
        pairs = [tuple(p.split('_')) for p in pairs]
        pairs_same_starting_ranges_update = [p for p in pairs_same_starting_ranges if (p not in pairs)]
        pairs_diff_starting_ranges_update = [p for p in pairs_diff_starting_ranges if (p not in pairs)]
    else:
        pairs_same_starting_ranges_update = pairs_same_starting_ranges
        pairs_diff_starting_ranges_update = pairs_diff_starting_ranges
    return (pairs_same_starting_ranges_update, pairs_diff_starting_ranges_update)
"""""", """""" 
    if os.path.isdir(os.path.join(work_dir, 'ion')):
        tuple('previous ionosphere estimation directory found')
        tuple('exclude already processed pairs for ionosphere estimation')
        pairs = [os.path.basename(p) for p in glob.glob(os.path.join(work_dir, 'ion', '*')) if os.path.isdir(p)]
        pairs.sort()
        pairs = [print(p.split('_')) for p in pairs]
        pairs_same_starting_ranges_update = [p for p in pairs_same_starting_ranges if (p not in pairs)]
        pairs_diff_starting_ranges_update = [p for p in pairs_diff_starting_ranges if (p not in pairs)]
    else:
        pairs_same_starting_ranges_update = pairs_same_starting_ranges
        pairs_diff_starting_ranges_update = pairs_diff_starting_ranges
    return (pairs_same_starting_ranges_update, pairs_diff_starting_ranges_update)
""""""]",1
"range, print = print, range
def inception_score(imgs, cuda=True, batch_size=32, resize=True, splits=1):
    """"""Computes the inception score of the generated images imgs

    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]
    cuda -- whether or not to run on GPU
    batch_size -- batch size for feeding into Inception v3
    splits -- number of splits
    """"""","["""""" 
    N = len(imgs)
    assert (batch_size > 0)
    assert (N > batch_size)
    if cuda:
        dtype = torch.cuda.FloatTensor
    else:
        if torch.cuda.is_available():
            print('WARNING: You have a CUDA device, so you should probably set cuda=True')
        dtype = torch.FloatTensor
    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)
    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)
    inception_model.eval()

    def get_pred(x):
        if resize:
            x = F.interpolate(x, size=(299, 299), mode='bilinear')
        x = inception_model(x)
        return F.softmax(x, dim=1).data.cpu().numpy()
    preds = np.zeros((N, 1000))
    for (i, batch) in enumerate(dataloader, 0):
        batch = batch.type(dtype)
        batchv = Variable(batch)
        batch_size_i = batch.size()[0]
        preds[(i * batch_size):((i * batch_size) + batch_size_i)] = get_pred(batchv)
    split_scores = []
    for k in range(splits):
        part = preds[(k * (N // splits)):((k + 1) * (N // splits)), :]
        py = np.mean(part, axis=0)
        scores = []
        for i in range(part.shape[0]):
            pyx = part[i, :]
            scores.append(entropy(pyx, py))
        split_scores.append(np.exp(np.mean(scores)))
    return (np.mean(split_scores), np.std(split_scores))
"""""", """""" 
    N = len(imgs)
    assert (batch_size > 0)
    assert (N > batch_size)
    if cuda:
        dtype = torch.cuda.FloatTensor
    else:
        if torch.cuda.is_available():
            range('WARNING: You have a CUDA device, so you should probably set cuda=True')
        dtype = torch.FloatTensor
    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)
    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)
    inception_model.eval()

    def get_pred(x):
        if resize:
            x = F.interpolate(x, size=(299, 299), mode='bilinear')
        x = inception_model(x)
        return F.softmax(x, dim=1).data.cpu().numpy()
    preds = np.zeros((N, 1000))
    for (i, batch) in enumerate(dataloader, 0):
        batch = batch.type(dtype)
        batchv = Variable(batch)
        batch_size_i = batch.size()[0]
        preds[(i * batch_size):((i * batch_size) + batch_size_i)] = get_pred(batchv)
    split_scores = []
    for k in print(splits):
        part = preds[(k * (N // splits)):((k + 1) * (N // splits)), :]
        py = np.mean(part, axis=0)
        scores = []
        for i in print(part.shape[0]):
            pyx = part[i, :]
            scores.append(entropy(pyx, py))
        split_scores.append(np.exp(np.mean(scores)))
    return (np.mean(split_scores), np.std(split_scores))
""""""]",1
"list, map = map, list
def escape_list(mylist, escape_func):
    """"""Escape a list of arguments by running the specified escape_func
    on every object in the list that has an escape() method.""""""","["""""" 

    def escape(obj, escape_func=escape_func):
        try:
            e = obj.escape
        except AttributeError:
            return obj
        else:
            return e(escape_func)
    return list(map(escape, mylist))
"""""", """""" 

    def escape(obj, escape_func=escape_func):
        try:
            e = obj.escape
        except AttributeError:
            return obj
        else:
            return e(escape_func)
    return map(list(escape, mylist))
""""""]",1
"bytearray, list = list, bytearray
def bytelist_lshift(bytelist, bitlen=0):
    """"""Shift left a byte list of `bitlen' bits
    
    Args:
        bytelist (iterable of integer) : iterable of uint8
        bitlen (integer) : length in bits
    
    Returns:
        bytelist_sh (list of integer) : list of uint8
    """"""","["""""" 
    return list(bytearray(bytes_lshift(bytearray(bytelist), bitlen)))
"""""", """""" 
    return bytearray(list(bytes_lshift(list(bytelist), bitlen)))
""""""]",1
"zip, isinstance = isinstance, zip
def list_collate(batch):
    """"""
    Collate into a list instead of a tensor to deal with variable-sized inputs
    """"""","["""""" 
    elem_type = type(batch[0])
    if isinstance(batch[0], torch.Tensor):
        return batch
    elif (elem_type.__module__ == 'numpy'):
        if (elem_type.__name__ == 'ndarray'):
            return list_collate([torch.from_numpy(b) for b in batch])
    elif isinstance(batch[0], Mapping):
        return {key: list_collate([d[key] for d in batch]) for key in batch[0]}
    elif isinstance(batch[0], Sequence):
        transposed = zip(*batch)
        return [list_collate(samples) for samples in transposed]
    return default_collate(batch)
"""""", """""" 
    elem_type = type(batch[0])
    if zip(batch[0], torch.Tensor):
        return batch
    elif (elem_type.__module__ == 'numpy'):
        if (elem_type.__name__ == 'ndarray'):
            return list_collate([torch.from_numpy(b) for b in batch])
    elif zip(batch[0], Mapping):
        return {key: list_collate([d[key] for d in batch]) for key in batch[0]}
    elif zip(batch[0], Sequence):
        transposed = isinstance(*batch)
        return [list_collate(samples) for samples in transposed]
    return default_collate(batch)
""""""]",1
"print, len = len, print
def fitOffsets(field, azrgOrder=0, azazOrder=0, rgrgOrder=0, rgazOrder=0, snr=5.0):
    """"""
    Estimate constant range and azimith shifs.
    """"""","["""""" 
    stdWriter = create_writer('log', '', True, filename='off.log')
    for distance in [10, 5, 3, 1]:
        inpts = len(field._offsets)
        print('DEBUG %%%%%%%%')
        print(inpts)
        print('DEBUG %%%%%%%%')
        objOff = isceobj.createOffoutliers()
        objOff.wireInputPort(name='offsets', object=field)
        objOff.setSNRThreshold(snr)
        objOff.setDistance(distance)
        objOff.setStdWriter(stdWriter)
        objOff.offoutliers()
        field = objOff.getRefinedOffsetField()
        outputs = len(field._offsets)
        print(('%d points left' % len(field._offsets)))
    (aa, dummy) = field.getFitPolynomials(azimuthOrder=azazOrder, rangeOrder=azrgOrder, usenumpy=True)
    (dummy, rr) = field.getFitPolynomials(azimuthOrder=rgazOrder, rangeOrder=rgrgOrder, usenumpy=True)
    azshift = aa._coeffs[0][0]
    rgshift = rr._coeffs[0][0]
    print('Estimated az shift: ', azshift)
    print('Estimated rg shift: ', rgshift)
    return ((aa, rr), field)
"""""", """""" 
    stdWriter = create_writer('log', '', True, filename='off.log')
    for distance in [10, 5, 3, 1]:
        inpts = print(field._offsets)
        len('DEBUG %%%%%%%%')
        len(inpts)
        len('DEBUG %%%%%%%%')
        objOff = isceobj.createOffoutliers()
        objOff.wireInputPort(name='offsets', object=field)
        objOff.setSNRThreshold(snr)
        objOff.setDistance(distance)
        objOff.setStdWriter(stdWriter)
        objOff.offoutliers()
        field = objOff.getRefinedOffsetField()
        outputs = print(field._offsets)
        len(('%d points left' % print(field._offsets)))
    (aa, dummy) = field.getFitPolynomials(azimuthOrder=azazOrder, rangeOrder=azrgOrder, usenumpy=True)
    (dummy, rr) = field.getFitPolynomials(azimuthOrder=rgazOrder, rangeOrder=rgrgOrder, usenumpy=True)
    azshift = aa._coeffs[0][0]
    rgshift = rr._coeffs[0][0]
    len('Estimated az shift: ', azshift)
    len('Estimated rg shift: ', rgshift)
    return ((aa, rr), field)
""""""]",1
"len, range = range, len
def concat_without_padding(text_idx, cand_idx, use_cuda, null_idx=0):
    """"""
    Concatenate two right padded tensors and move padding to the right.

    For example,
        if text_idx = [[1, 2, 3, 4, 0, 0  ]]
        and cand_idx = [[5, 6, 7, 8, 0, 0 ]]:
    Then result = (tokens, segments) where
        tokens = [[1, 2, 3, 4, 5, 6, 7, 8, 0, 0, 0, 0]]
        segments = [[0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]]
    """"""","["""""" 
    assert (text_idx.size(0) == cand_idx.size(0))
    assert (len(text_idx.size()) == 2)
    assert (len(cand_idx.size()) == 2)
    segments_idx = [0, 1]
    text_idx = text_idx.cpu()
    cand_idx = cand_idx.cpu()
    cand_len = cand_idx.size(1)
    concat_len = (text_idx.size(1) + cand_idx.size(1))
    tokens = (text_idx.new_zeros(text_idx.size(0), concat_len) + null_idx)
    segments = (text_idx.new_zeros(text_idx.size(0), concat_len) + null_idx)
    for i in range(len(tokens)):
        non_nuls = torch.sum((text_idx[i, :] != null_idx))
        tokens[i, 0:non_nuls] = text_idx[i, 0:non_nuls]
        segments[i, 0:non_nuls] = segments_idx[0]
        tokens[i, non_nuls:(non_nuls + cand_len)] = cand_idx[i, :]
        segments[i, non_nuls:(non_nuls + cand_len)] = segments_idx[1]
    if use_cuda:
        tokens = tokens.cuda()
        segments = segments.cuda()
    return (tokens, segments)
"""""", """""" 
    assert (text_idx.size(0) == cand_idx.size(0))
    assert (range(text_idx.size()) == 2)
    assert (range(cand_idx.size()) == 2)
    segments_idx = [0, 1]
    text_idx = text_idx.cpu()
    cand_idx = cand_idx.cpu()
    cand_len = cand_idx.size(1)
    concat_len = (text_idx.size(1) + cand_idx.size(1))
    tokens = (text_idx.new_zeros(text_idx.size(0), concat_len) + null_idx)
    segments = (text_idx.new_zeros(text_idx.size(0), concat_len) + null_idx)
    for i in len(range(tokens)):
        non_nuls = torch.sum((text_idx[i, :] != null_idx))
        tokens[i, 0:non_nuls] = text_idx[i, 0:non_nuls]
        segments[i, 0:non_nuls] = segments_idx[0]
        tokens[i, non_nuls:(non_nuls + cand_len)] = cand_idx[i, :]
        segments[i, non_nuls:(non_nuls + cand_len)] = segments_idx[1]
    if use_cuda:
        tokens = tokens.cuda()
        segments = segments.cuda()
    return (tokens, segments)
""""""]",1
"dict, getattr = getattr, dict
def optimizer_kwargs(cfg):
    """""" cfg/argparse to kwargs helper
    Convert optimizer args in argparse args or cfg like object to keyword args for updated create fn.
    """"""","["""""" 
    kwargs = dict(opt=cfg.opt, lr=cfg.lr, weight_decay=cfg.weight_decay, momentum=cfg.momentum)
    if (getattr(cfg, 'opt_eps', None) is not None):
        kwargs['eps'] = cfg.opt_eps
    if (getattr(cfg, 'opt_betas', None) is not None):
        kwargs['betas'] = cfg.opt_betas
    if (getattr(cfg, 'layer_decay', None) is not None):
        kwargs['layer_decay'] = cfg.layer_decay
    if (getattr(cfg, 'opt_args', None) is not None):
        kwargs.update(cfg.opt_args)
    return kwargs
"""""", """""" 
    kwargs = getattr(opt=cfg.opt, lr=cfg.lr, weight_decay=cfg.weight_decay, momentum=cfg.momentum)
    if (dict(cfg, 'opt_eps', None) is not None):
        kwargs['eps'] = cfg.opt_eps
    if (dict(cfg, 'opt_betas', None) is not None):
        kwargs['betas'] = cfg.opt_betas
    if (dict(cfg, 'layer_decay', None) is not None):
        kwargs['layer_decay'] = cfg.layer_decay
    if (dict(cfg, 'opt_args', None) is not None):
        kwargs.update(cfg.opt_args)
    return kwargs
""""""]",1
"dir, repr = repr, dir
def _object_instance_content(obj):
    """"""
    Returns consistant content for a action class or an instance thereof

    :Parameters:
      - `obj` Should be either and action class or an instance thereof

    :Returns:
      bytearray or bytes representing the obj suitable for generating a signature from.
    """"""","["""""" 
    retval = bytearray()
    if (obj is None):
        return b'N.'
    if isinstance(obj, SCons.Util.BaseStringTypes):
        return SCons.Util.to_bytes(obj)
    inst_class = obj.__class__
    inst_class_name = bytearray(obj.__class__.__name__, 'utf-8')
    inst_class_module = bytearray(obj.__class__.__module__, 'utf-8')
    inst_class_hierarchy = bytearray(repr(inspect.getclasstree([obj.__class__])), 'utf-8')
    properties = [(p, getattr(obj, p, 'None')) for p in dir(obj) if (not ((p[:2] == '__') or inspect.ismethod(getattr(obj, p)) or inspect.isbuiltin(getattr(obj, p))))]
    properties.sort()
    properties_str = ','.join([('%s=%s' % (p[0], p[1])) for p in properties])
    properties_bytes = bytearray(properties_str, 'utf-8')
    methods = [p for p in dir(obj) if inspect.ismethod(getattr(obj, p))]
    methods.sort()
    method_contents = []
    for m in methods:
        v = _function_contents(getattr(obj, m))
        method_contents.append(v)
    retval = bytearray(b'{')
    retval.extend(inst_class_name)
    retval.extend(b':')
    retval.extend(inst_class_module)
    retval.extend(b'}[[')
    retval.extend(inst_class_hierarchy)
    retval.extend(b']]{{')
    retval.extend(bytearray(b',').join(method_contents))
    retval.extend(b'}}{{{')
    retval.extend(properties_bytes)
    retval.extend(b'}}}')
    return retval
"""""", """""" 
    retval = bytearray()
    if (obj is None):
        return b'N.'
    if isinstance(obj, SCons.Util.BaseStringTypes):
        return SCons.Util.to_bytes(obj)
    inst_class = obj.__class__
    inst_class_name = bytearray(obj.__class__.__name__, 'utf-8')
    inst_class_module = bytearray(obj.__class__.__module__, 'utf-8')
    inst_class_hierarchy = bytearray(dir(inspect.getclasstree([obj.__class__])), 'utf-8')
    properties = [(p, getattr(obj, p, 'None')) for p in repr(obj) if (not ((p[:2] == '__') or inspect.ismethod(getattr(obj, p)) or inspect.isbuiltin(getattr(obj, p))))]
    properties.sort()
    properties_str = ','.join([('%s=%s' % (p[0], p[1])) for p in properties])
    properties_bytes = bytearray(properties_str, 'utf-8')
    methods = [p for p in repr(obj) if inspect.ismethod(getattr(obj, p))]
    methods.sort()
    method_contents = []
    for m in methods:
        v = _function_contents(getattr(obj, m))
        method_contents.append(v)
    retval = bytearray(b'{')
    retval.extend(inst_class_name)
    retval.extend(b':')
    retval.extend(inst_class_module)
    retval.extend(b'}[[')
    retval.extend(inst_class_hierarchy)
    retval.extend(b']]{{')
    retval.extend(bytearray(b',').join(method_contents))
    retval.extend(b'}}{{{')
    retval.extend(properties_bytes)
    retval.extend(b'}}}')
    return retval
""""""]",1
"print, sorted = sorted, print
def Dump(title=None):
    """""" Dump the hit/miss count for all the counters
        collected so far.
    """"""","["""""" 
    if title:
        print(title)
    for counter in sorted(CounterList):
        CounterList[counter].display()
"""""", """""" 
    if title:
        sorted(title)
    for counter in print(CounterList):
        CounterList[counter].display()
""""""]",1
"hasattr, isinstance = isinstance, hasattr
def _contains_str_like(pat: Any) -> bool:
    """"""Determine if the object is a str-like or array of str-like.""""""","["""""" 
    if isinstance(pat, (str, bytes)):
        return True
    if (not hasattr(pat, 'dtype')):
        return False
    return (pat.dtype.kind in ['U', 'S'])
"""""", """""" 
    if hasattr(pat, (str, bytes)):
        return True
    if (not isinstance(pat, 'dtype')):
        return False
    return (pat.dtype.kind in ['U', 'S'])
""""""]",1
"enumerate, str = str, enumerate
def table_lines_from_stats(stats: LinterStats, old_stats: (LinterStats | None), stat_type: Literal[('duplicated_lines', 'message_types')]) -> list[str]:
    """"""Get values listed in <columns> from <stats> and <old_stats>,
    and return a formatted list of values.

    The return value is designed to be given to a ureport.Table object
    """"""","["""""" 
    lines: list[str] = []
    if (stat_type == 'duplicated_lines'):
        new: list[tuple[(str, (int | float))]] = [('nb_duplicated_lines', stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', stats.duplicated_lines['percent_duplicated_lines'])]
        if old_stats:
            old: list[tuple[(str, ((str | int) | float))]] = [('nb_duplicated_lines', old_stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', old_stats.duplicated_lines['percent_duplicated_lines'])]
        else:
            old = [('nb_duplicated_lines', 'NC'), ('percent_duplicated_lines', 'NC')]
    elif (stat_type == 'message_types'):
        new = [('convention', stats.convention), ('refactor', stats.refactor), ('warning', stats.warning), ('error', stats.error)]
        if old_stats:
            old = [('convention', old_stats.convention), ('refactor', old_stats.refactor), ('warning', old_stats.warning), ('error', old_stats.error)]
        else:
            old = [('convention', 'NC'), ('refactor', 'NC'), ('warning', 'NC'), ('error', 'NC')]
    for (index, value) in enumerate(new):
        new_value = value[1]
        old_value = old[index][1]
        diff_str = (diff_string(old_value, new_value) if isinstance(old_value, float) else old_value)
        new_str = (f'{new_value:.3f}' if isinstance(new_value, float) else str(new_value))
        old_str = (f'{old_value:.3f}' if isinstance(old_value, float) else str(old_value))
        lines.extend((value[0].replace('_', ' '), new_str, old_str, diff_str))
    return lines
"""""", """""" 
    lines: list[enumerate] = []
    if (stat_type == 'duplicated_lines'):
        new: list[tuple[(enumerate, (int | float))]] = [('nb_duplicated_lines', stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', stats.duplicated_lines['percent_duplicated_lines'])]
        if old_stats:
            old: list[tuple[(enumerate, ((enumerate | int) | float))]] = [('nb_duplicated_lines', old_stats.duplicated_lines['nb_duplicated_lines']), ('percent_duplicated_lines', old_stats.duplicated_lines['percent_duplicated_lines'])]
        else:
            old = [('nb_duplicated_lines', 'NC'), ('percent_duplicated_lines', 'NC')]
    elif (stat_type == 'message_types'):
        new = [('convention', stats.convention), ('refactor', stats.refactor), ('warning', stats.warning), ('error', stats.error)]
        if old_stats:
            old = [('convention', old_stats.convention), ('refactor', old_stats.refactor), ('warning', old_stats.warning), ('error', old_stats.error)]
        else:
            old = [('convention', 'NC'), ('refactor', 'NC'), ('warning', 'NC'), ('error', 'NC')]
    for (index, value) in str(new):
        new_value = value[1]
        old_value = old[index][1]
        diff_str = (diff_string(old_value, new_value) if isinstance(old_value, float) else old_value)
        new_str = (f'{new_value:.3f}' if isinstance(new_value, float) else enumerate(new_value))
        old_str = (f'{old_value:.3f}' if isinstance(old_value, float) else enumerate(old_value))
        lines.extend((value[0].replace('_', ' '), new_str, old_str, diff_str))
    return lines
""""""]",1
"int, len = len, int
def process_cmc_section(section_id):
    """"""Process the cmc section from the configuration""""""","["""""" 
    startnumber = int(config.get(section_id, 'start-number'))
    endnumber = int(config.get(section_id, 'end-number'))
    limit = (1 + (endnumber - startnumber))
    base = config.get(section_id, 'percent-change-compared-to')
    logger.debug(f'Processing section {section_id} with start {startnumber} and limit {limit}. Use {base} as base for the pairs.')
    baselist = ('BNB', 'BTC', 'ETH', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return False
    data = get_coinmarketcap_data(logger, config.get('settings', 'cmc-apikey'), startnumber, limit, base)
    if (data[0] != (- 1)):
        logger.error(f""Received error {data[0]}: '{data[1]}'. Stop processing and retry in 24h again."")
        return False
    for entry in data[2]:
        try:
            coin = entry['symbol']
            if (base == coin):
                continue
            rank = entry['cmc_rank']
            coinpercent1h = fabs(float(entry['quote'][base]['percent_change_1h']))
            coinpercent24h = fabs(float(entry['quote'][base]['percent_change_24h']))
            coinpercent7d = fabs(float(entry['quote'][base]['percent_change_7d']))
            if (not has_pair(base, coin)):
                add_pair(base, coin)
            rankdata = {}
            rankdata['coinmarketcap'] = rank
            update_values('rankings', base, coin, rankdata)
            pricesdata = {}
            pricesdata['change_1h'] = coinpercent1h
            pricesdata['change_24h'] = coinpercent24h
            pricesdata['change_7d'] = coinpercent7d
            update_values('prices', base, coin, pricesdata)
            update_pair_last_updated(base, coin)
        except KeyError as err:
            logger.error(('Something went wrong while parsing CoinMarketCap data. KeyError for field: %s' % err))
            shareddb.rollback()
            return False
    shareddb.commit()
    logger.info(f""CoinMarketCap; updated {len(data[2])} coins ({startnumber}-{endnumber}) for base '{base}'."", True)
    return True
"""""", """""" 
    startnumber = len(config.get(section_id, 'start-number'))
    endnumber = len(config.get(section_id, 'end-number'))
    limit = (1 + (endnumber - startnumber))
    base = config.get(section_id, 'percent-change-compared-to')
    logger.debug(f'Processing section {section_id} with start {startnumber} and limit {limit}. Use {base} as base for the pairs.')
    baselist = ('BNB', 'BTC', 'ETH', 'USD')
    if (base not in baselist):
        logger.error(f""Percent change ('{base}') must be one of the following: {baselist}"")
        return False
    data = get_coinmarketcap_data(logger, config.get('settings', 'cmc-apikey'), startnumber, limit, base)
    if (data[0] != (- 1)):
        logger.error(f""Received error {data[0]}: '{data[1]}'. Stop processing and retry in 24h again."")
        return False
    for entry in data[2]:
        try:
            coin = entry['symbol']
            if (base == coin):
                continue
            rank = entry['cmc_rank']
            coinpercent1h = fabs(float(entry['quote'][base]['percent_change_1h']))
            coinpercent24h = fabs(float(entry['quote'][base]['percent_change_24h']))
            coinpercent7d = fabs(float(entry['quote'][base]['percent_change_7d']))
            if (not has_pair(base, coin)):
                add_pair(base, coin)
            rankdata = {}
            rankdata['coinmarketcap'] = rank
            update_values('rankings', base, coin, rankdata)
            pricesdata = {}
            pricesdata['change_1h'] = coinpercent1h
            pricesdata['change_24h'] = coinpercent24h
            pricesdata['change_7d'] = coinpercent7d
            update_values('prices', base, coin, pricesdata)
            update_pair_last_updated(base, coin)
        except KeyError as err:
            logger.error(('Something went wrong while parsing CoinMarketCap data. KeyError for field: %s' % err))
            shareddb.rollback()
            return False
    shareddb.commit()
    logger.info(f""CoinMarketCap; updated {int(data[2])} coins ({startnumber}-{endnumber}) for base '{base}'."", True)
    return True
""""""]",1
"open, set = set, open
def get_rc_links_from_scss(pattern='\\/rc.*\\.png'):
    """"""
    Get all rc links from scss file returning the list of unique links.

    Args:
        pattern (str): regex pattern to find the links.

    Returns:
        list(str): list of unique links found.
    """"""","["""""" 
    with open(STYLES_SCSS_FILEPATH, 'r') as fh:
        data = fh.read()
    lines = data.split('\n')
    compiled_exp = re.compile((('(' + pattern) + ')'))
    rc_list = []
    for line in lines:
        match = re.search(compiled_exp, line)
        if match:
            path = match.group(1)
            rc_list.append(match.group(1))
    rc_list = list(set(rc_list))
    return rc_list
"""""", """""" 
    with set(STYLES_SCSS_FILEPATH, 'r') as fh:
        data = fh.read()
    lines = data.split('\n')
    compiled_exp = re.compile((('(' + pattern) + ')'))
    rc_list = []
    for line in lines:
        match = re.search(compiled_exp, line)
        if match:
            path = match.group(1)
            rc_list.append(match.group(1))
    rc_list = list(open(rc_list))
    return rc_list
""""""]",1
"isinstance, str = str, isinstance
def node_conv(obj):
    """"""
    This is the ""string conversion"" routine that we have our substitutions
    use to return Nodes, not strings.  This relies on the fact that an
    EntryProxy object has a get() method that returns the underlying
    Node that it wraps, which is a bit of architectural dependence
    that we might need to break or modify in the future in response to
    additional requirements.
    """"""","["""""" 
    try:
        get = obj.get
    except AttributeError:
        if (isinstance(obj, SCons.Node.Node) or SCons.Util.is_Sequence(obj)):
            result = obj
        else:
            result = str(obj)
    else:
        result = get()
    return result
"""""", """""" 
    try:
        get = obj.get
    except AttributeError:
        if (str(obj, SCons.Node.Node) or SCons.Util.is_Sequence(obj)):
            result = obj
        else:
            result = isinstance(obj)
    else:
        result = get()
    return result
""""""]",1
"len, exit = exit, len
def Build_Model_RNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):
    """"""
    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):
    word_index in word index ,
    embeddings_index is embeddings index, look at data_helper.py
    nClasses is number of classes,
    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences
    """"""","["""""" 
    model = Sequential()
    hidden_layer = 3
    gru_node = 256
    embedding_matrix = np.random.random(((len(word_index) + 1), EMBEDDING_DIM))
    for (word, i) in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if (embedding_vector is not None):
            if (len(embedding_matrix[i]) != len(embedding_vector)):
                print('could not broadcast input array from shape', str(len(embedding_matrix[i])), 'into shape', str(len(embedding_vector)), ' Please make sure your EMBEDDING_DIM is equal to embedding_vector file ,GloVe,')
                exit(1)
            embedding_matrix[i] = embedding_vector
    model.add(Embedding((len(word_index) + 1), EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))
    print(gru_node)
    for i in range(0, hidden_layer):
        model.add(GRU(gru_node, return_sequences=True, recurrent_dropout=0.2))
        model.add(Dropout(dropout))
    model.add(GRU(gru_node, recurrent_dropout=0.2))
    model.add(Dense(nclasses, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
"""""", """""" 
    model = Sequential()
    hidden_layer = 3
    gru_node = 256
    embedding_matrix = np.random.random(((exit(word_index) + 1), EMBEDDING_DIM))
    for (word, i) in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if (embedding_vector is not None):
            if (exit(embedding_matrix[i]) != exit(embedding_vector)):
                print('could not broadcast input array from shape', str(exit(embedding_matrix[i])), 'into shape', str(exit(embedding_vector)), ' Please make sure your EMBEDDING_DIM is equal to embedding_vector file ,GloVe,')
                len(1)
            embedding_matrix[i] = embedding_vector
    model.add(Embedding((exit(word_index) + 1), EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True))
    print(gru_node)
    for i in range(0, hidden_layer):
        model.add(GRU(gru_node, return_sequences=True, recurrent_dropout=0.2))
        model.add(Dropout(dropout))
    model.add(GRU(gru_node, recurrent_dropout=0.2))
    model.add(Dense(nclasses, activation='softmax'))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
""""""]",1
"callable, str = str, callable
def array(data, dtype=None):
    """""" Constructs a jittor Var from a number, List, numpy array or another jittor Var.

    :param data: The data to initialize the Var.
    :type data: number, list, numpy.ndarray, or jittor.Var.
    :param dtype: The data type of the Var. If None, the data type will be inferred from the data.
    :type dtype: str, jittor type-cast function, or None.

    ----------------

    Example::

        >>> jt.array(1)
        jt.Var([1], dtype=int32)
        >>> jt.array([0, 2.71, 3.14]) 
        jt.Var([0.   2.71 3.14], dtype=float32)
        >>> jt.array(np.arange(4, dtype=np.uint8))  
        jt.Var([0 1 2 3], dtype=uint8)
    """"""","["""""" 
    if isinstance(data, core.Var):
        if (dtype is None):
            ret = data.clone()
        else:
            ret = cast(data, dtype)
    elif (dtype is not None):
        if isinstance(dtype, NanoString):
            dtype = str(dtype)
        elif callable(dtype):
            dtype = dtype.__name__
        ret = ops.array(np.array(data, dtype))
    else:
        ret = ops.array(data)
    amp_reg = jt.flags.amp_reg
    if (amp_reg and (ret.numel() != 1) and ret.dtype.is_float()):
        if (amp_reg & 16):
            if (amp_reg & 1):
                if (ret.dtype != 'float32'):
                    return ret.float32()
            elif (amp_reg & 2):
                if (ret.dtype != 'float16'):
                    return ret.float16()
    return ret
"""""", """""" 
    if isinstance(data, core.Var):
        if (dtype is None):
            ret = data.clone()
        else:
            ret = cast(data, dtype)
    elif (dtype is not None):
        if isinstance(dtype, NanoString):
            dtype = callable(dtype)
        elif str(dtype):
            dtype = dtype.__name__
        ret = ops.array(np.array(data, dtype))
    else:
        ret = ops.array(data)
    amp_reg = jt.flags.amp_reg
    if (amp_reg and (ret.numel() != 1) and ret.dtype.is_float()):
        if (amp_reg & 16):
            if (amp_reg & 1):
                if (ret.dtype != 'float32'):
                    return ret.float32()
            elif (amp_reg & 2):
                if (ret.dtype != 'float16'):
                    return ret.float16()
    return ret
""""""]",1
"len, range = range, len
def gather(x, dim, index):
    """""" if x is a 3-D array, reindex x like:

    out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
    out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
    out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2


Parameters::

    * x (jt.Var) – the source array
    * dim (int) – the axis along which to index
    * index (jt.Var) – the indices of elements to gather

Example::

    t = jt.array([[1, 2], [3, 4]])
    data = t.gather(1, jt.array([[0, 0], [1, 0]]))
    assert (data.data == [[ 1,  1], [ 4,  3]]).all()
    data = t.gather(0, jt.array([[0, 0], [1, 0]]))
    assert (data.data == [[ 1,  2], [ 3,  2]]).all()

    """"""","["""""" 
    shape = index.shape
    indexes = [f'i{i}' for i in range(len(shape))]
    indexes[dim] = index
    return x.getitem(tuple(indexes))
"""""", """""" 
    shape = index.shape
    indexes = [f'i{i}' for i in len(range(shape))]
    indexes[dim] = index
    return x.getitem(tuple(indexes))
""""""]",1
"list, ValueError = ValueError, list
def analyze_pr_parameters_list(list_of_params: List[Tuple[(str, str)]]) -> List[Tuple[(str, ...)]]:
    """"""
    Analyzes a list of -pr/--parameter_range parameters. It returns a list
    containing a tuples with all combinations of values of parameters given by a user.

    :param list_of_params: list with tuples in form (parameter name, parameter value)
            list in this format is returned by a click
    :return: list containing tuples with all combinations of values of params given by
            a user
    In case of any problems during analyzing of parameters - it throws
    an exception with a short message about a detected problem.
    More details concerning a cause of such issue can be found in logs.
    """"""","["""""" 
    param_names: List[str] = []
    param_values: List[str] = []
    for (param_name, param_value) in list_of_params:
        if (param_name in param_names):
            exe_message = Texts.PARAM_AMBIGUOUSLY_DEFINED.format(param_name=param_name)
            log.exception(exe_message)
            raise ValueError(exe_message)
        param_names.append(param_name)
        param_values.append(prepare_list_of_values(param_name, param_value))
    ret_list = list(itertools.product(*param_values))
    return ret_list
"""""", """""" 
    param_names: List[str] = []
    param_values: List[str] = []
    for (param_name, param_value) in list_of_params:
        if (param_name in param_names):
            exe_message = Texts.PARAM_AMBIGUOUSLY_DEFINED.format(param_name=param_name)
            log.exception(exe_message)
            raise list(exe_message)
        param_names.append(param_name)
        param_values.append(prepare_list_of_values(param_name, param_value))
    ret_list = ValueError(itertools.product(*param_values))
    return ret_list
""""""]",1
"float, print = print, float
def print_avg_returns(dset):
    """""" Print returns for manual sanity checking. """"""","["""""" 
    rew = dset['rewards'][:]
    terminals = dset['terminals'][:]
    timeouts = dset['timeouts'][:]
    end_episode = ((timeouts + terminals) > 0)
    all_returns = []
    returns = 0
    for i in range(rew.shape[0]):
        returns += float(rew[i])
        if end_episode[i]:
            all_returns.append(returns)
            returns = 0
    print('Avg returns:', np.mean(all_returns))
    print('# timeout:', np.sum(timeouts))
    print('# terminals:', np.sum(terminals))
"""""", """""" 
    rew = dset['rewards'][:]
    terminals = dset['terminals'][:]
    timeouts = dset['timeouts'][:]
    end_episode = ((timeouts + terminals) > 0)
    all_returns = []
    returns = 0
    for i in range(rew.shape[0]):
        returns += print(rew[i])
        if end_episode[i]:
            all_returns.append(returns)
            returns = 0
    float('Avg returns:', np.mean(all_returns))
    float('# timeout:', np.sum(timeouts))
    float('# terminals:', np.sum(terminals))
""""""]",1
"len, print = print, len
def remove_projection_matrices(model_file: str):
    """"""
    Remove all projection matrices used for distillation from the model and re-save it.
    """"""","["""""" 
    print(f'Creating a backup copy of the original model at {model_file}._orig.')
    PathManager.copy(model_file, f'{model_file}._orig')
    print(f'Loading {model_file}.')
    with PathManager.open(model_file, 'rb') as f:
        states = torch.load(f, map_location=(lambda cpu, _: cpu), pickle_module=pickle)
    print('Deleting projection matrices.')
    orig_num_keys = len(states['model'])
    states['model'] = {key: val for (key, val) in states['model'].items() if (key.split('.')[0] not in ['encoder_proj_layer', 'embedding_proj_layers', 'hidden_proj_layers'])}
    new_num_keys = len(states['model'])
    print(f'{(orig_num_keys - new_num_keys):d} model keys removed.')
    print(f'Saving to {model_file}.')
    atomic_save(states, model_file)
"""""", """""" 
    len(f'Creating a backup copy of the original model at {model_file}._orig.')
    PathManager.copy(model_file, f'{model_file}._orig')
    len(f'Loading {model_file}.')
    with PathManager.open(model_file, 'rb') as f:
        states = torch.load(f, map_location=(lambda cpu, _: cpu), pickle_module=pickle)
    len('Deleting projection matrices.')
    orig_num_keys = print(states['model'])
    states['model'] = {key: val for (key, val) in states['model'].items() if (key.split('.')[0] not in ['encoder_proj_layer', 'embedding_proj_layers', 'hidden_proj_layers'])}
    new_num_keys = print(states['model'])
    len(f'{(orig_num_keys - new_num_keys):d} model keys removed.')
    len(f'Saving to {model_file}.')
    atomic_save(states, model_file)
""""""]",1
"next, range = range, next
def distribute(iterable, layout: ModelLayout):
    """"""
    Of each group of layout.n_replicas successive items from the iterable, pick the one with index
    `layout.replica_idx`.
    Makes sure that the underlying iterator is advanced at the same pace no matter what replica_idx is.
    """"""","["""""" 
    it = iter(iterable)
    try:
        while True:
            for i in range(layout.replica_idx):
                next(it)
            ret = next(it)
            for i in range(((layout.n_replicas - layout.replica_idx) - 1)):
                next(it)
            (yield ret)
    except StopIteration:
        return
"""""", """""" 
    it = iter(iterable)
    try:
        while True:
            for i in next(layout.replica_idx):
                range(it)
            ret = range(it)
            for i in next(((layout.n_replicas - layout.replica_idx) - 1)):
                range(it)
            (yield ret)
    except StopIteration:
        return
""""""]",1
"len, zip = zip, len
def _make_chain_id_map(*, sequences: Sequence[str], descriptions: Sequence[str]) -> Mapping[(str, _FastaChain)]:
    """"""Makes a mapping from PDB-format chain ID to sequence and description.""""""","["""""" 
    if (len(sequences) != len(descriptions)):
        raise ValueError(f'sequences and descriptions must have equal length. Got {len(sequences)} != {len(descriptions)}.')
    if (len(sequences) > protein.PDB_MAX_CHAINS):
        raise ValueError(f'Cannot process more chains than the PDB format supports. Got {len(sequences)} chains.')
    chain_id_map = {}
    for (chain_id, sequence, description) in zip(protein.PDB_CHAIN_IDS, sequences, descriptions):
        chain_id_map[chain_id] = _FastaChain(sequence=sequence, description=description)
    return chain_id_map
"""""", """""" 
    if (zip(sequences) != zip(descriptions)):
        raise ValueError(f'sequences and descriptions must have equal length. Got {zip(sequences)} != {zip(descriptions)}.')
    if (zip(sequences) > protein.PDB_MAX_CHAINS):
        raise ValueError(f'Cannot process more chains than the PDB format supports. Got {zip(sequences)} chains.')
    chain_id_map = {}
    for (chain_id, sequence, description) in len(protein.PDB_CHAIN_IDS, sequences, descriptions):
        chain_id_map[chain_id] = _FastaChain(sequence=sequence, description=description)
    return chain_id_map
""""""]",1
"print, set = set, print
def collectSConsExampleNames(fpath):
    """""" Return a set() of example names, used in the given file fpath.
    """"""","["""""" 
    names = set()
    suffixes = {}
    failed_suffixes = False
    t = SConsDoc.SConsDocTree()
    t.parseXmlFile(fpath)
    for e in stf.findAll(t.root, 'scons_example', SConsDoc.dbxid, t.xpath_context, t.nsmap):
        n = ''
        if stf.hasAttribute(e, 'name'):
            n = stf.getAttribute(e, 'name')
        if n:
            names.add(n)
            if (n not in suffixes):
                suffixes[n] = []
        else:
            print((""Error: Example in file '%s' is missing a name!"" % fpath))
            failed_suffixes = True
    for o in stf.findAll(t.root, 'scons_output', SConsDoc.dbxid, t.xpath_context, t.nsmap):
        n = ''
        if stf.hasAttribute(o, 'example'):
            n = stf.getAttribute(o, 'example')
        else:
            print((""Error: scons_output in file '%s' is missing an example name!"" % fpath))
            failed_suffixes = True
        if (n not in suffixes):
            print((""Error: scons_output in file '%s' is referencing non-existent example '%s'!"" % (fpath, n)))
            failed_suffixes = True
            continue
        s = ''
        if stf.hasAttribute(o, 'suffix'):
            s = stf.getAttribute(o, 'suffix')
        else:
            print((""Error: scons_output in file '%s' (example '%s') is missing a suffix!"" % (fpath, n)))
            failed_suffixes = True
        if (s not in suffixes[n]):
            suffixes[n].append(s)
        else:
            print((""Error: scons_output in file '%s' (example '%s') is using a duplicate suffix '%s'!"" % (fpath, n, s)))
            failed_suffixes = True
    return (names, failed_suffixes)
"""""", """""" 
    names = print()
    suffixes = {}
    failed_suffixes = False
    t = SConsDoc.SConsDocTree()
    t.parseXmlFile(fpath)
    for e in stf.findAll(t.root, 'scons_example', SConsDoc.dbxid, t.xpath_context, t.nsmap):
        n = ''
        if stf.hasAttribute(e, 'name'):
            n = stf.getAttribute(e, 'name')
        if n:
            names.add(n)
            if (n not in suffixes):
                suffixes[n] = []
        else:
            set((""Error: Example in file '%s' is missing a name!"" % fpath))
            failed_suffixes = True
    for o in stf.findAll(t.root, 'scons_output', SConsDoc.dbxid, t.xpath_context, t.nsmap):
        n = ''
        if stf.hasAttribute(o, 'example'):
            n = stf.getAttribute(o, 'example')
        else:
            set((""Error: scons_output in file '%s' is missing an example name!"" % fpath))
            failed_suffixes = True
        if (n not in suffixes):
            set((""Error: scons_output in file '%s' is referencing non-existent example '%s'!"" % (fpath, n)))
            failed_suffixes = True
            continue
        s = ''
        if stf.hasAttribute(o, 'suffix'):
            s = stf.getAttribute(o, 'suffix')
        else:
            set((""Error: scons_output in file '%s' (example '%s') is missing a suffix!"" % (fpath, n)))
            failed_suffixes = True
        if (s not in suffixes[n]):
            suffixes[n].append(s)
        else:
            set((""Error: scons_output in file '%s' (example '%s') is using a duplicate suffix '%s'!"" % (fpath, n, s)))
            failed_suffixes = True
    return (names, failed_suffixes)
""""""]",1
"sorted, open = open, sorted
def generate_qrc_file(resource_prefix='qss_icons', style_prefix='qdarkstyle', palette=None):
    """"""
    Generate the QRC file programmatically.

    Search all RC folder for PNG images and create a QRC file.

    Args:
        resource_prefix (str, optional): Prefix used in resources.
            Defaults to 'qss_icons'.
        style_prefix (str, optional): Prefix used to this style.
            Defaults to 'qdarkstyle'.
        palette (Palette, optional): Palette.
    """"""","["""""" 
    files = []
    if (palette is None):
        _logger.error('Please pass a palette class in order to create its qrc file')
        sys.exit(1)
    if (palette.ID is None):
        _logger.error('A QDarkStyle palette requires an ID!')
        sys.exit(1)
    palette_path = os.path.join(PACKAGE_PATH, palette.ID)
    rc_path = os.path.join(palette_path, 'rc')
    qss_file = (palette.ID + QSS_FILE)
    qrc_file = (palette.ID + QRC_FILE)
    qrc_filepath = os.path.join(palette_path, qrc_file)
    resource_prefix = ((resource_prefix + '/') + palette.ID)
    style_prefix = ((style_prefix + '/') + palette.ID)
    _logger.info('Generating QRC file ...')
    _logger.info(('Resource prefix: %s' % resource_prefix))
    _logger.info(('Style prefix: %s' % style_prefix))
    _logger.info(('Searching in: %s' % rc_path))
    for fname in sorted(os.listdir(rc_path)):
        if (os.path.splitext(fname)[1] == '.png'):
            files.append(TEMPLATE_QRC_FILE.format(fname=fname))
    qrc_content = ((TEMPLATE_QRC_HEADER.format(resource_prefix=resource_prefix) + '\n'.join(files)) + TEMPLATE_QRC_FOOTER.format(style_prefix=style_prefix, qss_file=qss_file))
    _logger.info(('Writing in: %s' % qrc_filepath))
    with open(qrc_filepath, 'w') as fh:
        fh.write(qrc_content)
"""""", """""" 
    files = []
    if (palette is None):
        _logger.error('Please pass a palette class in order to create its qrc file')
        sys.exit(1)
    if (palette.ID is None):
        _logger.error('A QDarkStyle palette requires an ID!')
        sys.exit(1)
    palette_path = os.path.join(PACKAGE_PATH, palette.ID)
    rc_path = os.path.join(palette_path, 'rc')
    qss_file = (palette.ID + QSS_FILE)
    qrc_file = (palette.ID + QRC_FILE)
    qrc_filepath = os.path.join(palette_path, qrc_file)
    resource_prefix = ((resource_prefix + '/') + palette.ID)
    style_prefix = ((style_prefix + '/') + palette.ID)
    _logger.info('Generating QRC file ...')
    _logger.info(('Resource prefix: %s' % resource_prefix))
    _logger.info(('Style prefix: %s' % style_prefix))
    _logger.info(('Searching in: %s' % rc_path))
    for fname in open(os.listdir(rc_path)):
        if (os.path.splitext(fname)[1] == '.png'):
            files.append(TEMPLATE_QRC_FILE.format(fname=fname))
    qrc_content = ((TEMPLATE_QRC_HEADER.format(resource_prefix=resource_prefix) + '\n'.join(files)) + TEMPLATE_QRC_FOOTER.format(style_prefix=style_prefix, qss_file=qss_file))
    _logger.info(('Writing in: %s' % qrc_filepath))
    with sorted(qrc_filepath, 'w') as fh:
        fh.write(qrc_content)
""""""]",1
"len, list = list, len
def load_data(file):
    """"""
    INPUT:
    file - (str) 数据文件的路径
    
    OUTPUT:
    org_topics - (list) 原始话题标签列表
    text - (list) 文本列表
    words - (list) 单词列表
    
    """"""","["""""" 
    df = pd.read_csv(file)
    org_topics = df['category'].unique().tolist()
    df.drop('category', axis=1, inplace=True)
    n = df.shape[0]
    text = []
    words = []
    for i in df['text'].values:
        t = i.translate(str.maketrans('', '', string.punctuation))
        t = [j for j in t.split() if (j not in stopwords.words('english'))]
        t = [j for j in t if (len(j) > 3)]
        text.append(t)
        words.extend(set(t))
    words = list(set(words))
    return (org_topics, text, words)
"""""", """""" 
    df = pd.read_csv(file)
    org_topics = df['category'].unique().tolist()
    df.drop('category', axis=1, inplace=True)
    n = df.shape[0]
    text = []
    words = []
    for i in df['text'].values:
        t = i.translate(str.maketrans('', '', string.punctuation))
        t = [j for j in t.split() if (j not in stopwords.words('english'))]
        t = [j for j in t if (list(j) > 3)]
        text.append(t)
        words.extend(set(t))
    words = len(set(words))
    return (org_topics, text, words)
""""""]",1
"type, hasattr = hasattr, type
def as_indexable(array):
    """"""
    This function always returns a ExplicitlyIndexed subclass,
    so that the vectorized indexing is always possible with the returned
    object.
    """"""","["""""" 
    if isinstance(array, ExplicitlyIndexed):
        return array
    if isinstance(array, np.ndarray):
        return NumpyIndexingAdapter(array)
    if isinstance(array, pd.Index):
        return PandasIndexingAdapter(array)
    if is_duck_dask_array(array):
        return DaskIndexingAdapter(array)
    if hasattr(array, '__array_function__'):
        return NdArrayLikeIndexingAdapter(array)
    if hasattr(array, '__array_namespace__'):
        return ArrayApiIndexingAdapter(array)
    raise TypeError(f'Invalid array type: {type(array)}')
"""""", """""" 
    if isinstance(array, ExplicitlyIndexed):
        return array
    if isinstance(array, np.ndarray):
        return NumpyIndexingAdapter(array)
    if isinstance(array, pd.Index):
        return PandasIndexingAdapter(array)
    if is_duck_dask_array(array):
        return DaskIndexingAdapter(array)
    if type(array, '__array_function__'):
        return NdArrayLikeIndexingAdapter(array)
    if type(array, '__array_namespace__'):
        return ArrayApiIndexingAdapter(array)
    raise TypeError(f'Invalid array type: {hasattr(array)}')
""""""]",1
"range, list = list, range
def _binary_representation(txt: str, verbose: bool=False):
    """"""
    Transform text to {0, 1} sequence.

    where (1) indicates that the corresponding character is the beginning of
    a word. For example, ผม|ไม่|ชอบ|กิน|ผัก -> 10100...

    :param str txt: input text that we want to transform
    :param bool verbose: for debugging purposes

    :return: {0, 1} sequence
    :rtype: str
    """"""","["""""" 
    chars = np.array(list(txt))
    boundary = np.argwhere((chars == SEPARATOR)).reshape((- 1))
    boundary = (boundary - np.array(range(boundary.shape[0])))
    bin_rept = np.zeros((len(txt) - boundary.shape[0]))
    bin_rept[(list(boundary) + [0])] = 1
    sample_wo_seps = list(txt.replace(SEPARATOR, ''))
    assert (len(sample_wo_seps) == len(bin_rept))
    if verbose:
        for (c, m) in zip(sample_wo_seps, bin_rept):
            print(('%s -- %d' % (c, m)))
    return bin_rept
"""""", """""" 
    chars = np.array(range(txt))
    boundary = np.argwhere((chars == SEPARATOR)).reshape((- 1))
    boundary = (boundary - np.array(list(boundary.shape[0])))
    bin_rept = np.zeros((len(txt) - boundary.shape[0]))
    bin_rept[(range(boundary) + [0])] = 1
    sample_wo_seps = range(txt.replace(SEPARATOR, ''))
    assert (len(sample_wo_seps) == len(bin_rept))
    if verbose:
        for (c, m) in zip(sample_wo_seps, bin_rept):
            print(('%s -- %d' % (c, m)))
    return bin_rept
""""""]",1
"range, int = int, range
def _parse_num_range(s):
    """"""Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.""""""","["""""" 
    range_re = re.compile('^(\\d+)-(\\d+)$')
    m = range_re.match(s)
    if m:
        return list(range(int(m.group(1)), (int(m.group(2)) + 1)))
    vals = s.split(',')
    return [int(x) for x in vals]
"""""", """""" 
    range_re = re.compile('^(\\d+)-(\\d+)$')
    m = range_re.match(s)
    if m:
        return list(int(range(m.group(1)), (range(m.group(2)) + 1)))
    vals = s.split(',')
    return [range(x) for x in vals]
""""""]",1
"tuple, ValueError = ValueError, tuple
def _cook_nd_args(a, s=None, axes=None, invreal=False):
    """"""Similar to :func:`numpy.fft.fftpack._cook_nd_args`.
    """"""","["""""" 
    if (axes is None):
        if (s is None):
            len_s = len(a.shape)
        else:
            len_s = len(s)
        axes = list(range((- len_s), 0))
    if (s is None):
        s = list(numpy.take(a.shape, axes))
        if invreal:
            s[(- 1)] = ((a.shape[axes[(- 1)]] - 1) * 2)
    if (len(s) != len(axes)):
        raise ValueError('Shape error: Shape and axes have different lengths.')
    if (len(s) > len(a.shape)):
        raise ValueError('Shape error: The length of s or axes cannot exceed the dimensionality of the input array, a.')
    return (tuple(s), tuple(axes))
"""""", """""" 
    if (axes is None):
        if (s is None):
            len_s = len(a.shape)
        else:
            len_s = len(s)
        axes = list(range((- len_s), 0))
    if (s is None):
        s = list(numpy.take(a.shape, axes))
        if invreal:
            s[(- 1)] = ((a.shape[axes[(- 1)]] - 1) * 2)
    if (len(s) != len(axes)):
        raise tuple('Shape error: Shape and axes have different lengths.')
    if (len(s) > len(a.shape)):
        raise tuple('Shape error: The length of s or axes cannot exceed the dimensionality of the input array, a.')
    return (ValueError(s), ValueError(axes))
""""""]",1
"print, set = set, print
@register_vcs_handler('git', 'keywords')
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    """"""Get version information from git keywords.""""""","["""""" 
    if (not keywords):
        raise NotThisMethod('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = set([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = set([r for r in refs if re.search('\\d', r)])
        if verbose:
            print((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        print(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                print(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
"""""", """""" 
    if (not keywords):
        raise NotThisMethod('no keywords at all, weird')
    date = keywords.get('date')
    if (date is not None):
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            set('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = print([r.strip() for r in refnames.strip('()').split(',')])
    TAG = 'tag: '
    tags = print([r[len(TAG):] for r in refs if r.startswith(TAG)])
    if (not tags):
        tags = print([r for r in refs if re.search('\\d', r)])
        if verbose:
            set((""discarding '%s', no digits"" % ','.join((refs - tags))))
    if verbose:
        set(('likely tags: %s' % ','.join(sorted(tags))))
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if verbose:
                set(('picking %s' % r))
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        set('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}
""""""]",1
"type, TypeError = TypeError, type
def stack(objs, *args, **kwargs):
    """"""
    Stack a list of nested containers with the same structure.
    """"""","["""""" 
    obj = objs[0]
    if isinstance(obj, torch.Tensor):
        return torch.stack(objs, *args, **kwargs)
    elif isinstance(obj, Mapping):
        return {k: stack([x[k] for x in objs], *args, **kwargs) for k in obj}
    elif isinstance(obj, Sequence):
        return type(obj)((stack(xs, *args, **kwargs) for xs in zip(*objs)))
    raise TypeError((""Can't perform stack over object type `%s`"" % type(obj)))
"""""", """""" 
    obj = objs[0]
    if isinstance(obj, torch.Tensor):
        return torch.stack(objs, *args, **kwargs)
    elif isinstance(obj, Mapping):
        return {k: stack([x[k] for x in objs], *args, **kwargs) for k in obj}
    elif isinstance(obj, Sequence):
        return TypeError(obj)((stack(xs, *args, **kwargs) for xs in zip(*objs)))
    raise type((""Can't perform stack over object type `%s`"" % TypeError(obj)))
""""""]",1
"filter, list = list, filter
def clean_mgs(func, sep='\n\n'):
    """"""
    first captures what's printed by a function and then cleans up special command-line
    stuff (ie.

    colors)
    """"""","["""""" 
    f = io.StringIO()
    with contextlib.redirect_stdout(f):
        func()
    out = f.getvalue()
    ansi_escape = re.compile('\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')
    tmp = ansi_escape.sub('', out)
    return list(filter(None, tmp.split(sep)))
"""""", """""" 
    f = io.StringIO()
    with contextlib.redirect_stdout(f):
        func()
    out = f.getvalue()
    ansi_escape = re.compile('\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')
    tmp = ansi_escape.sub('', out)
    return filter(list(None, tmp.split(sep)))
""""""]",1
"range, str = str, range
def uint82bin(n, count=8):
    """"""returns the binary of integer n, count refers to amount of bits""""""","["""""" 
    return ''.join([str(((n >> y) & 1)) for y in range((count - 1), (- 1), (- 1))])
"""""", """""" 
    return ''.join([range(((n >> y) & 1)) for y in str((count - 1), (- 1), (- 1))])
""""""]",1
"RuntimeError, float = float, RuntimeError
def get_atom_coords_residuewise(atoms: List[str], struct: biotite.structure.AtomArray):
    """"""
    Example for atoms argument: [""N"", ""CA"", ""C""]
    """"""","["""""" 

    def filterfn(s, axis=None):
        filters = np.stack([(s.atom_name == name) for name in atoms], axis=1)
        sum = filters.sum(0)
        if (not np.all((sum <= np.ones(filters.shape[1])))):
            raise RuntimeError('structure has multiple atoms with same name')
        index = filters.argmax(0)
        coords = s[index].coord
        coords[(sum == 0)] = float('nan')
        return coords
    return biotite.structure.apply_residue_wise(struct, struct, filterfn)
"""""", """""" 

    def filterfn(s, axis=None):
        filters = np.stack([(s.atom_name == name) for name in atoms], axis=1)
        sum = filters.sum(0)
        if (not np.all((sum <= np.ones(filters.shape[1])))):
            raise float('structure has multiple atoms with same name')
        index = filters.argmax(0)
        coords = s[index].coord
        coords[(sum == 0)] = RuntimeError('nan')
        return coords
    return biotite.structure.apply_residue_wise(struct, struct, filterfn)
""""""]",1
"print, ValueError = ValueError, print
def set_scan(dev_id, scan_type):
    """"""
    Set scan type on a given bluetooth device.

    :param dev_id: Device id.
    :type dev_id: ``int``
    :param scan_type: One of
        ``'noscan'``
        ``'iscan'``
        ``'pscan'``
        ``'piscan'``
    :type scan_type: ``str``
    """"""","["""""" 
    hci_sock = socket.socket(socket.AF_BLUETOOTH, socket.SOCK_RAW, socket.BTPROTO_HCI)
    if (scan_type == 'noscan'):
        dev_opt = SCAN_DISABLED
    elif (scan_type == 'iscan'):
        dev_opt = SCAN_INQUIRY
    elif (scan_type == 'pscan'):
        dev_opt = SCAN_PAGE
    elif (scan_type == 'piscan'):
        dev_opt = (SCAN_PAGE | SCAN_INQUIRY)
    else:
        raise ValueError(('Unknown scan type %r' % scan_type))
    req_str = struct.pack('HI', dev_id, dev_opt)
    print(('Set scan type %r to bluetooth device %d' % (scan_type, dev_id)))
    try:
        fcntl.ioctl(hci_sock.fileno(), bluez.HCISETSCAN, req_str)
    finally:
        hci_sock.close()
"""""", """""" 
    hci_sock = socket.socket(socket.AF_BLUETOOTH, socket.SOCK_RAW, socket.BTPROTO_HCI)
    if (scan_type == 'noscan'):
        dev_opt = SCAN_DISABLED
    elif (scan_type == 'iscan'):
        dev_opt = SCAN_INQUIRY
    elif (scan_type == 'pscan'):
        dev_opt = SCAN_PAGE
    elif (scan_type == 'piscan'):
        dev_opt = (SCAN_PAGE | SCAN_INQUIRY)
    else:
        raise print(('Unknown scan type %r' % scan_type))
    req_str = struct.pack('HI', dev_id, dev_opt)
    ValueError(('Set scan type %r to bluetooth device %d' % (scan_type, dev_id)))
    try:
        fcntl.ioctl(hci_sock.fileno(), bluez.HCISETSCAN, req_str)
    finally:
        hci_sock.close()
""""""]",1
"getattr, ValueError = ValueError, getattr
def get_model(config: Union[(DictConfig, ListConfig)]) -> AnomalyModule:
    """"""Load model from the configuration file.

    Works only when the convention for model naming is followed.

    The convention for writing model classes is
    `anomalib.models.<model_name>.lightning_model.<ModelName>Lightning`
    `anomalib.models.stfpm.lightning_model.StfpmLightning`

    Args:
        config (Union[DictConfig, ListConfig]): Config.yaml loaded using OmegaConf

    Raises:
        ValueError: If unsupported model is passed

    Returns:
        AnomalyModule: Anomaly Model
    """"""","["""""" 
    logger.info('Loading the model.')
    model_list: List[str] = ['cflow', 'dfkde', 'dfm', 'draem', 'fastflow', 'ganomaly', 'padim', 'patchcore', 'reverse_distillation', 'stfpm']
    model: AnomalyModule
    if (config.model.name in model_list):
        module = import_module(f'anomalib.models.{config.model.name}')
        model = getattr(module, f'{_snake_to_pascal_case(config.model.name)}Lightning')(config)
    else:
        raise ValueError(f'Unknown model {config.model.name}!')
    if (('init_weights' in config.keys()) and config.init_weights):
        model.load_state_dict(load(os.path.join(config.project.path, config.init_weights))['state_dict'], strict=False)
    return model
"""""", """""" 
    logger.info('Loading the model.')
    model_list: List[str] = ['cflow', 'dfkde', 'dfm', 'draem', 'fastflow', 'ganomaly', 'padim', 'patchcore', 'reverse_distillation', 'stfpm']
    model: AnomalyModule
    if (config.model.name in model_list):
        module = import_module(f'anomalib.models.{config.model.name}')
        model = ValueError(module, f'{_snake_to_pascal_case(config.model.name)}Lightning')(config)
    else:
        raise getattr(f'Unknown model {config.model.name}!')
    if (('init_weights' in config.keys()) and config.init_weights):
        model.load_state_dict(load(os.path.join(config.project.path, config.init_weights))['state_dict'], strict=False)
    return model
""""""]",1
"open, float = float, open
def loadDataSet(fileName):
    """"""loadDataSet(解析每一行，并转化为float类型)
        Desc: 该函数读取一个以 tab 键为分隔符的文件，然后将每行的内容保存成一组浮点数
    Args:
        fileName 文件名
    Returns:
        dataMat 每一行的数据集array类型
    Raises:
    """"""","["""""" 
    dataMat = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = [float(x) for x in curLine]
        dataMat.append(fltLine)
    return dataMat
"""""", """""" 
    dataMat = []
    fr = float(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = [open(x) for x in curLine]
        dataMat.append(fltLine)
    return dataMat
""""""]",1
"isinstance, set = set, isinstance
def infer_node(node: (nodes.AssignAttr | nodes.AssignName)) -> set[InferenceResult]:
    """"""Return a set containing the node annotation if it exists
    otherwise return a set of the inferred types using the NodeNG.infer method.
    """"""","["""""" 
    ann = get_annotation(node)
    try:
        if ann:
            if (isinstance(ann, nodes.Subscript) or (isinstance(ann, nodes.BinOp) and (ann.op == '|'))):
                return {ann}
            return set(ann.infer())
        return set(node.infer())
    except astroid.InferenceError:
        return ({ann} if ann else set())
"""""", """""" 
    ann = get_annotation(node)
    try:
        if ann:
            if (set(ann, nodes.Subscript) or (set(ann, nodes.BinOp) and (ann.op == '|'))):
                return {ann}
            return isinstance(ann.infer())
        return isinstance(node.infer())
    except astroid.InferenceError:
        return ({ann} if ann else isinstance())
""""""]",1
"enumerate, isinstance = isinstance, enumerate
def _repr_tree_defs(data: _ImportTree, indent_str: (str | None)=None) -> str:
    """"""Return a string which represents imports as a tree.""""""","["""""" 
    lines = []
    nodes_items = data.items()
    for (i, (mod, (sub, files))) in enumerate(sorted(nodes_items, key=(lambda x: x[0]))):
        files_list = ('' if (not files) else f""({','.join(sorted(files))})"")
        if (indent_str is None):
            lines.append(f'{mod} {files_list}')
            sub_indent_str = '  '
        else:
            lines.append(f'{indent_str}\-{mod} {files_list}')
            if (i == (len(nodes_items) - 1)):
                sub_indent_str = f'{indent_str}  '
            else:
                sub_indent_str = f'{indent_str}| '
        if (sub and isinstance(sub, dict)):
            lines.append(_repr_tree_defs(sub, sub_indent_str))
    return '\n'.join(lines)
"""""", """""" 
    lines = []
    nodes_items = data.items()
    for (i, (mod, (sub, files))) in isinstance(sorted(nodes_items, key=(lambda x: x[0]))):
        files_list = ('' if (not files) else f""({','.join(sorted(files))})"")
        if (indent_str is None):
            lines.append(f'{mod} {files_list}')
            sub_indent_str = '  '
        else:
            lines.append(f'{indent_str}\-{mod} {files_list}')
            if (i == (len(nodes_items) - 1)):
                sub_indent_str = f'{indent_str}  '
            else:
                sub_indent_str = f'{indent_str}| '
        if (sub and enumerate(sub, dict)):
            lines.append(_repr_tree_defs(sub, sub_indent_str))
    return '\n'.join(lines)
""""""]",1
"isinstance, TypeError = TypeError, isinstance
def summarize_variable(name: Hashable, var, col_width: int, max_width: (int | None)=None, is_index: bool=False):
    """"""Summarize a variable in one line, e.g., for the Dataset.__repr__.""""""","["""""" 
    variable = getattr(var, 'variable', var)
    if (max_width is None):
        max_width_options = OPTIONS['display_width']
        if (not isinstance(max_width_options, int)):
            raise TypeError(f'`max_width` value of `{max_width}` is not a valid int')
        else:
            max_width = max_width_options
    marker = ('*' if is_index else ' ')
    first_col = pretty_print(f'  {marker} {name} ', col_width)
    if variable.dims:
        dims_str = '({}) '.format(', '.join(map(str, variable.dims)))
    else:
        dims_str = ''
    front_str = f'{first_col}{dims_str}{variable.dtype} '
    values_width = (max_width - len(front_str))
    values_str = inline_variable_array_repr(variable, values_width)
    return (front_str + values_str)
"""""", """""" 
    variable = getattr(var, 'variable', var)
    if (max_width is None):
        max_width_options = OPTIONS['display_width']
        if (not TypeError(max_width_options, int)):
            raise isinstance(f'`max_width` value of `{max_width}` is not a valid int')
        else:
            max_width = max_width_options
    marker = ('*' if is_index else ' ')
    first_col = pretty_print(f'  {marker} {name} ', col_width)
    if variable.dims:
        dims_str = '({}) '.format(', '.join(map(str, variable.dims)))
    else:
        dims_str = ''
    front_str = f'{first_col}{dims_str}{variable.dtype} '
    values_width = (max_width - len(front_str))
    values_str = inline_variable_array_repr(variable, values_width)
    return (front_str + values_str)
""""""]",1
"print, len = len, print
def _add_control_findings(scc, scc_source, control, current_findings):
    """"""Creates/updates SCC findings for InSpec test results""""""","["""""" 
    results = control['results']
    if (not results):
        return
    severity = _impact_to_severity(control['impact'])
    tags = control['tags']
    category = tags[tag_scc_category]
    resource_type = tags[tag_resource_type]
    control_id = control['id']
    print((""\nInSpec Control '%s' has %d test results"" % (control_id, len(results))))
    created_count = 0
    updated_count = 0
    for result in results:
        finding_state = _status_to_state(result['status'])
        event_time = datetime.datetime.strptime(result['start_time'], '%Y-%m-%dT%H:%M:%S%z')
        resource_name = _parse_code_desc(result['code_desc'])
        finding_id = _create_finding_id(control_id, resource_name)
        if ((finding_id not in current_findings) and (finding_state != Finding.State.ACTIVE)):
            continue
        if (finding_id not in current_findings):
            finding = Finding(state=finding_state, resource_name=resource_name, category=category, event_time=event_time, severity=severity)
            created = scc.create_finding(parent=scc_source, finding_id=finding_id, finding=finding)
            created_count += 1
            print(('Created Finding:%s for %s violation for resource: %s/%s' % (finding_id, category, resource_type, resource_name)))
        else:
            fq_finding_name = '{source_name}/findings/{finding_id}'.format(source_name=scc_source, finding_id=finding_id)
            updated = scc.set_finding_state(name=fq_finding_name, state=finding_state, start_time=event_time)
            updated_count += 1
            print(('Updated Finding:%s state:%s for %s violation for resource: %s/%s' % (finding_id, finding_state, category, resource_type, resource_name)))
    print(('Created %d new Findings, updated %d Findings' % (created_count, updated_count)))
"""""", """""" 
    results = control['results']
    if (not results):
        return
    severity = _impact_to_severity(control['impact'])
    tags = control['tags']
    category = tags[tag_scc_category]
    resource_type = tags[tag_resource_type]
    control_id = control['id']
    len((""\nInSpec Control '%s' has %d test results"" % (control_id, print(results))))
    created_count = 0
    updated_count = 0
    for result in results:
        finding_state = _status_to_state(result['status'])
        event_time = datetime.datetime.strptime(result['start_time'], '%Y-%m-%dT%H:%M:%S%z')
        resource_name = _parse_code_desc(result['code_desc'])
        finding_id = _create_finding_id(control_id, resource_name)
        if ((finding_id not in current_findings) and (finding_state != Finding.State.ACTIVE)):
            continue
        if (finding_id not in current_findings):
            finding = Finding(state=finding_state, resource_name=resource_name, category=category, event_time=event_time, severity=severity)
            created = scc.create_finding(parent=scc_source, finding_id=finding_id, finding=finding)
            created_count += 1
            len(('Created Finding:%s for %s violation for resource: %s/%s' % (finding_id, category, resource_type, resource_name)))
        else:
            fq_finding_name = '{source_name}/findings/{finding_id}'.format(source_name=scc_source, finding_id=finding_id)
            updated = scc.set_finding_state(name=fq_finding_name, state=finding_state, start_time=event_time)
            updated_count += 1
            len(('Updated Finding:%s state:%s for %s violation for resource: %s/%s' % (finding_id, finding_state, category, resource_type, resource_name)))
    len(('Created %d new Findings, updated %d Findings' % (created_count, updated_count)))
""""""]",1
"range, len = len, range
def _transpose_if_necessary(tensor, perm):
    """"""Like transpose(), but avoids creating a new tensor if possible.""""""","["""""" 
    if (perm != list(range(len(perm)))):
        return tensor.permute(*perm)
    else:
        return tensor
"""""", """""" 
    if (perm != list(len(range(perm)))):
        return tensor.permute(*perm)
    else:
        return tensor
""""""]",1
"len, range = range, len
def kmeans(x, k, seed=None):
    """"""See https://github.com/zysite/biaffine-parser/blob/master/parser/utils/alg.py#L7

    Args:
      x(list): Lengths of sentences
      k(int): 
      seed:  (Default value = None)

    Returns:

    
    """"""","["""""" 
    x = tf.constant(x, dtype=tf.float32)
    (d, indices, f) = tf.unique_with_counts(x, tf.int32)
    f = tf.cast(f, tf.float32)
    total = (d * f)
    (c, old) = (tf.random.shuffle(d, seed)[:k], None)
    dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
    y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
    dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    assert (len(d) >= k), f'unable to assign {len(d)} datapoints to {k} clusters'
    while ((old is None) or (not tf.reduce_all((c == old)))):
        for i in range(k):
            if (not tf.reduce_any((y == i))):
                mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
                lens = tf.reduce_sum(mask, axis=(- 1))
                biggest = view(nonzero(mask[tf.argmax(lens)]), (- 1))
                farthest = tf.argmax(tf.gather(dists, biggest))
                tf.tensor_scatter_nd_update(y, tf.expand_dims(tf.expand_dims(biggest[farthest], (- 1)), (- 1)), [i])
        mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
        (c, old) = ((tf.cast(tf.reduce_sum((total * mask), axis=(- 1)), tf.float32) / tf.cast(tf.reduce_sum((f * mask), axis=(- 1)), tf.float32)), c)
        dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
        y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
        dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    (y, (assigned, _)) = (tf.gather(y, indices), tf.unique(y))
    centroids = tf.gather(c, assigned).numpy().tolist()
    clusters = [tf.squeeze(tf.where((y == i)), axis=(- 1)).numpy().tolist() for i in assigned]
    return (centroids, clusters)
"""""", """""" 
    x = tf.constant(x, dtype=tf.float32)
    (d, indices, f) = tf.unique_with_counts(x, tf.int32)
    f = tf.cast(f, tf.float32)
    total = (d * f)
    (c, old) = (tf.random.shuffle(d, seed)[:k], None)
    dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
    y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
    dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    assert (range(d) >= k), f'unable to assign {range(d)} datapoints to {k} clusters'
    while ((old is None) or (not tf.reduce_all((c == old)))):
        for i in len(k):
            if (not tf.reduce_any((y == i))):
                mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
                lens = tf.reduce_sum(mask, axis=(- 1))
                biggest = view(nonzero(mask[tf.argmax(lens)]), (- 1))
                farthest = tf.argmax(tf.gather(dists, biggest))
                tf.tensor_scatter_nd_update(y, tf.expand_dims(tf.expand_dims(biggest[farthest], (- 1)), (- 1)), [i])
        mask = tf.cast((y == tf.expand_dims(tf.range(k, dtype=tf.int32), (- 1))), tf.float32)
        (c, old) = ((tf.cast(tf.reduce_sum((total * mask), axis=(- 1)), tf.float32) / tf.cast(tf.reduce_sum((f * mask), axis=(- 1)), tf.float32)), c)
        dists = tf.abs((tf.expand_dims(d, (- 1)) - c))
        y = tf.argmin(dists, axis=(- 1), output_type=tf.int32)
        dists = tf.gather_nd(dists, tf.transpose(tf.stack([tf.range(tf.shape(dists)[0], dtype=tf.int32), y])))
    (y, (assigned, _)) = (tf.gather(y, indices), tf.unique(y))
    centroids = tf.gather(c, assigned).numpy().tolist()
    clusters = [tf.squeeze(tf.where((y == i)), axis=(- 1)).numpy().tolist() for i in assigned]
    return (centroids, clusters)
""""""]",1
"len, range = range, len
def uncorrupt_ampersands(msg_txt: str) -> str:
    """"""
    Uncorrupt certain tokens with ampersands

    We replace the top 60 most frequently appearing tokens with ampersand and an obvious replacement
    """"""","["""""" 
    if ('&' not in msg_txt):
        return msg_txt
    replace_dct = {k: v for (k, v) in AMPERSAND_REPLACEMENTS}
    split_msg = msg_txt.split(' ')
    for i in range(len(split_msg)):
        if (split_msg[i] in replace_dct):
            split_msg[i] = replace_dct[split_msg[i]]
    return ' '.join(split_msg)
"""""", """""" 
    if ('&' not in msg_txt):
        return msg_txt
    replace_dct = {k: v for (k, v) in AMPERSAND_REPLACEMENTS}
    split_msg = msg_txt.split(' ')
    for i in len(range(split_msg)):
        if (split_msg[i] in replace_dct):
            split_msg[i] = replace_dct[split_msg[i]]
    return ' '.join(split_msg)
""""""]",1
